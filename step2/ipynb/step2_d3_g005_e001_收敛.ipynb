{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2044b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 0. set-up part:  import necessary libraries and set up environment \"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from threading import Thread\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "import json\n",
    "import cProfile\n",
    "\n",
    "# download nltk data once time\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "#  chinese character support in matplotlib\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS' 'SimHei' 'DejaVu Sans']  \n",
    "plt.rcParams['axes.unicode_minus'] = False  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a654762",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 1.1 Data Preprocessing: load data, clean text, lemmatization, remove low-frequency words\"\"\"\n",
    "\n",
    "# Map POS tags to WordNet formatÔºå Penn Treebank annotation: fine-grained (45 tags), WordNet annotation: coarse-grained (4 tags: a, v, n, r)\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'  # ÂΩ¢ÂÆπËØç\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'  # Âä®ËØç\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n'  # ÂêçËØç\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'  # ÂâØËØç\n",
    "    else:\n",
    "        return 'n'  # ÈªòËÆ§ÂêçËØç\n",
    "\n",
    "# Text cleaning and lemmatization preprocessing function\n",
    "def clean_and_lemmatize(text):\n",
    "    if pd.isnull(text):\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove non-alphabetic characters using regex\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmatized = [lemmatizer.lemmatize(w, get_wordnet_pos(pos)) for w, pos in pos_tags]\n",
    "    return lemmatized  \n",
    "\n",
    "#-----------------Load data----------------\n",
    "data = pd.read_excel('./data/raw/papers_CM.xlsx', usecols=['PaperID', 'Abstract', 'Keywords', 'Year'])\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# clean and lemmatize the abstracts\n",
    "data['Lemmatized_Tokens'] = data['Abstract'].apply(clean_and_lemmatize)\n",
    "\n",
    "# count word frequencies\n",
    "all_tokens = [word for tokens in data['Lemmatized_Tokens'] for word in tokens]\n",
    "word_counts = Counter(all_tokens)\n",
    "\n",
    "# set a minimum frequency threshold for valid words\n",
    "min_freq = 10\n",
    "valid_words = set([word for word, freq in word_counts.items() if freq >= min_freq])\n",
    "\n",
    "# remove rare words based on frequency threshold\n",
    "def remove_rare_words(tokens):\n",
    "    return [word for word in tokens if word in valid_words]\n",
    "\n",
    "data['Filtered_Tokens'] = data['Lemmatized_Tokens'].apply(remove_rare_words)\n",
    "\n",
    "# join tokens back into cleaned abstracts\n",
    "data['Cleaned_Abstract'] = data['Filtered_Tokens'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "# create a cleaned DataFrame with relevant columns\n",
    "cleaned_data = data[['PaperID', 'Year', 'Cleaned_Abstract']]\n",
    "cleaned_data = cleaned_data[~(cleaned_data['PaperID'] == 57188)] # this paper has no abstract\n",
    "cleaned_data = cleaned_data.reset_index(drop=True) \n",
    "cleaned_data.insert(0, 'Document_ID', range(len(cleaned_data))) \n",
    "abstract_list = cleaned_data['Cleaned_Abstract'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "corpus = {doc_id: abstract_list for doc_id, abstract_list in enumerate(abstract_list)}\n",
    "# cleaned_data.to_csv('./data/processed/cleaned_data.xlsx', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c0e112a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Corpus word frequency information:\n",
      "total words: 83,202\n",
      "num of unique words: 1,490\n",
      "average num of words for each doc: 55.84\n",
      "standard deviation of word frequency among docs: 104.60\n",
      "\n",
      "üèÜ Top 20 High-frequency words:\n",
      " Rank          Word  Frequency  Percentage  Cumulative_Percentage  Document_Count  Document_Percentage Frequency_Category\n",
      "    1        method       1598      1.9206                 1.9206             654              67.4227               High\n",
      "    2         model       1554      1.8677                 3.7883             579              59.6907               High\n",
      "    3       element       1112      1.3365                 5.1248             513              52.8866               High\n",
      "    4           use        999      1.2007                 6.3255             572              58.9691               High\n",
      "    5       propose        823      0.9892                 7.3147             510              52.5773             Medium\n",
      "    6     numerical        799      0.9603                 8.2750             535              55.1546             Medium\n",
      "    7       problem        774      0.9303                 9.2053             442              45.5670             Medium\n",
      "    8        finite        712      0.8557                10.0610             447              46.0825             Medium\n",
      "    9      material        695      0.8353                10.8963             339              34.9485             Medium\n",
      "   10       present        642      0.7716                11.6679             468              48.2474             Medium\n",
      "   11      approach        608      0.7308                12.3987             368              37.9381             Medium\n",
      "   12   formulation        575      0.6911                13.0898             305              31.4433             Medium\n",
      "   13          base        554      0.6658                13.7556             410              42.2680             Medium\n",
      "   14        result        526      0.6322                14.3878             402              41.4433             Medium\n",
      "   15 computational        486      0.5841                14.9719             333              34.3299             Medium\n",
      "   16      analysis        442      0.5312                15.5031             256              26.3918             Medium\n",
      "   17      solution        415      0.4988                16.0019             285              29.3814             Medium\n",
      "   18    simulation        405      0.4868                16.4887             261              26.9072             Medium\n",
      "   19      equation        404      0.4856                16.9743             251              25.8763             Medium\n",
      "   20          mesh        396      0.4760                17.4503             194              20.0000             Medium\n",
      "\n",
      "üìà Word Rank Information:\n",
      "Frequency_Category\n",
      "Low       1264\n",
      "Medium     222\n",
      "High         4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìã create doc-words matrix, shape of matrix is : (970, 1003)\n",
      "\n",
      "üìä Corpus detailed statistics info:\n",
      "==================================================\n",
      "\n",
      "BASIC_STATS:\n",
      "  total_documents: 970\n",
      "  total_words: 83202\n",
      "  unique_words: 1490\n",
      "  vocabulary_richness: 0.0179\n",
      "  average_doc_length: 85.7753\n",
      "  median_doc_length: 81.0000\n",
      "  min_doc_length: 4\n",
      "  max_doc_length: 216\n",
      "  std_doc_length: 26.3635\n",
      "\n",
      "FREQUENCY_DISTRIBUTION:\n",
      "  words_appearing_once: 0\n",
      "  words_appearing_2_5_times: 0\n",
      "  words_appearing_6_20_times: 608\n",
      "  words_appearing_more_than_20: 882\n",
      "\n",
      "TOP_WORDS:\n",
      "  Top words:\n",
      "    method: 1598\n",
      "    model: 1554\n",
      "    element: 1112\n",
      "    use: 999\n",
      "    propose: 823\n",
      "    numerical: 799\n",
      "    problem: 774\n",
      "    finite: 712\n",
      "    material: 695\n",
      "    present: 642\n",
      "    approach: 608\n",
      "    formulation: 575\n",
      "    base: 554\n",
      "    result: 526\n",
      "    computational: 486\n",
      "    analysis: 442\n",
      "    solution: 415\n",
      "    simulation: 405\n",
      "    equation: 404\n",
      "    mesh: 396\n",
      "\n",
      "üíæ save statistics tables...\n",
      "‚úÖ save <- corpus_word_frequency_table.csv\n",
      "‚úÖ  save <- document_word_matrix.csv\n",
      "‚úÖ save <- corpus_statistics_summary.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 1.2 Corpus Analysis: create corpus word frequency table, document-word matrix, and generate corpus statistics \"\"\"\n",
    "\n",
    "# Create corpus word frequency table\n",
    "def create_corpus_word_frequency_table(corpus):\n",
    "    # count all words in the corpus\n",
    "    all_words = [word for doc in corpus.values() for word in doc]\n",
    "    word_freq_counter = Counter(all_words)\n",
    "    \n",
    "    # create a DataFrame for word frequencies\n",
    "    word_freq_df = pd.DataFrame([\n",
    "        {'Word': word, 'Frequency': freq}\n",
    "        for word, freq in word_freq_counter.items()\n",
    "    ]).sort_values('Frequency', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # add statistics like total words, unique words, and percentages\n",
    "    total_words = len(all_words)\n",
    "    unique_words = len(word_freq_counter)\n",
    "    \n",
    "    word_freq_df['Percentage'] = (word_freq_df['Frequency'] / total_words * 100).round(4)\n",
    "    word_freq_df['Cumulative_Percentage'] = word_freq_df['Percentage'].cumsum().round(4)\n",
    "    \n",
    "    # add word distribution across documents like document count and percentage\n",
    "    word_doc_count = {}\n",
    "    for word in word_freq_counter.keys():\n",
    "        doc_count = sum(1 for doc in corpus.values() if word in doc)\n",
    "        word_doc_count[word] = doc_count\n",
    "    \n",
    "    word_freq_df['Document_Count'] = word_freq_df['Word'].map(word_doc_count)\n",
    "    word_freq_df['Document_Percentage'] = (word_freq_df['Document_Count'] / len(corpus) * 100).round(4)\n",
    "    \n",
    "    # categorize frequency into High, Medium, Low, Very Low\n",
    "    def categorize_frequency(freq, total):\n",
    "        if freq >= total * 0.01:  # >1%\n",
    "            return 'High'\n",
    "        elif freq >= total * 0.001:  # 0.1%-1%\n",
    "            return 'Medium'\n",
    "        elif freq >= total * 0.0001:  # 0.01%-0.1%\n",
    "            return 'Low'\n",
    "        else:\n",
    "            return 'Very Low'\n",
    "    \n",
    "    word_freq_df['Frequency_Category'] = word_freq_df['Frequency'].apply(\n",
    "        lambda x: categorize_frequency(x, total_words)\n",
    "    )\n",
    "    \n",
    "    # add ranking of words based on frequency\n",
    "    word_freq_df['Rank'] = range(1, len(word_freq_df) + 1)\n",
    "    \n",
    "    # return DataFrame with selected columns in a specific order\n",
    "    word_freq_df = word_freq_df[[\n",
    "        'Rank', 'Word', 'Frequency', 'Percentage', 'Cumulative_Percentage',\n",
    "        'Document_Count', 'Document_Percentage', 'Frequency_Category'\n",
    "    ]]\n",
    "    \n",
    "    return word_freq_df\n",
    "\n",
    "# Create document-word matrix\n",
    "def create_document_word_matrix(corpus, cleaned_data=None):\n",
    "    # get all unique words (sorted by frequency, keeping only the top 1000 most common words)\n",
    "    all_words = [word for doc in corpus.values() for word in doc]\n",
    "    word_counter = Counter(all_words)\n",
    "    top_words = [word for word, _ in word_counter.most_common(1000)]\n",
    "    \n",
    "    # create a document-word matrix, where each row corresponds to a document and each column corresponds to a word\n",
    "    # the value is the frequency of that word in the document\n",
    "    doc_word_matrix = []\n",
    "    \n",
    "    for doc_id, doc in corpus.items():\n",
    "        doc_counter = Counter(doc)\n",
    "        row = [doc_counter.get(word, 0) for word in top_words]\n",
    "        doc_word_matrix.append(row)\n",
    "    \n",
    "    # create DataFrame from the document-word matrix\n",
    "    df = pd.DataFrame(doc_word_matrix, columns=top_words)\n",
    "    \n",
    "    # insert Document_ID and other metadata if available\n",
    "    if cleaned_data is not None and len(cleaned_data) == len(corpus):\n",
    "        df.insert(0, 'Document_ID', range(len(corpus)))\n",
    "        df.insert(1, 'PaperID', cleaned_data['PaperID'].values)\n",
    "        df.insert(2, 'Year', cleaned_data['Year'].values)\n",
    "    else:\n",
    "        df.insert(0, 'Document_ID', range(len(corpus)))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate corpus statistics like total documents, total words, unique words, vocabulary richness, average document length, etc.\n",
    "def generate_corpus_statistics(corpus, cleaned_data=None):\n",
    "\n",
    "    # word frequency statistics\n",
    "    total_docs = len(corpus)\n",
    "    all_words = [word for doc in corpus.values() for word in doc]\n",
    "    total_words = len(all_words)\n",
    "    unique_words = len(set(all_words))\n",
    "    \n",
    "    # document length statistics\n",
    "    doc_lengths = [len(doc) for doc in corpus.values()]\n",
    "    \n",
    "    # count word frequencies and return statistics\n",
    "    word_counter = Counter(all_words)\n",
    "    \n",
    "    statistics = {\n",
    "        'basic_stats': {\n",
    "            'total_documents': total_docs,\n",
    "            'total_words': total_words,\n",
    "            'unique_words': unique_words,\n",
    "            'vocabulary_richness': unique_words / total_words,\n",
    "            'average_doc_length': np.mean(doc_lengths),\n",
    "            'median_doc_length': np.median(doc_lengths),\n",
    "            'min_doc_length': min(doc_lengths),\n",
    "            'max_doc_length': max(doc_lengths),\n",
    "            'std_doc_length': np.std(doc_lengths)\n",
    "        },\n",
    "        'frequency_distribution': {\n",
    "            'words_appearing_once': sum(1 for freq in word_counter.values() if freq == 1),\n",
    "            'words_appearing_2_5_times': sum(1 for freq in word_counter.values() if 2 <= freq <= 5),\n",
    "            'words_appearing_6_20_times': sum(1 for freq in word_counter.values() if 6 <= freq <= 20),\n",
    "            'words_appearing_more_than_20': sum(1 for freq in word_counter.values() if freq > 20),\n",
    "        },\n",
    "        'top_words': word_counter.most_common(20)\n",
    "    }\n",
    "    \n",
    "    return statistics\n",
    "\n",
    "#-------------------create word frequency summary-----------------\n",
    "word_freq_table = create_corpus_word_frequency_table(corpus)\n",
    "\n",
    "print(f\"\\nüìä Corpus word frequency information:\")\n",
    "print(f\"total words: {word_freq_table['Frequency'].sum():,}\")\n",
    "print(f\"num of unique words: {len(word_freq_table):,}\")\n",
    "print(f\"average num of words for each doc: {word_freq_table['Frequency'].mean():.2f}\")\n",
    "print(f\"standard deviation of word frequency among docs: {word_freq_table['Frequency'].std():.2f}\")\n",
    "\n",
    "print(f\"\\nüèÜ Top 20 High-frequency words:\")\n",
    "print(word_freq_table.head(20).to_string(index=False))\n",
    "\n",
    "print(f\"\\nüìà Word Rank Information:\")\n",
    "freq_category_stats = word_freq_table['Frequency_Category'].value_counts()\n",
    "print(freq_category_stats)\n",
    "\n",
    "#-------------------create document-word matrix and generate corpus statistics----------------\n",
    "# create document-word matrix (optional, suitable for smaller datasets)\n",
    "\n",
    "doc_word_matrix = create_document_word_matrix(corpus, cleaned_data)\n",
    "print(f\"\\nüìã create doc-words matrix, shape of matrix is : {doc_word_matrix.shape}\")\n",
    "\n",
    "# generate detailed statistics\n",
    "corpus_stats = generate_corpus_statistics(corpus, cleaned_data)\n",
    "\n",
    "print(f\"\\nüìä Corpus detailed statistics info:\")\n",
    "print(\"=\" * 50)\n",
    "for category, stats in corpus_stats.items():\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    if isinstance(stats, dict):\n",
    "        for key, value in stats.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"  {key}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"  {key}: {value}\")\n",
    "    elif isinstance(stats, list):\n",
    "        print(\"  Top words:\")\n",
    "        for word, freq in stats:\n",
    "            print(f\"    {word}: {freq}\")\n",
    "\n",
    "# ---------------------save statistics tables-------------------\n",
    "print(f\"\\nüíæ save statistics tables...\")\n",
    "\n",
    "# word_freq_table.to_csv('corpus_word_frequency_table.csv', index=False, encoding='utf-8')\n",
    "print(\"‚úÖ save <- corpus_word_frequency_table.csv\")\n",
    "\n",
    "if 'doc_word_matrix' in locals():\n",
    "    # doc_word_matrix.to_csv('document_word_matrix.csv', index=False, encoding='utf-8')\n",
    "    print(\"‚úÖ  save <- document_word_matrix.csv\")\n",
    "\n",
    "# ‰øùÂ≠òÁªüËÆ°ÊëòË¶Å\n",
    "stats_df = pd.DataFrame([\n",
    "    {'Metric': key, 'Value': value} \n",
    "    for category_stats in corpus_stats.values() \n",
    "    if isinstance(category_stats, dict)\n",
    "    for key, value in category_stats.items()\n",
    "])\n",
    "\n",
    "# stats_df.to_csv('corpus_statistics_summary.csv', index=False, encoding='utf-8')\n",
    "print(\"‚úÖ save <- corpus_statistics_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6702e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"2. Core logic function: Chain Rule Process (CRP)\"\"\"\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    Tree node class for hierarchical topic modeling using nested Chinese Restaurant Process (nCRP).\n",
    "    Each node represents a topic in the hierarchy, with the tree structure representing\n",
    "    the nested relationship between general and specific topics.\n",
    "    \"\"\"\n",
    "    last_node_id = 0\n",
    "    total_node = 0\n",
    "    node_with_id = {}\n",
    "\n",
    "    def __init__(self, parent=None, layer=0):\n",
    "        self.node_id = Node.last_node_id # Unique identifier for this node\n",
    "        Node.last_node_id += 1\n",
    "        Node.total_node += 1\n",
    "        self.layer = layer # layer (int): Depth level in the hierarchy (0=root, 1=first level, etc.)\n",
    "        self.children = [] # children (list): List of child Node objects\n",
    "        self.parent = parent # parent (Node): Reference to parent node (None for root)\n",
    "        self.docs_list = [] # record index of documents reaching this node\n",
    "        Node.node_with_id[self.node_id] = self \n",
    "\n",
    "    def add_child(self):\n",
    "        child = Node(parent=self, layer=self.layer+1)\n",
    "        self.children.append(child)\n",
    "        return child\n",
    "    \n",
    "    def remove_child(self, child):\n",
    "        self.children.remove(child)\n",
    "        Node.node_with_id[child.node_id] = None \n",
    "        child.parent = None \n",
    "        Node.total_node -= 1\n",
    "\n",
    "def nCRP(corpus, depth, gamma):\n",
    "    \"\"\"\n",
    "    Nested Chinese Restaurant Process (nCRP) for initializing hierarchical topic structure.\n",
    "    \n",
    "    This function implements the generative process for creating a tree structure where:\n",
    "    1. Each document follows a path from root to a leaf node\n",
    "    2. At each level, documents choose to either create a new topic or join an existing one\n",
    "    3. Words in documents are randomly assigned to topics along their path\n",
    "    \"\"\"\n",
    "    # Initialize the root Node class each time nCRP is called\n",
    "    Node.last_node_id = 0\n",
    "    Node.total_node = 0\n",
    "    Node.node_with_id = {}\n",
    "    \n",
    "    \"\"\"\n",
    "     Args:\n",
    "        corpus (dict): Document collection {doc_id: [word1, word2, ...]}\n",
    "        depth (int): Maximum depth of the topic hierarchy (number of levels)\n",
    "        gamma (float): Concentration parameter controlling topic creation probability\n",
    "                      Higher gamma -> more likely to create new topics\n",
    "                      \n",
    "    Returns: [root_node, path_list, doc_path, doc_word_allocation] where:\n",
    "        - root_node (Node): Root node of the topic tree\n",
    "        - path_list (dict): {leaf_node_id: [node0, node1, ...]} - Complete paths from root to each leaf\n",
    "        - doc_path (dict): {doc_id: leaf_node_id} - Maps each document to its assigned leaf node\n",
    "        - doc_word_allocation (dict): {doc_id: [layer0, layer1, ...]} - Word-to-layer assignments\n",
    "    \"\"\"\n",
    "\n",
    "    root_node = Node()\n",
    "    path_list = {} # {leaf_node_id: [node0, node2,...]} - record each path from root to leaf nodes\n",
    "    doc_word_allocation = {} # {doc_id: {word: layer}} - record word allocation for each document\n",
    "    doc_path = {} # [leaf_node_id, leaf_node_id, ...] - record only the leaf node id of the pathÔºå list indexed by doc_id\n",
    "    \n",
    "    for c, doc in corpus.items(): # c is the index, d is the document\n",
    "        # print(doc)\n",
    "        # all docs starts from the root node\n",
    "        path = [root_node]\n",
    "        root_node.docs_list.append(c)\n",
    "\n",
    "        for i in range(1, depth):\n",
    "            # chose node based on CRP\n",
    "            parent_node = path[i-1]\n",
    "\n",
    "            CRP_probs = [gamma/(gamma + len(parent_node.docs_list) - 1)] # choose a new node: gamma/(gamma + n - 1)\n",
    "            for child in parent_node.children:\n",
    "                CRP_probs.append(len(child.docs_list)/(gamma + len(parent_node.docs_list) - 1))\n",
    "\n",
    "            chosen_index = np.random.choice(len(CRP_probs), p=CRP_probs) \n",
    "            if chosen_index == 0: # create a new node\n",
    "                current_node = parent_node.add_child()\n",
    "            else: # chose an existing node\n",
    "                current_node = parent_node.children[chosen_index - 1]\n",
    "\n",
    "            path.append(current_node)\n",
    "            current_node.docs_list.append(c)\n",
    "\n",
    "        if path[-1].node_id not in path_list.keys():\n",
    "            path_list[path[-1].node_id] = path\n",
    "        doc_path[c] = path[-1].node_id\n",
    "\n",
    "        # assign the words location to the document\n",
    "        word_allocation = []\n",
    "        for word in doc:\n",
    "            allocate_layer = np.random.randint(0,depth) # randomly allocate word to a layer\n",
    "            word_allocation.append(allocate_layer)\n",
    "\n",
    "        doc_word_allocation[c] = word_allocation\n",
    "    return [root_node, path_list, doc_path, doc_word_allocation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feb2366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"3. Core logic functions: Gibbs sampling, and relevant sub-function: word distribution, likelihood calculation, etc.\"\"\"\n",
    "def _create_default_dict_int():\n",
    "    return defaultdict(int)\n",
    "\n",
    "def aggregate_words(corpus, doc_word_allocation):\n",
    "    \"\"\" \n",
    "    Convert document word-layer assignments into hierarchical node word count.\n",
    "\n",
    "    Args:\n",
    "        corpus: {doc_id: [word1, word2, ...]} - Document collection\n",
    "        doc_word_allocation : {doc_id: [layer0, layer1, ...]} - Word-to-layer assignments\n",
    "    \n",
    "    Returns:\n",
    "        doc_node_allocation: {doc_id: {layer: {word: count}}} - Nested word counts by document and layer\n",
    "    \"\"\"\n",
    "    doc_node_allocation = {}\n",
    "\n",
    "    for doc_id, doc in corpus.items(): # c is the index, d is the document\n",
    "        allocation = doc_word_allocation[doc_id]\n",
    "        \n",
    "        # Use the named function here\n",
    "        layer_counts = defaultdict(_create_default_dict_int)\n",
    "        \n",
    "        for word, layer in zip(doc, allocation):\n",
    "            layer_counts[layer][word] += 1\n",
    "\n",
    "        doc_node_allocation[doc_id] = layer_counts\n",
    "    return doc_node_allocation\n",
    "\n",
    "def node_word_distribution(doc_node_allocation, doc_path, path_list, exclude_docs=None):\n",
    "    \"\"\"\n",
    "    Calculate global node-word distribution by aggregating word counts from all documents.\n",
    "    \n",
    "    Args:\n",
    "        doc_node_allocation (dict): {doc_id: {layer: {word: count}}} - Document word counts by layer\n",
    "        doc_path (dict): {doc_id: leaf_id} - Document to leaf node mapping\n",
    "        path_list (dict): {leaf_id: [Node1, Node2, ...]} - Complete paths from root to leaf\n",
    "        exclude_docs (set, optional): Document IDs to exclude from calculation\n",
    "    \n",
    "    Returns:\n",
    "        node_word_dist: {node_id: {word: count}} - Word count distribution for each node\n",
    "    \"\"\"\n",
    "    node_word_dist = defaultdict(lambda: defaultdict(int))\n",
    "    exclude_docs = set() if exclude_docs is None else set(exclude_docs)\n",
    "    \n",
    "    for doc_id, leaf_id in doc_path.items():\n",
    "        if doc_id in exclude_docs:\n",
    "            continue\n",
    "            \n",
    "        path = path_list[leaf_id]\n",
    "        \n",
    "        doc_allocation = doc_node_allocation[doc_id]\n",
    "        \n",
    "        for node in path:\n",
    "            node_layer = node.layer\n",
    "            if node_layer in doc_allocation:\n",
    "                for word, count in doc_allocation[node_layer].items():\n",
    "                    node_word_dist[node.node_id][word] += count\n",
    "    \n",
    "    return node_word_dist\n",
    "\n",
    "def calc_node_likelihood(compare_dist, target_dist, eta, len_W):\n",
    "    \"\"\"\n",
    "    Calculate likelihood of generating target word distribution from comparison node \n",
    "    to any tree node using Dirichlet-multinomial model, providing a probability estimate to choose the best path.\n",
    "   \n",
    "    Args:\n",
    "        compare_dist: {word: count} - Current word distribution of the comparison node\n",
    "        target_dist: {word: count} - Target word distribution to be generated  \n",
    "        eta (int): Dirichlet prior parameter (smoothing parameter)\n",
    "        len_W (int): Vocabulary size\n",
    "    \n",
    "    Returns:\n",
    "        float: Likelihood probability (or -inf for numerical overflow)\n",
    "    \"\"\"\n",
    "    sum_A = sum(compare_dist.values()) + eta * len_W\n",
    "    sum_B = sum(compare_dist.values()) + sum(target_dist.values()) + eta * len_W\n",
    "    \n",
    "    lgamma_sum_A = math.lgamma(sum_A)\n",
    "    lgamma_sum_B = math.lgamma(sum_B)\n",
    "    \n",
    "    lgamma_prod_A = 0.0\n",
    "    lgamma_prod_B = 0.0\n",
    "    \n",
    "    for word, count in target_dist.items():\n",
    "        comp_val = compare_dist.get(word, 0)\n",
    "        A = comp_val + eta\n",
    "        B = comp_val + count + eta\n",
    "        \n",
    "        lgamma_prod_A += math.lgamma(A)\n",
    "        lgamma_prod_B += math.lgamma(B)\n",
    "\n",
    "    try:\n",
    "        log_likelihood = (lgamma_sum_A - lgamma_prod_A) + (lgamma_prod_B - lgamma_sum_B)\n",
    "        return math.exp(log_likelihood)\n",
    "    except (OverflowError, ValueError):\n",
    "        return float('-inf')  # ËøîÂõûË¥üÊó†Á©∑Ë°®Á§∫ÊûÅÂ∞èÊ¶ÇÁéá    \n",
    "\n",
    "def create_new_path(base_node, doc_id, depth):\n",
    "    new_path = []\n",
    "    current = base_node\n",
    "    while current:\n",
    "        new_path.insert(0, current)\n",
    "        current = current.parent\n",
    "    \n",
    "    current = base_node\n",
    "    for _ in range(base_node.layer, depth-1):\n",
    "        new_node = current.add_child()\n",
    "        new_path.append(new_node)\n",
    "        current = new_node\n",
    "        \n",
    "    return current.node_id, new_path\n",
    "\n",
    "def exclude_doc_from_node_dist(global_node_word_dist, doc_node_allocation, doc_path, path_list, doc_id):\n",
    "    doc_path_lst = path_list[doc_path[doc_id]]\n",
    "    for node in doc_path_lst:\n",
    "        if node.node_id in global_node_word_dist:\n",
    "            for word, count in doc_node_allocation[doc_id].get(node.layer, {}).items():\n",
    "                global_node_word_dist[node.node_id][word] -= count\n",
    "                if global_node_word_dist[node.node_id][word] <= 0:\n",
    "                    del global_node_word_dist[node.node_id][word]\n",
    "    return global_node_word_dist\n",
    "\n",
    "def add_doc_to_node_dist(global_node_word_dist, doc_node_allocation, doc_path, path_list, doc_id):\n",
    "    \"\"\"Â∞ÜÊñáÊ°£ÁöÑËØçÂàÜÂ∏ÉÊ∑ªÂä†Âà∞ÂÖ®Â±ÄËØçÂàÜÂ∏É‰∏≠\"\"\"\n",
    "    doc_path_lst = path_list[doc_path[doc_id]]\n",
    "    for node in doc_path_lst:\n",
    "        node_layer = node.layer\n",
    "        if node_layer in doc_node_allocation[doc_id]:\n",
    "            for word, count in doc_node_allocation[doc_id][node_layer].items():\n",
    "                if node.node_id not in global_node_word_dist:\n",
    "                    global_node_word_dist[node.node_id] = defaultdict(int)\n",
    "                global_node_word_dist[node.node_id][word] += count\n",
    "    return global_node_word_dist\n",
    "\n",
    "def Gibbs_sampling(corpus, root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation, global_node_word_dist,\n",
    "                   gamma, eta, alpha, depth, iteration, iter_start=None, iter_end=None):\n",
    "    W = set(itertools.chain.from_iterable(corpus.values()))\n",
    "\n",
    "    for doc_id, doc in corpus.items():\n",
    "        # print('doc_id', doc_id)\n",
    "        current_path = path_list[doc_path[doc_id]]\n",
    "        current_node_allocation = doc_node_allocation[doc_id]\n",
    "        node_word_dist = exclude_doc_from_node_dist(global_node_word_dist, doc_node_allocation, doc_path, path_list, doc_id)\n",
    "        # print('g2',global_node_word_dist[0])\n",
    "\n",
    "        # isolate doc_c from current situation\n",
    "        nodes_to_remove = []\n",
    "        for node in current_path[::-1]:\n",
    "            node.docs_list.remove(doc_id)\n",
    "            if len(node.docs_list) == 0 and node != root_node:\n",
    "                nodes_to_remove.append(node)\n",
    "        \n",
    "        if nodes_to_remove:\n",
    "            del path_list[doc_path[doc_id]]\n",
    "            for node in nodes_to_remove:\n",
    "                if node.parent:  # ÂÜçÊ¨°Ê£ÄÊü•‰ª•Á°Æ‰øùÂÆâÂÖ®\n",
    "                    node.parent.remove_child(node)\n",
    "\n",
    "        # sample path\n",
    "        path_prior_lst = {}\n",
    "        path_likelihood_lst = {}\n",
    "\n",
    "        target_prior = {}\n",
    "        target_likelihood = {}\n",
    "        for path_id, path in path_list.items():\n",
    "            node_prior_lst = [1.0]\n",
    "            node_likelihood_lst = []\n",
    "            \n",
    "            for node in path:\n",
    "                if node.parent:\n",
    "                    prior = (len(node.docs_list)) / (gamma + len(node.parent.docs_list)) \n",
    "                    node_prior_lst.append(prior)\n",
    "                \n",
    "                if node.layer in current_node_allocation:\n",
    "                    likelihood = calc_node_likelihood(node_word_dist[node.node_id], current_node_allocation[node.layer], eta, len(W))\n",
    "                    node_likelihood_lst.append(likelihood)\n",
    "                else:\n",
    "                    node_likelihood_lst.append(0)\n",
    "\n",
    "            target_prior[path_id] = node_prior_lst\n",
    "            target_likelihood[path_id] = node_likelihood_lst\n",
    "\n",
    "            path_prior = reduce(operator.mul, (x for x in node_prior_lst if x != 0), 1)\n",
    "            path_likelihood = reduce(operator.mul, (x for x in node_likelihood_lst if x != 0), 1)\n",
    "            path_prior_lst[path_id] = path_prior\n",
    "            path_likelihood_lst[path_id] = path_likelihood\n",
    "\n",
    "        multiplied_path_dict = {key: path_prior_lst[key] * path_likelihood_lst[key] for key in path_likelihood_lst}\n",
    "        \n",
    "        # create new path,\n",
    "        new_path_prior_lst = {}\n",
    "        new_path_likelihood_lst = {}\n",
    "\n",
    "        new_target_prior = {}\n",
    "        new_target_likelihood = {}\n",
    "\n",
    "        for node_id, node in Node.node_with_id.items():\n",
    "            if node != None and node.layer < depth-1:\n",
    "                new_node_prior_lst = [gamma / (gamma + len(node.docs_list))]\n",
    "                \n",
    "                if 0 in current_node_allocation:\n",
    "                    new_node_likelihood_lst = [calc_node_likelihood(node_word_dist[0], current_node_allocation[0], eta, len(W))]\n",
    "                else:\n",
    "                    new_node_likelihood_lst = [0]\n",
    "                \n",
    "                temp_node = node\n",
    "                while temp_node.parent:\n",
    "                    new_node_prior_lst.insert(0, (len(temp_node.docs_list)) / (gamma + len(temp_node.parent.docs_list)))             \n",
    "                \n",
    "                    if temp_node.layer in current_node_allocation:\n",
    "                        new_node_likelihood_lst.insert(1, calc_node_likelihood(node_word_dist[temp_node.node_id], current_node_allocation[temp_node.layer], eta, len(W)))\n",
    "                    else:\n",
    "                        new_node_likelihood_lst.insert(1, 0)\n",
    "\n",
    "                    temp_node = temp_node.parent\n",
    "\n",
    "                for layer in range(node.layer+1, depth):\n",
    "                    if layer in current_node_allocation:\n",
    "                        new_node_likelihood_lst.append(calc_node_likelihood({}, current_node_allocation[layer], eta, len(W)))\n",
    "\n",
    "                new_target_prior[node_id] = new_node_prior_lst\n",
    "                new_target_likelihood[node_id] = new_node_likelihood_lst\n",
    "                new_path_prior = reduce(operator.mul, (x for x in new_node_prior_lst if x != 0), 1)\n",
    "                new_path_likelihood = reduce(operator.mul, (x for x in new_node_likelihood_lst if x != 0), 1)\n",
    "                new_path_prior_lst[node_id] = new_path_prior\n",
    "                new_path_likelihood_lst[node_id] = new_path_likelihood\n",
    "        \n",
    "        multiplied_new_path_dict = {f'create{key}': new_path_prior_lst[key] * new_path_likelihood_lst[key] for key in new_path_likelihood_lst}\n",
    "        all_probs = {**multiplied_path_dict, **multiplied_new_path_dict}\n",
    "\n",
    "        total_prob = sum(v for v in all_probs.values())\n",
    "        if total_prob > 0:\n",
    "            normalized_probs = {k: v/total_prob for k, v in all_probs.items()}\n",
    "        else:\n",
    "            normalized_probs = {k: 1.0/len(all_probs) for k in all_probs}\n",
    "        # print(normalized_probs)\n",
    "        \n",
    "        chosen_path = np.random.choice(list(normalized_probs.keys()),p=list(normalized_probs.values()))\n",
    "        if chosen_path.startswith('create'):\n",
    "            base_node = Node.node_with_id[int(chosen_path[6:])]\n",
    "            leaf_id, added_path = create_new_path(base_node, doc_id, depth)\n",
    "            path_list.update({leaf_id:added_path})\n",
    "            doc_path[doc_id] = leaf_id\n",
    "        else:\n",
    "            leaf_id = int(chosen_path)\n",
    "            added_path = path_list[int(chosen_path)]\n",
    "            doc_path[doc_id] = int(chosen_path)\n",
    "        \n",
    "        for node in added_path:\n",
    "            node.docs_list.append(doc_id)\n",
    "        \n",
    "        \"\"\"sample topic\"\"\"\n",
    "        # node_word_dist_update = node_word_distribution(doc_node_allocation, doc_path, path_list, exclude_docs=None)\n",
    "        node_word_dist_update = add_doc_to_node_dist(global_node_word_dist, doc_node_allocation, doc_path, path_list, doc_id)\n",
    "        # print('g3',global_node_word_dist[0])\n",
    "        \n",
    "        doc_word2node = doc_word_allocation[doc_id]\n",
    "        current_path = path_list[doc_path[doc_id]]\n",
    "        update_doc_word2node = []\n",
    "        for word, old_layer in zip(doc, doc_word2node):\n",
    "            doc_node_allocation[doc_id][old_layer][word] -= 1\n",
    "            node_word_dist_update[current_path[old_layer].node_id][word] -= 1\n",
    "            topic_probs = {}\n",
    "\n",
    "            for layer, topic in enumerate(current_path):\n",
    "                word_in_topic = node_word_dist_update[topic.node_id].get(word,0)\n",
    "                topic_in_doc = sum(doc_node_allocation[doc_id].get(layer,{}).values())\n",
    "\n",
    "                word_prop = (word_in_topic+eta)/(sum(node_word_dist_update[topic.node_id].values())+len(W)*eta)\n",
    "                topic_prop = (topic_in_doc+alpha)/(len(doc)+depth*alpha)\n",
    "                topic_probs[layer] = word_prop*topic_prop\n",
    "            \n",
    "            total_topic_prob = sum(v for v in topic_probs.values() if v > 0)\n",
    "            if total_topic_prob > 0:\n",
    "                normalized_topic_probs = {k: v/total_topic_prob for k, v in topic_probs.items()}\n",
    "            else:\n",
    "                normalized_topic_probs = {k: 1.0/len(topic_probs) for k in topic_probs}\n",
    "\n",
    "            chosen_layer = np.random.choice(list(normalized_topic_probs.keys()),p=list(normalized_topic_probs.values()))\n",
    "            update_doc_word2node.append(chosen_layer)\n",
    "            doc_node_allocation[doc_id][chosen_layer][word] += 1\n",
    "            node_word_dist_update[current_path[chosen_layer].node_id][word] += 1\n",
    "\n",
    "        doc_word_allocation[doc_id] = update_doc_word2node\n",
    "        # print('g4',global_node_word_dist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caa69ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recorder:\n",
    "    \"\"\"GibbsÈááÊ†∑Ëø≠‰ª£ËÆ∞ÂΩïÂô®\"\"\"\n",
    "    \n",
    "    def __init__(self, corpus, depth, eta, alpha):\n",
    "        self.corpus = corpus\n",
    "        self.depth = depth\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.iteration_records = []\n",
    "        self.vocab = set(itertools.chain.from_iterable(corpus.values()))\n",
    "        \n",
    "    def record_iteration(self, iteration_num, root_node, path_list, doc_path, \n",
    "                        doc_word_allocation, doc_node_allocation, global_node_word_dist,\n",
    "                        newly_created_paths=None, iter_start_for_detailed_log=float('inf')): # Ê∑ªÂä†Êñ∞ÂèÇÊï∞Âπ∂ËÆæÈªòËÆ§ÂÄº\n",
    "        \"\"\"ËÆ∞ÂΩïÂçïÊ¨°Ëø≠‰ª£ÁöÑÊâÄÊúâ‰ø°ÊÅØ\"\"\"\n",
    "        \n",
    "        should_record_word_node_details_to_file = (iteration_num >= iter_start_for_detailed_log)\n",
    "\n",
    "        # ËÆ°ÁÆóÊîπÂèòË∑ØÂæÑÁöÑÊñáÊ°£Êï∞\n",
    "        changed_docs_count = 0\n",
    "        if iteration_num > 0:  # ÂØπÁ¨¨‰∏ÄËΩÆ‰πãÂêéÁöÑËø≠‰ª£ËøõË°åÁªüËÆ°\n",
    "            previous_doc_paths = {doc['document_id']: doc['leaf_node_id'] \n",
    "                                for doc in self.iteration_records[-1]['doc_path_assignments']}\n",
    "            changed_docs_count = sum(1 for doc_id, leaf_id in doc_path.items() \n",
    "                                if doc_id in previous_doc_paths and previous_doc_paths[doc_id] != leaf_id)\n",
    "        \n",
    "        # 1. ËÆ∞ÂΩïÊñáÊ°£Ë∑ØÂæÑÂàÜÈÖç\n",
    "        # file_name: gibbs_iteration_records_document_paths - \n",
    "        doc_path_records = []\n",
    "        for doc_id, leaf_id in doc_path.items():\n",
    "            doc_path_records.append({\n",
    "                'iteration': iteration_num,\n",
    "                'document_id': doc_id,\n",
    "                'leaf_node_id': leaf_id,\n",
    "                'path_created_this_iteration': leaf_id in (newly_created_paths or [])\n",
    "            })\n",
    "        \n",
    "        # 2. ËÆ∞ÂΩïË∑ØÂæÑ/Ê†ëÁªìÊûÑ \n",
    "        # path_list: {leaf_id: [Node1, Node2, ...]}\n",
    "        # doc_path: {doc_id: leaf_id}\n",
    "        # file_name:gibbs_iteration_records_path_structures - \n",
    "        path_structure_records = []\n",
    "        for leaf_id, path_nodes in path_list.items():\n",
    "            node_ids = [node.node_id for node in path_nodes]\n",
    "            \n",
    "            docs_in_this_path = [doc_id for doc_id, assigned_leaf_id in doc_path.items()\n",
    "                                if assigned_leaf_id == leaf_id]\n",
    "            \n",
    "            path_record = {\n",
    "                'iteration': iteration_num,\n",
    "                'leaf_node_id': leaf_id,\n",
    "                'document_count': len(docs_in_this_path),\n",
    "                'documents_in_path': docs_in_this_path,\n",
    "                'path_created_this_iteration': leaf_id in (newly_created_paths or [])\n",
    "            }\n",
    "\n",
    "            for i in range(self.depth):\n",
    "                layer_key = f'layer_{i}_node_id'\n",
    "                if i < len(node_ids):\n",
    "                    path_record[layer_key] = node_ids[i]\n",
    "                else:\n",
    "                    path_record[layer_key] = None \n",
    "            \n",
    "            path_structure_records.append(path_record),\n",
    "        \n",
    "        # 3. ËÆ∞ÂΩïËØçËØ≠ÂàÜÈÖç (‰ªÖÂú®ËØ¶ÁªÜËÆ∞ÂΩïÁ™óÂè£ÂÜÖÂ°´ÂÖÖ)\n",
    "        # file_name: gibbs_iteration_records_word_allocations - \n",
    "        # doc_word_allocation: {doc_id: [layer_for_word1, layer_for_word2,...]}\n",
    "        word_allocation_records = []\n",
    "        # attention: should_record_word_node_details_to_file = (iteration_num >= iter_start_for_detailed_log)\n",
    "        if should_record_word_node_details_to_file:\n",
    "            for doc_id, word_assignments in doc_word_allocation.items():\n",
    "                doc_words = self.corpus[doc_id]\n",
    "                for word_idx, (word, layer) in enumerate(zip(doc_words, word_assignments)):\n",
    "                    word_allocation_records.append({\n",
    "                        'iteration': iteration_num,\n",
    "                        'document_id': doc_id,\n",
    "                        'word_index': word_idx,\n",
    "                        'word': word,\n",
    "                        'assigned_layer': layer,\n",
    "                        'leaf_node_id': doc_path[doc_id],\n",
    "                        'assigned_node_id': path_list[doc_path[doc_id]][layer].node_id\n",
    "                    })\n",
    "        \n",
    "        # 4. ËÆ°ÁÆóËäÇÁÇπËØçÂàÜÂ∏É (ÂßãÁªàÈúÄË¶Å‰∏∫log-likelihoodËÆ°ÁÆó)\n",
    "        # file_name: gibbs_iteration_records_node_word_distributions - \n",
    "        # doc_node_allocation: ?\n",
    "        # node_word_dist = self._calculate_node_word_distribution(\n",
    "        #     doc_node_allocation, doc_path, path_list\n",
    "        # )\n",
    "        # node_word_dist: # {node_id: {word: count}}\n",
    "        #   ËÆ∞ÂΩïËäÇÁÇπËØçÂàÜÂ∏É (‰ªÖÂú®ËØ¶ÁªÜËÆ∞ÂΩïÁ™óÂè£ÂÜÖÂ°´ÂÖÖ)\n",
    "        node_word_records = []\n",
    "        # attention: should_record_word_node_details_to_file = (iteration_num >= iter_start_for_detailed_log)\n",
    "        if should_record_word_node_details_to_file:\n",
    "            for node_id, current_word_dist in global_node_word_dist.items(): # ‰ΩøÁî®ËÆ°ÁÆóÂæóÂà∞ÁöÑ node_word_dist\n",
    "                for word, count in current_word_dist.items():\n",
    "                    node_word_records.append({\n",
    "                        'iteration': iteration_num,\n",
    "                        'node_id': node_id,\n",
    "                        'word': word,\n",
    "                        'count': count\n",
    "                    })\n",
    "        \n",
    "        # 5. ËÆ°ÁÆóÁîüÊàêÊ¶ÇÁéá/log-likelihood (ÂßãÁªà‰ΩøÁî®ËÆ°ÁÆóÂá∫ÁöÑ node_word_dist)\n",
    "        # file_name: gibbs_iteration_records_iteration_summaries - \n",
    "        log_likelihood = self._calculate_log_likelihood(\n",
    "            doc_path, path_list, doc_word_allocation, global_node_word_dist\n",
    "        )\n",
    "        \n",
    "        # 6. ËÆ∞ÂΩïÊï¥‰ΩìÁªüËÆ°‰ø°ÊÅØ,\n",
    "        # file_name: gibbs_iteration_records_iteration_summaries -\n",
    "        iteration_summary = {\n",
    "            'iteration': iteration_num,\n",
    "            'total_paths': len(path_list),\n",
    "            'total_documents': len(doc_path),\n",
    "            'log_likelihood': log_likelihood,\n",
    "            'changed_docs_count': changed_docs_count, \n",
    "            'newly_created_paths': len(newly_created_paths or []),\n",
    "            'avg_path_size': np.mean([len([doc_id for doc_id, assigned_leaf_id in doc_path.items() \n",
    "                                            if assigned_leaf_id == leaf_id]) for leaf_id in path_list.keys()]) if path_list else 0,\n",
    "            'max_path_size': max([len([doc_id for doc_id, assigned_leaf_id in doc_path.items() \n",
    "                                        if assigned_leaf_id == leaf_id]) for leaf_id in path_list.keys()]) if path_list else 0,\n",
    "            'min_path_size': min([len([doc_id for doc_id, assigned_leaf_id in doc_path.items() \n",
    "                                        if assigned_leaf_id == leaf_id]) for leaf_id in path_list.keys()]) if path_list else 0,\n",
    "        #     'doc_path': doc_path,\n",
    "        #     'doc_node_allocation':doc_node_allocation\n",
    "        }\n",
    "            \n",
    "        self.iteration_records.append({\n",
    "            'iteration': iteration_num,\n",
    "            'doc_path_assignments': doc_path_records,\n",
    "            'path_structures': path_structure_records,\n",
    "            'word_allocations': word_allocation_records, # Â¶ÇÊûú‰∏çÊª°Ë∂≥Êù°‰ª∂ÔºåÂàô‰∏∫Á©∫ÂàóË°®\n",
    "            'node_word_distributions': node_word_records, # Â¶ÇÊûú‰∏çÊª°Ë∂≥Êù°‰ª∂ÔºåÂàô‰∏∫Á©∫ÂàóË°®\n",
    "            'iteration_summary': iteration_summary\n",
    "        })\n",
    "        \n",
    "        return iteration_summary\n",
    "    \n",
    "    # def _calculate_node_word_distribution(self, doc_node_allocation, doc_path, path_list):\n",
    "    #     \"\"\"ËÆ°ÁÆóËäÇÁÇπËØçÂàÜÂ∏É\"\"\"\n",
    "    #     node_word_dist = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "    #     for doc_id, leaf_id in doc_path.items():\n",
    "    #         path = path_list[leaf_id]\n",
    "    #         doc_allocation = doc_node_allocation[doc_id]\n",
    "            \n",
    "    #         for node in path:\n",
    "    #             node_layer = node.layer\n",
    "    #             if node_layer in doc_allocation:\n",
    "    #                 for word, count in doc_allocation[node_layer].items():\n",
    "    #                     node_word_dist[node.node_id][word] += count\n",
    "        \n",
    "    #     return node_word_dist\n",
    "\n",
    "    def _calculate_log_likelihood(self, doc_path, path_list, \n",
    "                                    doc_word_allocation, # {doc_id: [layer_for_word1, layer_for_word2,...]}\n",
    "                                    node_word_dist):     # {node_id: {word: count}}\n",
    "        \"\"\"\n",
    "        ËÆ°ÁÆóÂú®ÂΩìÂâçÊñáÊ°£ËØçËØ≠Â±ÇÁ∫ßÂàÜÈÖçÂíåË∑ØÂæÑ‰∏ãÔºåÈáçÊñ∞ÁîüÊàêÊâÄÊúâÊñáÊ°£ËØçËØ≠ÁöÑÂØπÊï∞Ê¶ÇÁéá„ÄÇ\n",
    "        log P(Words | WordLayerAssignments, Paths, eta)\n",
    "        \"\"\"\n",
    "        total_log_likelihood = 0.0\n",
    "        vocab_size = len(self.vocab)\n",
    "        \n",
    "        # È¢ÑËÆ°ÁÆóÊØè‰∏™ËäÇÁÇπÁöÑÊÄªËØçÊï∞\n",
    "        node_total_words_map = defaultdict(int)\n",
    "        for node_id, dist in node_word_dist.items():\n",
    "            node_total_words_map[node_id] = sum(dist.values())\n",
    "\n",
    "        for doc_id, words_in_doc in self.corpus.items():\n",
    "            if not words_in_doc:\n",
    "                continue\n",
    "\n",
    "            current_doc_word_layer_assignments = doc_word_allocation[doc_id]\n",
    "            current_doc_path_nodes = path_list[doc_path[doc_id]] # List of Node objects\n",
    "\n",
    "            for i, word in enumerate(words_in_doc):\n",
    "                assigned_layer = current_doc_word_layer_assignments[i]\n",
    "\n",
    "                # P(word | assigned_node, eta)\n",
    "                # = (count_of_this_word_in_node + eta) / (total_words_in_node + vocab_size * eta)\n",
    "                if assigned_layer >= len(current_doc_path_nodes):\n",
    "                    # print(f\\Warning: Likelihood calc - doc_id {doc_id}, word '{word}' assigned_layer {assigned_layer} out of bounds for path length {len(current_doc_path_nodes)}.\\)\n",
    "                    total_log_likelihood += -float('inf') # Penalize for inconsistent assignment\n",
    "                    continue\n",
    "\n",
    "                assigned_node_object = current_doc_path_nodes[assigned_layer]\n",
    "                assigned_node_id = assigned_node_object.node_id\n",
    "                \n",
    "                count_word_in_assigned_node = node_word_dist.get(assigned_node_id, {}).get(word, 0)\n",
    "                total_words_in_assigned_node = node_total_words_map.get(assigned_node_id, 0)\n",
    "\n",
    "                denominator_val = total_words_in_assigned_node + vocab_size * self.eta\n",
    "                if denominator_val <= 0: \n",
    "                    log_prob_word_generation = -float('inf') \n",
    "                else:\n",
    "                    numerator_val = count_word_in_assigned_node + self.eta\n",
    "                    if numerator_val <=0: # Should not happen if eta > 0\n",
    "                        log_prob_word_generation = -float('inf')\n",
    "                    else:\n",
    "                        log_prob_word_generation = math.log(numerator_val) - math.log(denominator_val)\n",
    "                \n",
    "                total_log_likelihood += log_prob_word_generation\n",
    "                \n",
    "        return total_log_likelihood\n",
    "    \n",
    "    def save_to_files(self, base_filename=\"iteration\", last_n_iterations=20):\n",
    "        \"\"\"‰øùÂ≠òÊâÄÊúâËÆ∞ÂΩïÂà∞CSVÊñá‰ª∂\"\"\"\n",
    "        \n",
    "        if not self.iteration_records:\n",
    "            print(\"No iteration records to save.\")\n",
    "            return\n",
    "        \n",
    "        # 1. ‰øùÂ≠òÊñáÊ°£Ë∑ØÂæÑÂàÜÈÖçËÆ∞ÂΩï (‰øùÊåÅ‰∏çÂèò)\n",
    "        all_doc_paths = []\n",
    "        for record in self.iteration_records:\n",
    "            all_doc_paths.extend(record['doc_path_assignments'])\n",
    "        \n",
    "        doc_path_df = pd.DataFrame(all_doc_paths)\n",
    "        doc_path_df.to_csv(f'{base_filename}_document_paths.csv', index=False, encoding='utf-8')\n",
    "        print(f\"‚úÖ doc-path allocation is saved : {base_filename}_document_paths.csv\")\n",
    "    \n",
    "        # 2. ‰øùÂ≠òË∑ØÂæÑÁªìÊûÑËÆ∞ÂΩï - Â§ÑÁêÜÂàóË°®Â≠óÊÆµ\n",
    "        # ÂÅáËÆæ path_record (Êù•Ëá™ record['path_structures']) Â∑≤ÁªèÂåÖÂê´‰∫Ü layer_X_node_id Âàó\n",
    "        all_path_structures = []\n",
    "        for record in self.iteration_records:\n",
    "            for path_record_item in record['path_structures']: # path_record_item ÊòØÂåÖÂê´ÂàÜÂ±Ç‰ø°ÊÅØÁöÑÂ≠óÂÖ∏\n",
    "                path_record_copy = path_record_item.copy()\n",
    "                all_path_structures.append(path_record_copy)\n",
    "        \n",
    "        path_structure_df = pd.DataFrame(all_path_structures)\n",
    "        path_structure_df.to_csv(f'{base_filename}_path_structures.csv', index=False, encoding='utf-8')\n",
    "        print(f\"‚úÖ structure of tree path is saved: {base_filename}_path_structures.csv\")\n",
    "        \n",
    "        # 5. ‰øùÂ≠òËø≠‰ª£ÊëòË¶Å (‰øùÊåÅ‰∏çÂèò)\n",
    "        iteration_summaries = [record['iteration_summary'] for record in self.iteration_records]\n",
    "        summary_df = pd.DataFrame(iteration_summaries)\n",
    "        summary_df.to_csv(f'{base_filename}_iteration_summaries.csv', index=False, encoding='utf-8')\n",
    "        print(f\"‚úÖ info recoreded in iteration is saved: {base_filename}_iteration_summaries.csv\")\n",
    "        \n",
    "        # 6. È¢ùÂ§ñ‰øùÂ≠ò‰∏Ä‰∏™ËØ¶ÁªÜÁöÑË∑ØÂæÑ-ÊñáÊ°£Êò†Â∞ÑË°® - ‰øÆÊîπÊ≠§Â§Ñ‰ª•ÈÄÇÂ∫îÂàÜÂ±ÇÁªìÊûÑ   \n",
    "        path_document_mapping = []\n",
    "        for record in self.iteration_records:\n",
    "            for path_record_item in record['path_structures']: # path_record_item ÊòØÂåÖÂê´ÂàÜÂ±Ç‰ø°ÊÅØÁöÑÂ≠óÂÖ∏\n",
    "                iteration_num = path_record_item['iteration']\n",
    "                leaf_id = path_record_item['leaf_node_id']\n",
    "\n",
    "                for doc_id_in_path in path_record_item['documents_in_path']:\n",
    "                    mapping_entry = {\n",
    "                        'iteration': iteration_num,\n",
    "                        'leaf_node_id': leaf_id,\n",
    "                        'document_id': doc_id_in_path,\n",
    "                    }\n",
    "                    # Ê∑ªÂä†ÂàÜÂ±ÇËäÇÁÇπIDÔºåÁõ¥Âà∞Ê®°ÂûãÂÆö‰πâÁöÑÊ∑±Â∫¶ self.depth\n",
    "                    # Á°Æ‰øù self.depth Âú® __init__ ‰∏≠Ë¢´ËÆæÁΩÆ\n",
    "                    if hasattr(self, 'depth'):\n",
    "                        for i in range(self.depth):\n",
    "                            layer_key = f'layer_{i}_node_id'\n",
    "                            mapping_entry[layer_key] = path_record_item.get(layer_key) # ‰ªé path_record_item Ëé∑Âèñ\n",
    "                    else:\n",
    "                        # Â¶ÇÊûú self.depth ‰∏çÂèØÁî®ÔºåÂèØ‰ª•Â∞ùËØï‰ªé path_record_item ÁöÑÈîÆÊé®Êñ≠Ôºå‰ΩÜËøô‰∏çÂ§™ÁêÜÊÉ≥\n",
    "                        # ÊàñËÄÖÂè™ËÆ∞ÂΩïÂÆûÈôÖÂ≠òÂú®ÁöÑÂ±ÇÁ∫ß\n",
    "                        pass # Ê†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂ§ÑÁêÜ self.depth ‰∏çÂèØÁî®ÁöÑÊÉÖÂÜµ\n",
    "                    \n",
    "                    path_document_mapping.append(mapping_entry)\n",
    "        \n",
    "        path_doc_mapping_df = pd.DataFrame(path_document_mapping)\n",
    "        path_doc_mapping_df.to_csv(f'{base_filename}_path_document_mapping.csv', index=False, encoding='utf-8')\n",
    "        print(f\"‚úÖ doc-path mapping is saved: {base_filename}_path_document_mapping.csv\")\n",
    "        \n",
    "        filtered_records_for_detail = self.iteration_records[-last_n_iterations:]\n",
    "        \n",
    "        # 3. ‰øùÂ≠òËØçËØ≠ÂàÜÈÖçËÆ∞ÂΩï (‰øùÊåÅ‰∏çÂèò)\n",
    "        all_word_allocations = []\n",
    "        for record in filtered_records_for_detail:\n",
    "            all_word_allocations.extend(record['word_allocations'])\n",
    "        \n",
    "        word_allocation_df = pd.DataFrame(all_word_allocations)\n",
    "        word_allocation_df.to_csv(f'{base_filename}_word_allocations.csv', index=False, encoding='utf-8')\n",
    "        print(f\"‚úÖ doc's words allocation to node is saved: {base_filename}_word_allocations.csv\")\n",
    "        \n",
    "        # 4. ‰øùÂ≠òËäÇÁÇπËØçÂàÜÂ∏ÉËÆ∞ÂΩï (‰øùÊåÅ‰∏çÂèò)\n",
    "        all_node_words = []\n",
    "        for record in filtered_records_for_detail:\n",
    "            all_node_words.extend(record['node_word_distributions'])\n",
    "        \n",
    "        node_word_df = pd.DataFrame(all_node_words)\n",
    "        node_word_df.to_csv(f'{base_filename}_node_word_distributions.csv', index=False, encoding='utf-8')\n",
    "        print(f\"‚úÖ node word distribution is saved: {base_filename}_node_word_distributions.csv\")\n",
    "        \n",
    "        return {\n",
    "            'doc_path_df': doc_path_df,\n",
    "            'path_structure_df': path_structure_df,\n",
    "            'word_allocation_df': word_allocation_df,\n",
    "            'node_word_df': node_word_df,\n",
    "            'summary_df': summary_df,\n",
    "            'path_doc_mapping_df': path_doc_mapping_df\n",
    "        }\n",
    "\n",
    "    def get_iteration_summary(self):\n",
    "        \"\"\"Ëé∑ÂèñËø≠‰ª£ËøáÁ®ãÊëòË¶Å\"\"\"\n",
    "        if not self.iteration_records:\n",
    "            return None\n",
    "        \n",
    "        summaries = [record['iteration_summary'] for record in self.iteration_records]\n",
    "        return pd.DataFrame(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf397ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "def save_gibbs_checkpoint(filename, recorder, root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation, iteration):\n",
    "    \"\"\"‰øùÂ≠òGibbsÈááÊ†∑ÁöÑÊñ≠ÁÇπÔºàÊâÄÊúâÂÖ≥ÈîÆÂèòÈáèÔºâ\"\"\"\n",
    "    checkpoint = {\n",
    "        'recorder': recorder,\n",
    "        'root_node': root_node,\n",
    "        'path_list': path_list,\n",
    "        'doc_path': doc_path,\n",
    "        'doc_word_allocation': doc_word_allocation,\n",
    "        'doc_node_allocation': doc_node_allocation,\n",
    "        'iteration': iteration\n",
    "    }\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(checkpoint, f)\n",
    "    print(f\"‚úÖ checkpoint is saved to: {filename}ÔºàËø≠‰ª£ {iteration}Ôºâ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daaf8f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gibbs_checkpoint(filename):\n",
    "    \"\"\"‰ªéÊñ≠ÁÇπÊñá‰ª∂ÊÅ¢Â§çGibbsÈááÊ†∑ÁöÑÊâÄÊúâÂÖ≥ÈîÆÂèòÈáè\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        checkpoint = pickle.load(f)\n",
    "    print(f\"‚úÖ from checkpoint file {filename} recover to iteration {checkpoint['iteration']}\"),\n",
    "    return (checkpoint['recorder'], checkpoint['root_node'], checkpoint['path_list'],\n",
    "            checkpoint['doc_path'], checkpoint['doc_word_allocation'],\n",
    "            checkpoint['doc_node_allocation'], checkpoint['iteration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "076ea874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually, this function is designed to be used after all iterations are done, to get stable documents across the last `window` iterations\n",
    "# but we also can use it in a sliding window manner during iterations, if we input the windowed portion of doc_path_lst\n",
    "def get_stable_docs_sliding_window(doc_path_lst_portion, back_window=5): # Ëø≠‰ª£ÂÆåÊàêÂêéËÆ°ÁÆó\n",
    "    \"\"\"\n",
    "    ËøîÂõûÊØè‰∏ÄËΩÆÔºà‰ªéwindowËΩÆÂºÄÂßãÔºâËøûÁª≠windowËΩÆË∑ØÂæÑÈÉΩÊ≤°ÂèòÁöÑÊñáÊ°£IDÈõÜÂêà\n",
    "    ËøîÂõûÊ†ºÂºèÔºö{Ëø≠‰ª£Âè∑: set(Á®≥ÂÆöÊñáÊ°£ID)}\n",
    "    \"\"\"\n",
    "    iter_portion = sorted(doc_path_lst_portion.keys())\n",
    "\n",
    "    doc_ids = sorted(doc_path_lst_portion[iter_portion[-1]].keys())\n",
    "    doc_idx_map = {doc_id: idx for idx, doc_id in enumerate(doc_ids)} # Ëøô‰∏™ÊòØÊãÖÂøÉdoc_ids‰∏çËøûÁª≠\n",
    "    n_docs = len(doc_ids)\n",
    "    n_iters = len(iter_portion) # 15\n",
    "    # ÊûÑÈÄ†ÂÆåÊï¥Ë∑ØÂæÑÁü©Èòµ shape=(n_iters, n_docs)\n",
    "    path_matrix = np.full((n_iters, n_docs), -1, dtype=np.int32)\n",
    "    for i, iter_num in enumerate(iter_portion):\n",
    "        for doc_id, leaf_id in doc_path_lst_portion[iter_num].items():\n",
    "            path_matrix[i, doc_idx_map[doc_id]] = leaf_id\n",
    "    # ÊªëÂä®Á™óÂè£ÁªüËÆ°\n",
    "    stable_dict = {}\n",
    "    # sliding window from 4 and review back from [0,5] to find stable docs\n",
    "    for i in range(back_window-1, n_iters): # range from 4, and review back, is okay\n",
    "        window_matrix = path_matrix[i-back_window+1:i+1, :]  # [0,5] rows # shape=(window, n_docs)\n",
    "        first_row = window_matrix[0]\n",
    "        stable_mask = np.all(window_matrix == first_row, axis=0) & (first_row != -1)\n",
    "        stable_doc_ids = {doc_ids[j] for j in np.where(stable_mask)[0]}\n",
    "        stable_dict[iter_portion[i]] = stable_doc_ids # ‰ªéÁ¨¨Âõõ‰∏™ÂºÄÂßãÁÆóÁ®≥ÂÆöÊñáÊ°£\n",
    "    return stable_dict\n",
    "## now, it can be used for check_inner_convergence\n",
    "\n",
    "def get_jaccard_list_from_stable_dict(stable_dict, doc_node_allocation_lst_portion, back_window=5, depth=3):\n",
    "    \"\"\"\n",
    "    ËøîÂõûÊØè‰∏ÄËΩÆ‰∏éÂâç‰∏ÄËΩÆÁ®≥ÂÆöÊñáÊ°£ÈõÜÂêàÁöÑJaccardÁõ∏‰ººÂ∫¶ÂàóË°®\n",
    "    Âπ∂ÁªüËÆ°Á™óÂè£ÂÜÖÊØèÂ±ÇÊØèËØçÊúÄÂ∞èËØçÈ¢ëÂÄºÂá∫Áé∞Ê¨°Êï∞ÊâÄÂç†ÊØî‰æãÔºàÊúÄÂ∞èÂÄºÁ®≥ÂÆöÊÄßÔºâ\n",
    "    \"\"\"\n",
    "    iters = sorted(stable_dict.keys()) # ‰ªé4ÂºÄÂßã\n",
    "    jaccard_list = []\n",
    "    stable_word_ratio_list = []\n",
    "    for i in range(1, len(iters)):\n",
    "        set1 = stable_dict[iters[i-1]]\n",
    "        set2 = stable_dict[iters[i]]\n",
    "        intersection = len(set1 & set2)\n",
    "        union = len(set1 | set2)\n",
    "        jaccard_index = intersection / union if union != 0 else 0\n",
    "        jaccard_list.append(jaccard_index)\n",
    "\n",
    "    for i in range(len(iters)):\n",
    "        if doc_node_allocation_lst_portion is not None and iters[0] >= back_window-1: # Á°Æ‰øùÊúâË∂≥Â§üÁöÑËø≠‰ª£Ê¨°Êï∞\n",
    "            stable_docs = stable_dict[iters[i]]\n",
    "            min_freq_sum = 0\n",
    "            total_freq_sum = 0\n",
    "            for doc_id in stable_docs:\n",
    "                window_allocs = [doc_node_allocation_lst_portion[j].get(doc_id, {}) for j in range(iters[i]-back_window+1, iters[i]+1)] # Á™óÂè£ÂÜÖÁöÑÂàÜÈÖç\n",
    "                # print(f\"window_allocs: {window_allocs}\")\n",
    "                for layer in range(depth): \n",
    "                    # ÂêàÂπ∂Á™óÂè£ÂÜÖÊâÄÊúâËØç\n",
    "                    words = set()\n",
    "                    for alloc in window_allocs:\n",
    "                        words.update(alloc.get(layer, {}).keys())\n",
    "                    for word in words:\n",
    "                        freq_list = [alloc.get(layer, {}).get(word, 0) for alloc in window_allocs]\n",
    "                        min_freq = min(freq_list)\n",
    "                        min_freq_sum += min_freq\n",
    "                        total_freq_sum += sum(freq_list)\n",
    "            ratio = min_freq_sum / total_freq_sum if total_freq_sum > 0 else 0\n",
    "            stable_word_ratio_list.append(ratio)\n",
    "            # print(f\"iter={iters[i]}, min_freq_sum={min_freq_sum}, total_freq_sum={total_freq_sum}, ratio={ratio}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è unable to calculate word ratio stability, missing doc_node_allocation_lst or not enough iterations\")\n",
    "            exit()\n",
    "    return jaccard_list, stable_word_ratio_list\n",
    "\n",
    "def calculate_gelman_rubin(chain_values, use_differences=False):\n",
    "    \"\"\"\n",
    "    ËÆ°ÁÆó Gelman-Rubin ÁªüËÆ°Èáè (R-hat)\n",
    "    \n",
    "    Args:\n",
    "        chain_values: Â≠óÂÖ∏ÔºåÈîÆ‰∏∫ÈìæIDÔºåÂÄº‰∏∫ËØ•ÈìæÁöÑjaccard list\n",
    "        warmup: È¢ÑÁÉ≠ÊúüÊØî‰æãÔºå‰∏¢ÂºÉÊØèÊù°ÈìæÂâç warmup% ÁöÑÊ†∑Êú¨\n",
    "        \n",
    "    Returns:\n",
    "        float: Gelman-Rubin ÁªüËÆ°Èáè\n",
    "    \"\"\"\n",
    "     # Â§ÑÁêÜÂéüÂßãÊï∞ÊçÆ\n",
    "    if use_differences:\n",
    "        # ËÆ°ÁÆóÂ∑ÆÂÄºÔºàÂΩìÂâçËΩÆÊ¨°ÂáèÂéªÂâç‰∏ÄËΩÆÊ¨°Ôºâ\n",
    "        diff_chains = {}\n",
    "        for chain_id, values in chain_values.items():\n",
    "            diff_values = [abs(values[i] - values[i-1]) for i in range(1, len(values))]\n",
    "            diff_chains[chain_id] = diff_values\n",
    "        chain_values = diff_chains\n",
    "\n",
    "    chains = [chain for chain in chain_values.values()]\n",
    "    \n",
    "    n_chains = len(chains)\n",
    "    n_samples = len(chains[0])\n",
    "    \n",
    "    # ËÆ°ÁÆóÈìæÂÜÖÂùáÂÄº\n",
    "    chain_means = [np.mean(chain) for chain in chains]\n",
    "    \n",
    "    # ËÆ°ÁÆóÂÖ®Â±ÄÂùáÂÄº\n",
    "    global_mean = np.mean(chain_means)\n",
    "    \n",
    "    # ËÆ°ÁÆóÈìæÈó¥ÊñπÂ∑Æ B\n",
    "    between_chain_var = n_samples * np.sum([(mean - global_mean)**2 for mean in chain_means]) / (n_chains - 1)\n",
    "    \n",
    "    # ËÆ°ÁÆóÈìæÂÜÖÊñπÂ∑Æ W\n",
    "    within_chain_var = np.mean([np.var(chain, ddof=1) for chain in chains])\n",
    "    \n",
    "    # ËÆ°ÁÆóÊñπÂ∑Æ‰º∞ËÆ°\n",
    "    var_estimate = ((n_samples - 1) / n_samples) * within_chain_var + (1 / n_samples) * between_chain_var\n",
    "    \n",
    "    # ËÆ°ÁÆó R-hat\n",
    "    r_hat = np.sqrt(var_estimate / within_chain_var)\n",
    "    \n",
    "    return r_hat\n",
    "\n",
    "def check_convergence_with_gelman_rubin(chain_len_docs, mean_jaccard, mean_ratio, iteration, r_hat_threshold=1.1):\n",
    "    \"\"\"\n",
    "    Ë∑ØÂæÑÈÄâÊã©ÂíåÁ®≥ÂÆöËØçÂàÜÈÖçratioÈÉΩË¶ÅR-hatÊî∂ÊïõÊâçËÆ§‰∏∫Êï¥‰ΩìÊî∂Êïõ\n",
    "    \"\"\"\n",
    "    print(f\"check_dual_convergence_with_gelman_rubin: chain_len_docs:{chain_len_docs}, mean_jaccard:{mean_jaccard}, mean_ratio:{mean_ratio}\")\n",
    "    r_hat_len_docs = calculate_gelman_rubin(chain_len_docs, use_differences=False)\n",
    "    is_converged = (r_hat_len_docs < r_hat_threshold)\n",
    "    \n",
    "    convergence_info = {\n",
    "        \"iteration\": iteration,\n",
    "        \"r_hat_len_docs\": r_hat_len_docs,\n",
    "        \"mean_jaccard\": mean_jaccard,\n",
    "        \"mean_ratio\": mean_ratio,\n",
    "        \"threshold\": r_hat_threshold,\n",
    "        \"converged\": is_converged,\n",
    "    }\n",
    "    print(f\"ü•∞ Convergence status: {'‚úÖ Converged!' if is_converged else '‚ùå Not converged...'}\")\n",
    "    print(f\"ü•∞  R-hat info: {r_hat_len_docs:.4f}Ôºåmean_jaccard:{mean_jaccard}, mean_ratio:{mean_ratio}\")\n",
    "    return is_converged, convergence_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0692653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedState:\n",
    "    \"\"\"Â§öËøõÁ®ãÈó¥ÂÖ±‰∫´Áä∂ÊÄÅÁÆ°ÁêÜÁ±ªÔºåÊîØÊåÅJaccardÂíåratioÂéÜÂè≤ÁöÑÂ≠òÂÇ®‰∏éËØªÂèñ\"\"\"\n",
    "    def __init__(self, base_dir):\n",
    "        self.base_dir = base_dir\n",
    "        self.state_file = os.path.join(base_dir, \"shared_state.json\")\n",
    "        self.ensure_dir()\n",
    "        self.init_state()\n",
    "\n",
    "    def ensure_dir(self):\n",
    "        os.makedirs(self.base_dir, exist_ok=True)\n",
    "\n",
    "    def init_state(self):\n",
    "        state = {\n",
    "            \"chains\": {},\n",
    "            \"last_update\": time.time(),\n",
    "            \"rhat_history\": [],\n",
    "            \"convergence\": False\n",
    "        }\n",
    "        self.save_state(state)\n",
    "\n",
    "    def save_state(self, state):\n",
    "        import json\n",
    "        def convert_numpy_types(obj):\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            elif isinstance(obj, np.bool_):\n",
    "                return bool(obj)\n",
    "            elif isinstance(obj, dict):\n",
    "                return {k: convert_numpy_types(v) for k, v in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [convert_numpy_types(i) for i in obj]\n",
    "            return obj\n",
    "        converted_state = convert_numpy_types(state)\n",
    "        with open(self.state_file, \"w\") as f:\n",
    "            json.dump(converted_state, f)\n",
    "\n",
    "    def load_state(self):\n",
    "        import json\n",
    "        try:\n",
    "            with open(self.state_file, \"r\") as f:\n",
    "                return json.load(f)\n",
    "        except (FileNotFoundError, json.JSONDecodeError):\n",
    "            self.init_state()\n",
    "            return self.load_state()\n",
    "\n",
    "    def update_chain_data(self, chain_id, len_stable_docs, jaccard_list, ratio_list, iteration, record_time): # update a single chain's data\n",
    "        \"\"\"\n",
    "        Êõ¥Êñ∞ÊüêÊù°ÈìæÁöÑÊï∞ÊçÆÔºàJaccardÂíåratioÂéÜÂè≤Ôºâ\n",
    "        \"\"\"\n",
    "        state = self.load_state()\n",
    "        if str(chain_id) not in state[\"chains\"]:\n",
    "            state[\"chains\"][str(chain_id)] = {\n",
    "                \"len_stable_docs\":{},\n",
    "                \"jaccard\": {},\n",
    "                \"ratio\": {},\n",
    "                \"record_time\": -1,\n",
    "                \"converged\": False\n",
    "            }\n",
    "        chain_data = state[\"chains\"][str(chain_id)]\n",
    "        \n",
    "        # ÊÑüËßâÂ¶ÇÊûúÊòØÁ®≥ÂÆöË∑ØÂæÑÊï∞ÁöÑËØùÔºåÂÄí‰πü‰∏çÁî®Áî®till nowÁöÑmeanÂíåstdÔºåÂõ†‰∏∫Êï∞ÈáèÊØîËæÉÂ•ΩË°°ÈáèÊòØ‰∏çÊòØÔºü\n",
    "        chain_data[\"len_stable_docs\"][str(record_time)] = len_stable_docs\n",
    "        chain_data[\"jaccard\"][str(record_time)] = jaccard_list\n",
    "        chain_data[\"ratio\"][str(record_time)] = ratio_list\n",
    "        chain_data[\"record_time\"] = record_time\n",
    "        state[\"last_update\"] = time.time()\n",
    "        self.save_state(state)\n",
    "\n",
    "    def update_rhat(self, rhat_info, iteration, converged=False):\n",
    "        state = self.load_state()\n",
    "        state[\"rhat_history\"].append({\n",
    "            \"rhat_len_docs\": rhat_info.get(\"r_hat_len_docs\"),\n",
    "            \"mean_jaccard\": rhat_info.get(\"mean_jaccard\"),\n",
    "            \"mean_ratio\": rhat_info.get(\"mean_ratio\"),\n",
    "            \"rhat_threshold\": rhat_info.get(\"r_hat_threshold\"),\n",
    "            \"converged\": converged,\n",
    "            \"iteration\": iteration,\n",
    "            \"timestamp\": time.time()\n",
    "        })\n",
    "        state[\"convergence\"] = converged\n",
    "        self.save_state(state)\n",
    "\n",
    "    def update_convergence_status(self, converged):\n",
    "        state = self.load_state()\n",
    "        state[\"convergence\"] = converged\n",
    "        self.save_state(state)\n",
    "\n",
    "    def get_chain_data_histories(self, record_time, max_retry=5, wait_sec=2):\n",
    "        for _ in range(max_retry):\n",
    "            state = self.load_state()\n",
    "            len_stable_docs = {}\n",
    "            jaccard_lists = {}\n",
    "            ratio_lists = {}\n",
    "\n",
    "            for chain_id, data in state[\"chains\"].items():\n",
    "                # Âè™ÂèñÂΩìÂâçrecord_timeÂØπÂ∫îÁöÑÊï∞ÊçÆ\n",
    "                len_stable_docs[str(chain_id)] = data[\"len_stable_docs\"][str(record_time)]\n",
    "                jaccard_lists[str(chain_id)] = data[\"jaccard\"][str(record_time)]\n",
    "                ratio_lists[str(chain_id)] = data[\"ratio\"][str(record_time)]\n",
    "\n",
    "        return len_stable_docs, jaccard_lists, ratio_lists\n",
    "    \n",
    "    def get_latest_completed_record(self, n_chains): # find the min last_iteration across all chains\n",
    "        state = self.load_state() # state is a dict from JSON file\n",
    "        \"\"\"\n",
    "        state = {\n",
    "        \"chains\": {},\n",
    "        \"last_update\": time.time(),\n",
    "        \"rhat_history\": [],\n",
    "        \"convergence\": False\n",
    "        }\n",
    "        \"\"\"\n",
    "        if len(state[\"chains\"]) < n_chains: # must have all chains\n",
    "            return -1\n",
    "        chain_record = []\n",
    "        for data in state[\"chains\"].values():\n",
    "            chain_record.append(data.get(\"record_time\", -1))\n",
    "        return min(chain_record)\n",
    "\n",
    "    def get_convergence_status(self):\n",
    "        state = self.load_state()\n",
    "        return state[\"convergence\"]\n",
    "\n",
    "    def all_chains_finished(self, n_chains):\n",
    "        state = self.load_state()\n",
    "        if len(state[\"chains\"]) < n_chains:\n",
    "            return False\n",
    "        return all(data.get(\"converged\", False) for data in state[\"chains\"].values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64aee7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rhat_monitor_process(shared_state, n_chains, max_iterations, burn_in, check_interval, back_window, r_hat_threshold=1.1):\n",
    "    \"\"\"\n",
    "    ÁõëÊéßJaccardÂíåratioÁöÑR-hatÔºåÂèäÊó∂Âà§Êñ≠Â§öÈìæÊî∂Êïõ\n",
    "    shared_state: SharedStateÂÆû‰æã\n",
    "    n_chains: ÈìæÊï∞Èáè\n",
    "    max_iterations: ÊúÄÂ§ßËø≠‰ª£Ê¨°Êï∞\n",
    "    r_hat_threshold: R-hatÊî∂ÊïõÈòàÂÄº\n",
    "    ËØ•ËøõÁ®ãÊåÅÁª≠ËøêË°åÔºåÁõ¥Âà∞Ê£ÄÊµãÂà∞Êî∂ÊïõÊàñÊâÄÊúâÈìæÂÆåÊàê\n",
    "    \"\"\"\n",
    "    print(\"üîç Initialize R-hat independent monitor thread (stable docs & Jaccard & Ratio)\")\n",
    "    convergence_detected = False\n",
    "    record_time = 0\n",
    "\n",
    "    while True:\n",
    "        # ÂÖ∂ÂÆûËøôÈáåÂ∞±‰∏çÈúÄË¶Å‰∫ÜÔºåÂõ†‰∏∫_checkpointÂ∑≤ÁªèÂÅöÂá∫Âà§Êñ≠‰∫ÜÔºåËÄå‰∏îÂæàÂç±Èô©ÁöÑÊòØÔºåÂ¶ÇÊûúmonitorÊôöÁÇπÊ£ÄÊü•Â∞±‰ºöË¢´Êõø‰ª£ÔºåÊâÄ‰ª•Ë¶ÅÊääupdateÂáΩÊï∞Êîπ‰∏∫append\n",
    "        current_record_time = shared_state.get_latest_completed_record(n_chains=n_chains)\n",
    "        # last replaced lsts, iterationÊòØÂêëÂâçÂõûÊ∫ØÁöÑ\n",
    "        \n",
    "        if not current_record_time >= record_time:\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "        \n",
    "        chain_len_docs, chain_jaccard_lists, chain_ratio_lists = shared_state.get_chain_data_histories(record_time = record_time)\n",
    "        # print(f\"chain_len_docs:{chain_len_docs}, chain_ratio_lists: {chain_ratio_lists}, chain_jaccard_lists: {chain_jaccard_lists}\")\n",
    "        \n",
    "        mean_jaccard = [np.mean(jaccard_lst) for jaccard_lst in chain_jaccard_lists.values()]\n",
    "        mean_ratio = [np.mean(ratio_lst) for ratio_lst in chain_ratio_lists.values()]\n",
    "        # print(mean_jaccard, mean_ratio)                   \n",
    "        \n",
    "        current_last_iteration = min(burn_in+back_window+current_record_time*check_interval, max_iterations)\n",
    "        is_converged, convergence_info = check_convergence_with_gelman_rubin(chain_len_docs=chain_len_docs, mean_jaccard=mean_jaccard, mean_ratio=mean_ratio, iteration=current_last_iteration, r_hat_threshold=r_hat_threshold)\n",
    "\n",
    "        \n",
    "        # print(f\"chain_len_docs:{chain_len_docs}, chain_ratio_lists: {chain_ratio_lists}, chain_jaccard_lists: {chain_jaccard_lists}\")\n",
    " \n",
    "        \"\"\"\n",
    "        convergence_info = {\n",
    "        \"iteration\": iteration,\n",
    "        \"r_hat_len_docs\": r_hat_len_docs,\n",
    "        \"mean_jaccard\": mean_jaccard,\n",
    "        \"mean_ratio\": mean_ratio,\n",
    "        \"threshold\": r_hat_threshold,\n",
    "        \"converged\": is_converged,\n",
    "    }\n",
    "        \"\"\"\n",
    "\n",
    "        # ËÆ∞ÂΩïR-hatÂÄº\n",
    "        shared_state.update_rhat(convergence_info, current_last_iteration, converged=is_converged)\n",
    "\n",
    "        if is_converged:\n",
    "            print(f\"‚úÖ r-hat of multi-chains is detected to convergence {burn_in+back_window+record_time*check_interval} in iteration:{current_last_iteration}, R-hat={convergence_info['r_hat_len_docs']:.4f}, JaccardÂπ≥ÂùáÂÄº={convergence_info['mean_jaccard']}, RatioÂπ≥ÂùáÂÄº={convergence_info['mean_ratio']}\")\n",
    "            convergence_detected = True\n",
    "            shared_state.update_convergence_status(True)\n",
    "\n",
    "        record_time += 1\n",
    "\n",
    "        # Ê£ÄÊü•ÊòØÂê¶ÊâÄÊúâÈìæÈÉΩÂ∑≤ÂÆåÊàê\n",
    "        if shared_state.all_chains_finished(n_chains) or current_last_iteration >= max_iterations:\n",
    "            print(\"‚úä All chains finished or max iterations reached, exiting r-hat monitor process.\")\n",
    "            break\n",
    "\n",
    "        if convergence_detected:\n",
    "            print(\"‚úÖ R-hat convergence is detected, waiting for all chains to finish.\")\n",
    "\n",
    "        time.sleep(2)\n",
    "    print(\"‚úÖ R-hat monitor process finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "726e2a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gibbs_with_checkpoint(\n",
    "    corpus, depth=3, gamma=None, eta=None, alpha=None,\n",
    "    max_iterations=200, save_every_n_iter=10, checkpoint_dir='gibbs_checkpoints',\n",
    "    check_interval=10, resume_from_checkpoint=None, base_dir=None,\n",
    "    shared_state=None, chain_id=None, back_window=5, burn_in = 50):\n",
    "    \"\"\"\n",
    "    ÊîØÊåÅÊñ≠ÁÇπ‰øùÂ≠ò‰∏éÊÅ¢Â§çÁöÑGibbsÈááÊ†∑‰∏ªÂæ™ÁéØ\n",
    "    - resume_from_checkpoint: Êñ≠ÁÇπÊñá‰ª∂Ë∑ØÂæÑÔºàÂ¶ÇÈúÄÊÅ¢Â§çÂàôÂ°´ÂÜôÔºåÂê¶Âàô‰ªéÂ§¥ÂºÄÂßã)\n",
    "    \n",
    "     result = run_gibbs_with_checkpoint(\n",
    "        corpus=corpus,\n",
    "        depth=depth,\n",
    "        gamma=gamma,\n",
    "        eta=eta,\n",
    "        alpha=alpha,\n",
    "        max_iterations=max_iterations,\n",
    "        checkpoint_dir=os.path.join(chain_dir, \"checkpoints\"),\n",
    "        base_dir=chain_dir,\n",
    "        chain_id=chain_id,\n",
    "        shared_state=shared_state,\n",
    "        back_window=back_window,\n",
    "        burn_in=burn_in,  # ËÆæÁΩÆburn-inÊúü‰∏∫50ËΩÆ\n",
    "        check_interval=check_interval  # ËÆæÁΩÆÊªëÂä®Á™óÂè£Â§ßÂ∞è\n",
    "    \"\"\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    if resume_from_checkpoint is not None and os.path.exists(resume_from_checkpoint):\n",
    "        # ‰ªéÊñ≠ÁÇπÊÅ¢Â§ç\n",
    "        (recorder, root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation, start_iter) = load_gibbs_checkpoint(resume_from_checkpoint)\n",
    "        print(f\"Resume from checkpoint and continue sampling (from iteration {start_iter+1})\")\n",
    "    else:\n",
    "        # ÂÖ®Êñ∞ÂºÄÂßã,\n",
    "        print(\"üìä Initialize nCRP Process...\")\n",
    "        [root_node, path_list, doc_path, doc_word_allocation] = nCRP(\n",
    "            corpus=corpus, depth=depth, gamma=gamma,\n",
    "        )\n",
    "        # path_list: {leaf_id: [Node1, Node2, ...]}\n",
    "        # doc_path: {doc_id: leaf_id}\n",
    "        # doc_word_allocation: {doc_id: [layer_for_word1, layer_for_word2,...]}\n",
    "        \n",
    "        recorder = Recorder(corpus, depth, eta, alpha) # for Recorder self._init_ setting\n",
    "        doc_node_allocation = aggregate_words(corpus, doc_word_allocation)\n",
    "        # doc_node_allocation: {doc_id: {layer: {word: count}}}\n",
    "        \n",
    "        global_node_word_dist = node_word_distribution(doc_node_allocation, doc_path, path_list, exclude_docs=None)\n",
    "        # global_node_word_dist[node_id]: {layer: {word: count}}\n",
    "        \n",
    "        # ÂÆö‰πâËØ¶ÁªÜËÆ∞ÂΩïÂºÄÂßãÁöÑËø≠‰ª£Ôºà‰∏éGibbsÈááÊ†∑‰∏≠ÁöÑiter_start‰∏ÄËá¥Ôºâ\n",
    "        iter_start_for_log = burn_in + back_window\n",
    "\n",
    "        initial_summary = recorder.record_iteration(\n",
    "            0, root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation,global_node_word_dist,\n",
    "            iter_start_for_detailed_log=iter_start_for_log # ‰º†ÈÄíÁªôÂàùÂßãËÆ∞ÂΩï\n",
    "        ) # record the initial state\n",
    "        \"\"\"\n",
    "        recorder_iteration return: some basic infomation\n",
    "        iteration_summary = {\n",
    "                    'iteration': iteration_num,\n",
    "                    'total_paths': len(path_list),\n",
    "                    'total_documents': len(doc_path),\n",
    "                    'log_likelihood': log_likelihood,\n",
    "                    'changed_docs_count': changed_docs_count, \n",
    "                    'newly_created_paths': len(newly_created_paths or []),\n",
    "                    'avg_path_size',\n",
    "                    'max_path_size',\n",
    "                    'min_path_size'}\n",
    "        \"\"\"\n",
    "        print(f\"üìù Chain {chain_id} initial state after nCRP is recorded: {initial_summary['total_paths']} path, \", \n",
    "                f\"Log-likelihood: {initial_summary['log_likelihood']:.2f}\")\n",
    "        start_iter = 1\n",
    "\n",
    "    previous_log_likelihood = recorder.iteration_records[-1]['iteration_summary']['log_likelihood']\n",
    "    loglikelihood_list = [previous_log_likelihood]\n",
    "    change_docs_list = []\n",
    "\n",
    "    doc_path_lst = {0:doc_path.copy()}# Áî®‰∫éÂ≠òÂÇ®ÊØèÊù°ÈìæÁöÑjaccardÂéÜÂè≤ËÆ∞ÂΩï\n",
    "    doc_node_allocation_lst = {0:doc_node_allocation.copy()} # Áî®‰∫éÂ≠òÂÇ®ÊØèÊù°ÈìæÁöÑdoc_node_allocationÂéÜÂè≤ËÆ∞ÂΩï\n",
    "\n",
    "    record_time = 0\n",
    "    start_window = burn_in + back_window \n",
    "    \n",
    "    for iteration in range(start_iter, max_iterations + 1): # start_iter=1\n",
    "        old_paths = set(path_list.keys())\n",
    "        doc_path_lst[iteration] = doc_path.copy()\n",
    "        \n",
    "        # ÂÆö‰πâËØ¶ÁªÜËÆ∞ÂΩïÂºÄÂßãÁöÑËø≠‰ª£Ôºà‰∏éGibbsÈááÊ†∑‰∏≠ÁöÑiter_start‰∏ÄËá¥Ôºâ\n",
    "        # ‰πüÁî®‰∫éÊéßÂà∂RecorderÁöÑËØ¶ÁªÜÊó•ÂøóËÆ∞ÂΩï           \n",
    "        current_iter_start_for_detailed_log = burn_in + back_window\n",
    "\n",
    "        Gibbs_sampling(\n",
    "            corpus, root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation, global_node_word_dist,\n",
    "            gamma, eta, alpha, depth, iteration, \n",
    "            iter_start=current_iter_start_for_detailed_log, # GibbsÈááÊ†∑ÂÜÖÈÉ®‰ΩøÁî®\n",
    "            iter_end=max_iterations\n",
    "        )\n",
    "        \"\"\"\n",
    "        detail_record.items() = {\n",
    "            'iteration': iteration,\n",
    "            'doc_id': doc_id,\n",
    "            'deleted_path_list': {leaf_id: [node.node_id for node in path_nodes] for leaf_id, path_nodes in path_list.items()}, # Ê≥®ÊÑèÊ∑±Êã∑Ë¥ùÁöÑÊÄßËÉΩÂΩ±Âìç\n",
    "            'doc_path': {k:v for k, v in doc_path.items()},\n",
    "            'doc_word_allocation': list(doc_word_allocation[doc_id]),\n",
    "            'doc_node_allocation': {layer: {word: count for word, count in word_counts.items()} \n",
    "                                    for layer, word_counts in doc_node_allocation[doc_id].items()}\n",
    "        }\n",
    "\n",
    "        jump_record.items() = {\n",
    "            'iteration': iteration,\n",
    "            'doc_id': doc_id,\n",
    "            'old_leaf_id': current_path[-1].node_id,\n",
    "            'old_path': [n.node_id for n in current_path],\n",
    "            'new_leaf_id': added_path[-1].node_id,\n",
    "            'new_path': [n.node_id for n in added_path],\n",
    "            'origal_probs':all_probs,\n",
    "            'normalized_probs': normalized_probs,\n",
    "            'chosen_path_prob': chosen_path_prob,\n",
    "            'rank': f'{rank} out of {len(values)}',\n",
    "            'create_path': True\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "        doc_path_lst[iteration] = doc_path.copy() # ËÆ∞ÂΩïÂΩìÂâçËø≠‰ª£ÁöÑdoc_pathÔºåÁî®‰∫éÂêéÁª≠ËÆ°ÁÆójaccard\n",
    "        doc_node_allocation_lst[iteration] = doc_node_allocation.copy() # ËÆ∞ÂΩïÂΩìÂâçËø≠‰ª£ÁöÑdoc_node_allocation\n",
    "\n",
    "        new_paths = set(path_list.keys()) - old_paths\n",
    "        iteration_summary = recorder.record_iteration(\n",
    "            iteration, root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation, global_node_word_dist,\n",
    "            new_paths, iter_start_for_detailed_log=current_iter_start_for_detailed_log # ‰º†ÈÄíÁªôRecorder\n",
    "        )\n",
    "        print(f\"üîÑ Chain {chain_id} in iteration {iteration}/{max_iterations},\\n\",\n",
    "                f\"üìä path: {iteration_summary['total_paths']}, \",\n",
    "                f\"new path: {iteration_summary['newly_created_paths']}, \",\n",
    "                f\"docs changed path: {iteration_summary['changed_docs_count']}, \",\n",
    "                f\"Log-likelihood: {iteration_summary['log_likelihood']:.2f}\")\n",
    "        \n",
    "        # Êñ≠ÁÇπ‰øùÂ≠ò\n",
    "        # if (iteration % save_every_n_iter == 0) or (iteration == max_iterations):\n",
    "        #     checkpoint_path = os.path.join(checkpoint_dir, f'gibbs_checkpoint_iter{iteration}.pkl')\n",
    "        #     save_gibbs_checkpoint(\n",
    "        #         checkpoint_path, recorder, root_node, path_list, doc_path,\n",
    "        #         doc_word_allocation, doc_node_allocation, iteration,\n",
    "        #     )\n",
    "\n",
    "        # Ê£ÄÊü•Êî∂Êïõ\n",
    "        change_docs_list.append(iteration_summary['changed_docs_count'])\n",
    "        loglikelihood_list.append(iteration_summary['log_likelihood'])\n",
    "        \n",
    "        # iteration starts from 1, burn_in + back_window + check_interval \n",
    "        # print((iteration - start_window) / check_interval)\n",
    "        if (iteration - start_window) / check_interval == 1 or  (iteration == max_iterations): # iteration starts from 1\n",
    "            # ÊØèÊ¨°Ëø≠‰ª£ÂºÄÂßãÊó∂Ê£ÄÊü•ÊòØÂê¶Â∑≤Êî∂ÊïõÔºàÂø´ÈÄüÊ£ÄÊü•ÔºåÊó†ÈúÄÈáçÊñ∞ËÆ°ÁÆóÔºâ\n",
    "            convergence_status = shared_state.get_convergence_status() if shared_state else False\n",
    "            if convergence_status:\n",
    "                final_iteration = iteration\n",
    "                print(f\"üéâ Chain {chain_id} is detected to convergence in iteration {iteration}, sampling ends prematurely.\")\n",
    "                break\n",
    "                \n",
    "            \"\"\"    \n",
    "            check_interval=5,\n",
    "            back_window=3,\n",
    "            burn_in=2,\n",
    "            \"\"\"\n",
    "            window_keys = list(range(iteration-check_interval-back_window+1, iteration+1))\n",
    "            # print(f\"window_keys:{window_keys}\")\n",
    "            \n",
    "            doc_path_window = {k: doc_path_lst[k] for k in window_keys} # 6-20\n",
    "            \n",
    "            stable_docs = get_stable_docs_sliding_window(doc_path_window, back_window=back_window) # interval+1ËΩÆÁöÑstable docs\n",
    "            chain_len_docs = [len(doc_list) for iterd, doc_list in stable_docs.items()]\n",
    "            \n",
    "            doc_node_allocation_window = {k: doc_node_allocation_lst[k] for k in window_keys}\n",
    "            \n",
    "            jaccard_lst, ratio_lst = get_jaccard_list_from_stable_dict(stable_docs, doc_node_allocation_window, back_window=back_window, depth=depth)\n",
    "            \n",
    "            shared_state.update_chain_data(chain_id, chain_len_docs, jaccard_lst, ratio_lst, iteration, record_time)\n",
    "            start_window = iteration\n",
    "            record_time += 1\n",
    "            \n",
    "            # print(f\"stable_docs:{stable_docs},jaccard_lst{jaccard_lst}, ratio_lst{ratio_lst}\")\n",
    "            \n",
    "            \"\"\" def update_chain_data(self, chain_id, len_stable_docs, jaccard_list, ratio_list, iteration, record_time): # update a single chain's data\"\"\"\n",
    "    \n",
    "    print(f\"üéØ Chain {chain_id} finish Gibbs Sampling.\")\n",
    "    print(\"üíæ Save iterations info...\")\n",
    "    saved_files = recorder.save_to_files(base_filename=os.path.join(base_dir, \"iteration\") if base_dir else \"iteration\")\n",
    "    return recorder, root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation, saved_files, change_docs_list, loglikelihood_list, doc_path_lst, doc_node_allocation_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c02a9399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_renyi_entropy(prob_dist, q=2.0):\n",
    "    total = sum(prob_dist.values())\n",
    "    if total <= 0:\n",
    "        return 0.0\n",
    "    moment_q = 0.0\n",
    "    for v in prob_dist.values():\n",
    "        p = v / total\n",
    "        if p > 0:\n",
    "            moment_q += p ** q\n",
    "    if moment_q <= 0:\n",
    "        return float('inf')\n",
    "    return (1.0 / (1.0 - q)) * math.log(moment_q)  # Ëá™ÁÑ∂ÂØπÊï∞\n",
    "\n",
    "def jensen_shannon_divergence(dist1, dist2):\n",
    "    # Áªü‰∏ÄËØçË°®Âπ∂ÂΩí‰∏ÄÂåñ\n",
    "    keys = set(dist1.keys()) | set(dist2.keys())\n",
    "    s1 = float(sum(dist1.get(k, 0.0) for k in keys))\n",
    "    s2 = float(sum(dist2.get(k, 0.0) for k in keys))\n",
    "    if s1 == 0 or s2 == 0:\n",
    "        return 1.0\n",
    "    p = {k: dist1.get(k, 0.0) / s1 for k in keys}\n",
    "    q = {k: dist2.get(k, 0.0) / s2 for k in keys}\n",
    "    m = {k: 0.5 * (p[k] + q[k]) for k in keys}\n",
    "\n",
    "    def _kl(a, b):\n",
    "        val = 0.0\n",
    "        for k in keys:\n",
    "            ak = a[k]\n",
    "            bk = b[k]\n",
    "            if ak > 0 and bk > 0:\n",
    "                val += ak * math.log(ak / bk)\n",
    "        return val\n",
    "\n",
    "    return 0.5 * _kl(p, m) + 0.5 * _kl(q, m)\n",
    "\n",
    "def evaluate_tree_structure_with_nodes(root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation):\n",
    "    \"\"\"\n",
    "    ËøîÂõûÔºö\n",
    "      - node_records: ÊØèËäÇÁÇπ‰∏ÄË°åÔºàlayer/node_id/parent_id/entropy/doc_count/pathÔºâ\n",
    "      - layer_entropy_wavg: {layer: ÊñáÊ°£Êï∞Âä†ÊùÉÁöÑR√©nyiÁÜµ}\n",
    "      - layer_distinctiveness_wavg: {layer: ÊñáÊ°£Êï∞Âä†ÊùÉJSD(‰∏ªÈ¢òÂºÇË¥®ÊÄß)}\n",
    "      - nodes_per_layer: {layer: ËäÇÁÇπÊï∞}\n",
    "    ËØ¥ÊòéÔºö\n",
    "      - ËäÇÁÇπÊñáÊ°£Êï∞ÔºöÁªüËÆ°ËêΩÂú®ËØ•ËäÇÁÇπÔºàËØ•Â±ÇÔºâ‰∏äÁöÑÂéªÈáçÊñáÊ°£Êï∞\n",
    "      - Âä†ÊùÉÂπ≥ÂùáÔºöÁî®ÊàêÂØπÊùÉÈáç m_i*m_jÔºàÂºÇË¥®ÊÄßÔºâ‰∏éËäÇÁÇπÊùÉÈáç m_iÔºàÁÜµÔºâ\n",
    "    \"\"\"\n",
    "    # ËØçÈ¢ë‰∏éÊñáÊ°£ÈõÜÂêà\n",
    "    layer_word_dist = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))  # layer -> node_id -> word -> cnt\n",
    "    node_doc_sets = defaultdict(set)  # node_id -> {doc_ids}\n",
    "\n",
    "    # Ëã• root_node ÊòØ dict[node_id]->obj ‰∏îÂê´ parent_idÔºåÁî®ÂÆÉÔºõÂê¶Âàô parent_id ÁΩÆ None\n",
    "    get_parent_id = (lambda nid: getattr(root_node.get(nid), 'parent_id', None)) if isinstance(root_node, dict) else (lambda nid: None)\n",
    "\n",
    "    # Á¥ØÁßØÊØèËäÇÁÇπÁöÑËØçÈ¢ë‰∏éÊñáÊ°£ÈõÜÂêà\n",
    "    for doc_id, leaf_id in doc_path.items():\n",
    "        path_nodes = path_list[leaf_id]  # [Node1, Node2, ...]\n",
    "        per_doc_alloc = doc_node_allocation.get(doc_id, {})\n",
    "        for node in path_nodes:\n",
    "            lyr = node.layer\n",
    "            if lyr in per_doc_alloc:\n",
    "                # ËÆ∞ÂΩïËØçÈ¢ë\n",
    "                for w, c in per_doc_alloc[lyr].items():\n",
    "                    layer_word_dist[lyr][node.node_id][w] += c\n",
    "                # ËÆ∞ÂΩïÊñáÊ°£\n",
    "                node_doc_sets[node.node_id].add(doc_id)\n",
    "\n",
    "#     # Ê±áÊÄªÈÄêËäÇÁÇπ\n",
    "#     node_records = []\n",
    "#     for layer, nodes_dist in layer_word_dist.items():\n",
    "#         for nid, wdist in nodes_dist.items():\n",
    "#             entropy = calculate_renyi_entropy(wdist, q=2.0)\n",
    "#             doc_count = len(node_doc_sets[nid])\n",
    "#             node_records.append({\n",
    "#                 \"layer\": layer,\n",
    "#                 \"node_id\": nid,\n",
    "#                 \"parent_id\": get_parent_id(nid),\n",
    "#                 \"entropy\": entropy,\n",
    "#                 \"doc_count\": doc_count,\n",
    "#                 # ÂèØÈÄâÔºö‰øùÂ≠òË∑ØÂæÑÂ≠óÁ¨¶‰∏≤ÔºàÂ¶ÇÈúÄË¶ÅÔºå‰Ω†ÂèØ‰ª•Âú®Â§ñÈÉ®È¢ÑÂÖàÊûÑÈÄ† node_id->path_str ÁöÑÊò†Â∞Ñ‰º†ÂÖ•Ôºâ\n",
    "#                 \"path\": None\n",
    "#             })\n",
    "            \n",
    "    # Âú® evaluate_tree_structure ÂáΩÊï∞‰∏≠Ê∑ªÂä†Ë∞ÉËØï‰ø°ÊÅØ\n",
    "    node_records = []\n",
    "    for layer, nodes_dist in layer_word_dist.items():\n",
    "        for nid, wdist in nodes_dist.items():\n",
    "            entropy = calculate_renyi_entropy(wdist, q=2.0)\n",
    "            doc_count = len(node_doc_sets[nid]) if nid in node_doc_sets else 0\n",
    "\n",
    "            # Ê∑ªÂä†Ë∞ÉËØï‰ø°ÊÅØ\n",
    "            if entropy == 0:\n",
    "                print(f\"‚ö†Ô∏è Node {nid} (Layer {layer}) has entropy=0:\")\n",
    "                print(f\"   Word distribution: {dict(wdist)}\")\n",
    "                print(f\"   Number of unique words: {len(wdist)}\")\n",
    "                print(f\"   Total word count: {sum(wdist.values())}\")\n",
    "                print(f\"   Document count: {doc_count}\")\n",
    "                print()\n",
    "\n",
    "            node_records.append({\n",
    "                \"layer\": layer,\n",
    "                \"node_id\": nid,\n",
    "                \"parent_id\": get_parent_id(nid),\n",
    "                \"entropy\": entropy,\n",
    "                \"doc_count\": doc_count,\n",
    "                \"unique_words\": len(wdist),\n",
    "                \"total_words\": sum(wdist.values()),\n",
    "                \"path\": None\n",
    "            })\n",
    "        \n",
    "    # Â±ÇÁ∫ßÊñáÊ°£Âä†ÊùÉÁÜµ\n",
    "    layer_entropy_wavg = {}\n",
    "    nodes_per_layer = {}\n",
    "    for layer, nodes_dist in layer_word_dist.items():\n",
    "        nodes = list(nodes_dist.keys())\n",
    "        nodes_per_layer[layer] = len(nodes)\n",
    "        total_docs = sum(len(node_doc_sets[nid]) for nid in nodes)\n",
    "        if total_docs == 0:\n",
    "            layer_entropy_wavg[layer] = 0.0\n",
    "            continue\n",
    "        wsum = 0.0\n",
    "        for nid in nodes:\n",
    "            H = calculate_renyi_entropy(nodes_dist[nid], q=2.0)\n",
    "            w = len(node_doc_sets[nid])\n",
    "            wsum += H * w\n",
    "        layer_entropy_wavg[layer] = wsum / total_docs\n",
    "\n",
    "    # Â±ÇÁ∫ßÊñáÊ°£Âä†ÊùÉ‰∏ªÈ¢òÂºÇË¥®ÊÄßÔºàÂä†ÊùÉJSDÔºâ\n",
    "    layer_distinctiveness_wavg = {}\n",
    "    for layer, nodes_dist in layer_word_dist.items():\n",
    "        nids = list(nodes_dist.keys())\n",
    "        if len(nids) < 2:\n",
    "            layer_distinctiveness_wavg[layer] = 0.0\n",
    "            continue\n",
    "        wsum_jsd = 0.0\n",
    "        wsum = 0.0\n",
    "        for i in range(len(nids)):\n",
    "            for j in range(i+1, len(nids)):\n",
    "                ni, nj = nids[i], nids[j]\n",
    "                mi, mj = len(node_doc_sets[ni]), len(node_doc_sets[nj])\n",
    "                if mi == 0 or mj == 0:\n",
    "                    continue\n",
    "                jsd = jensen_shannon_divergence(nodes_dist[ni], nodes_dist[nj])\n",
    "                w = mi * mj\n",
    "                wsum_jsd += jsd * w\n",
    "                wsum += w\n",
    "        layer_distinctiveness_wavg[layer] = (wsum_jsd / wsum) if wsum > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"node_records\": node_records,\n",
    "        \"layer_entropy_wavg\": layer_entropy_wavg,\n",
    "        \"layer_distinctiveness_wavg\": layer_distinctiveness_wavg,\n",
    "        \"nodes_per_layer\": nodes_per_layer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1366be79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_single_chain(args):\n",
    "    \"\"\"\n",
    "    ËøêË°åÂçïÊù° hLDA ÈìæÁöÑÂáΩÊï∞ÔºàÁî®‰∫é joblib Âπ∂Ë°åÔºâ\n",
    "    \n",
    "    Args:\n",
    "        args: ÂÖÉÁªÑ (chain_id, corpus, depth, gamma, eta, alpha, max_iterations, general_dir, seed)\n",
    "        \n",
    "    Returns:\n",
    "        dict: ÂåÖÂê´ÈìæÁªìÊûúÁöÑÂ≠óÂÖ∏\n",
    "    \"\"\"\n",
    "    (chain_id, corpus, depth, gamma, eta, alpha, max_iterations, general_dir, seed, shared_state, back_window, check_interval, burn_in) = args\n",
    "\n",
    "    print(f\"‚õìÔ∏è Chain {chain_id} startsÔºàPID: {os.getpid()}Ôºâ\")\n",
    "    \n",
    "    # ËÆæÁΩÆÈöèÊú∫ÁßçÂ≠ê\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # ‰∏∫ÈìæÂàõÂª∫ÁõÆÂΩï\n",
    "    chain_dir = os.path.join(general_dir, f\"depth_{depth}_gamma_{gamma}_run_{chain_id}\")\n",
    "    os.makedirs(chain_dir, exist_ok=True)\n",
    "    \n",
    "    # ËøêË°åGibbsÈááÊ†∑\n",
    "    \"\"\"\n",
    "    def run_gibbs_with_checkpoint(\n",
    "    corpus, depth=3, gamma=None, eta=None, alpha=None,\n",
    "    max_iterations=200, save_every_n_iter=10, checkpoint_dir='gibbs_checkpoints',\n",
    "    check_interval=10, resume_from_checkpoint=None, base_dir=None,\n",
    "    shared_state=None, chain_id=None, back_window=5, burn_in = 50):\n",
    "    \"\"\"\n",
    "    result = run_gibbs_with_checkpoint(\n",
    "        corpus=corpus,\n",
    "        depth=depth,\n",
    "        gamma=gamma,\n",
    "        eta=eta,\n",
    "        alpha=alpha,\n",
    "        max_iterations=max_iterations,\n",
    "        checkpoint_dir=os.path.join(chain_dir, \"checkpoints\"),\n",
    "        base_dir=chain_dir,\n",
    "        chain_id=chain_id,\n",
    "        shared_state=shared_state,\n",
    "        back_window=back_window,\n",
    "        burn_in=burn_in,  # ËÆæÁΩÆburn-inÊúü‰∏∫50ËΩÆ\n",
    "        check_interval=check_interval  # ËÆæÁΩÆÊªëÂä®Á™óÂè£Â§ßÂ∞è\n",
    "    )\n",
    "    # 1. converged: return recorder, root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation, saved_files\n",
    "    # 2. not converged: return recorder, root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation, saved_files, change_docs_list, loglikelihood_list, converge_or_not, doc_path_lst, doc_node_allocation_lst\n",
    "    \n",
    "    # ÊèêÂèñÁªìÊûú\n",
    "    recorder, root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation, saved_files, change_docs_list, loglikelihood_list, doc_path_lst, doc_node_allocation_lst = result\n",
    "    \n",
    "    # ‰øùÂ≠òËΩªÈáèÁ∫ßÁªìÊûúÂà∞Á£ÅÁõò‰ª•‰æø‰∏ªËøõÁ®ãËØªÂèñ\n",
    "    result_file = os.path.join(chain_dir, \"final_checkpoint.pkl\")\n",
    "    with open(result_file, 'wb') as f:\n",
    "        chain_result = {\n",
    "            'chain_id': chain_id,\n",
    "            'loglikelihood_history': loglikelihood_list,\n",
    "            'changed_docs_history': change_docs_list\n",
    "            # ‰∏ç‰øùÂ≠òÂ§™Â§ßÁöÑÂØπË±°\n",
    "        }\n",
    "        pickle.dump(chain_result, f)\n",
    "    \n",
    "    print(f\"‚úÖ Chain {chain_id} Finished !\")\n",
    "    \n",
    "    # 1. ÊåâÂ±Ç‰øùÂ≠ò\n",
    "    \n",
    "    res = evaluate_tree_structure_with_nodes(root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation)\n",
    "    \n",
    "    layers = sorted(res[\"layer_entropy_wavg\"].keys())\n",
    "    layer_rows = []\n",
    "    for L in layers:\n",
    "        layer_rows.append({\n",
    "            \"depth\": depth,\n",
    "            \"gamma\": gamma,\n",
    "            \"eta\": eta,\n",
    "            \"alpha\": alpha,\n",
    "            \"layer\": L,\n",
    "            \"entropy_wavg\": res[\"layer_entropy_wavg\"].get(L, 0.0),\n",
    "            \"distinctiveness_wavg_jsd\": res[\"layer_distinctiveness_wavg\"].get(L, 0.0),\n",
    "            \"nodes_in_layer\": res[\"nodes_per_layer\"].get(L, 0)\n",
    "        })\n",
    "    df_layers = pd.DataFrame(layer_rows)\n",
    "    layers_csv = os.path.join(chain_dir, \"result_layers.csv\")\n",
    "    os.makedirs(chain_dir, exist_ok=True)\n",
    "    if not os.path.exists(layers_csv):\n",
    "        df_layers.to_csv(layers_csv, index=False, mode='w', header=True)\n",
    "    else:\n",
    "        df_layers.to_csv(layers_csv, index=False, mode='a', header=False)\n",
    "\n",
    "    # 2. ÊåâËäÇÁÇπ‰øùÂ≠ò\n",
    "    df_nodes = pd.DataFrame(res[\"node_records\"])\n",
    "    if not df_nodes.empty:\n",
    "        df_nodes.insert(0, \"depth\", depth)\n",
    "        df_nodes.insert(1, \"gamma\", gamma)\n",
    "        df_nodes.insert(2, \"eta\", eta)\n",
    "        df_nodes.insert(3, \"alpha\", alpha)\n",
    "        nodes_csv = os.path.join(chain_dir, \"result_nodes.csv\")\n",
    "        if not os.path.exists(nodes_csv):\n",
    "            df_nodes.to_csv(nodes_csv, index=False, mode='w', header=True)\n",
    "        else:\n",
    "            df_nodes.to_csv(nodes_csv, index=False, mode='a', header=False)\n",
    "\n",
    "    # 3. ÂçïË°åÊëòË¶ÅÔºàÂéüÊù•ÁöÑ result_metrics.csv Ôºâ\n",
    "    summary = {\n",
    "        \"depth\": depth,\n",
    "        \"gamma\": gamma,\n",
    "        \"eta\": eta,\n",
    "        \"alpha\": alpha,\n",
    "        \"avg_entropy_wavg_over_layers\": float(np.mean([res[\"layer_entropy_wavg\"][L] for L in layers])) if layers else 0.0,\n",
    "        \"avg_distinctiveness_wavg_over_layers\": float(np.mean([res[\"layer_distinctiveness_wavg\"][L] for L in layers])) if layers else 0.0,\n",
    "        \"total_layers\": len(layers),\n",
    "        \"total_nodes\": int(sum(res[\"nodes_per_layer\"].values())) if layers else 0\n",
    "    }\n",
    "    df_metrics = pd.DataFrame([summary])\n",
    "    metrics_csv = os.path.join(chain_dir, \"result_metrics.csv\")\n",
    "    if not os.path.exists(metrics_csv):\n",
    "        df_metrics.to_csv(metrics_csv, index=False, mode='w', header=True)\n",
    "    else:\n",
    "        df_metrics.to_csv(metrics_csv, index=False, mode='a', header=False)\n",
    "\n",
    "#     # ËÆ°ÁÆóÊåáÊ†áÂπ∂‰øùÂ≠ò\n",
    "#     metrics = evaluate_tree_structure(root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation)\n",
    "\n",
    "#     metrics.update({\n",
    "#         'gamma': gamma,\n",
    "#         'eta': eta,\n",
    "#         'depth': depth,\n",
    "#         'alpha': alpha,\n",
    "#     })     \n",
    "    \n",
    "#     metrics_df = pd.DataFrame([metrics])  # Â∞ÜÂ≠óÂÖ∏ËΩ¨Êç¢‰∏∫DataFrame\n",
    "#     metrics_df.to_csv(os.path.join(chain_dir, \"result_metrics.csv\"), index=False)\n",
    "    \n",
    "    # ËøîÂõûËΩªÈáèÁ∫ßÁªìÊûú\n",
    "    return {\n",
    "        'chain_id': chain_id,\n",
    "        'loglikelihood_history': loglikelihood_list,\n",
    "        'result_file': result_file,\n",
    "        'changed_docs_history': change_docs_list,\n",
    "        'doc_path_lst': doc_path_lst,\n",
    "        'doc_node_allocation_lst': doc_node_allocation_lst\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c7397f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "import json\n",
    "\n",
    "def run_multi_chain_hlda(\n",
    "    corpus, depth=3, gamma=0.01, eta=0.01, alpha=0.1,\n",
    "    n_chains=3, max_iterations=20, r_hat_threshold=1.1,\n",
    "    general_dir=\"multi_chain_hlda_results\",\n",
    "    back_window=5, check_interval=10, burn_in=50\n",
    "):\n",
    "    \"\"\"\n",
    "    Âπ∂Ë°åËøêË°åÂ§öÊù° hLDA ÈìæÔºåÊî∂ÊïõÊ£ÄÊü•Áî± monitor Á∫øÁ®ãÂÆûÊó∂ÂÆåÊàê„ÄÇ\n",
    "    ËøêË°åÁªìÊùüÂêéËá™Âä®‰øùÂ≠òÊâÄÊúâÊî∂ÊïõÊ£ÄÊü•ÂéÜÂè≤Êï∞ÊçÆ‰∏∫ CSV„ÄÇ\n",
    "    \n",
    "    result = run_multi_chain_hlda(\n",
    "    corpus=corpus,\n",
    "    depth=depth,\n",
    "    gamma=gamma,\n",
    "    eta=eta,\n",
    "    alpha=alpha,\n",
    "    n_chains=n_chains,\n",
    "    max_iterations=30,\n",
    "    check_interval=5,\n",
    "    back_window=3,\n",
    "    burn_in=2,\n",
    "    r_hat_threshold=1.1,\n",
    "    general_dir=\"0809_multi_len_docs\")\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(general_dir, exist_ok=True)\n",
    "\n",
    "    # ÂêØÂä®ÂÖ±‰∫´Áä∂ÊÄÅÂíåÁõëÊéßÁ∫øÁ®ã\n",
    "    shared_state = SharedState(general_dir)\n",
    "    monitor = Thread(\n",
    "        target=rhat_monitor_process,\n",
    "        args=(shared_state, n_chains, max_iterations, burn_in, check_interval, back_window, r_hat_threshold)\n",
    "    )\n",
    "    #  # args=(shared_state, n_chains, max_iterations, burn_in, r_hat_threshold, check_interval)\n",
    "    \n",
    "    monitor.daemon = False\n",
    "    monitor.start()\n",
    "    print(\"üöÄ Start R-hat monitor process\")\n",
    "\n",
    "    # ÊûÑÈÄ†ÊØèÊù°ÈìæÁöÑÂèÇÊï∞\n",
    "    args_list = []\n",
    "    for i in range(1, n_chains + 1):\n",
    "        seed = i * 1000 + int(time.time()) % 1000\n",
    "        args_list.append((\n",
    "            i, corpus, depth, gamma, eta, alpha,\n",
    "            max_iterations, general_dir, seed,\n",
    "            shared_state, back_window, check_interval, burn_in\n",
    "        ))\n",
    "        \n",
    "    \"\"\"\n",
    "    (chain_id, corpus, depth, gamma, eta, alpha, max_iterations, general_dir, seed, shared_state, back_window, check_interval, burn_in) = args\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"üöÄ Start {n_chains} hLDA chainÔºåeach chain could have {max_iterations} max iterations...\")\n",
    "\n",
    "    # Âπ∂Ë°åËøêË°åÊâÄÊúâÈìæ\n",
    "    chain_results = Parallel(n_jobs=n_chains, backend='multiprocessing', verbose=10)(\n",
    "        delayed(_run_single_chain)(args) for args in args_list\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ All chains finish sampling and waiting for monitor process finish...\")\n",
    "\n",
    "    # Á≠âÂæÖÁõëÊéßÁ∫øÁ®ãÁªìÊùüÔºàÂç≥ÊâÄÊúâÈìæÈÉΩÊî∂ÊïõÊàñËææÂà∞ÊúÄÂ§ßËø≠‰ª£Ôºâ\n",
    "    monitor.join(timeout=60)\n",
    "    print(\"‚úÖ R-hat monitor process finished !\")\n",
    "\n",
    "    # Êî∂ÈõÜÊØèÊù°ÈìæÁöÑÁªìÊûú\n",
    "    full_results = {}\n",
    "    for result in chain_results:\n",
    "        with open(result['result_file'], 'rb') as f:\n",
    "            full_results[result['chain_id']] = pickle.load(f)\n",
    "\n",
    "    # ‰øùÂ≠òÊî∂ÊïõÊ£ÄÊü•ÂéÜÂè≤Êï∞ÊçÆ\n",
    "    rhat_file = os.path.join(general_dir, \"shared_state.json\")\n",
    "    if os.path.exists(rhat_file):\n",
    "        with open(rhat_file, \"r\") as f:\n",
    "            shared_state_data = json.load(f)\n",
    "        \n",
    "        # 1. ‰øùÂ≠òÊÄª‰ΩìR-hatÊî∂ÊïõÂéÜÂè≤Âà∞general_dir\n",
    "        rhat_history = shared_state_data.get(\"rhat_history\", [])\n",
    "        if rhat_history:\n",
    "            convergence_df = pd.DataFrame(rhat_history)\n",
    "            convergence_csv = os.path.join(general_dir, \"convergence_info.csv\")\n",
    "            convergence_df.to_csv(convergence_csv, index=False, encoding='utf-8')\n",
    "            print(f\"‚úÖ Overall convergence history is saved to CSV: {convergence_csv}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Does not find any convergence history in shared_state.json.\")\n",
    "        \n",
    "        # 2. ‰∏∫ÊØè‰∏™ÈìæÂçïÁã¨‰øùÂ≠òÂÖ∂Êî∂Êïõ‰ø°ÊÅØÂà∞ÂêÑËá™ÁöÑchain_dir\n",
    "        for result in chain_results:\n",
    "            chain_id = result['chain_id']\n",
    "            chain_dir = os.path.join(general_dir, f\"depth_{depth}_gamma_{gamma}_run_{chain_id}\")\n",
    "            \n",
    "            # ÊèêÂèñËØ•ÈìæÁâπÂÆöÁöÑÊï∞ÊçÆ\n",
    "            chain_data = shared_state_data.get(\"chains\", {}).get(str(chain_id), {})\n",
    "            if chain_data:\n",
    "                # ËΩ¨Êç¢‰∏∫DataFrameÊ†ºÂºèÊñπ‰æøÂàÜÊûê\n",
    "                chain_records = []\n",
    "                for record_time, len_docs in chain_data.get('len_stable_docs', {}).items():\n",
    "                    jaccard_list = chain_data.get('jaccard', {}).get(record_time, [])\n",
    "                    ratio_list = chain_data.get('ratio', {}).get(record_time, [])\n",
    "                    \n",
    "                    chain_records.append({\n",
    "                        'record_time': int(record_time),\n",
    "                        'len_stable_docs': len_docs,\n",
    "                        'jaccard_mean': np.mean(jaccard_list) if jaccard_list else 0,\n",
    "                        'jaccard_std': np.std(jaccard_list) if jaccard_list else 0,\n",
    "                        'ratio_mean': np.mean(ratio_list) if ratio_list else 0,\n",
    "                        'ratio_std': np.std(ratio_list) if ratio_list else 0,\n",
    "                        'jaccard_list': jaccard_list,\n",
    "                        'ratio_list': ratio_list\n",
    "                    })\n",
    "                \n",
    "                if chain_records:\n",
    "                    chain_df = pd.DataFrame(chain_records)\n",
    "                    chain_convergence_csv = os.path.join(chain_dir, \"chain_convergence_info.csv\")\n",
    "                    chain_df.to_csv(chain_convergence_csv, index=False, encoding='utf-8')\n",
    "                    print(f\"‚úÖ Chain {chain_id} convergence info saved to: {chain_convergence_csv}\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è No convergence data found for chain {chain_id}\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è No chain data found for chain {chain_id}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Does not find shared_state.json, no convergence history saved.\")\n",
    "\n",
    "    return full_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4aceb7c-65c7-4e60-8bab-dba8e7a39cbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import cProfile\n",
    "# # ËÆæÁΩÆÂèÇÊï∞Ôºà‰ΩøÁî®Â∞èÂÜôÂèòÈáèÂêçÔºâ\n",
    "# depth = 3\n",
    "# gamma = 0.05\n",
    "# eta = 0.02\n",
    "# alpha = 0.1\n",
    "# n_chains = 3  # 3Êù°ÈìæÂπ∂Ë°åËøêË°å\n",
    "\n",
    "# # ‰ΩøÁî® joblib ËøêË°åÂ§öÈìæ\n",
    "# result = run_multi_chain_hlda(\n",
    "#     corpus=corpus,\n",
    "#     depth=depth,\n",
    "#     gamma=gamma,\n",
    "#     eta=eta,\n",
    "#     alpha=alpha,\n",
    "#     n_chains=n_chains,\n",
    "#     max_iterations=300,\n",
    "#     check_interval=10,\n",
    "#     back_window=5,\n",
    "#     burn_in=50,\n",
    "#     r_hat_threshold=1.1,\n",
    "#     general_dir=\"step2_d3_g005_e002_check10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f44b2d2-8de8-41c1-b143-a845f65b97b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def restore_from_csv_files(chain_dir, corpus, depth):\n",
    "    \"\"\"\n",
    "    ‰ªéCSVÊñá‰ª∂ÊÅ¢Â§çÂçïÊù°ÈìæÁöÑÊâÄÊúâÂèòÈáè\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Ê£ÄÊü•ÂøÖË¶ÅÊñá‰ª∂ÊòØÂê¶Â≠òÂú®\n",
    "    path_struct_file = os.path.join(chain_dir, 'iteration_path_structures.csv')\n",
    "    word_alloc_file = os.path.join(chain_dir, 'iteration_word_allocations.csv')\n",
    "    \n",
    "    if not os.path.exists(path_struct_file):\n",
    "        raise FileNotFoundError(f\"Path structure file not found: {path_struct_file}\")\n",
    "    if not os.path.exists(word_alloc_file):\n",
    "        raise FileNotFoundError(f\"Word allocation file not found: {word_alloc_file}\")\n",
    "    \n",
    "    print(f\"üìÑ Reading files from {chain_dir}\")\n",
    "    print(f\"   - Path structure: {path_struct_file}\")\n",
    "    print(f\"   - Word allocation: {word_alloc_file}\")\n",
    "    \n",
    "    # ËØªÂèñË∑ØÂæÑÁªìÊûÑCSV\n",
    "    path_struct_df = pd.read_csv(path_struct_file)\n",
    "    max_iter = path_struct_df['iteration'].max()\n",
    "    df_path = path_struct_df[path_struct_df['iteration'] == max_iter]\n",
    "    print(f\"üìä Using iteration {max_iter} from path structure file\")\n",
    "\n",
    "    # Êî∂ÈõÜÊâÄÊúâËäÇÁÇπIDÂèäÂÖ∂Â±ÇÁ∫ß‰ø°ÊÅØ\n",
    "    node_info = {}\n",
    "    for _, row in df_path.iterrows():\n",
    "        for layer in range(depth):\n",
    "            node_id = row[f'layer_{layer}_node_id']\n",
    "            if pd.notna(node_id) and node_id not in node_info:\n",
    "                node_info[node_id] = {\n",
    "                    'layer': layer,\n",
    "                    'parent_id': row[f'layer_{layer-1}_node_id'] if layer > 0 else None,\n",
    "                    'children': set(),\n",
    "                    'docs_list': []\n",
    "                }\n",
    "\n",
    "    # Âª∫Á´ãÁà∂Â≠êÂÖ≥Á≥ª\n",
    "    for _, row in df_path.iterrows():\n",
    "        for layer in range(1, depth):\n",
    "            parent_id = row[f'layer_{layer-1}_node_id']\n",
    "            child_id = row[f'layer_{layer}_node_id']\n",
    "            if (pd.notna(parent_id) and pd.notna(child_id) and \n",
    "                parent_id in node_info and child_id in node_info):\n",
    "                node_info[parent_id]['children'].add(child_id)\n",
    "                node_info[child_id]['parent_id'] = parent_id\n",
    "\n",
    "    # ÈáçÁΩÆNodeÁ±ªÁöÑÈùôÊÄÅÂèòÈáèÂπ∂ÈáçÂª∫ÊâÄÊúâNodeÂØπË±°\n",
    "    Node.last_node_id = max(node_info.keys()) + 1 if node_info else 0\n",
    "    Node.total_node = len(node_info)\n",
    "    Node.node_with_id = {}\n",
    "\n",
    "    node_dict = {}\n",
    "    for node_id, info in node_info.items():\n",
    "        node = Node.__new__(Node)\n",
    "        node.node_id = node_id\n",
    "        node.layer = info['layer']\n",
    "        node.parent = None\n",
    "        node.children = []\n",
    "        node.docs_list = []\n",
    "        Node.node_with_id[node_id] = node\n",
    "        node_dict[node_id] = node\n",
    "\n",
    "    # Âª∫Á´ãNodeÂØπË±°‰πãÈó¥ÁöÑÁà∂Â≠êÂÖ≥Á≥ª\n",
    "    for node_id, info in node_info.items():\n",
    "        node = node_dict[node_id]\n",
    "        if info['parent_id'] is not None and info['parent_id'] in node_dict:\n",
    "            node.parent = node_dict[info['parent_id']]\n",
    "        node.children = [node_dict[child_id] for child_id in info['children'] if child_id in node_dict]\n",
    "\n",
    "    # ÊâæÂà∞Ê†πËäÇÁÇπ\n",
    "    root_node = None\n",
    "    for node in node_dict.values():\n",
    "        if node.layer == 0 and node.parent is None:\n",
    "            root_node = node\n",
    "            break\n",
    "\n",
    "    if root_node is None:\n",
    "        raise ValueError(\"No root node found in the restored tree\")\n",
    "\n",
    "    # ÈáçÂª∫path_list\n",
    "    path_list = {}\n",
    "    for _, row in df_path.iterrows():\n",
    "        leaf_id = row['leaf_node_id']\n",
    "        node_ids = [row[f'layer_{i}_node_id'] for i in range(depth)]\n",
    "        if all(pd.notna(nid) and nid in node_dict for nid in node_ids):\n",
    "            path_list[leaf_id] = [node_dict[nid] for nid in node_ids]\n",
    "\n",
    "    # ËØªÂèñËØçËØ≠ÂàÜÈÖçÊï∞ÊçÆ\n",
    "    word_alloc_df = pd.read_csv(word_alloc_file)\n",
    "    max_iter_word = word_alloc_df['iteration'].max()\n",
    "    df_word = word_alloc_df[word_alloc_df['iteration'] == max_iter_word]\n",
    "    print(f\"üìä Using iteration {max_iter_word} from word allocation file\")\n",
    "\n",
    "    # ËøòÂéüdoc_word_allocationÔºöÊåâword_index‰ªéÂ∞èÂà∞Â§ßÊéíÂ∫èÔºåÂèñassigned_layer\n",
    "    doc_word_allocation = {}\n",
    "    for doc_id, group in df_word.groupby('document_id'):\n",
    "        sorted_group = group.sort_values('word_index')\n",
    "        doc_word_allocation[doc_id] = list(sorted_group['assigned_layer'])\n",
    "\n",
    "    # ËøòÂéüdoc_pathÔºöÂèñdocument_idÂíåleaf_node_idÁöÑÊò†Â∞Ñ\n",
    "    doc_path = {}\n",
    "    for _, row in df_word.drop_duplicates('document_id').iterrows():\n",
    "        doc_path[row['document_id']] = row['leaf_node_id']\n",
    "\n",
    "    # ÈáçÂª∫doc_node_allocation\n",
    "    doc_node_allocation = aggregate_words(corpus, doc_word_allocation)\n",
    "\n",
    "    # Ê†πÊçÆdoc_pathÈáçÊñ∞ÊûÑÂª∫ÊØè‰∏™ËäÇÁÇπÁöÑdocs_list\n",
    "    for doc_id, leaf_id in doc_path.items():\n",
    "        if leaf_id in path_list:\n",
    "            path_nodes = path_list[leaf_id]\n",
    "            for node in path_nodes:\n",
    "                if doc_id not in node.docs_list:\n",
    "                    node.docs_list.append(doc_id)\n",
    "\n",
    "    print(f\"‚úÖ Successfully restored from {chain_dir}\")\n",
    "    print(f\"   - Nodes: {len(node_dict)}\")\n",
    "    print(f\"   - Paths: {len(path_list)}\")\n",
    "    print(f\"   - Documents: {len(doc_path)}\")\n",
    "    \n",
    "    return root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation\n",
    "\n",
    "def Gibbs_sampling_simple(corpus, root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation, global_node_word_dist,\n",
    "                   gamma, eta, alpha, depth, iteration, iter_start=None, iter_end=None):\n",
    "    \"\"\"\n",
    "    ÁÆÄÂåñÁâàÊú¨ÁöÑGibbsÈááÊ†∑ÂáΩÊï∞ÔºåÂéªÈô§jumpÂíådetailËÆ∞ÂΩï\n",
    "    \"\"\"\n",
    "    W = set(itertools.chain.from_iterable(corpus.values()))\n",
    "\n",
    "    for doc_id, doc in corpus.items():\n",
    "        current_path = path_list[doc_path[doc_id]]\n",
    "        current_node_allocation = doc_node_allocation[doc_id]\n",
    "        \n",
    "        # ‰ªéÂÖ®Â±ÄÂàÜÂ∏É‰∏≠ÊéíÈô§ÂΩìÂâçÊñáÊ°£\n",
    "        node_word_dist = exclude_doc_from_node_dist(global_node_word_dist, doc_node_allocation, doc_path, path_list, doc_id)\n",
    "\n",
    "        # ‰ªéÂΩìÂâçË∑ØÂæÑÁßªÈô§ÊñáÊ°£\n",
    "        nodes_to_remove = []\n",
    "        for node in current_path[::-1]:\n",
    "            if doc_id in node.docs_list:\n",
    "                node.docs_list.remove(doc_id)\n",
    "            if len(node.docs_list) == 0 and node != root_node:\n",
    "                nodes_to_remove.append(node)\n",
    "        \n",
    "        if nodes_to_remove:\n",
    "            del path_list[doc_path[doc_id]]\n",
    "            for node in nodes_to_remove:\n",
    "                if node.parent:\n",
    "                    node.parent.remove_child(node)\n",
    "\n",
    "        # ËÆ°ÁÆóÁé∞ÊúâË∑ØÂæÑÁöÑÊ¶ÇÁéá\n",
    "        path_prior_lst = {}\n",
    "        path_likelihood_lst = {}\n",
    "\n",
    "        for path_id, path in path_list.items():\n",
    "            node_prior_lst = [1.0]\n",
    "            node_likelihood_lst = []\n",
    "            \n",
    "            for node in path:\n",
    "                if node.parent:\n",
    "                    prior = (len(node.docs_list)) / (gamma + len(node.parent.docs_list)) \n",
    "                    node_prior_lst.append(prior)\n",
    "                \n",
    "                if node.layer in current_node_allocation:\n",
    "                    likelihood = calc_node_likelihood(node_word_dist[node.node_id], current_node_allocation[node.layer], eta, len(W))\n",
    "                    node_likelihood_lst.append(likelihood)\n",
    "                else:\n",
    "                    node_likelihood_lst.append(0)\n",
    "\n",
    "            path_prior = reduce(operator.mul, (x for x in node_prior_lst if x != 0), 1)\n",
    "            path_likelihood = reduce(operator.mul, (x for x in node_likelihood_lst if x != 0), 1)\n",
    "            path_prior_lst[path_id] = path_prior\n",
    "            path_likelihood_lst[path_id] = path_likelihood\n",
    "\n",
    "        multiplied_path_dict = {key: path_prior_lst[key] * path_likelihood_lst[key] for key in path_likelihood_lst}\n",
    "        \n",
    "        # ËÆ°ÁÆóÊñ∞Ë∑ØÂæÑÁöÑÊ¶ÇÁéá\n",
    "        new_path_prior_lst = {}\n",
    "        new_path_likelihood_lst = {}\n",
    "\n",
    "        for node_id, node in Node.node_with_id.items():\n",
    "            if node != None and node.layer < depth-1:\n",
    "                new_node_prior_lst = [gamma / (gamma + len(node.docs_list))]\n",
    "                \n",
    "                if 0 in current_node_allocation:\n",
    "                    new_node_likelihood_lst = [calc_node_likelihood(node_word_dist[0], current_node_allocation[0], eta, len(W))]\n",
    "                else:\n",
    "                    new_node_likelihood_lst = [0]\n",
    "                \n",
    "                temp_node = node\n",
    "                while temp_node.parent:\n",
    "                    new_node_prior_lst.insert(0, (len(temp_node.docs_list)) / (gamma + len(temp_node.parent.docs_list)))             \n",
    "                \n",
    "                    if temp_node.layer in current_node_allocation:\n",
    "                        new_node_likelihood_lst.insert(1, calc_node_likelihood(node_word_dist[temp_node.node_id], current_node_allocation[temp_node.layer], eta, len(W)))\n",
    "                    else:\n",
    "                        new_node_likelihood_lst.insert(1, 0)\n",
    "\n",
    "                    temp_node = temp_node.parent\n",
    "\n",
    "                for layer in range(node.layer+1, depth):\n",
    "                    if layer in current_node_allocation:\n",
    "                        new_node_likelihood_lst.append(calc_node_likelihood({}, current_node_allocation[layer], eta, len(W)))\n",
    "\n",
    "                new_path_prior = reduce(operator.mul, (x for x in new_node_prior_lst if x != 0), 1)\n",
    "                new_path_likelihood = reduce(operator.mul, (x for x in new_node_likelihood_lst if x != 0), 1)\n",
    "                new_path_prior_lst[node_id] = new_path_prior\n",
    "                new_path_likelihood_lst[node_id] = new_path_likelihood\n",
    "        \n",
    "        multiplied_new_path_dict = {f'create{key}': new_path_prior_lst[key] * new_path_likelihood_lst[key] for key in new_path_likelihood_lst}\n",
    "        all_probs = {**multiplied_path_dict, **multiplied_new_path_dict}\n",
    "\n",
    "        total_prob = sum(v for v in all_probs.values())\n",
    "        if total_prob > 0:\n",
    "            normalized_probs = {k: v/total_prob for k, v in all_probs.items()}\n",
    "        else:\n",
    "            normalized_probs = {k: 1.0/len(all_probs) for k in all_probs}\n",
    "        \n",
    "        chosen_path = np.random.choice(list(normalized_probs.keys()),p=list(normalized_probs.values()))\n",
    "        if chosen_path.startswith('create'):\n",
    "            base_node = Node.node_with_id[int(chosen_path[6:])]\n",
    "            leaf_id, added_path = create_new_path(base_node, doc_id, depth)\n",
    "            path_list.update({leaf_id:added_path})\n",
    "            doc_path[doc_id] = leaf_id\n",
    "        else:\n",
    "            leaf_id = int(chosen_path)\n",
    "            added_path = path_list[int(chosen_path)]\n",
    "            doc_path[doc_id] = int(chosen_path)\n",
    "        \n",
    "        # Â∞ÜÊñáÊ°£Ê∑ªÂä†Âà∞Êñ∞Ë∑ØÂæÑ\n",
    "        for node in added_path:\n",
    "            node.docs_list.append(doc_id)\n",
    "        \n",
    "        # Êõ¥Êñ∞ÂÖ®Â±ÄËØçÂàÜÂ∏É\n",
    "        node_word_dist_update = add_doc_to_node_dist(global_node_word_dist, doc_node_allocation, doc_path, path_list, doc_id)\n",
    "        \n",
    "        # ËØçËØ≠‰∏ªÈ¢òÈááÊ†∑\n",
    "        doc_word2node = doc_word_allocation[doc_id]\n",
    "        current_path = path_list[doc_path[doc_id]]\n",
    "        update_doc_word2node = []\n",
    "        \n",
    "        for word, old_layer in zip(doc, doc_word2node):\n",
    "            doc_node_allocation[doc_id][old_layer][word] -= 1\n",
    "            node_word_dist_update[current_path[old_layer].node_id][word] -= 1\n",
    "            topic_probs = {}\n",
    "\n",
    "            for layer, topic in enumerate(current_path):\n",
    "                word_in_topic = node_word_dist_update[topic.node_id].get(word,0)\n",
    "                topic_in_doc = sum(doc_node_allocation[doc_id].get(layer,{}).values())\n",
    "\n",
    "                word_prop = (word_in_topic+eta)/(sum(node_word_dist_update[topic.node_id].values())+len(W)*eta)\n",
    "                topic_prop = (topic_in_doc+alpha)/(len(doc)+depth*alpha)\n",
    "                topic_probs[layer] = word_prop*topic_prop\n",
    "            \n",
    "            total_topic_prob = sum(v for v in topic_probs.values() if v > 0)\n",
    "            if total_topic_prob > 0:\n",
    "                normalized_topic_probs = {k: v/total_topic_prob for k, v in topic_probs.items()}\n",
    "            else:\n",
    "                normalized_topic_probs = {k: 1.0/len(topic_probs) for k in topic_probs}\n",
    "\n",
    "            chosen_layer = np.random.choice(list(normalized_topic_probs.keys()),p=list(normalized_topic_probs.values()))\n",
    "            update_doc_word2node.append(chosen_layer)\n",
    "            doc_node_allocation[doc_id][chosen_layer][word] += 1\n",
    "            node_word_dist_update[current_path[chosen_layer].node_id][word] += 1\n",
    "\n",
    "        doc_word_allocation[doc_id] = update_doc_word2node\n",
    "\n",
    "def run_gibbs_with_checkpoint_restore(\n",
    "    corpus, depth=3, gamma=None, eta=None, alpha=None,\n",
    "    max_iterations=200, checkpoint_dir='gibbs_checkpoints',\n",
    "    check_interval=10, base_dir=None,\n",
    "    shared_state=None, chain_id=None, back_window=5, burn_in=50,\n",
    "    restored_state=None\n",
    "):\n",
    "    \"\"\"\n",
    "    ‰∏ìÈó®Áî®‰∫éÊÅ¢Â§çÊ®°ÂºèÁöÑGibbsÈááÊ†∑‰∏ªÂæ™ÁéØÔºàË∑≥ËøánCRPÂàùÂßãÂåñÔºâ\n",
    "    \"\"\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    if restored_state is None:\n",
    "        raise ValueError(\"restored_state is required for restore mode\")\n",
    "    \n",
    "    # ‰ªéÊÅ¢Â§çÁä∂ÊÄÅÂºÄÂßã\n",
    "    root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation = restored_state\n",
    "    \n",
    "    # ÂàõÂª∫Êñ∞ÁöÑrecorder\n",
    "    recorder = Recorder(corpus, depth, eta, alpha)\n",
    "    \n",
    "    # ËÆ°ÁÆóÂÖ®Â±ÄËäÇÁÇπËØçÂàÜÂ∏É\n",
    "    global_node_word_dist = node_word_distribution(doc_node_allocation, doc_path, path_list, exclude_docs=None)\n",
    "    \n",
    "    # ËÆ∞ÂΩïÊÅ¢Â§çÂêéÁöÑÂàùÂßãÁä∂ÊÄÅ\n",
    "    iter_start_for_log = burn_in + back_window\n",
    "    initial_summary = recorder.record_iteration(\n",
    "        0, root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation, global_node_word_dist,\n",
    "        iter_start_for_detailed_log=iter_start_for_log\n",
    "    )\n",
    "    \n",
    "    print(f\"üìù Chain {chain_id} restored state recorded: {initial_summary['total_paths']} paths, \", \n",
    "          f\"Log-likelihood: {initial_summary['log_likelihood']:.2f}\")\n",
    "    \n",
    "    start_iter = 1\n",
    "    previous_log_likelihood = recorder.iteration_records[-1]['iteration_summary']['log_likelihood']\n",
    "    loglikelihood_list = [previous_log_likelihood]\n",
    "    change_docs_list = []\n",
    "\n",
    "    doc_path_lst = {0: doc_path.copy()}\n",
    "    doc_node_allocation_lst = {0: doc_node_allocation.copy()}\n",
    "\n",
    "    record_time = 0\n",
    "    start_window = burn_in + back_window \n",
    "    \n",
    "    for iteration in range(start_iter, max_iterations + 1):\n",
    "        old_paths = set(path_list.keys())\n",
    "        doc_path_lst[iteration] = doc_path.copy()\n",
    "        \n",
    "        current_iter_start_for_detailed_log = burn_in + back_window\n",
    "\n",
    "        try:\n",
    "            # ‰ΩøÁî®ÁÆÄÂåñÁâàÊú¨ÁöÑGibbsÈááÊ†∑ÔºàÂéªÈô§jumpÂíådetailËÆ∞ÂΩïÔºâ\n",
    "            Gibbs_sampling_simple(\n",
    "                corpus, root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation, global_node_word_dist,\n",
    "                gamma, eta, alpha, depth, iteration, \n",
    "                iter_start=current_iter_start_for_detailed_log,\n",
    "                iter_end=max_iterations\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in Gibbs sampling at iteration {iteration}: {e}\")\n",
    "            break\n",
    "\n",
    "        doc_path_lst[iteration] = doc_path.copy()\n",
    "        doc_node_allocation_lst[iteration] = doc_node_allocation.copy()\n",
    "\n",
    "        new_paths = set(path_list.keys()) - old_paths\n",
    "        iteration_summary = recorder.record_iteration(\n",
    "            iteration, root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation, global_node_word_dist,\n",
    "            new_paths, iter_start_for_detailed_log=current_iter_start_for_detailed_log\n",
    "        )\n",
    "        print(f\"üîÑ Chain {chain_id} in iteration {iteration}/{max_iterations}, \\n\",\n",
    "              f\"path: {iteration_summary['total_paths']}, \",\n",
    "              f\"new path: {iteration_summary['newly_created_paths']}, \",\n",
    "              f\"docs changed path: {iteration_summary['changed_docs_count']}, \",\n",
    "              f\"Log-likelihood: {iteration_summary['log_likelihood']:.2f}\")\n",
    "        \n",
    "        # Ê£ÄÊü•Êî∂Êïõ\n",
    "        change_docs_list.append(iteration_summary['changed_docs_count'])\n",
    "        loglikelihood_list.append(iteration_summary['log_likelihood'])\n",
    "        \n",
    "        if (iteration - start_window) / check_interval == 1 or (iteration == max_iterations):\n",
    "            convergence_status = shared_state.get_convergence_status() if shared_state else False\n",
    "            if convergence_status:\n",
    "                final_iteration = iteration\n",
    "                print(f\"üéâ Chain {chain_id} is detected to convergence in iteration {iteration}, sampling ends prematurely.\")\n",
    "                break\n",
    "                \n",
    "            window_keys = list(range(iteration-check_interval-back_window+1, iteration+1))\n",
    "            doc_path_window = {k: doc_path_lst[k] for k in window_keys}\n",
    "            \n",
    "            stable_docs = get_stable_docs_sliding_window(doc_path_window, back_window=back_window)\n",
    "            chain_len_docs = [len(doc_list) for iterd, doc_list in stable_docs.items()]\n",
    "            \n",
    "            doc_node_allocation_window = {k: doc_node_allocation_lst[k] for k in window_keys}\n",
    "            \n",
    "            jaccard_lst, ratio_lst = get_jaccard_list_from_stable_dict(stable_docs, doc_node_allocation_window, back_window=back_window, depth=depth)\n",
    "            \n",
    "            if shared_state:\n",
    "                shared_state.update_chain_data(chain_id, chain_len_docs, jaccard_lst, ratio_lst, iteration, record_time)\n",
    "            start_window = iteration\n",
    "            record_time += 1\n",
    "    \n",
    "    print(f\"üéØ Chain {chain_id} finish Gibbs Sampling.\")\n",
    "    print(\"üíæ Save iterations info...\")\n",
    "    saved_files = recorder.save_to_files(base_filename=os.path.join(base_dir, \"iteration\") if base_dir else \"iteration\")\n",
    "    return recorder, root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation, saved_files, change_docs_list, loglikelihood_list, doc_path_lst, doc_node_allocation_lst\n",
    "\n",
    "def _run_single_chain_with_restore(args):\n",
    "    \"\"\"\n",
    "    ËøêË°åÂçïÊù°ÈìæÔºåÊîØÊåÅ‰ªéCSVÊñá‰ª∂ÊÅ¢Â§ç\n",
    "    \"\"\"\n",
    "    (chain_id, corpus, depth, gamma, eta, alpha, max_iterations, general_dir, seed, \n",
    "     shared_state, back_window, check_interval, burn_in, restore_from_existing, source_dir) = args\n",
    "\n",
    "    # Â¶ÇÊûú‰∏çÈúÄË¶ÅÊÅ¢Â§çÔºåÁõ¥Êé•ËøîÂõû\n",
    "    if not restore_from_existing:\n",
    "        return {\n",
    "            'chain_id': chain_id,\n",
    "            'status': 'stopped',\n",
    "            'message': 'restore_from_existing=False, execution stopped'\n",
    "        }\n",
    "\n",
    "    print(f\"‚õìÔ∏è Chain {chain_id} starts (PID: {os.getpid()})\")\n",
    "    \n",
    "    # ËÆæÁΩÆÈöèÊú∫ÁßçÂ≠ê\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # ‰∏∫Êñ∞ËøêË°åÂàõÂª∫ÁõÆÂΩï\n",
    "    chain_dir = os.path.join(general_dir, f\"depth_{depth}_gamma_{gamma}_run_{chain_id}\")\n",
    "    os.makedirs(chain_dir, exist_ok=True)\n",
    "    \n",
    "    # Â∞ùËØï‰ªéÁé∞ÊúâÊï∞ÊçÆÊÅ¢Â§ç\n",
    "    try:\n",
    "        # ÊûÑÈÄ†Ê∫êÁõÆÂΩï‰∏≠ÂØπÂ∫îÈìæÁöÑË∑ØÂæÑ\n",
    "        source_chain_dir = os.path.join(source_dir, f\"depth_{depth}_gamma_{gamma}_run_{chain_id}\")\n",
    "        \n",
    "        if not os.path.exists(source_chain_dir):\n",
    "            raise FileNotFoundError(f\"Source chain directory not found: {source_chain_dir}\")\n",
    "        \n",
    "        print(f\"üîÑ Attempting to restore Chain {chain_id} from {source_chain_dir}...\")\n",
    "        restored_state = restore_from_csv_files(source_chain_dir, corpus, depth)\n",
    "        \n",
    "        print(f\"‚úÖ Chain {chain_id} restored successfully from {source_chain_dir}\")\n",
    "        \n",
    "        # ‰ΩøÁî®ÊÅ¢Â§çÁöÑÁä∂ÊÄÅÁªßÁª≠ËøêË°åGibbsÈááÊ†∑\n",
    "        result = run_gibbs_with_checkpoint_restore(\n",
    "            corpus=corpus,\n",
    "            depth=depth,\n",
    "            gamma=gamma,\n",
    "            eta=eta,\n",
    "            alpha=alpha,\n",
    "            max_iterations=max_iterations,\n",
    "            checkpoint_dir=os.path.join(chain_dir, \"checkpoints\"),\n",
    "            base_dir=chain_dir,\n",
    "            chain_id=chain_id,\n",
    "            shared_state=shared_state,\n",
    "            back_window=back_window,\n",
    "            burn_in=burn_in,\n",
    "            check_interval=check_interval,\n",
    "            restored_state=restored_state\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to restore Chain {chain_id}: {e}\")\n",
    "        import traceback\n",
    "        print(f\"üìã Full traceback: {traceback.format_exc()}\")\n",
    "        return {\n",
    "            'chain_id': chain_id,\n",
    "            'status': 'failed',\n",
    "            'message': f'Failed to restore: {e}'\n",
    "        }\n",
    "    \n",
    "    # ÊèêÂèñÁªìÊûú\n",
    "    recorder, root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation = result[:6]\n",
    "    \n",
    "    # ËÆ°ÁÆóÊåáÊ†áÂπ∂‰øùÂ≠ò\n",
    "    res = evaluate_tree_structure_with_nodes(root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation)\n",
    "    \n",
    "    # ‰øùÂ≠òÁªìÊûúÊñá‰ª∂\n",
    "    layers = sorted(res[\"layer_entropy_wavg\"].keys())\n",
    "    layer_rows = []\n",
    "    for L in layers:\n",
    "        layer_rows.append({\n",
    "            \"depth\": depth,\n",
    "            \"gamma\": gamma,\n",
    "            \"eta\": eta,\n",
    "            \"alpha\": alpha,\n",
    "            \"layer\": L,\n",
    "            \"entropy_wavg\": res[\"layer_entropy_wavg\"].get(L, 0.0),\n",
    "            \"distinctiveness_wavg_jsd\": res[\"layer_distinctiveness_wavg\"].get(L, 0.0),\n",
    "            \"nodes_in_layer\": res[\"nodes_per_layer\"].get(L, 0)\n",
    "        })\n",
    "    \n",
    "    df_layers = pd.DataFrame(layer_rows)\n",
    "    layers_csv = os.path.join(chain_dir, \"result_layers.csv\")\n",
    "    df_layers.to_csv(layers_csv, index=False)\n",
    "    \n",
    "    # ‰øùÂ≠òËäÇÁÇπËØ¶ÁªÜ‰ø°ÊÅØ\n",
    "    df_nodes = pd.DataFrame(res[\"node_records\"])\n",
    "    if not df_nodes.empty:\n",
    "        df_nodes.insert(0, \"depth\", depth)\n",
    "        df_nodes.insert(1, \"gamma\", gamma)\n",
    "        df_nodes.insert(2, \"eta\", eta)\n",
    "        df_nodes.insert(3, \"alpha\", alpha)\n",
    "        nodes_csv = os.path.join(chain_dir, \"result_nodes.csv\")\n",
    "        df_nodes.to_csv(nodes_csv, index=False)\n",
    "    \n",
    "    # ‰øùÂ≠òÊëòË¶ÅÊåáÊ†á\n",
    "    summary = {\n",
    "        \"depth\": depth,\n",
    "        \"gamma\": gamma,\n",
    "        \"eta\": eta,\n",
    "        \"alpha\": alpha,\n",
    "        \"avg_entropy_wavg_over_layers\": float(np.mean([res[\"layer_entropy_wavg\"][L] for L in layers])) if layers else 0.0,\n",
    "        \"avg_distinctiveness_wavg_over_layers\": float(np.mean([res[\"layer_distinctiveness_wavg\"][L] for L in layers])) if layers else 0.0,\n",
    "        \"total_layers\": len(layers),\n",
    "        \"total_nodes\": int(sum(res[\"nodes_per_layer\"].values())) if layers else 0\n",
    "    }\n",
    "    df_metrics = pd.DataFrame([summary])\n",
    "    metrics_csv = os.path.join(chain_dir, \"result_metrics.csv\")\n",
    "    df_metrics.to_csv(metrics_csv, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Chain {chain_id} Finished and results saved to {chain_dir}\")\n",
    "    \n",
    "    return {\n",
    "        'chain_id': chain_id,\n",
    "        'chain_dir': chain_dir,\n",
    "        'metrics': res,\n",
    "        'status': 'completed'\n",
    "    }\n",
    "\n",
    "def run_multi_chain_hlda_with_restore(\n",
    "    corpus, depth=3, gamma=0.01, eta=0.01, alpha=0.1,\n",
    "    n_chains=3, max_iterations=20, r_hat_threshold=1.1,\n",
    "    general_dir=\"multi_chain_hlda_results\",\n",
    "    back_window=5, check_interval=10, burn_in=50,\n",
    "    restore_from_existing=True,\n",
    "    source_dir=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Âπ∂Ë°åËøêË°åÂ§öÊù° hLDA ÈìæÔºåÊîØÊåÅ‰ªéÁé∞ÊúâCSVÊñá‰ª∂ÊÅ¢Â§ç\n",
    "    \"\"\"\n",
    "    # Â¶ÇÊûú‰∏çÈúÄË¶ÅÊÅ¢Â§çÔºåÁõ¥Êé•ËøîÂõû\n",
    "    if not restore_from_existing:\n",
    "        print(\"‚ùå restore_from_existing=False, stopping execution\")\n",
    "        return None\n",
    "    \n",
    "    os.makedirs(general_dir, exist_ok=True)\n",
    "    \n",
    "    # Â¶ÇÊûúÊ≤°ÊúâÊåáÂÆöÊ∫êÁõÆÂΩïÔºå‰ΩøÁî®ÈªòËÆ§ËßÑÂàôÊé®Êñ≠\n",
    "    if restore_from_existing and source_dir is None:\n",
    "        # Ê†πÊçÆÂèÇÊï∞Ëá™Âä®ÁîüÊàêÊ∫êÁõÆÂΩïÂêçÁß∞\n",
    "        gamma_str = str(gamma).replace('0.', '').replace('0', '0')\n",
    "        source_dir = f\"d{depth}_g{gamma_str}_Êî∂Êïõ\"\n",
    "        print(f\"‚ö†Ô∏è No source_dir specified, using inferred path: {source_dir}\")\n",
    "\n",
    "    if not os.path.exists(source_dir):\n",
    "        print(f\"‚ùå Source directory does not exist: {source_dir}\")\n",
    "        return None\n",
    "\n",
    "    # ÂêØÂä®ÂÖ±‰∫´Áä∂ÊÄÅÂíåÁõëÊéßÁ∫øÁ®ã\n",
    "    shared_state = SharedState(general_dir)\n",
    "    monitor = Thread(\n",
    "        target=rhat_monitor_process,\n",
    "        args=(shared_state, n_chains, max_iterations, burn_in, check_interval, back_window, r_hat_threshold)\n",
    "    )\n",
    "    monitor.daemon = False\n",
    "    monitor.start()\n",
    "    print(\"üöÄ Start R-hat monitor process\")\n",
    "\n",
    "    # ÊûÑÈÄ†ÊØèÊù°ÈìæÁöÑÂèÇÊï∞\n",
    "    args_list = []\n",
    "    for i in range(1, n_chains + 1):\n",
    "        seed = i * 1000 + int(time.time()) % 1000\n",
    "        args_list.append((\n",
    "            i, corpus, depth, gamma, eta, alpha,\n",
    "            max_iterations, general_dir, seed,\n",
    "            shared_state, back_window, check_interval, burn_in,\n",
    "            restore_from_existing, source_dir\n",
    "        ))\n",
    "\n",
    "    print(f\"üöÄ Start {n_chains} hLDA chains with restore from {source_dir}\")\n",
    "    print(f\"   New results will be saved to: {general_dir}\")\n",
    "\n",
    "    # Âπ∂Ë°åËøêË°åÊâÄÊúâÈìæ\n",
    "    chain_results = Parallel(n_jobs=n_chains, backend='multiprocessing', verbose=10)(\n",
    "        delayed(_run_single_chain_with_restore)(args) for args in args_list\n",
    "    )\n",
    "\n",
    "    # Ê£ÄÊü•ÊòØÂê¶ÊúâÈìæÂõ†‰∏∫ÈóÆÈ¢òËÄåÂÅúÊ≠¢\n",
    "    stopped_chains = [result for result in chain_results if result.get('status') in ['stopped', 'failed']]\n",
    "    if stopped_chains:\n",
    "        print(f\"‚ö†Ô∏è {len(stopped_chains)} chains were stopped or failed:\")\n",
    "        for result in stopped_chains:\n",
    "            print(f\"   Chain {result['chain_id']}: {result['message']}\")\n",
    "\n",
    "    print(\"‚úÖ All chains finish sampling and waiting for monitor process finish...\")\n",
    "\n",
    "    # Á≠âÂæÖÁõëÊéßÁ∫øÁ®ãÁªìÊùü\n",
    "    monitor.join(timeout=60)\n",
    "    print(\"‚úÖ R-hat monitor process finished !\")\n",
    "\n",
    "    # ‰øùÂ≠òÊî∂ÊïõÊ£ÄÊü•ÂéÜÂè≤Êï∞ÊçÆ\n",
    "    rhat_file = os.path.join(general_dir, \"shared_state.json\")\n",
    "    if os.path.exists(rhat_file):\n",
    "        with open(rhat_file, \"r\") as f:\n",
    "            shared_state_data = json.load(f)\n",
    "        \n",
    "        # ‰øùÂ≠òÊÄª‰ΩìR-hatÊî∂ÊïõÂéÜÂè≤\n",
    "        rhat_history = shared_state_data.get(\"rhat_history\", [])\n",
    "        if rhat_history:\n",
    "            convergence_df = pd.DataFrame(rhat_history)\n",
    "            convergence_csv = os.path.join(general_dir, \"convergence_info.csv\")\n",
    "            convergence_df.to_csv(convergence_csv, index=False, encoding='utf-8')\n",
    "            print(f\"‚úÖ Overall convergence history saved to: {convergence_csv}\")\n",
    "\n",
    "        # ‰∏∫ÊØè‰∏™Èìæ‰øùÂ≠òÊî∂Êïõ‰ø°ÊÅØ\n",
    "        for result in chain_results:\n",
    "            if result.get('status') == 'completed':\n",
    "                chain_id = result['chain_id']\n",
    "                chain_dir = result['chain_dir']\n",
    "                \n",
    "                chain_data = shared_state_data.get(\"chains\", {}).get(str(chain_id), {})\n",
    "                if chain_data:\n",
    "                    chain_records = []\n",
    "                    for record_time, len_docs in chain_data.get('len_stable_docs', {}).items():\n",
    "                        jaccard_list = chain_data.get('jaccard', {}).get(record_time, [])\n",
    "                        ratio_list = chain_data.get('ratio', {}).get(record_time, [])\n",
    "                        \n",
    "                        chain_records.append({\n",
    "                            'record_time': int(record_time),\n",
    "                            'len_stable_docs': len_docs,\n",
    "                            'jaccard_mean': np.mean(jaccard_list) if jaccard_list else 0,\n",
    "                            'jaccard_std': np.std(jaccard_list) if jaccard_list else 0,\n",
    "                            'ratio_mean': np.mean(ratio_list) if ratio_list else 0,\n",
    "                            'ratio_std': np.std(ratio_list) if ratio_list else 0,\n",
    "                            'jaccard_list': jaccard_list,\n",
    "                            'ratio_list': ratio_list\n",
    "                        })\n",
    "                    \n",
    "                    if chain_records:\n",
    "                        chain_df = pd.DataFrame(chain_records)\n",
    "                        chain_convergence_csv = os.path.join(chain_dir, \"chain_convergence_info.csv\")\n",
    "                        chain_df.to_csv(chain_convergence_csv, index=False, encoding='utf-8')\n",
    "                        print(f\"‚úÖ Chain {chain_id} convergence info saved to: {chain_convergence_csv}\")\n",
    "\n",
    "    return chain_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8372ac95-8c30-4e9a-8a98-4228cb39aa9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Initialize R-hat independent monitor thread (stable docs & Jaccard & Ratio)üöÄ Start R-hat monitor process\n",
      "üöÄ Start 3 hLDA chains with restore from machine3_step1_d3_g005_Êî∂Êïõ\n",
      "   New results will be saved to: step2_d3_g005_e001_Âü∫‰∫ée01\n",
      "\n",
      "‚õìÔ∏è Chain 1 starts (PID: 12926)\n",
      "üîÑ Attempting to restore Chain 1 from machine3_step1_d3_g005_Êî∂Êïõ/depth_3_gamma_0.05_run_1...\n",
      "üìÑ Reading files from machine3_step1_d3_g005_Êî∂Êïõ/depth_3_gamma_0.05_run_1\n",
      "   - Path structure: machine3_step1_d3_g005_Êî∂Êïõ/depth_3_gamma_0.05_run_1/iteration_path_structures.csv\n",
      "   - Word allocation: machine3_step1_d3_g005_Êî∂Êïõ/depth_3_gamma_0.05_run_1/iteration_word_allocations.csv\n",
      "‚õìÔ∏è Chain 2 starts (PID: 12927)\n",
      "üîÑ Attempting to restore Chain 2 from machine3_step1_d3_g005_Êî∂Êïõ/depth_3_gamma_0.05_run_2...\n",
      "üìÑ Reading files from machine3_step1_d3_g005_Êî∂Êïõ/depth_3_gamma_0.05_run_2\n",
      "   - Path structure: machine3_step1_d3_g005_Êî∂Êïõ/depth_3_gamma_0.05_run_2/iteration_path_structures.csv\n",
      "   - Word allocation: machine3_step1_d3_g005_Êî∂Êïõ/depth_3_gamma_0.05_run_2/iteration_word_allocations.csv\n",
      "‚õìÔ∏è Chain 3 starts (PID: 12928)\n",
      "üîÑ Attempting to restore Chain 3 from machine3_step1_d3_g005_Êî∂Êïõ/depth_3_gamma_0.05_run_3...\n",
      "üìÑ Reading files from machine3_step1_d3_g005_Êî∂Êïõ/depth_3_gamma_0.05_run_3\n",
      "   - Path structure: machine3_step1_d3_g005_Êî∂Êïõ/depth_3_gamma_0.05_run_3/iteration_path_structures.csv\n",
      "üìä Using iteration 175 from path structure file   - Word allocation: machine3_step1_d3_g005_Êî∂Êïõ/depth_3_gamma_0.05_run_3/iteration_word_allocations.csv\n",
      "\n",
      "üìä Using iteration 175 from path structure file\n",
      "üìä Using iteration 175 from path structure file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend MultiprocessingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Using iteration 175 from word allocation file\n",
      "üìä Using iteration 175 from word allocation file\n",
      "üìä Using iteration 175 from word allocation file\n",
      "‚úÖ Successfully restored from machine3_step1_d3_g005_Êî∂Êïõ/depth_3_gamma_0.05_run_1\n",
      "   - Nodes: 205\n",
      "   - Paths: 164\n",
      "   - Documents: 970\n",
      "‚úÖ Successfully restored from machine3_step1_d3_g005_Êî∂Êïõ/depth_3_gamma_0.05_run_2\n",
      "   - Nodes: 231\n",
      "   - Paths: 186\n",
      "‚úÖ Chain 1 restored successfully from machine3_step1_d3_g005_Êî∂Êïõ/depth_3_gamma_0.05_run_1   - Documents: 970\n",
      "\n",
      "‚úÖ Successfully restored from machine3_step1_d3_g005_Êî∂Êïõ/depth_3_gamma_0.05_run_3\n",
      "   - Nodes: 215\n",
      "   - Paths: 173\n",
      "   - Documents: 970\n",
      "‚úÖ Chain 2 restored successfully from machine3_step1_d3_g005_Êî∂Êïõ/depth_3_gamma_0.05_run_2\n",
      "‚úÖ Chain 3 restored successfully from machine3_step1_d3_g005_Êî∂Êïõ/depth_3_gamma_0.05_run_3\n",
      "üìù Chain 1 restored state recorded: 164 paths,  Log-likelihood: -432078.27\n",
      "üìù Chain 2 restored state recorded: 186 paths,  Log-likelihood: -426916.66\n",
      "üìù Chain 3 restored state recorded: 173 paths,  Log-likelihood: -428569.61\n",
      "üîÑ Chain 1 in iteration 1/200, \n",
      " path: 292,  new path: 178,  docs changed path: 404,  Log-likelihood: -430571.83\n",
      "üîÑ Chain 3 in iteration 1/200, \n",
      " path: 296,  new path: 175,  docs changed path: 379,  Log-likelihood: -427088.74\n",
      "üîÑ Chain 2 in iteration 1/200, \n",
      " path: 320,  new path: 188,  docs changed path: 403,  Log-likelihood: -423730.71\n",
      "üîÑ Chain 1 in iteration 2/200, \n",
      " path: 322,  new path: 149,  docs changed path: 386,  Log-likelihood: -428991.78\n",
      "üîÑ Chain 3 in iteration 2/200, \n",
      " path: 318,  new path: 142,  docs changed path: 363,  Log-likelihood: -424382.46\n",
      "üîÑ Chain 2 in iteration 2/200, \n",
      " path: 321,  new path: 140,  docs changed path: 370,  Log-likelihood: -423607.52\n",
      "üîÑ Chain 1 in iteration 3/200, \n",
      " path: 319,  new path: 130,  docs changed path: 368,  Log-likelihood: -427198.12\n",
      "üîÑ Chain 2 in iteration 3/200, \n",
      " path: 328,  new path: 138,  docs changed path: 353,  Log-likelihood: -423112.94\n",
      "üîÑ Chain 3 in iteration 3/200, \n",
      " path: 316,  new path: 145,  docs changed path: 339,  Log-likelihood: -424143.50\n",
      "üîÑ Chain 1 in iteration 4/200, \n",
      " path: 317,  new path: 127,  docs changed path: 338,  Log-likelihood: -427736.04\n",
      "üîÑ Chain 2 in iteration 4/200, \n",
      " path: 328,  new path: 146,  docs changed path: 357,  Log-likelihood: -424551.13\n",
      "üîÑ Chain 3 in iteration 4/200, \n",
      " path: 319,  new path: 138,  docs changed path: 324,  Log-likelihood: -425222.22\n",
      "üîÑ Chain 1 in iteration 5/200, \n",
      " path: 328,  new path: 136,  docs changed path: 336,  Log-likelihood: -428692.55\n",
      "üîÑ Chain 2 in iteration 5/200, \n",
      " path: 324,  new path: 146,  docs changed path: 346,  Log-likelihood: -425488.64\n",
      "üîÑ Chain 3 in iteration 5/200, \n",
      " path: 304,  new path: 120,  docs changed path: 312,  Log-likelihood: -426008.27\n",
      "üîÑ Chain 1 in iteration 6/200, \n",
      " path: 320,  new path: 133,  docs changed path: 346,  Log-likelihood: -428044.44\n",
      "üîÑ Chain 2 in iteration 6/200, \n",
      " path: 329,  new path: 144,  docs changed path: 344,  Log-likelihood: -425976.70\n",
      "üîÑ Chain 3 in iteration 6/200, \n",
      " path: 312,  new path: 129,  docs changed path: 313,  Log-likelihood: -426364.03\n",
      "üîÑ Chain 1 in iteration 7/200, \n",
      " path: 324,  new path: 135,  docs changed path: 347,  Log-likelihood: -427844.10\n",
      "üîÑ Chain 2 in iteration 7/200, \n",
      " path: 327,  new path: 127,  docs changed path: 337,  Log-likelihood: -425449.87\n",
      "üîÑ Chain 3 in iteration 7/200, \n",
      " path: 314,  new path: 131,  docs changed path: 293,  Log-likelihood: -427590.23\n",
      "üîÑ Chain 2 in iteration 8/200, \n",
      " path: 332,  new path: 144,  docs changed path: 346,  Log-likelihood: -425992.99\n",
      "üîÑ Chain 1 in iteration 8/200, \n",
      " path: 318,  new path: 123,  docs changed path: 321,  Log-likelihood: -429050.38\n",
      "üîÑ Chain 3 in iteration 8/200, \n",
      " path: 314,  new path: 134,  docs changed path: 307,  Log-likelihood: -426938.94\n",
      "üîÑ Chain 2 in iteration 9/200, \n",
      " path: 337,  new path: 143,  docs changed path: 347,  Log-likelihood: -426551.23\n",
      "üîÑ Chain 1 in iteration 9/200, \n",
      " path: 317,  new path: 126,  docs changed path: 325,  Log-likelihood: -428668.40\n",
      "üîÑ Chain 3 in iteration 9/200, \n",
      " path: 316,  new path: 127,  docs changed path: 313,  Log-likelihood: -428291.83\n",
      "üîÑ Chain 2 in iteration 10/200, \n",
      " path: 336,  new path: 137,  docs changed path: 337,  Log-likelihood: -427407.68\n",
      "üîÑ Chain 1 in iteration 10/200, \n",
      " path: 330,  new path: 141,  docs changed path: 327,  Log-likelihood: -428919.54\n",
      "üîÑ Chain 3 in iteration 10/200, \n",
      " path: 313,  new path: 120,  docs changed path: 301,  Log-likelihood: -428443.46\n",
      "üîÑ Chain 2 in iteration 11/200, \n",
      " path: 318,  new path: 117,  docs changed path: 330,  Log-likelihood: -428579.47\n",
      "üîÑ Chain 1 in iteration 11/200, \n",
      " path: 319,  new path: 126,  docs changed path: 321,  Log-likelihood: -428961.91\n",
      "üîÑ Chain 3 in iteration 11/200, \n",
      " path: 317,  new path: 124,  docs changed path: 302,  Log-likelihood: -428041.90\n",
      "üîÑ Chain 2 in iteration 12/200, \n",
      " path: 322,  new path: 135,  docs changed path: 336,  Log-likelihood: -427797.75\n",
      "üîÑ Chain 1 in iteration 12/200, \n",
      " path: 331,  new path: 142,  docs changed path: 337,  Log-likelihood: -429725.38\n",
      "üîÑ Chain 3 in iteration 12/200, \n",
      " path: 324,  new path: 132,  docs changed path: 304,  Log-likelihood: -428130.45\n",
      "üîÑ Chain 2 in iteration 13/200, \n",
      " path: 320,  new path: 135,  docs changed path: 333,  Log-likelihood: -428976.62\n",
      "üîÑ Chain 3 in iteration 13/200, \n",
      " path: 318,  new path: 133,  docs changed path: 310,  Log-likelihood: -428860.28\n",
      "üîÑ Chain 1 in iteration 13/200, \n",
      " path: 335,  new path: 144,  docs changed path: 341,  Log-likelihood: -429220.86\n",
      "üîÑ Chain 2 in iteration 14/200, \n",
      " path: 324,  new path: 138,  docs changed path: 330,  Log-likelihood: -428397.71\n",
      "üîÑ Chain 3 in iteration 14/200, \n",
      " path: 322,  new path: 136,  docs changed path: 307,  Log-likelihood: -428485.60\n",
      "üîÑ Chain 1 in iteration 14/200, \n",
      " path: 333,  new path: 141,  docs changed path: 345,  Log-likelihood: -429561.86\n",
      "üîÑ Chain 2 in iteration 15/200, \n",
      " path: 334,  new path: 155,  docs changed path: 340,  Log-likelihood: -428825.35\n",
      "üîÑ Chain 3 in iteration 15/200, \n",
      " path: 320,  new path: 128,  docs changed path: 304,  Log-likelihood: -428114.80\n",
      "üîÑ Chain 1 in iteration 15/200, \n",
      " path: 332,  new path: 141,  docs changed path: 340,  Log-likelihood: -430286.13\n",
      "üîÑ Chain 2 in iteration 16/200, \n",
      " path: 325,  new path: 138,  docs changed path: 335,  Log-likelihood: -428738.70\n",
      "üîÑ Chain 3 in iteration 16/200, \n",
      " path: 317,  new path: 114,  docs changed path: 294,  Log-likelihood: -427812.49\n",
      "üîÑ Chain 1 in iteration 16/200, \n",
      " path: 333,  new path: 137,  docs changed path: 322,  Log-likelihood: -429190.84\n",
      "üîÑ Chain 2 in iteration 17/200, \n",
      " path: 330,  new path: 144,  docs changed path: 327,  Log-likelihood: -428570.70\n",
      "üîÑ Chain 3 in iteration 17/200, \n",
      " path: 322,  new path: 132,  docs changed path: 312,  Log-likelihood: -428516.64\n",
      "üîÑ Chain 1 in iteration 17/200, \n",
      " path: 323,  new path: 133,  docs changed path: 328,  Log-likelihood: -430757.30\n",
      "üîÑ Chain 2 in iteration 18/200, \n",
      " path: 320,  new path: 135,  docs changed path: 333,  Log-likelihood: -429048.49\n",
      "üîÑ Chain 3 in iteration 18/200, \n",
      " path: 315,  new path: 111,  docs changed path: 307,  Log-likelihood: -429437.56\n",
      "üîÑ Chain 1 in iteration 18/200, \n",
      " path: 325,  new path: 127,  docs changed path: 313,  Log-likelihood: -429808.44\n",
      "üîÑ Chain 2 in iteration 19/200, \n",
      " path: 318,  new path: 134,  docs changed path: 326,  Log-likelihood: -429602.26\n",
      "üîÑ Chain 3 in iteration 19/200, \n",
      " path: 324,  new path: 129,  docs changed path: 302,  Log-likelihood: -428686.49\n",
      "üîÑ Chain 1 in iteration 19/200, \n",
      " path: 336,  new path: 141,  docs changed path: 328,  Log-likelihood: -429833.25\n",
      "üîÑ Chain 2 in iteration 20/200, \n",
      " path: 324,  new path: 128,  docs changed path: 313,  Log-likelihood: -428492.02\n",
      "üîÑ Chain 3 in iteration 20/200, \n",
      " path: 308,  new path: 117,  docs changed path: 279,  Log-likelihood: -428598.49\n",
      "üîÑ Chain 1 in iteration 20/200, \n",
      " path: 328,  new path: 124,  docs changed path: 323,  Log-likelihood: -429750.37\n",
      "üîÑ Chain 2 in iteration 21/200, \n",
      " path: 312,  new path: 124,  docs changed path: 306,  Log-likelihood: -431505.50\n",
      "üîÑ Chain 3 in iteration 21/200, \n",
      " path: 320,  new path: 126,  docs changed path: 302,  Log-likelihood: -429034.95\n",
      "üîÑ Chain 1 in iteration 21/200, \n",
      " path: 334,  new path: 138,  docs changed path: 335,  Log-likelihood: -431051.92\n",
      "üîÑ Chain 2 in iteration 22/200, \n",
      " path: 321,  new path: 122,  docs changed path: 304,  Log-likelihood: -430138.71\n",
      "üîÑ Chain 3 in iteration 22/200, \n",
      " path: 323,  new path: 122,  docs changed path: 290,  Log-likelihood: -428515.84\n",
      "üîÑ Chain 1 in iteration 22/200, \n",
      " path: 328,  new path: 129,  docs changed path: 333,  Log-likelihood: -431471.56\n",
      "üîÑ Chain 2 in iteration 23/200, \n",
      " path: 330,  new path: 139,  docs changed path: 307,  Log-likelihood: -428975.90\n",
      "üîÑ Chain 3 in iteration 23/200, \n",
      " path: 319,  new path: 110,  docs changed path: 290,  Log-likelihood: -429377.19\n",
      "üîÑ Chain 1 in iteration 23/200, \n",
      " path: 331,  new path: 125,  docs changed path: 338,  Log-likelihood: -430361.09\n",
      "üîÑ Chain 2 in iteration 24/200, \n",
      " path: 323,  new path: 135,  docs changed path: 313,  Log-likelihood: -429419.39\n",
      "üîÑ Chain 3 in iteration 24/200, \n",
      " path: 316,  new path: 117,  docs changed path: 295,  Log-likelihood: -429748.96\n",
      "üîÑ Chain 1 in iteration 24/200, \n",
      " path: 322,  new path: 121,  docs changed path: 311,  Log-likelihood: -430516.30\n",
      "üîÑ Chain 2 in iteration 25/200, \n",
      " path: 330,  new path: 133,  docs changed path: 316,  Log-likelihood: -430022.09\n",
      "üîÑ Chain 3 in iteration 25/200, \n",
      " path: 315,  new path: 116,  docs changed path: 297,  Log-likelihood: -430149.70\n",
      "üîÑ Chain 1 in iteration 25/200, \n",
      " path: 334,  new path: 133,  docs changed path: 311,  Log-likelihood: -430767.07\n",
      "üîÑ Chain 2 in iteration 26/200, \n",
      " path: 316,  new path: 129,  docs changed path: 316,  Log-likelihood: -430438.46\n",
      "üîÑ Chain 3 in iteration 26/200, \n",
      " path: 311,  new path: 122,  docs changed path: 309,  Log-likelihood: -430698.10\n",
      "üîÑ Chain 1 in iteration 26/200, \n",
      " path: 325,  new path: 124,  docs changed path: 312,  Log-likelihood: -431204.64\n",
      "üîÑ Chain 2 in iteration 27/200, \n",
      " path: 320,  new path: 135,  docs changed path: 302,  Log-likelihood: -429818.17\n",
      "üîÑ Chain 3 in iteration 27/200, \n",
      " path: 308,  new path: 117,  docs changed path: 297,  Log-likelihood: -430422.34\n",
      "üîÑ Chain 1 in iteration 27/200, \n",
      " path: 322,  new path: 124,  docs changed path: 314,  Log-likelihood: -431120.28\n",
      "üîÑ Chain 2 in iteration 28/200, \n",
      " path: 323,  new path: 127,  docs changed path: 307,  Log-likelihood: -429956.33\n",
      "üîÑ Chain 3 in iteration 28/200, \n",
      " path: 311,  new path: 120,  docs changed path: 295,  Log-likelihood: -430027.20\n",
      "üîÑ Chain 1 in iteration 28/200, \n",
      " path: 334,  new path: 126,  docs changed path: 300,  Log-likelihood: -430792.46\n",
      "üîÑ Chain 2 in iteration 29/200, \n",
      " path: 320,  new path: 132,  docs changed path: 319,  Log-likelihood: -430284.10\n",
      "üîÑ Chain 3 in iteration 29/200, \n",
      " path: 314,  new path: 113,  docs changed path: 288,  Log-likelihood: -430180.35\n",
      "üîÑ Chain 1 in iteration 29/200, \n",
      " path: 328,  new path: 128,  docs changed path: 314,  Log-likelihood: -430717.67\n",
      "üîÑ Chain 2 in iteration 30/200, \n",
      " path: 318,  new path: 133,  docs changed path: 323,  Log-likelihood: -429570.74\n",
      "üîÑ Chain 3 in iteration 30/200, \n",
      " path: 299,  new path: 99,  docs changed path: 292,  Log-likelihood: -431698.68\n",
      "üîÑ Chain 1 in iteration 30/200, \n",
      " path: 327,  new path: 117,  docs changed path: 306,  Log-likelihood: -431759.17\n",
      "üîÑ Chain 2 in iteration 31/200, \n",
      " path: 320,  new path: 123,  docs changed path: 321,  Log-likelihood: -431158.46\n",
      "üîÑ Chain 3 in iteration 31/200, \n",
      " path: 308,  new path: 109,  docs changed path: 286,  Log-likelihood: -430933.32\n",
      "üîÑ Chain 1 in iteration 31/200, \n",
      " path: 322,  new path: 118,  docs changed path: 298,  Log-likelihood: -432668.46\n",
      "üîÑ Chain 2 in iteration 32/200, \n",
      " path: 326,  new path: 133,  docs changed path: 321,  Log-likelihood: -430626.25\n",
      "üîÑ Chain 3 in iteration 32/200, \n",
      " path: 316,  new path: 115,  docs changed path: 297,  Log-likelihood: -429821.74\n",
      "üîÑ Chain 1 in iteration 32/200, \n",
      " path: 334,  new path: 127,  docs changed path: 320,  Log-likelihood: -431711.82\n",
      "üîÑ Chain 2 in iteration 33/200, \n",
      " path: 323,  new path: 133,  docs changed path: 316,  Log-likelihood: -430585.58\n",
      "üîÑ Chain 3 in iteration 33/200, \n",
      " path: 318,  new path: 121,  docs changed path: 300,  Log-likelihood: -430174.45\n",
      "üîÑ Chain 2 in iteration 34/200, \n",
      " path: 326,  new path: 125,  docs changed path: 312,  Log-likelihood: -429999.50\n",
      "üîÑ Chain 1 in iteration 33/200, \n",
      " path: 331,  new path: 121,  docs changed path: 320,  Log-likelihood: -431279.02\n",
      "üîÑ Chain 3 in iteration 34/200, \n",
      " path: 312,  new path: 120,  docs changed path: 288,  Log-likelihood: -430435.79\n",
      "üîÑ Chain 2 in iteration 35/200, \n",
      " path: 333,  new path: 141,  docs changed path: 319,  Log-likelihood: -429808.89\n",
      "üîÑ Chain 1 in iteration 34/200, \n",
      " path: 334,  new path: 124,  docs changed path: 313,  Log-likelihood: -431013.80\n",
      "üîÑ Chain 3 in iteration 35/200, \n",
      " path: 309,  new path: 116,  docs changed path: 293,  Log-likelihood: -430439.79\n",
      "üîÑ Chain 2 in iteration 36/200, \n",
      " path: 325,  new path: 123,  docs changed path: 311,  Log-likelihood: -431100.75\n",
      "üîÑ Chain 1 in iteration 35/200, \n",
      " path: 332,  new path: 119,  docs changed path: 314,  Log-likelihood: -430850.96\n",
      "üîÑ Chain 3 in iteration 36/200, \n",
      " path: 323,  new path: 130,  docs changed path: 314,  Log-likelihood: -429463.99\n",
      "üîÑ Chain 2 in iteration 37/200, \n",
      " path: 325,  new path: 125,  docs changed path: 295,  Log-likelihood: -430830.92\n",
      "üîÑ Chain 1 in iteration 36/200, \n",
      " path: 328,  new path: 118,  docs changed path: 317,  Log-likelihood: -431786.06\n",
      "üîÑ Chain 3 in iteration 37/200, \n",
      " path: 321,  new path: 119,  docs changed path: 320,  Log-likelihood: -429460.24\n",
      "üîÑ Chain 2 in iteration 38/200, \n",
      " path: 322,  new path: 127,  docs changed path: 306,  Log-likelihood: -429878.50\n",
      "üîÑ Chain 1 in iteration 37/200, \n",
      " path: 327,  new path: 125,  docs changed path: 324,  Log-likelihood: -432227.94\n",
      "üîÑ Chain 3 in iteration 38/200, \n",
      " path: 315,  new path: 115,  docs changed path: 301,  Log-likelihood: -430437.00\n",
      "üîÑ Chain 2 in iteration 39/200, \n",
      " path: 332,  new path: 139,  docs changed path: 331,  Log-likelihood: -429946.04\n",
      "üîÑ Chain 1 in iteration 38/200, \n",
      " path: 330,  new path: 124,  docs changed path: 316,  Log-likelihood: -431964.41\n",
      "üîÑ Chain 3 in iteration 39/200, \n",
      " path: 325,  new path: 120,  docs changed path: 304,  Log-likelihood: -430445.95\n",
      "üîÑ Chain 2 in iteration 40/200, \n",
      " path: 323,  new path: 126,  docs changed path: 317,  Log-likelihood: -430283.59\n",
      "üîÑ Chain 3 in iteration 40/200, \n",
      " path: 321,  new path: 115,  docs changed path: 302,  Log-likelihood: -430386.72\n",
      "üîÑ Chain 1 in iteration 39/200, \n",
      " path: 323,  new path: 112,  docs changed path: 318,  Log-likelihood: -431750.86\n",
      "üîÑ Chain 2 in iteration 41/200, \n",
      " path: 315,  new path: 135,  docs changed path: 312,  Log-likelihood: -431548.62\n",
      "üîÑ Chain 3 in iteration 41/200, \n",
      " path: 325,  new path: 118,  docs changed path: 307,  Log-likelihood: -429627.47\n",
      "üîÑ Chain 1 in iteration 40/200, \n",
      " path: 335,  new path: 121,  docs changed path: 324,  Log-likelihood: -432639.46\n",
      "üîÑ Chain 2 in iteration 42/200, \n",
      " path: 325,  new path: 134,  docs changed path: 316,  Log-likelihood: -430683.72\n",
      "üîÑ Chain 3 in iteration 42/200, \n",
      " path: 322,  new path: 111,  docs changed path: 311,  Log-likelihood: -429953.65\n",
      "üîÑ Chain 1 in iteration 41/200, \n",
      " path: 330,  new path: 119,  docs changed path: 318,  Log-likelihood: -431767.31\n",
      "üîÑ Chain 2 in iteration 43/200, \n",
      " path: 325,  new path: 134,  docs changed path: 322,  Log-likelihood: -430685.23\n",
      "üîÑ Chain 3 in iteration 43/200, \n",
      " path: 332,  new path: 126,  docs changed path: 316,  Log-likelihood: -429064.71\n",
      "üîÑ Chain 1 in iteration 42/200, \n",
      " path: 320,  new path: 115,  docs changed path: 323,  Log-likelihood: -432577.73\n",
      "üîÑ Chain 2 in iteration 44/200, \n",
      " path: 325,  new path: 131,  docs changed path: 330,  Log-likelihood: -431372.41\n",
      "üîÑ Chain 3 in iteration 44/200, \n",
      " path: 327,  new path: 130,  docs changed path: 314,  Log-likelihood: -429269.39\n",
      "üîÑ Chain 1 in iteration 43/200, \n",
      " path: 331,  new path: 117,  docs changed path: 310,  Log-likelihood: -433351.86\n",
      "üîÑ Chain 2 in iteration 45/200, \n",
      " path: 333,  new path: 128,  docs changed path: 327,  Log-likelihood: -430584.93\n",
      "üîÑ Chain 3 in iteration 45/200, \n",
      " path: 327,  new path: 120,  docs changed path: 312,  Log-likelihood: -429355.64\n",
      "üîÑ Chain 1 in iteration 44/200, \n",
      " path: 330,  new path: 118,  docs changed path: 314,  Log-likelihood: -432409.79\n",
      "üîÑ Chain 2 in iteration 46/200, \n",
      " path: 331,  new path: 133,  docs changed path: 325,  Log-likelihood: -430807.35\n",
      "üîÑ Chain 3 in iteration 46/200, \n",
      " path: 316,  new path: 115,  docs changed path: 294,  Log-likelihood: -429970.78\n",
      "üîÑ Chain 1 in iteration 45/200, \n",
      " path: 330,  new path: 116,  docs changed path: 305,  Log-likelihood: -432033.06\n",
      "üîÑ Chain 2 in iteration 47/200, \n",
      " path: 325,  new path: 126,  docs changed path: 326,  Log-likelihood: -431815.98\n",
      "üîÑ Chain 3 in iteration 47/200, \n",
      " path: 321,  new path: 114,  docs changed path: 290,  Log-likelihood: -429243.46\n",
      "üîÑ Chain 1 in iteration 46/200, \n",
      " path: 337,  new path: 133,  docs changed path: 323,  Log-likelihood: -431488.60\n",
      "üîÑ Chain 2 in iteration 48/200, \n",
      " path: 328,  new path: 134,  docs changed path: 318,  Log-likelihood: -430668.15\n",
      "üîÑ Chain 3 in iteration 48/200, \n",
      " path: 326,  new path: 116,  docs changed path: 295,  Log-likelihood: -429466.25\n",
      "üîÑ Chain 1 in iteration 47/200, \n",
      " path: 328,  new path: 116,  docs changed path: 331,  Log-likelihood: -431769.43\n",
      "üîÑ Chain 2 in iteration 49/200, \n",
      " path: 326,  new path: 134,  docs changed path: 324,  Log-likelihood: -431140.23\n",
      "üîÑ Chain 3 in iteration 49/200, \n",
      " path: 323,  new path: 114,  docs changed path: 314,  Log-likelihood: -429134.67\n",
      "üîÑ Chain 1 in iteration 48/200, \n",
      " path: 326,  new path: 121,  docs changed path: 320,  Log-likelihood: -431397.81\n",
      "üîÑ Chain 2 in iteration 50/200, \n",
      " path: 324,  new path: 135,  docs changed path: 327,  Log-likelihood: -430840.81\n",
      "üîÑ Chain 3 in iteration 50/200, \n",
      " path: 329,  new path: 116,  docs changed path: 310,  Log-likelihood: -429016.78\n",
      "üîÑ Chain 1 in iteration 49/200, \n",
      " path: 325,  new path: 115,  docs changed path: 324,  Log-likelihood: -431903.72\n",
      "üîÑ Chain 2 in iteration 51/200, \n",
      " path: 325,  new path: 127,  docs changed path: 320,  Log-likelihood: -432678.90\n",
      "üîÑ Chain 3 in iteration 51/200, \n",
      " path: 329,  new path: 121,  docs changed path: 301,  Log-likelihood: -429852.44\n",
      "üîÑ Chain 1 in iteration 50/200, \n",
      " path: 331,  new path: 121,  docs changed path: 330,  Log-likelihood: -430842.47\n",
      "üîÑ Chain 2 in iteration 52/200, \n",
      " path: 322,  new path: 126,  docs changed path: 310,  Log-likelihood: -431624.80\n",
      "üîÑ Chain 3 in iteration 52/200, \n",
      " path: 325,  new path: 116,  docs changed path: 299,  Log-likelihood: -430592.87\n",
      "üîÑ Chain 1 in iteration 51/200, \n",
      " path: 332,  new path: 112,  docs changed path: 324,  Log-likelihood: -430978.02\n",
      "üîÑ Chain 2 in iteration 53/200, \n",
      " path: 326,  new path: 131,  docs changed path: 332,  Log-likelihood: -431880.45\n",
      "üîÑ Chain 3 in iteration 53/200, \n",
      " path: 318,  new path: 108,  docs changed path: 307,  Log-likelihood: -430740.54\n",
      "üîÑ Chain 1 in iteration 52/200, \n",
      " path: 325,  new path: 122,  docs changed path: 327,  Log-likelihood: -431848.49\n",
      "üîÑ Chain 2 in iteration 54/200, \n",
      " path: 320,  new path: 123,  docs changed path: 317,  Log-likelihood: -431844.35\n",
      "üîÑ Chain 3 in iteration 54/200, \n",
      " path: 321,  new path: 118,  docs changed path: 304,  Log-likelihood: -430658.82\n",
      "üîÑ Chain 1 in iteration 53/200, \n",
      " path: 327,  new path: 121,  docs changed path: 319,  Log-likelihood: -431184.07\n",
      "üîÑ Chain 2 in iteration 55/200, \n",
      " path: 323,  new path: 127,  docs changed path: 322,  Log-likelihood: -432264.36\n",
      "üîÑ Chain 3 in iteration 55/200, \n",
      " path: 322,  new path: 116,  docs changed path: 320,  Log-likelihood: -431175.09\n",
      "üîÑ Chain 1 in iteration 54/200, \n",
      " path: 338,  new path: 132,  docs changed path: 320,  Log-likelihood: -430285.77\n",
      "üîÑ Chain 2 in iteration 56/200, \n",
      " path: 327,  new path: 135,  docs changed path: 318,  Log-likelihood: -431976.25\n",
      "üîÑ Chain 3 in iteration 56/200, \n",
      " path: 321,  new path: 110,  docs changed path: 308,  Log-likelihood: -429957.68\n",
      "üîÑ Chain 1 in iteration 55/200, \n",
      " path: 330,  new path: 118,  docs changed path: 316,  Log-likelihood: -431310.31\n",
      "üîÑ Chain 2 in iteration 57/200, \n",
      " path: 323,  new path: 129,  docs changed path: 321,  Log-likelihood: -432101.02\n",
      "üîÑ Chain 3 in iteration 57/200, \n",
      " path: 330,  new path: 121,  docs changed path: 302,  Log-likelihood: -429617.04\n",
      "üîÑ Chain 1 in iteration 56/200, \n",
      " path: 336,  new path: 132,  docs changed path: 324,  Log-likelihood: -430815.92\n",
      "üîÑ Chain 2 in iteration 58/200, \n",
      " path: 323,  new path: 133,  docs changed path: 307,  Log-likelihood: -431933.22\n",
      "üîÑ Chain 3 in iteration 58/200, \n",
      " path: 324,  new path: 114,  docs changed path: 302,  Log-likelihood: -429365.77\n",
      "üîÑ Chain 1 in iteration 57/200, \n",
      " path: 333,  new path: 124,  docs changed path: 337,  Log-likelihood: -431166.82\n",
      "üîÑ Chain 2 in iteration 59/200, \n",
      " path: 323,  new path: 126,  docs changed path: 316,  Log-likelihood: -431912.37\n",
      "üîÑ Chain 3 in iteration 59/200, \n",
      " path: 315,  new path: 109,  docs changed path: 314,  Log-likelihood: -430322.90\n",
      "üîÑ Chain 2 in iteration 60/200, \n",
      " path: 325,  new path: 133,  docs changed path: 318,  Log-likelihood: -431947.00\n",
      "üîÑ Chain 1 in iteration 58/200, \n",
      " path: 328,  new path: 115,  docs changed path: 336,  Log-likelihood: -431559.89\n",
      "üîÑ Chain 3 in iteration 60/200, \n",
      " path: 320,  new path: 114,  docs changed path: 305,  Log-likelihood: -429722.42\n",
      "üîÑ Chain 2 in iteration 61/200, \n",
      " path: 318,  new path: 131,  docs changed path: 318,  Log-likelihood: -432422.15\n",
      "üîÑ Chain 1 in iteration 59/200, \n",
      " path: 323,  new path: 120,  docs changed path: 327,  Log-likelihood: -430998.31\n",
      "üîÑ Chain 3 in iteration 61/200, \n",
      " path: 319,  new path: 118,  docs changed path: 330,  Log-likelihood: -429108.35\n",
      "üîÑ Chain 2 in iteration 62/200, \n",
      " path: 325,  new path: 127,  docs changed path: 313,  Log-likelihood: -432423.17\n",
      "üîÑ Chain 1 in iteration 60/200, \n",
      " path: 332,  new path: 131,  docs changed path: 332,  Log-likelihood: -430786.51\n",
      "üîÑ Chain 3 in iteration 62/200, \n",
      " path: 321,  new path: 111,  docs changed path: 316,  Log-likelihood: -430724.84\n",
      "üîÑ Chain 2 in iteration 63/200, \n",
      " path: 322,  new path: 131,  docs changed path: 333,  Log-likelihood: -433027.94\n",
      "üîÑ Chain 1 in iteration 61/200, \n",
      " path: 329,  new path: 114,  docs changed path: 331,  Log-likelihood: -431154.61\n",
      "üîÑ Chain 3 in iteration 63/200, \n",
      " path: 317,  new path: 118,  docs changed path: 334,  Log-likelihood: -429598.04\n",
      "üîÑ Chain 2 in iteration 64/200, \n",
      " path: 315,  new path: 124,  docs changed path: 319,  Log-likelihood: -433200.00\n",
      "üîÑ Chain 1 in iteration 62/200, \n",
      " path: 329,  new path: 120,  docs changed path: 305,  Log-likelihood: -431168.10\n",
      "üîÑ Chain 3 in iteration 64/200, \n",
      " path: 332,  new path: 122,  docs changed path: 324,  Log-likelihood: -429457.05\n",
      "üîÑ Chain 2 in iteration 65/200, \n",
      " path: 321,  new path: 128,  docs changed path: 315,  Log-likelihood: -433197.89\n",
      "üîÑ Chain 1 in iteration 63/200, \n",
      " path: 318,  new path: 105,  docs changed path: 317,  Log-likelihood: -430898.26\n",
      "üîÑ Chain 3 in iteration 65/200, \n",
      " path: 319,  new path: 103,  docs changed path: 318,  Log-likelihood: -429043.04\n",
      "üîÑ Chain 2 in iteration 66/200, \n",
      " path: 322,  new path: 132,  docs changed path: 314,  Log-likelihood: -433181.83\n",
      "üîÑ Chain 1 in iteration 64/200, \n",
      " path: 327,  new path: 116,  docs changed path: 295,  Log-likelihood: -430728.22\n",
      "üîÑ Chain 3 in iteration 66/200, \n",
      " path: 332,  new path: 135,  docs changed path: 311,  Log-likelihood: -429116.68\n",
      "üîÑ Chain 2 in iteration 67/200, \n",
      " path: 319,  new path: 127,  docs changed path: 318,  Log-likelihood: -433314.67\n",
      "üîÑ Chain 1 in iteration 65/200, \n",
      " path: 322,  new path: 114,  docs changed path: 319,  Log-likelihood: -432082.58\n",
      "üîÑ Chain 3 in iteration 67/200, \n",
      " path: 332,  new path: 127,  docs changed path: 307,  Log-likelihood: -429260.29\n",
      "üîÑ Chain 2 in iteration 68/200, \n",
      " path: 318,  new path: 128,  docs changed path: 317,  Log-likelihood: -434019.07\n",
      "üîÑ Chain 1 in iteration 66/200, \n",
      " path: 329,  new path: 128,  docs changed path: 307,  Log-likelihood: -431253.44\n",
      "üîÑ Chain 3 in iteration 68/200, \n",
      " path: 322,  new path: 122,  docs changed path: 307,  Log-likelihood: -429295.90\n",
      "üîÑ Chain 2 in iteration 69/200, \n",
      " path: 327,  new path: 134,  docs changed path: 327,  Log-likelihood: -433027.90\n",
      "üîÑ Chain 1 in iteration 67/200, \n",
      " path: 331,  new path: 122,  docs changed path: 312,  Log-likelihood: -431182.91\n",
      "üîÑ Chain 3 in iteration 69/200, \n",
      " path: 331,  new path: 120,  docs changed path: 317,  Log-likelihood: -429299.35\n",
      "üîÑ Chain 2 in iteration 70/200, \n",
      " path: 328,  new path: 134,  docs changed path: 315,  Log-likelihood: -432628.47\n",
      "üîÑ Chain 1 in iteration 68/200, \n",
      " path: 328,  new path: 118,  docs changed path: 319,  Log-likelihood: -431303.03\n",
      "üîÑ Chain 3 in iteration 70/200, \n",
      " path: 328,  new path: 125,  docs changed path: 314,  Log-likelihood: -429619.73\n",
      "üîÑ Chain 2 in iteration 71/200, \n",
      " path: 325,  new path: 128,  docs changed path: 314,  Log-likelihood: -432070.59\n",
      "üîÑ Chain 1 in iteration 69/200, \n",
      " path: 333,  new path: 132,  docs changed path: 316,  Log-likelihood: -431197.80\n",
      "üîÑ Chain 3 in iteration 71/200, \n",
      " path: 330,  new path: 123,  docs changed path: 311,  Log-likelihood: -429488.72\n",
      "üîÑ Chain 2 in iteration 72/200, \n",
      " path: 325,  new path: 127,  docs changed path: 316,  Log-likelihood: -432779.07\n",
      "üîÑ Chain 1 in iteration 70/200, \n",
      " path: 332,  new path: 126,  docs changed path: 315,  Log-likelihood: -430846.38\n",
      "üîÑ Chain 3 in iteration 72/200, \n",
      " path: 324,  new path: 115,  docs changed path: 308,  Log-likelihood: -428896.05\n",
      "üîÑ Chain 2 in iteration 73/200, \n",
      " path: 326,  new path: 127,  docs changed path: 312,  Log-likelihood: -432859.33\n",
      "üîÑ Chain 1 in iteration 71/200, \n",
      " path: 331,  new path: 131,  docs changed path: 319,  Log-likelihood: -430595.81\n",
      "üîÑ Chain 3 in iteration 73/200, \n",
      " path: 326,  new path: 106,  docs changed path: 309,  Log-likelihood: -430001.04\n",
      "üîÑ Chain 2 in iteration 74/200, \n",
      " path: 319,  new path: 120,  docs changed path: 310,  Log-likelihood: -432870.46\n",
      "üîÑ Chain 1 in iteration 72/200, \n",
      " path: 332,  new path: 122,  docs changed path: 316,  Log-likelihood: -430970.45\n",
      "üîÑ Chain 3 in iteration 74/200, \n",
      " path: 329,  new path: 114,  docs changed path: 311,  Log-likelihood: -428473.22\n",
      "üîÑ Chain 2 in iteration 75/200, \n",
      " path: 318,  new path: 118,  docs changed path: 314,  Log-likelihood: -433403.75\n",
      "üîÑ Chain 1 in iteration 73/200, \n",
      " path: 331,  new path: 121,  docs changed path: 317,  Log-likelihood: -429579.02\n",
      "üîÑ Chain 3 in iteration 75/200, \n",
      " path: 329,  new path: 129,  docs changed path: 297,  Log-likelihood: -429197.55\n",
      "üîÑ Chain 2 in iteration 76/200, \n",
      " path: 321,  new path: 121,  docs changed path: 302,  Log-likelihood: -432994.64\n",
      "üîÑ Chain 1 in iteration 74/200, \n",
      " path: 335,  new path: 117,  docs changed path: 326,  Log-likelihood: -430527.77\n",
      "üîÑ Chain 3 in iteration 76/200, \n",
      " path: 332,  new path: 124,  docs changed path: 305,  Log-likelihood: -429685.35\n",
      "üîÑ Chain 2 in iteration 77/200, \n",
      " path: 327,  new path: 130,  docs changed path: 306,  Log-likelihood: -432684.69\n",
      "üîÑ Chain 1 in iteration 75/200, \n",
      " path: 328,  new path: 124,  docs changed path: 323,  Log-likelihood: -430791.44\n",
      "check_dual_convergence_with_gelman_rubin: chain_len_docs:{'2': [556, 548, 556, 560, 556, 550, 557, 554, 566, 563, 566, 561, 560, 563, 559, 558, 559, 560, 565, 564, 564], '3': [556, 555, 552, 558, 559, 566, 554, 547, 549, 542, 542, 538, 546, 545, 547, 561, 565, 554, 554, 552, 559], '1': [534, 532, 527, 522, 519, 522, 525, 524, 528, 533, 545, 540, 545, 535, 534, 537, 535, 542, 536, 531, 536]}, mean_jaccard:[0.948466922294273, 0.9403888629251697, 0.9290941567238067], mean_ratio:[0.20000000000000004, 0.20000000000000004, 0.20000000000000004]\n",
      "ü•∞ Convergence status: ‚ùå Not converged...\n",
      "ü•∞  R-hat info: 2.2808Ôºåmean_jaccard:[0.948466922294273, 0.9403888629251697, 0.9290941567238067], mean_ratio:[0.20000000000000004, 0.20000000000000004, 0.20000000000000004]\n",
      "üîÑ Chain 3 in iteration 77/200, \n",
      " path: 338,  new path: 140,  docs changed path: 313,  Log-likelihood: -429665.05\n",
      "üîÑ Chain 2 in iteration 78/200, \n",
      " path: 325,  new path: 122,  docs changed path: 321,  Log-likelihood: -432793.66\n",
      "üîÑ Chain 1 in iteration 76/200, \n",
      " path: 332,  new path: 126,  docs changed path: 324,  Log-likelihood: -431070.96\n",
      "üîÑ Chain 2 in iteration 79/200, \n",
      " path: 334,  new path: 134,  docs changed path: 324,  Log-likelihood: -432610.76\n",
      "üîÑ Chain 3 in iteration 78/200, \n",
      " path: 334,  new path: 116,  docs changed path: 319,  Log-likelihood: -429507.16\n",
      "üîÑ Chain 1 in iteration 77/200, \n",
      " path: 335,  new path: 120,  docs changed path: 317,  Log-likelihood: -430127.18\n",
      "üîÑ Chain 2 in iteration 80/200, \n",
      " path: 322,  new path: 121,  docs changed path: 322,  Log-likelihood: -432448.99\n",
      "üîÑ Chain 3 in iteration 79/200, \n",
      " path: 335,  new path: 119,  docs changed path: 294,  Log-likelihood: -428701.32\n",
      "üîÑ Chain 1 in iteration 78/200, \n",
      " path: 333,  new path: 123,  docs changed path: 321,  Log-likelihood: -431523.59\n",
      "üîÑ Chain 2 in iteration 81/200, \n",
      " path: 323,  new path: 130,  docs changed path: 313,  Log-likelihood: -432244.38\n",
      "üîÑ Chain 3 in iteration 80/200, \n",
      " path: 339,  new path: 139,  docs changed path: 323,  Log-likelihood: -428769.99\n",
      "üîÑ Chain 2 in iteration 82/200, \n",
      " path: 317,  new path: 118,  docs changed path: 306,  Log-likelihood: -432304.90\n",
      "üîÑ Chain 1 in iteration 79/200, \n",
      " path: 335,  new path: 120,  docs changed path: 318,  Log-likelihood: -431252.16\n",
      "üîÑ Chain 3 in iteration 81/200, \n",
      " path: 343,  new path: 135,  docs changed path: 328,  Log-likelihood: -428259.79\n",
      "üîÑ Chain 2 in iteration 83/200, \n",
      " path: 318,  new path: 121,  docs changed path: 310,  Log-likelihood: -432745.82\n",
      "üîÑ Chain 1 in iteration 80/200, \n",
      " path: 328,  new path: 114,  docs changed path: 323,  Log-likelihood: -431525.58\n",
      "üîÑ Chain 3 in iteration 82/200, \n",
      " path: 338,  new path: 128,  docs changed path: 315,  Log-likelihood: -428032.68\n",
      "üîÑ Chain 2 in iteration 84/200, \n",
      " path: 315,  new path: 115,  docs changed path: 297,  Log-likelihood: -433478.71\n",
      "üîÑ Chain 1 in iteration 81/200, \n",
      " path: 334,  new path: 123,  docs changed path: 323,  Log-likelihood: -430738.97\n",
      "üîÑ Chain 3 in iteration 83/200, \n",
      " path: 333,  new path: 127,  docs changed path: 315,  Log-likelihood: -428503.31\n",
      "üîÑ Chain 2 in iteration 85/200, \n",
      " path: 320,  new path: 116,  docs changed path: 296,  Log-likelihood: -433969.74\n",
      "üîÑ Chain 1 in iteration 82/200, \n",
      " path: 334,  new path: 115,  docs changed path: 310,  Log-likelihood: -431058.58\n",
      "üîÑ Chain 3 in iteration 84/200, \n",
      " path: 333,  new path: 129,  docs changed path: 338,  Log-likelihood: -428732.65\n",
      "üîÑ Chain 2 in iteration 86/200, \n",
      " path: 320,  new path: 110,  docs changed path: 300,  Log-likelihood: -433534.83\n",
      "üîÑ Chain 1 in iteration 83/200, \n",
      " path: 330,  new path: 109,  docs changed path: 298,  Log-likelihood: -431245.95\n",
      "üîÑ Chain 3 in iteration 85/200, \n",
      " path: 339,  new path: 128,  docs changed path: 330,  Log-likelihood: -428178.77\n",
      "üîÑ Chain 2 in iteration 87/200, \n",
      " path: 318,  new path: 116,  docs changed path: 318,  Log-likelihood: -432710.85\n",
      "üîÑ Chain 1 in iteration 84/200, \n",
      " path: 333,  new path: 119,  docs changed path: 316,  Log-likelihood: -431454.22\n",
      "üîÑ Chain 3 in iteration 86/200, \n",
      " path: 333,  new path: 132,  docs changed path: 323,  Log-likelihood: -428613.93\n",
      "üîÑ Chain 2 in iteration 88/200, \n",
      " path: 316,  new path: 112,  docs changed path: 307,  Log-likelihood: -432662.40\n",
      "üîÑ Chain 1 in iteration 85/200, \n",
      " path: 326,  new path: 106,  docs changed path: 304,  Log-likelihood: -431321.30\n",
      "üîÑ Chain 3 in iteration 87/200, \n",
      " path: 336,  new path: 119,  docs changed path: 312,  Log-likelihood: -428163.17\n",
      "üîÑ Chain 2 in iteration 89/200, \n",
      " path: 317,  new path: 113,  docs changed path: 290,  Log-likelihood: -432575.70\n",
      "üîÑ Chain 1 in iteration 86/200, \n",
      " path: 328,  new path: 117,  docs changed path: 311,  Log-likelihood: -430898.43\n",
      "üîÑ Chain 3 in iteration 88/200, \n",
      " path: 344,  new path: 128,  docs changed path: 316,  Log-likelihood: -427791.04\n",
      "üîÑ Chain 2 in iteration 90/200, \n",
      " path: 319,  new path: 116,  docs changed path: 317,  Log-likelihood: -433079.10\n",
      "üîÑ Chain 1 in iteration 87/200, \n",
      " path: 323,  new path: 109,  docs changed path: 305,  Log-likelihood: -432031.33\n",
      "üîÑ Chain 3 in iteration 89/200, \n",
      " path: 335,  new path: 118,  docs changed path: 324,  Log-likelihood: -428117.13\n",
      "üîÑ Chain 2 in iteration 91/200, \n",
      " path: 328,  new path: 123,  docs changed path: 325,  Log-likelihood: -432775.62\n",
      "üîÑ Chain 1 in iteration 88/200, \n",
      " path: 333,  new path: 116,  docs changed path: 297,  Log-likelihood: -431212.42\n",
      "üîÑ Chain 3 in iteration 90/200, \n",
      " path: 338,  new path: 130,  docs changed path: 310,  Log-likelihood: -428784.80\n",
      "üîÑ Chain 2 in iteration 92/200, \n",
      " path: 320,  new path: 116,  docs changed path: 313,  Log-likelihood: -432997.99\n",
      "üîÑ Chain 1 in iteration 89/200, \n",
      " path: 323,  new path: 112,  docs changed path: 307,  Log-likelihood: -431212.66\n",
      "üîÑ Chain 3 in iteration 91/200, \n",
      " path: 343,  new path: 127,  docs changed path: 309,  Log-likelihood: -428056.04\n",
      "üîÑ Chain 2 in iteration 93/200, \n",
      " path: 326,  new path: 119,  docs changed path: 321,  Log-likelihood: -432169.20\n",
      "üîÑ Chain 1 in iteration 90/200, \n",
      " path: 327,  new path: 99,  docs changed path: 300,  Log-likelihood: -431676.38\n",
      "üîÑ Chain 3 in iteration 92/200, \n",
      " path: 340,  new path: 127,  docs changed path: 320,  Log-likelihood: -427687.83\n",
      "üîÑ Chain 2 in iteration 94/200, \n",
      " path: 327,  new path: 121,  docs changed path: 323,  Log-likelihood: -432968.76\n",
      "üîÑ Chain 1 in iteration 91/200, \n",
      " path: 329,  new path: 118,  docs changed path: 313,  Log-likelihood: -431663.72\n",
      "üîÑ Chain 3 in iteration 93/200, \n",
      " path: 334,  new path: 124,  docs changed path: 305,  Log-likelihood: -428916.30\n",
      "üîÑ Chain 2 in iteration 95/200, \n",
      " path: 330,  new path: 126,  docs changed path: 328,  Log-likelihood: -432263.24\n",
      "üîÑ Chain 1 in iteration 92/200, \n",
      " path: 330,  new path: 125,  docs changed path: 314,  Log-likelihood: -431094.65\n",
      "üîÑ Chain 3 in iteration 94/200, \n",
      " path: 356,  new path: 141,  docs changed path: 312,  Log-likelihood: -427227.80\n",
      "üîÑ Chain 2 in iteration 96/200, \n",
      " path: 318,  new path: 121,  docs changed path: 310,  Log-likelihood: -432925.10\n",
      "üîÑ Chain 1 in iteration 93/200, \n",
      " path: 322,  new path: 106,  docs changed path: 313,  Log-likelihood: -431846.45\n",
      "üîÑ Chain 3 in iteration 95/200, \n",
      " path: 344,  new path: 130,  docs changed path: 323,  Log-likelihood: -427809.53\n",
      "üîÑ Chain 2 in iteration 97/200, \n",
      " path: 326,  new path: 121,  docs changed path: 314,  Log-likelihood: -432678.92\n",
      "üîÑ Chain 1 in iteration 94/200, \n",
      " path: 324,  new path: 115,  docs changed path: 299,  Log-likelihood: -431509.19\n",
      "üîÑ Chain 2 in iteration 98/200, \n",
      " path: 333,  new path: 123,  docs changed path: 311,  Log-likelihood: -431867.87\n",
      "üîÑ Chain 3 in iteration 96/200, \n",
      " path: 336,  new path: 119,  docs changed path: 304,  Log-likelihood: -427979.52\n",
      "üîÑ Chain 1 in iteration 95/200, \n",
      " path: 324,  new path: 109,  docs changed path: 297,  Log-likelihood: -431507.46\n",
      "check_dual_convergence_with_gelman_rubin: chain_len_docs:{'2': [564, 565, 563, 564, 557, 556, 562, 556, 566, 568, 566, 575, 573, 568, 565, 557, 562, 561, 558, 553, 553], '3': [559, 557, 561, 550, 546, 537, 543, 550, 557, 551, 549, 540, 541, 549, 547, 551, 548, 541, 554, 560, 547], '1': [536, 534, 535, 545, 541, 525, 533, 539, 545, 545, 549, 552, 550, 562, 555, 562, 554, 556, 553, 552, 557]}, mean_jaccard:[0.9438286662960349, 0.9324210631928859, 0.9353135105469443], mean_ratio:[0.20000000000000004, 0.20000000000000004, 0.20000000000000004]\n",
      "ü•∞ Convergence status: ‚ùå Not converged...\n",
      "ü•∞  R-hat info: 1.4505Ôºåmean_jaccard:[0.9438286662960349, 0.9324210631928859, 0.9353135105469443], mean_ratio:[0.20000000000000004, 0.20000000000000004, 0.20000000000000004]\n",
      "üîÑ Chain 2 in iteration 99/200, \n",
      " path: 325,  new path: 115,  docs changed path: 314,  Log-likelihood: -431973.30\n",
      "üîÑ Chain 3 in iteration 97/200, \n",
      " path: 335,  new path: 119,  docs changed path: 292,  Log-likelihood: -428912.99\n",
      "üîÑ Chain 1 in iteration 96/200, \n",
      " path: 331,  new path: 123,  docs changed path: 305,  Log-likelihood: -430495.82\n",
      "üîÑ Chain 2 in iteration 100/200, \n",
      " path: 321,  new path: 110,  docs changed path: 313,  Log-likelihood: -432978.47\n",
      "üîÑ Chain 3 in iteration 98/200, \n",
      " path: 343,  new path: 137,  docs changed path: 318,  Log-likelihood: -427561.81\n",
      "üîÑ Chain 1 in iteration 97/200, \n",
      " path: 327,  new path: 117,  docs changed path: 303,  Log-likelihood: -430681.94\n",
      "üîÑ Chain 2 in iteration 101/200, \n",
      " path: 330,  new path: 126,  docs changed path: 339,  Log-likelihood: -431999.32\n",
      "üîÑ Chain 3 in iteration 99/200, \n",
      " path: 333,  new path: 119,  docs changed path: 313,  Log-likelihood: -428289.54\n",
      "üîÑ Chain 1 in iteration 98/200, \n",
      " path: 329,  new path: 111,  docs changed path: 298,  Log-likelihood: -430785.22\n",
      "üîÑ Chain 2 in iteration 102/200, \n",
      " path: 335,  new path: 123,  docs changed path: 330,  Log-likelihood: -431864.27\n",
      "üîÑ Chain 3 in iteration 100/200, \n",
      " path: 345,  new path: 133,  docs changed path: 311,  Log-likelihood: -427370.81\n",
      "üîÑ Chain 1 in iteration 99/200, \n",
      " path: 335,  new path: 123,  docs changed path: 302,  Log-likelihood: -431029.67\n",
      "üîÑ Chain 2 in iteration 103/200, \n",
      " path: 333,  new path: 126,  docs changed path: 341,  Log-likelihood: -432467.49\n",
      "üîÑ Chain 3 in iteration 101/200, \n",
      " path: 335,  new path: 133,  docs changed path: 305,  Log-likelihood: -428399.34\n",
      "üîÑ Chain 1 in iteration 100/200, \n",
      " path: 333,  new path: 112,  docs changed path: 309,  Log-likelihood: -431048.84\n",
      "üîÑ Chain 2 in iteration 104/200, \n",
      " path: 329,  new path: 124,  docs changed path: 336,  Log-likelihood: -432143.14\n",
      "üîÑ Chain 3 in iteration 102/200, \n",
      " path: 344,  new path: 139,  docs changed path: 324,  Log-likelihood: -427841.40\n",
      "üîÑ Chain 1 in iteration 101/200, \n",
      " path: 333,  new path: 116,  docs changed path: 306,  Log-likelihood: -431474.95\n",
      "üîÑ Chain 2 in iteration 105/200, \n",
      " path: 332,  new path: 127,  docs changed path: 332,  Log-likelihood: -432833.21\n",
      "üîÑ Chain 3 in iteration 103/200, \n",
      " path: 340,  new path: 124,  docs changed path: 311,  Log-likelihood: -428297.05\n",
      "üîÑ Chain 2 in iteration 106/200, \n",
      " path: 320,  new path: 108,  docs changed path: 330,  Log-likelihood: -433315.13\n",
      "üîÑ Chain 1 in iteration 102/200, \n",
      " path: 336,  new path: 119,  docs changed path: 299,  Log-likelihood: -431265.61\n",
      "üîÑ Chain 3 in iteration 104/200, \n",
      " path: 344,  new path: 131,  docs changed path: 323,  Log-likelihood: -427941.43\n",
      "üîÑ Chain 2 in iteration 107/200, \n",
      " path: 338,  new path: 136,  docs changed path: 329,  Log-likelihood: -432079.17\n",
      "üîÑ Chain 1 in iteration 103/200, \n",
      " path: 331,  new path: 108,  docs changed path: 298,  Log-likelihood: -431359.00\n",
      "üîÑ Chain 3 in iteration 105/200, \n",
      " path: 349,  new path: 132,  docs changed path: 313,  Log-likelihood: -427046.03\n",
      "üîÑ Chain 2 in iteration 108/200, \n",
      " path: 332,  new path: 120,  docs changed path: 343,  Log-likelihood: -432040.74\n",
      "üîÑ Chain 1 in iteration 104/200, \n",
      " path: 331,  new path: 110,  docs changed path: 316,  Log-likelihood: -431299.62\n",
      "üîÑ Chain 3 in iteration 106/200, \n",
      " path: 341,  new path: 124,  docs changed path: 322,  Log-likelihood: -428010.08\n",
      "üîÑ Chain 2 in iteration 109/200, \n",
      " path: 332,  new path: 128,  docs changed path: 324,  Log-likelihood: -432143.45\n",
      "üîÑ Chain 1 in iteration 105/200, \n",
      " path: 336,  new path: 107,  docs changed path: 301,  Log-likelihood: -430255.32\n",
      "üîÑ Chain 3 in iteration 107/200, \n",
      " path: 341,  new path: 128,  docs changed path: 326,  Log-likelihood: -427759.10\n",
      "üîÑ Chain 2 in iteration 110/200, \n",
      " path: 331,  new path: 116,  docs changed path: 336,  Log-likelihood: -432066.49\n",
      "üîÑ Chain 1 in iteration 106/200, \n",
      " path: 331,  new path: 111,  docs changed path: 292,  Log-likelihood: -431128.07\n",
      "üîÑ Chain 3 in iteration 108/200, \n",
      " path: 341,  new path: 116,  docs changed path: 328,  Log-likelihood: -428046.23\n",
      "üîÑ Chain 2 in iteration 111/200, \n",
      " path: 335,  new path: 127,  docs changed path: 330,  Log-likelihood: -432353.86\n",
      "üîÑ Chain 1 in iteration 107/200, \n",
      " path: 332,  new path: 112,  docs changed path: 293,  Log-likelihood: -430501.72\n",
      "üîÑ Chain 3 in iteration 109/200, \n",
      " path: 337,  new path: 127,  docs changed path: 318,  Log-likelihood: -427678.98\n",
      "üîÑ Chain 2 in iteration 112/200, \n",
      " path: 320,  new path: 116,  docs changed path: 320,  Log-likelihood: -432681.36\n",
      "üîÑ Chain 1 in iteration 108/200, \n",
      " path: 330,  new path: 122,  docs changed path: 320,  Log-likelihood: -429875.55\n",
      "üîÑ Chain 3 in iteration 110/200, \n",
      " path: 340,  new path: 126,  docs changed path: 297,  Log-likelihood: -427839.07\n",
      "üîÑ Chain 2 in iteration 113/200, \n",
      " path: 329,  new path: 121,  docs changed path: 308,  Log-likelihood: -432537.18\n",
      "üîÑ Chain 1 in iteration 109/200, \n",
      " path: 322,  new path: 109,  docs changed path: 296,  Log-likelihood: -431086.23\n",
      "üîÑ Chain 3 in iteration 111/200, \n",
      " path: 342,  new path: 123,  docs changed path: 298,  Log-likelihood: -427155.62\n",
      "üîÑ Chain 2 in iteration 114/200, \n",
      " path: 328,  new path: 119,  docs changed path: 318,  Log-likelihood: -432310.97\n",
      "üîÑ Chain 1 in iteration 110/200, \n",
      " path: 330,  new path: 115,  docs changed path: 306,  Log-likelihood: -431139.29\n",
      "üîÑ Chain 3 in iteration 112/200, \n",
      " path: 341,  new path: 123,  docs changed path: 297,  Log-likelihood: -427111.88\n",
      "üîÑ Chain 2 in iteration 115/200, \n",
      " path: 320,  new path: 119,  docs changed path: 319,  Log-likelihood: -433261.66\n",
      "üîÑ Chain 1 in iteration 111/200, \n",
      " path: 332,  new path: 111,  docs changed path: 302,  Log-likelihood: -430140.43\n",
      "üîÑ Chain 3 in iteration 113/200, \n",
      " path: 348,  new path: 137,  docs changed path: 321,  Log-likelihood: -426866.19\n",
      "üîÑ Chain 2 in iteration 116/200, \n",
      " path: 328,  new path: 123,  docs changed path: 313,  Log-likelihood: -432432.48\n",
      "üîÑ Chain 1 in iteration 112/200, \n",
      " path: 336,  new path: 114,  docs changed path: 310,  Log-likelihood: -430803.40\n",
      "üîÑ Chain 3 in iteration 114/200, \n",
      " path: 338,  new path: 120,  docs changed path: 304,  Log-likelihood: -427478.64\n",
      "üîÑ Chain 2 in iteration 117/200, \n",
      " path: 324,  new path: 116,  docs changed path: 307,  Log-likelihood: -433244.55\n",
      "üîÑ Chain 1 in iteration 113/200, \n",
      " path: 334,  new path: 118,  docs changed path: 307,  Log-likelihood: -430513.51\n",
      "üîÑ Chain 3 in iteration 115/200, \n",
      " path: 336,  new path: 115,  docs changed path: 315,  Log-likelihood: -428648.08\n",
      "üîÑ Chain 2 in iteration 118/200, \n",
      " path: 326,  new path: 119,  docs changed path: 309,  Log-likelihood: -432630.65\n",
      "üîÑ Chain 1 in iteration 114/200, \n",
      " path: 336,  new path: 126,  docs changed path: 308,  Log-likelihood: -429935.78\n",
      "üîÑ Chain 2 in iteration 119/200, \n",
      " path: 324,  new path: 129,  docs changed path: 313,  Log-likelihood: -433145.84\n",
      "üîÑ Chain 3 in iteration 116/200, \n",
      " path: 347,  new path: 138,  docs changed path: 320,  Log-likelihood: -427049.31\n",
      "üîÑ Chain 1 in iteration 115/200, \n",
      " path: 339,  new path: 117,  docs changed path: 319,  Log-likelihood: -430255.93\n",
      "check_dual_convergence_with_gelman_rubin: chain_len_docs:{'2': [553, 544, 547, 544, 551, 550, 551, 544, 540, 532, 524, 516, 521, 521, 515, 513, 528, 535, 535, 541, 548], '3': [547, 545, 549, 539, 553, 551, 545, 537, 542, 555, 558, 566, 562, 549, 551, 548, 545, 550, 552, 547, 538], '1': [557, 554, 564, 559, 556, 547, 550, 557, 558, 547, 544, 546, 545, 555, 556, 549, 553, 556, 543, 542, 544]}, mean_jaccard:[0.937759252879854, 0.9301974332985858, 0.9288266014624039], mean_ratio:[0.20000000000000004, 0.20000000000000004, 0.20000000000000004]\n",
      "ü•∞ Convergence status: ‚ùå Not converged...\n",
      "ü•∞  R-hat info: 1.3212Ôºåmean_jaccard:[0.937759252879854, 0.9301974332985858, 0.9288266014624039], mean_ratio:[0.20000000000000004, 0.20000000000000004, 0.20000000000000004]\n",
      "üîÑ Chain 2 in iteration 120/200, \n",
      " path: 322,  new path: 115,  docs changed path: 304,  Log-likelihood: -432963.86\n",
      "üîÑ Chain 3 in iteration 117/200, \n",
      " path: 350,  new path: 124,  docs changed path: 320,  Log-likelihood: -426988.84\n",
      "üîÑ Chain 1 in iteration 116/200, \n",
      " path: 336,  new path: 117,  docs changed path: 306,  Log-likelihood: -429785.60\n",
      "üîÑ Chain 2 in iteration 121/200, \n",
      " path: 323,  new path: 128,  docs changed path: 315,  Log-likelihood: -433442.73\n",
      "üîÑ Chain 3 in iteration 118/200, \n",
      " path: 335,  new path: 113,  docs changed path: 313,  Log-likelihood: -427623.28\n",
      "üîÑ Chain 1 in iteration 117/200, \n",
      " path: 346,  new path: 130,  docs changed path: 321,  Log-likelihood: -429503.44\n",
      "üîÑ Chain 2 in iteration 122/200, \n",
      " path: 333,  new path: 125,  docs changed path: 308,  Log-likelihood: -432574.95\n",
      "üîÑ Chain 3 in iteration 119/200, \n",
      " path: 347,  new path: 128,  docs changed path: 315,  Log-likelihood: -427592.76\n",
      "üîÑ Chain 1 in iteration 118/200, \n",
      " path: 330,  new path: 115,  docs changed path: 319,  Log-likelihood: -430669.35\n",
      "üîÑ Chain 2 in iteration 123/200, \n",
      " path: 330,  new path: 127,  docs changed path: 318,  Log-likelihood: -433057.18\n",
      "üîÑ Chain 3 in iteration 120/200, \n",
      " path: 345,  new path: 118,  docs changed path: 319,  Log-likelihood: -427012.30\n",
      "üîÑ Chain 1 in iteration 119/200, \n",
      " path: 340,  new path: 124,  docs changed path: 300,  Log-likelihood: -429719.65\n",
      "üîÑ Chain 2 in iteration 124/200, \n",
      " path: 325,  new path: 113,  docs changed path: 311,  Log-likelihood: -433560.01\n",
      "üîÑ Chain 3 in iteration 121/200, \n",
      " path: 343,  new path: 120,  docs changed path: 321,  Log-likelihood: -426712.99\n",
      "üîÑ Chain 1 in iteration 120/200, \n",
      " path: 330,  new path: 118,  docs changed path: 313,  Log-likelihood: -430689.39\n",
      "üîÑ Chain 2 in iteration 125/200, \n",
      " path: 315,  new path: 113,  docs changed path: 308,  Log-likelihood: -433901.27\n",
      "üîÑ Chain 3 in iteration 122/200, \n",
      " path: 342,  new path: 127,  docs changed path: 313,  Log-likelihood: -426708.52\n",
      "üîÑ Chain 1 in iteration 121/200, \n",
      " path: 350,  new path: 134,  docs changed path: 315,  Log-likelihood: -429979.29\n",
      "üîÑ Chain 2 in iteration 126/200, \n",
      " path: 326,  new path: 125,  docs changed path: 309,  Log-likelihood: -433888.51\n",
      "üîÑ Chain 3 in iteration 123/200, \n",
      " path: 341,  new path: 119,  docs changed path: 314,  Log-likelihood: -427157.80\n",
      "üîÑ Chain 1 in iteration 122/200, \n",
      " path: 338,  new path: 128,  docs changed path: 316,  Log-likelihood: -430035.68\n",
      "üîÑ Chain 2 in iteration 127/200, \n",
      " path: 331,  new path: 124,  docs changed path: 317,  Log-likelihood: -433509.37\n",
      "üîÑ Chain 3 in iteration 124/200, \n",
      " path: 333,  new path: 113,  docs changed path: 310,  Log-likelihood: -426900.89\n",
      "üîÑ Chain 2 in iteration 128/200, \n",
      " path: 322,  new path: 112,  docs changed path: 303,  Log-likelihood: -433473.33\n",
      "üîÑ Chain 1 in iteration 123/200, \n",
      " path: 345,  new path: 124,  docs changed path: 310,  Log-likelihood: -430202.20\n",
      "üîÑ Chain 3 in iteration 125/200, \n",
      " path: 349,  new path: 129,  docs changed path: 306,  Log-likelihood: -426198.31\n",
      "üîÑ Chain 2 in iteration 129/200, \n",
      " path: 324,  new path: 117,  docs changed path: 302,  Log-likelihood: -433038.43\n",
      "üîÑ Chain 1 in iteration 124/200, \n",
      " path: 333,  new path: 116,  docs changed path: 315,  Log-likelihood: -430859.27\n",
      "üîÑ Chain 3 in iteration 126/200, \n",
      " path: 336,  new path: 121,  docs changed path: 287,  Log-likelihood: -427575.85\n",
      "üîÑ Chain 2 in iteration 130/200, \n",
      " path: 320,  new path: 120,  docs changed path: 307,  Log-likelihood: -433908.17\n",
      "üîÑ Chain 1 in iteration 125/200, \n",
      " path: 339,  new path: 113,  docs changed path: 307,  Log-likelihood: -430632.30\n",
      "üîÑ Chain 3 in iteration 127/200, \n",
      " path: 342,  new path: 125,  docs changed path: 298,  Log-likelihood: -427698.26\n",
      "üîÑ Chain 2 in iteration 131/200, \n",
      " path: 320,  new path: 108,  docs changed path: 286,  Log-likelihood: -433685.37\n",
      "üîÑ Chain 1 in iteration 126/200, \n",
      " path: 339,  new path: 122,  docs changed path: 329,  Log-likelihood: -429662.81\n",
      "üîÑ Chain 3 in iteration 128/200, \n",
      " path: 344,  new path: 117,  docs changed path: 304,  Log-likelihood: -427340.92\n",
      "üîÑ Chain 2 in iteration 132/200, \n",
      " path: 319,  new path: 116,  docs changed path: 304,  Log-likelihood: -432937.91\n",
      "üîÑ Chain 1 in iteration 127/200, \n",
      " path: 344,  new path: 120,  docs changed path: 339,  Log-likelihood: -429584.63\n",
      "üîÑ Chain 3 in iteration 129/200, \n",
      " path: 329,  new path: 110,  docs changed path: 303,  Log-likelihood: -428300.93\n",
      "üîÑ Chain 2 in iteration 133/200, \n",
      " path: 317,  new path: 113,  docs changed path: 302,  Log-likelihood: -433424.66\n",
      "üîÑ Chain 1 in iteration 128/200, \n",
      " path: 343,  new path: 133,  docs changed path: 328,  Log-likelihood: -429809.23\n",
      "üîÑ Chain 3 in iteration 130/200, \n",
      " path: 328,  new path: 111,  docs changed path: 293,  Log-likelihood: -427445.03\n",
      "üîÑ Chain 2 in iteration 134/200, \n",
      " path: 313,  new path: 121,  docs changed path: 312,  Log-likelihood: -433561.13\n",
      "üîÑ Chain 1 in iteration 129/200, \n",
      " path: 340,  new path: 116,  docs changed path: 317,  Log-likelihood: -429053.65\n",
      "üîÑ Chain 3 in iteration 131/200, \n",
      " path: 334,  new path: 118,  docs changed path: 299,  Log-likelihood: -428222.03\n",
      "üîÑ Chain 2 in iteration 135/200, \n",
      " path: 326,  new path: 122,  docs changed path: 303,  Log-likelihood: -432963.67\n",
      "üîÑ Chain 1 in iteration 130/200, \n",
      " path: 342,  new path: 118,  docs changed path: 315,  Log-likelihood: -429006.44\n",
      "üîÑ Chain 3 in iteration 132/200, \n",
      " path: 332,  new path: 117,  docs changed path: 308,  Log-likelihood: -427441.10\n",
      "üîÑ Chain 2 in iteration 136/200, \n",
      " path: 332,  new path: 126,  docs changed path: 323,  Log-likelihood: -433173.36\n",
      "üîÑ Chain 1 in iteration 131/200, \n",
      " path: 347,  new path: 118,  docs changed path: 332,  Log-likelihood: -428606.12\n",
      "üîÑ Chain 2 in iteration 137/200, \n",
      " path: 323,  new path: 118,  docs changed path: 318,  Log-likelihood: -433196.18\n",
      "üîÑ Chain 3 in iteration 133/200, \n",
      " path: 338,  new path: 124,  docs changed path: 280,  Log-likelihood: -427687.92\n",
      "üîÑ Chain 1 in iteration 132/200, \n",
      " path: 347,  new path: 133,  docs changed path: 316,  Log-likelihood: -428147.85\n",
      "üîÑ Chain 2 in iteration 138/200, \n",
      " path: 314,  new path: 114,  docs changed path: 311,  Log-likelihood: -432907.77\n",
      "üîÑ Chain 3 in iteration 134/200, \n",
      " path: 330,  new path: 115,  docs changed path: 278,  Log-likelihood: -427650.73\n",
      "üîÑ Chain 1 in iteration 133/200, \n",
      " path: 352,  new path: 128,  docs changed path: 325,  Log-likelihood: -428329.68\n",
      "üîÑ Chain 2 in iteration 139/200, \n",
      " path: 315,  new path: 116,  docs changed path: 317,  Log-likelihood: -432528.16\n",
      "üîÑ Chain 3 in iteration 135/200, \n",
      " path: 331,  new path: 119,  docs changed path: 283,  Log-likelihood: -427774.31\n",
      "üîÑ Chain 1 in iteration 134/200, \n",
      " path: 345,  new path: 120,  docs changed path: 325,  Log-likelihood: -428079.91\n",
      "üîÑ Chain 2 in iteration 140/200, \n",
      " path: 329,  new path: 130,  docs changed path: 300,  Log-likelihood: -432635.09\n",
      "üîÑ Chain 3 in iteration 136/200, \n",
      " path: 333,  new path: 122,  docs changed path: 305,  Log-likelihood: -428723.56\n",
      "üîÑ Chain 2 in iteration 141/200, \n",
      " path: 327,  new path: 129,  docs changed path: 320,  Log-likelihood: -432842.14\n",
      "üîÑ Chain 1 in iteration 135/200, \n",
      " path: 348,  new path: 123,  docs changed path: 316,  Log-likelihood: -427737.15\n",
      "check_dual_convergence_with_gelman_rubin: chain_len_docs:{'2': [548, 554, 554, 545, 545, 543, 543, 547, 557, 561, 560, 556, 547, 554, 561, 565, 573, 576, 580, 581, 574], '3': [538, 535, 537, 531, 529, 536, 544, 546, 548, 543, 549, 558, 556, 552, 556, 558, 557, 564, 569, 574, 575], '1': [544, 547, 530, 535, 541, 532, 529, 536, 535, 537, 540, 537, 525, 530, 518, 519, 526, 532, 531, 530, 538]}, mean_jaccard:[0.9437584687337027, 0.9316095318376825, 0.923347725433284], mean_ratio:[0.20000000000000004, 0.20000000000000004, 0.20000000000000004]\n",
      "ü•∞ Convergence status: ‚ùå Not converged...\n",
      "ü•∞  R-hat info: 1.4966Ôºåmean_jaccard:[0.9437584687337027, 0.9316095318376825, 0.923347725433284], mean_ratio:[0.20000000000000004, 0.20000000000000004, 0.20000000000000004]\n",
      "üîÑ Chain 3 in iteration 137/200, \n",
      " path: 332,  new path: 124,  docs changed path: 304,  Log-likelihood: -427801.02\n",
      "üîÑ Chain 2 in iteration 142/200, \n",
      " path: 326,  new path: 119,  docs changed path: 306,  Log-likelihood: -432630.15\n",
      "üîÑ Chain 1 in iteration 136/200, \n",
      " path: 349,  new path: 116,  docs changed path: 311,  Log-likelihood: -428675.11\n",
      "üîÑ Chain 3 in iteration 138/200, \n",
      " path: 340,  new path: 124,  docs changed path: 300,  Log-likelihood: -427722.76\n",
      "üîÑ Chain 2 in iteration 143/200, \n",
      " path: 321,  new path: 118,  docs changed path: 311,  Log-likelihood: -433916.86\n",
      "üîÑ Chain 1 in iteration 137/200, \n",
      " path: 346,  new path: 121,  docs changed path: 313,  Log-likelihood: -427852.45\n",
      "üîÑ Chain 3 in iteration 139/200, \n",
      " path: 343,  new path: 118,  docs changed path: 295,  Log-likelihood: -427842.26\n",
      "üîÑ Chain 2 in iteration 144/200, \n",
      " path: 321,  new path: 112,  docs changed path: 296,  Log-likelihood: -433304.78\n",
      "üîÑ Chain 1 in iteration 138/200, \n",
      " path: 341,  new path: 125,  docs changed path: 309,  Log-likelihood: -428159.50\n",
      "üîÑ Chain 3 in iteration 140/200, \n",
      " path: 339,  new path: 113,  docs changed path: 296,  Log-likelihood: -427235.82\n",
      "üîÑ Chain 2 in iteration 145/200, \n",
      " path: 333,  new path: 126,  docs changed path: 322,  Log-likelihood: -432823.58\n",
      "üîÑ Chain 1 in iteration 139/200, \n",
      " path: 350,  new path: 132,  docs changed path: 319,  Log-likelihood: -427228.46\n",
      "üîÑ Chain 3 in iteration 141/200, \n",
      " path: 336,  new path: 122,  docs changed path: 298,  Log-likelihood: -427965.07\n",
      "üîÑ Chain 2 in iteration 146/200, \n",
      " path: 326,  new path: 128,  docs changed path: 319,  Log-likelihood: -432798.79\n",
      "üîÑ Chain 1 in iteration 140/200, \n",
      " path: 341,  new path: 122,  docs changed path: 327,  Log-likelihood: -428485.73\n",
      "üîÑ Chain 3 in iteration 142/200, \n",
      " path: 347,  new path: 121,  docs changed path: 310,  Log-likelihood: -426352.28\n",
      "üîÑ Chain 2 in iteration 147/200, \n",
      " path: 332,  new path: 116,  docs changed path: 312,  Log-likelihood: -432318.55\n",
      "üîÑ Chain 1 in iteration 141/200, \n",
      " path: 347,  new path: 130,  docs changed path: 346,  Log-likelihood: -428439.03\n",
      "üîÑ Chain 3 in iteration 143/200, \n",
      " path: 345,  new path: 122,  docs changed path: 322,  Log-likelihood: -427287.85\n",
      "üîÑ Chain 2 in iteration 148/200, \n",
      " path: 324,  new path: 114,  docs changed path: 301,  Log-likelihood: -432540.49\n",
      "üîÑ Chain 1 in iteration 142/200, \n",
      " path: 346,  new path: 119,  docs changed path: 329,  Log-likelihood: -428446.60\n",
      "üîÑ Chain 3 in iteration 144/200, \n",
      " path: 329,  new path: 117,  docs changed path: 312,  Log-likelihood: -428072.53\n",
      "üîÑ Chain 2 in iteration 149/200, \n",
      " path: 326,  new path: 125,  docs changed path: 297,  Log-likelihood: -432416.51\n",
      "üîÑ Chain 1 in iteration 143/200, \n",
      " path: 351,  new path: 117,  docs changed path: 329,  Log-likelihood: -427174.52\n",
      "üîÑ Chain 3 in iteration 145/200, \n",
      " path: 334,  new path: 119,  docs changed path: 302,  Log-likelihood: -427579.33\n",
      "üîÑ Chain 2 in iteration 150/200, \n",
      " path: 323,  new path: 118,  docs changed path: 308,  Log-likelihood: -432589.62\n",
      "üîÑ Chain 1 in iteration 144/200, \n",
      " path: 351,  new path: 120,  docs changed path: 324,  Log-likelihood: -427321.78\n",
      "üîÑ Chain 3 in iteration 146/200, \n",
      " path: 331,  new path: 111,  docs changed path: 303,  Log-likelihood: -427846.29\n",
      "üîÑ Chain 2 in iteration 151/200, \n",
      " path: 325,  new path: 126,  docs changed path: 312,  Log-likelihood: -433644.17\n",
      "üîÑ Chain 1 in iteration 145/200, \n",
      " path: 348,  new path: 124,  docs changed path: 311,  Log-likelihood: -428053.53\n",
      "üîÑ Chain 3 in iteration 147/200, \n",
      " path: 330,  new path: 107,  docs changed path: 304,  Log-likelihood: -428183.94\n",
      "üîÑ Chain 2 in iteration 152/200, \n",
      " path: 331,  new path: 128,  docs changed path: 315,  Log-likelihood: -431714.18\n",
      "üîÑ Chain 1 in iteration 146/200, \n",
      " path: 344,  new path: 122,  docs changed path: 312,  Log-likelihood: -428123.11\n",
      "üîÑ Chain 3 in iteration 148/200, \n",
      " path: 330,  new path: 114,  docs changed path: 302,  Log-likelihood: -427736.60\n",
      "üîÑ Chain 2 in iteration 153/200, \n",
      " path: 332,  new path: 121,  docs changed path: 322,  Log-likelihood: -431208.17\n",
      "üîÑ Chain 1 in iteration 147/200, \n",
      " path: 346,  new path: 119,  docs changed path: 313,  Log-likelihood: -428424.07\n",
      "üîÑ Chain 2 in iteration 154/200, \n",
      " path: 330,  new path: 129,  docs changed path: 313,  Log-likelihood: -431764.92\n",
      "üîÑ Chain 3 in iteration 149/200, \n",
      " path: 342,  new path: 122,  docs changed path: 309,  Log-likelihood: -427209.58\n",
      "üîÑ Chain 2 in iteration 155/200, \n",
      " path: 324,  new path: 119,  docs changed path: 317,  Log-likelihood: -432416.83\n",
      "üîÑ Chain 1 in iteration 148/200, \n",
      " path: 342,  new path: 116,  docs changed path: 318,  Log-likelihood: -428547.94\n",
      "üîÑ Chain 3 in iteration 150/200, \n",
      " path: 351,  new path: 136,  docs changed path: 306,  Log-likelihood: -426114.02\n",
      "üîÑ Chain 2 in iteration 156/200, \n",
      " path: 322,  new path: 122,  docs changed path: 313,  Log-likelihood: -431910.83\n",
      "üîÑ Chain 1 in iteration 149/200, \n",
      " path: 345,  new path: 127,  docs changed path: 314,  Log-likelihood: -429436.06\n",
      "üîÑ Chain 3 in iteration 151/200, \n",
      " path: 332,  new path: 107,  docs changed path: 299,  Log-likelihood: -427631.14\n",
      "üîÑ Chain 2 in iteration 157/200, \n",
      " path: 324,  new path: 126,  docs changed path: 324,  Log-likelihood: -432197.45\n",
      "üîÑ Chain 1 in iteration 150/200, \n",
      " path: 346,  new path: 124,  docs changed path: 318,  Log-likelihood: -428696.69\n",
      "üîÑ Chain 3 in iteration 152/200, \n",
      " path: 341,  new path: 120,  docs changed path: 297,  Log-likelihood: -426806.97\n",
      "üîÑ Chain 2 in iteration 158/200, \n",
      " path: 339,  new path: 132,  docs changed path: 321,  Log-likelihood: -431822.15\n",
      "üîÑ Chain 1 in iteration 151/200, \n",
      " path: 336,  new path: 109,  docs changed path: 306,  Log-likelihood: -428818.91\n",
      "üîÑ Chain 3 in iteration 153/200, \n",
      " path: 338,  new path: 120,  docs changed path: 306,  Log-likelihood: -426726.19\n",
      "üîÑ Chain 2 in iteration 159/200, \n",
      " path: 330,  new path: 129,  docs changed path: 325,  Log-likelihood: -433318.79\n",
      "üîÑ Chain 1 in iteration 152/200, \n",
      " path: 332,  new path: 106,  docs changed path: 307,  Log-likelihood: -429288.60\n",
      "üîÑ Chain 3 in iteration 154/200, \n",
      " path: 344,  new path: 113,  docs changed path: 305,  Log-likelihood: -426916.47\n",
      "üîÑ Chain 2 in iteration 160/200, \n",
      " path: 340,  new path: 139,  docs changed path: 342,  Log-likelihood: -431978.29\n",
      "üîÑ Chain 1 in iteration 153/200, \n",
      " path: 337,  new path: 113,  docs changed path: 288,  Log-likelihood: -428553.81\n",
      "üîÑ Chain 3 in iteration 155/200, \n",
      " path: 334,  new path: 120,  docs changed path: 306,  Log-likelihood: -427153.51\n",
      "üîÑ Chain 2 in iteration 161/200, \n",
      " path: 325,  new path: 124,  docs changed path: 326,  Log-likelihood: -432493.93\n",
      "üîÑ Chain 1 in iteration 154/200, \n",
      " path: 335,  new path: 118,  docs changed path: 308,  Log-likelihood: -428202.61\n",
      "üîÑ Chain 3 in iteration 156/200, \n",
      " path: 341,  new path: 126,  docs changed path: 303,  Log-likelihood: -426429.90\n",
      "üîÑ Chain 2 in iteration 162/200, \n",
      " path: 327,  new path: 113,  docs changed path: 307,  Log-likelihood: -433167.50\n",
      "üîÑ Chain 1 in iteration 155/200, \n",
      " path: 341,  new path: 124,  docs changed path: 313,  Log-likelihood: -428119.26\n",
      "check_dual_convergence_with_gelman_rubin: chain_len_docs:{'2': [574, 568, 561, 561, 558, 562, 562, 569, 583, 581, 580, 563, 556, 550, 558, 562, 558, 544, 544, 546, 552], '3': [575, 579, 566, 565, 560, 560, 568, 567, 564, 556, 549, 546, 543, 553, 547, 556, 560, 560, 549, 554, 560], '1': [538, 532, 536, 544, 541, 541, 524, 519, 518, 517, 531, 529, 530, 535, 540, 542, 549, 543, 552, 561, 561]}, mean_jaccard:[0.9457533945940444, 0.9332956343618722, 0.9305463315407806], mean_ratio:[0.20000000000000004, 0.20000000000000004, 0.20000000000000004]\n",
      "ü•∞ Convergence status: ‚ùå Not converged...\n",
      "ü•∞  R-hat info: 1.5417Ôºåmean_jaccard:[0.9457533945940444, 0.9332956343618722, 0.9305463315407806], mean_ratio:[0.20000000000000004, 0.20000000000000004, 0.20000000000000004]\n",
      "üîÑ Chain 3 in iteration 157/200, \n",
      " path: 347,  new path: 135,  docs changed path: 302,  Log-likelihood: -426303.47\n",
      "üîÑ Chain 2 in iteration 163/200, \n",
      " path: 326,  new path: 122,  docs changed path: 313,  Log-likelihood: -433014.43\n",
      "üîÑ Chain 1 in iteration 156/200, \n",
      " path: 341,  new path: 117,  docs changed path: 305,  Log-likelihood: -427778.46\n",
      "üîÑ Chain 3 in iteration 158/200, \n",
      " path: 342,  new path: 127,  docs changed path: 312,  Log-likelihood: -426559.14\n",
      "üîÑ Chain 2 in iteration 164/200, \n",
      " path: 328,  new path: 118,  docs changed path: 313,  Log-likelihood: -433027.48\n",
      "üîÑ Chain 1 in iteration 157/200, \n",
      " path: 339,  new path: 115,  docs changed path: 312,  Log-likelihood: -428118.24\n",
      "üîÑ Chain 3 in iteration 159/200, \n",
      " path: 333,  new path: 111,  docs changed path: 310,  Log-likelihood: -426616.45\n",
      "üîÑ Chain 2 in iteration 165/200, \n",
      " path: 324,  new path: 117,  docs changed path: 297,  Log-likelihood: -432779.20\n",
      "üîÑ Chain 1 in iteration 158/200, \n",
      " path: 339,  new path: 128,  docs changed path: 324,  Log-likelihood: -427737.37\n",
      "üîÑ Chain 3 in iteration 160/200, \n",
      " path: 338,  new path: 126,  docs changed path: 313,  Log-likelihood: -426247.08\n",
      "üîÑ Chain 2 in iteration 166/200, \n",
      " path: 325,  new path: 133,  docs changed path: 312,  Log-likelihood: -432619.18\n",
      "üîÑ Chain 1 in iteration 159/200, \n",
      " path: 333,  new path: 123,  docs changed path: 311,  Log-likelihood: -428260.32\n",
      "üîÑ Chain 3 in iteration 161/200, \n",
      " path: 349,  new path: 122,  docs changed path: 309,  Log-likelihood: -425954.35\n",
      "üîÑ Chain 2 in iteration 167/200, \n",
      " path: 329,  new path: 119,  docs changed path: 305,  Log-likelihood: -432582.92\n",
      "üîÑ Chain 1 in iteration 160/200, \n",
      " path: 327,  new path: 108,  docs changed path: 294,  Log-likelihood: -429827.44\n",
      "üîÑ Chain 3 in iteration 162/200, \n",
      " path: 338,  new path: 114,  docs changed path: 302,  Log-likelihood: -426526.80\n",
      "üîÑ Chain 2 in iteration 168/200, \n",
      " path: 318,  new path: 116,  docs changed path: 315,  Log-likelihood: -433438.15\n",
      "üîÑ Chain 1 in iteration 161/200, \n",
      " path: 329,  new path: 112,  docs changed path: 284,  Log-likelihood: -428795.17\n",
      "üîÑ Chain 3 in iteration 163/200, \n",
      " path: 333,  new path: 106,  docs changed path: 283,  Log-likelihood: -426548.63\n",
      "üîÑ Chain 2 in iteration 169/200, \n",
      " path: 323,  new path: 114,  docs changed path: 308,  Log-likelihood: -433635.94\n",
      "üîÑ Chain 1 in iteration 162/200, \n",
      " path: 336,  new path: 113,  docs changed path: 290,  Log-likelihood: -427713.79\n",
      "üîÑ Chain 2 in iteration 170/200, \n",
      " path: 321,  new path: 113,  docs changed path: 306,  Log-likelihood: -432757.01\n",
      "üîÑ Chain 3 in iteration 164/200, \n",
      " path: 350,  new path: 144,  docs changed path: 310,  Log-likelihood: -425313.93\n",
      "üîÑ Chain 1 in iteration 163/200, \n",
      " path: 337,  new path: 114,  docs changed path: 294,  Log-likelihood: -428188.04\n",
      "üîÑ Chain 2 in iteration 171/200, \n",
      " path: 326,  new path: 129,  docs changed path: 313,  Log-likelihood: -432188.91\n",
      "üîÑ Chain 3 in iteration 165/200, \n",
      " path: 353,  new path: 133,  docs changed path: 313,  Log-likelihood: -426205.22\n",
      "üîÑ Chain 2 in iteration 172/200, \n",
      " path: 318,  new path: 115,  docs changed path: 307,  Log-likelihood: -433384.22\n",
      "üîÑ Chain 1 in iteration 164/200, \n",
      " path: 332,  new path: 119,  docs changed path: 290,  Log-likelihood: -428222.46\n",
      "üîÑ Chain 3 in iteration 166/200, \n",
      " path: 339,  new path: 117,  docs changed path: 296,  Log-likelihood: -426994.54\n",
      "üîÑ Chain 2 in iteration 173/200, \n",
      " path: 323,  new path: 116,  docs changed path: 305,  Log-likelihood: -432839.86\n",
      "üîÑ Chain 1 in iteration 165/200, \n",
      " path: 338,  new path: 111,  docs changed path: 286,  Log-likelihood: -428028.54\n",
      "üîÑ Chain 3 in iteration 167/200, \n",
      " path: 348,  new path: 120,  docs changed path: 315,  Log-likelihood: -425770.52\n",
      "üîÑ Chain 2 in iteration 174/200, \n",
      " path: 329,  new path: 129,  docs changed path: 301,  Log-likelihood: -432019.99\n",
      "üîÑ Chain 1 in iteration 166/200, \n",
      " path: 334,  new path: 102,  docs changed path: 289,  Log-likelihood: -428573.43\n",
      "üîÑ Chain 3 in iteration 168/200, \n",
      " path: 333,  new path: 120,  docs changed path: 307,  Log-likelihood: -426346.90\n",
      "üîÑ Chain 2 in iteration 175/200, \n",
      " path: 314,  new path: 110,  docs changed path: 313,  Log-likelihood: -434962.66\n",
      "üîÑ Chain 1 in iteration 167/200, \n",
      " path: 333,  new path: 120,  docs changed path: 294,  Log-likelihood: -428781.08\n",
      "üîÑ Chain 3 in iteration 169/200, \n",
      " path: 341,  new path: 128,  docs changed path: 315,  Log-likelihood: -426220.11\n",
      "üîÑ Chain 2 in iteration 176/200, \n",
      " path: 336,  new path: 130,  docs changed path: 295,  Log-likelihood: -432091.23\n",
      "üîÑ Chain 1 in iteration 168/200, \n",
      " path: 345,  new path: 118,  docs changed path: 282,  Log-likelihood: -428474.07\n",
      "üîÑ Chain 3 in iteration 170/200, \n",
      " path: 340,  new path: 121,  docs changed path: 308,  Log-likelihood: -426041.40\n",
      "üîÑ Chain 2 in iteration 177/200, \n",
      " path: 324,  new path: 112,  docs changed path: 322,  Log-likelihood: -432775.99\n",
      "üîÑ Chain 1 in iteration 169/200, \n",
      " path: 342,  new path: 122,  docs changed path: 308,  Log-likelihood: -429500.87\n",
      "üîÑ Chain 3 in iteration 171/200, \n",
      " path: 337,  new path: 131,  docs changed path: 305,  Log-likelihood: -426009.93\n",
      "üîÑ Chain 2 in iteration 178/200, \n",
      " path: 329,  new path: 125,  docs changed path: 313,  Log-likelihood: -432959.45\n",
      "üîÑ Chain 1 in iteration 170/200, \n",
      " path: 348,  new path: 121,  docs changed path: 304,  Log-likelihood: -428311.22\n",
      "üîÑ Chain 3 in iteration 172/200, \n",
      " path: 339,  new path: 118,  docs changed path: 299,  Log-likelihood: -426322.53\n",
      "üîÑ Chain 2 in iteration 179/200, \n",
      " path: 331,  new path: 119,  docs changed path: 328,  Log-likelihood: -433221.59\n",
      "üîÑ Chain 1 in iteration 171/200, \n",
      " path: 349,  new path: 122,  docs changed path: 300,  Log-likelihood: -428280.11\n",
      "üîÑ Chain 3 in iteration 173/200, \n",
      " path: 350,  new path: 141,  docs changed path: 303,  Log-likelihood: -426313.58\n",
      "üîÑ Chain 2 in iteration 180/200, \n",
      " path: 323,  new path: 111,  docs changed path: 303,  Log-likelihood: -433362.43\n",
      "üîÑ Chain 1 in iteration 172/200, \n",
      " path: 342,  new path: 123,  docs changed path: 317,  Log-likelihood: -428901.85\n",
      "üîÑ Chain 3 in iteration 174/200, \n",
      " path: 334,  new path: 111,  docs changed path: 302,  Log-likelihood: -426897.32\n",
      "üîÑ Chain 2 in iteration 181/200, \n",
      " path: 320,  new path: 117,  docs changed path: 315,  Log-likelihood: -433675.23\n",
      "üîÑ Chain 1 in iteration 173/200, \n",
      " path: 343,  new path: 115,  docs changed path: 307,  Log-likelihood: -427538.05\n",
      "üîÑ Chain 3 in iteration 175/200, \n",
      " path: 339,  new path: 127,  docs changed path: 295,  Log-likelihood: -426973.09\n",
      "üîÑ Chain 2 in iteration 182/200, \n",
      " path: 331,  new path: 130,  docs changed path: 328,  Log-likelihood: -432729.63\n",
      "üîÑ Chain 1 in iteration 174/200, \n",
      " path: 336,  new path: 102,  docs changed path: 276,  Log-likelihood: -428574.56\n",
      "üîÑ Chain 3 in iteration 176/200, \n",
      " path: 330,  new path: 109,  docs changed path: 289,  Log-likelihood: -427102.78\n",
      "üîÑ Chain 2 in iteration 183/200, \n",
      " path: 322,  new path: 112,  docs changed path: 324,  Log-likelihood: -432337.54\n",
      "üîÑ Chain 1 in iteration 175/200, \n",
      " path: 335,  new path: 114,  docs changed path: 289,  Log-likelihood: -428728.48\n",
      "check_dual_convergence_with_gelman_rubin: chain_len_docs:{'2': [552, 557, 557, 546, 548, 539, 534, 533, 542, 558, 569, 562, 562, 567, 567, 569, 567, 565, 556, 551, 555], '3': [560, 561, 556, 551, 553, 540, 536, 541, 562, 555, 553, 561, 549, 553, 553, 549, 541, 554, 576, 576, 566], '1': [561, 558, 555, 544, 547, 545, 549, 563, 577, 577, 567, 563, 565, 576, 564, 554, 553, 539, 542, 548, 552]}, mean_jaccard:[0.9400769762810557, 0.9290141428371989, 0.9286162688903264], mean_ratio:[0.20000000000000004, 0.20000000000000004, 0.20000000000000004]\n",
      "ü•∞ Convergence status: ‚úÖ Converged!\n",
      "ü•∞  R-hat info: 0.9833Ôºåmean_jaccard:[0.9400769762810557, 0.9290141428371989, 0.9286162688903264], mean_ratio:[0.20000000000000004, 0.20000000000000004, 0.20000000000000004]\n",
      "‚úÖ r-hat of multi-chains is detected to convergence 155 in iteration:155, R-hat=0.9833, JaccardÂπ≥ÂùáÂÄº=[0.9400769762810557, 0.9290141428371989, 0.9286162688903264], RatioÂπ≥ÂùáÂÄº=[0.20000000000000004, 0.20000000000000004, 0.20000000000000004]\n",
      "‚úÖ R-hat convergence is detected, waiting for all chains to finish.\n",
      "üîÑ Chain 2 in iteration 184/200, \n",
      " path: 330,  new path: 123,  docs changed path: 324,  Log-likelihood: -433394.68\n",
      "üîÑ Chain 3 in iteration 177/200, \n",
      " path: 336,  new path: 121,  docs changed path: 301,  Log-likelihood: -426253.74\n",
      "üîÑ Chain 1 in iteration 176/200, \n",
      " path: 344,  new path: 116,  docs changed path: 307,  Log-likelihood: -427748.03\n",
      "üîÑ Chain 2 in iteration 185/200, \n",
      " path: 334,  new path: 120,  docs changed path: 334,  Log-likelihood: -432534.22\n",
      "üîÑ Chain 3 in iteration 178/200, \n",
      " path: 341,  new path: 122,  docs changed path: 306,  Log-likelihood: -426597.82\n",
      "üîÑ Chain 1 in iteration 177/200, \n",
      " path: 353,  new path: 128,  docs changed path: 311,  Log-likelihood: -427286.14\n",
      "üîÑ Chain 2 in iteration 186/200, \n",
      " path: 337,  new path: 124,  docs changed path: 322,  Log-likelihood: -432368.84\n",
      "üîÑ Chain 3 in iteration 179/200, \n",
      " path: 333,  new path: 122,  docs changed path: 301,  Log-likelihood: -427877.47\n",
      "üîÑ Chain 1 in iteration 178/200, \n",
      " path: 346,  new path: 114,  docs changed path: 302,  Log-likelihood: -427884.45\n",
      "üîÑ Chain 2 in iteration 187/200, \n",
      " path: 332,  new path: 120,  docs changed path: 318,  Log-likelihood: -433146.52\n",
      "üîÑ Chain 3 in iteration 180/200, \n",
      " path: 334,  new path: 123,  docs changed path: 298,  Log-likelihood: -427311.60\n",
      "üîÑ Chain 1 in iteration 179/200, \n",
      " path: 339,  new path: 118,  docs changed path: 302,  Log-likelihood: -427421.53\n",
      "üîÑ Chain 2 in iteration 188/200, \n",
      " path: 334,  new path: 119,  docs changed path: 311,  Log-likelihood: -433236.10\n",
      "üîÑ Chain 3 in iteration 181/200, \n",
      " path: 336,  new path: 121,  docs changed path: 291,  Log-likelihood: -426949.91\n",
      "üîÑ Chain 2 in iteration 189/200, \n",
      " path: 318,  new path: 109,  docs changed path: 314,  Log-likelihood: -433645.04\n",
      "üîÑ Chain 1 in iteration 180/200, \n",
      " path: 337,  new path: 115,  docs changed path: 292,  Log-likelihood: -427755.37\n",
      "üîÑ Chain 3 in iteration 182/200, \n",
      " path: 340,  new path: 121,  docs changed path: 302,  Log-likelihood: -426587.85\n",
      "üîÑ Chain 2 in iteration 190/200, \n",
      " path: 326,  new path: 116,  docs changed path: 314,  Log-likelihood: -432835.98\n",
      "üîÑ Chain 1 in iteration 181/200, \n",
      " path: 334,  new path: 108,  docs changed path: 296,  Log-likelihood: -428318.71\n",
      "üîÑ Chain 3 in iteration 183/200, \n",
      " path: 338,  new path: 114,  docs changed path: 303,  Log-likelihood: -427204.54\n",
      "üîÑ Chain 2 in iteration 191/200, \n",
      " path: 330,  new path: 112,  docs changed path: 306,  Log-likelihood: -432442.32\n",
      "üîÑ Chain 1 in iteration 182/200, \n",
      " path: 338,  new path: 112,  docs changed path: 304,  Log-likelihood: -427676.56\n",
      "üîÑ Chain 3 in iteration 184/200, \n",
      " path: 330,  new path: 112,  docs changed path: 308,  Log-likelihood: -427642.21\n",
      "üîÑ Chain 2 in iteration 192/200, \n",
      " path: 332,  new path: 115,  docs changed path: 307,  Log-likelihood: -432148.72\n",
      "üîÑ Chain 1 in iteration 183/200, \n",
      " path: 347,  new path: 130,  docs changed path: 322,  Log-likelihood: -427557.45\n",
      "üîÑ Chain 3 in iteration 185/200, \n",
      " path: 342,  new path: 113,  docs changed path: 314,  Log-likelihood: -426721.11\n",
      "üîÑ Chain 2 in iteration 193/200, \n",
      " path: 334,  new path: 129,  docs changed path: 321,  Log-likelihood: -431941.63\n",
      "üîÑ Chain 1 in iteration 184/200, \n",
      " path: 336,  new path: 100,  docs changed path: 294,  Log-likelihood: -427792.34\n",
      "üîÑ Chain 3 in iteration 186/200, \n",
      " path: 336,  new path: 108,  docs changed path: 294,  Log-likelihood: -427703.70\n",
      "üîÑ Chain 2 in iteration 194/200, \n",
      " path: 323,  new path: 115,  docs changed path: 309,  Log-likelihood: -433168.00\n",
      "üîÑ Chain 1 in iteration 185/200, \n",
      " path: 341,  new path: 114,  docs changed path: 294,  Log-likelihood: -427053.22\n",
      "üîÑ Chain 3 in iteration 187/200, \n",
      " path: 344,  new path: 120,  docs changed path: 290,  Log-likelihood: -426374.63\n",
      "üîÑ Chain 2 in iteration 195/200, \n",
      " path: 326,  new path: 122,  docs changed path: 312,  Log-likelihood: -432874.01\n",
      "üéâ Chain 2 is detected to convergence in iteration 195, sampling ends prematurely.\n",
      "üéØ Chain 2 finish Gibbs Sampling.\n",
      "üíæ Save iterations info...\n",
      "‚úÖ doc-path allocation is saved : step2_d3_g005_e001_Âü∫‰∫ée01/depth_3_gamma_0.05_run_2/iteration_document_paths.csv\n",
      "‚úÖ structure of tree path is saved: step2_d3_g005_e001_Âü∫‰∫ée01/depth_3_gamma_0.05_run_2/iteration_path_structures.csv\n",
      "‚úÖ info recoreded in iteration is saved: step2_d3_g005_e001_Âü∫‰∫ée01/depth_3_gamma_0.05_run_2/iteration_iteration_summaries.csv\n",
      "‚úÖ doc-path mapping is saved: step2_d3_g005_e001_Âü∫‰∫ée01/depth_3_gamma_0.05_run_2/iteration_path_document_mapping.csv\n",
      "‚úÖ doc's words allocation to node is saved: step2_d3_g005_e001_Âü∫‰∫ée01/depth_3_gamma_0.05_run_2/iteration_word_allocations.csv\n",
      "‚úÖ node word distribution is saved: step2_d3_g005_e001_Âü∫‰∫ée01/depth_3_gamma_0.05_run_2/iteration_node_word_distributions.csv\n",
      "‚ö†Ô∏è Node 28249 (Layer 1) has entropy=0:\n",
      "   Word distribution: {'grain': 0, 'influence': 0, 'diffusion': 0, 'system': 0, 'small': 0, 'large': 0, 'author': 0, 'whereas': 0, 'cause': 0, 'assembly': 0, 'joint': 0, 'specify': 0, 'significant': 0, 'size': 0, 'developed': 0, 'complex': 0, 'knowledge': 0, 'high': 0, 'current': 0, 'open': 0, 'application': 0, 'shape': 0, 'importance': 0, 'assume': 0, 'critical': 0, 'nucleation': 0, 'near': 0, 'mechanical': 0, 'surface': 0, 'account': 0, 'consideration': 0, 'different': 0, 'approach': 0, 'study': 0, 'thermal': 0, 'commonly': 0, 'model': 0, 'direction': 0, 'coefficient': 0, 'perform': 0, 'maximum': 0, 'rigid': 0, 'wall': 0, 'calibrate': 0, 'hybrid': 0, 'unknown': 0, 'agreement': 0, 'deformation': 0}\n",
      "   Number of unique words: 48\n",
      "   Total word count: 0\n",
      "   Document count: 2\n",
      "\n",
      "‚ö†Ô∏è Node 29363 (Layer 2) has entropy=0:\n",
      "   Word distribution: {'computational': 0, 'homogenization': 0, 'couple': 0, 'material': 0, 'macroscopic': 0, 'microscale': 0, 'fast': 0, 'fourier': 0, 'fft': 0, 'context': 0, 'medium': 0, 'multiscale': 0, 'provide': 0, 'consistent': 0, 'tangent': 0, 'operator': 0, 'solution': 0, 'turn': 0, 'scheme': 0, 'short': 0, 'time': 0, 'certain': 0, 'condition': 0, 'fftbased': 0, 'general': 0, 'efficient': 0, 'reference': 0, 'strain': 0, 'quadratic': 0, 'applicability': 0, 'carry': 0, 'computation': 0, 'fully': 0, 'nonlinear': 0, 'convergence': 0, 'investigate': 0, 'impact': 0, 'address': 0, 'solve': 0, 'algorithmic': 0, 'order': 0, 'work': 0, 'accuracy': 0, 'may': 0, 'example': 0, 'allow': 0, 'value': 0, 'derive': 0, 'problem': 0, 'boundary': 0, 'employ': 0}\n",
      "   Number of unique words: 51\n",
      "   Total word count: 0\n",
      "   Document count: 1\n",
      "\n",
      "üîÑ Chain 1 in iteration 186/200, \n",
      " path: 340,  new path: 111,  docs changed path: 297,  Log-likelihood: -427591.93\n",
      "‚úÖ Chain 2 Finished and results saved to step2_d3_g005_e001_Âü∫‰∫ée01/depth_3_gamma_0.05_run_2\n",
      "üîÑ Chain 3 in iteration 188/200, \n",
      " path: 344,  new path: 124,  docs changed path: 288,  Log-likelihood: -426904.33\n",
      "üîÑ Chain 1 in iteration 187/200, \n",
      " path: 351,  new path: 120,  docs changed path: 296,  Log-likelihood: -426353.73\n",
      "üîÑ Chain 3 in iteration 189/200, \n",
      " path: 341,  new path: 117,  docs changed path: 273,  Log-likelihood: -426672.84\n",
      "üîÑ Chain 1 in iteration 188/200, \n",
      " path: 349,  new path: 122,  docs changed path: 321,  Log-likelihood: -426610.50\n",
      "üîÑ Chain 3 in iteration 190/200, \n",
      " path: 343,  new path: 118,  docs changed path: 291,  Log-likelihood: -426949.13\n",
      "üîÑ Chain 1 in iteration 189/200, \n",
      " path: 348,  new path: 127,  docs changed path: 299,  Log-likelihood: -426697.59\n",
      "üîÑ Chain 3 in iteration 191/200, \n",
      " path: 338,  new path: 104,  docs changed path: 294,  Log-likelihood: -427438.40\n",
      "üîÑ Chain 1 in iteration 190/200, \n",
      " path: 342,  new path: 103,  docs changed path: 296,  Log-likelihood: -426520.30\n",
      "üîÑ Chain 3 in iteration 192/200, \n",
      " path: 343,  new path: 115,  docs changed path: 294,  Log-likelihood: -427487.04\n",
      "üîÑ Chain 1 in iteration 191/200, \n",
      " path: 338,  new path: 116,  docs changed path: 295,  Log-likelihood: -427460.39\n",
      "üîÑ Chain 3 in iteration 193/200, \n",
      " path: 344,  new path: 122,  docs changed path: 302,  Log-likelihood: -427193.38\n",
      "üîÑ Chain 1 in iteration 192/200, \n",
      " path: 349,  new path: 118,  docs changed path: 304,  Log-likelihood: -426196.33\n",
      "üîÑ Chain 3 in iteration 194/200, \n",
      " path: 342,  new path: 115,  docs changed path: 293,  Log-likelihood: -427347.81\n",
      "üîÑ Chain 1 in iteration 193/200, \n",
      " path: 342,  new path: 110,  docs changed path: 294,  Log-likelihood: -427290.62\n",
      "üîÑ Chain 3 in iteration 195/200, \n",
      " path: 351,  new path: 134,  docs changed path: 307,  Log-likelihood: -427077.98\n",
      "üéâ Chain 3 is detected to convergence in iteration 195, sampling ends prematurely.\n",
      "üéØ Chain 3 finish Gibbs Sampling.\n",
      "üíæ Save iterations info...\n",
      "‚úÖ doc-path allocation is saved : step2_d3_g005_e001_Âü∫‰∫ée01/depth_3_gamma_0.05_run_3/iteration_document_paths.csv\n",
      "‚úÖ structure of tree path is saved: step2_d3_g005_e001_Âü∫‰∫ée01/depth_3_gamma_0.05_run_3/iteration_path_structures.csv\n",
      "‚úÖ info recoreded in iteration is saved: step2_d3_g005_e001_Âü∫‰∫ée01/depth_3_gamma_0.05_run_3/iteration_iteration_summaries.csv\n",
      "‚úÖ doc-path mapping is saved: step2_d3_g005_e001_Âü∫‰∫ée01/depth_3_gamma_0.05_run_3/iteration_path_document_mapping.csv\n",
      "‚úÖ doc's words allocation to node is saved: step2_d3_g005_e001_Âü∫‰∫ée01/depth_3_gamma_0.05_run_3/iteration_word_allocations.csv\n",
      "‚úÖ node word distribution is saved: step2_d3_g005_e001_Âü∫‰∫ée01/depth_3_gamma_0.05_run_3/iteration_node_word_distributions.csv\n",
      "‚ö†Ô∏è Node 28881 (Layer 2) has entropy=0:\n",
      "   Word distribution: {'family': 0, 'viscosity': 0, 'finescale': 0, 'identify': 0, 'either': 0, 'previously': 0, 'derive': 0, 'exist': 0, 'turbulent': 0, 'also': 0, 'modeling': 0, 'various': 0, 'combine': 0, 'equation': 0, 'useful': 0, 'suitable': 0, 'explore': 0, 'lead': 0, 'extend': 0, 'classical': 0, 'domain': 0, 'new': 0, 'space': 0, 'dynamic': 0, 'propose': 0, 'idea': 0, 'construct': 0, 'evolution': 0, 'model': 0, 'investigation': 0, 'method': 0, 'framework': 0, 'variational': 0, 'energy': 0, 'multiscale': 0, 'general': 0, 'simulation': 0, 'parameter': 0, 'within': 0, 'approach': 0}\n",
      "   Number of unique words: 40\n",
      "   Total word count: 0\n",
      "   Document count: 1\n",
      "\n",
      "‚ö†Ô∏è Node 28886 (Layer 2) has entropy=0:\n",
      "   Word distribution: {'end': 0, 'external': 0, 'paper': 0, 'map': 0, 'potential': 0, 'coordinate': 0, 'region': 0, 'satisfy': 0, 'tension': 0, 'specify': 0, 'system': 0, 'simultaneously': 0, 'type': 0, 'exist': 0, 'parameter': 0, 'form': 0, 'calculate': 0, 'dimension': 0, 'singular': 0, 'metamaterials': 0, 'various': 0}\n",
      "   Number of unique words: 21\n",
      "   Total word count: 0\n",
      "   Document count: 1\n",
      "\n",
      "‚ö†Ô∏è Node 28046 (Layer 2) has entropy=0:\n",
      "   Word distribution: {'domain': 0, 'method': 0, 'affect': 0, 'dynamic': 0, 'elasticity': 0, 'description': 0, 'nonlinear': 0, 'study': 0, 'flow': 0, 'molecular': 0, 'within': 0, 'chain': 0, 'discretize': 0, 'eg': 0, 'field': 0}\n",
      "   Number of unique words: 15\n",
      "   Total word count: 0\n",
      "   Document count: 1\n",
      "\n",
      "‚ö†Ô∏è Node 28735 (Layer 2) has entropy=0:\n",
      "   Word distribution: {'level': 0, 'set': 0, 'zero': 0, 'experiment': 0, 'singular': 0, 'give': 0, 'global': 0, 'furthermore': 0, 'element': 0, 'hierarchical': 0, 'map': 0, 'displacement': 0, 'treatment': 0, 'function': 0, 'triangular': 0, 'finite': 0, 'parameter': 0, 'normal': 0, 'coordinate': 0, 'polynomial': 0, 'stiffness': 0, 'threedimensional': 0, 'term': 0, 'permit': 0, 'derive': 0, 'poisson': 0, 'include': 0, 'propose': 0, 'inversion': 0, 'new': 0, 'coefficient': 0, 'avoid': 0, 'spectral': 0, 'boundary': 0, 'jacobian': 0, 'along': 0, 'neumann': 0, 'quadrilateral': 0, 'represent': 0, 'mass': 0, 'thickness': 0, 'highorder': 0, 'applicability': 0, 'evaluate': 0, 'treat': 0}\n",
      "   Number of unique words: 45\n",
      "   Total word count: 0\n",
      "   Document count: 2\n",
      "\n",
      "‚úÖ Chain 3 Finished and results saved to step2_d3_g005_e001_Âü∫‰∫ée01/depth_3_gamma_0.05_run_3\n",
      "üîÑ Chain 1 in iteration 194/200, \n",
      " path: 334,  new path: 110,  docs changed path: 278,  Log-likelihood: -427521.29\n",
      "üîÑ Chain 1 in iteration 195/200, \n",
      " path: 344,  new path: 114,  docs changed path: 288,  Log-likelihood: -427276.32\n",
      "üéâ Chain 1 is detected to convergence in iteration 195, sampling ends prematurely.\n",
      "üéØ Chain 1 finish Gibbs Sampling.\n",
      "üíæ Save iterations info...\n",
      "‚úÖ doc-path allocation is saved : step2_d3_g005_e001_Âü∫‰∫ée01/depth_3_gamma_0.05_run_1/iteration_document_paths.csv\n",
      "‚úÖ structure of tree path is saved: step2_d3_g005_e001_Âü∫‰∫ée01/depth_3_gamma_0.05_run_1/iteration_path_structures.csv\n",
      "‚úÖ info recoreded in iteration is saved: step2_d3_g005_e001_Âü∫‰∫ée01/depth_3_gamma_0.05_run_1/iteration_iteration_summaries.csv\n",
      "‚úÖ doc-path mapping is saved: step2_d3_g005_e001_Âü∫‰∫ée01/depth_3_gamma_0.05_run_1/iteration_path_document_mapping.csv\n",
      "‚úÖ doc's words allocation to node is saved: step2_d3_g005_e001_Âü∫‰∫ée01/depth_3_gamma_0.05_run_1/iteration_word_allocations.csv\n",
      "‚úÖ node word distribution is saved: step2_d3_g005_e001_Âü∫‰∫ée01/depth_3_gamma_0.05_run_1/iteration_node_word_distributions.csv\n",
      "‚ö†Ô∏è Node 28724 (Layer 1) has entropy=0:\n",
      "   Word distribution: {'beam': 0, 'finite': 0, 'employ': 0, 'lead': 0, 'possible': 0, 'present': 0, 'lagrange': 0, 'multiplier': 0, 'contribution': 0, 'weak': 0, 'form': 0, 'perform': 0, 'propose': 0, 'formulation': 0, 'example': 0, 'use': 0, 'element': 0, 'describe': 0, 'parameter': 0, 'approach': 0, 'involve': 0, 'spherical': 0, 'numerical': 0, 'load': 0, 'rotation': 0, 'enforce': 0, 'scheme': 0, 'show': 0, 'tensor': 0, 'overcome': 0, 'nonlinear': 0, 'detail': 0, 'operator': 0, 'joint': 0, 'complex': 0, 'spatial': 0, 'turn': 0, 'stress': 0, 'smoothed': 0, 'incompressible': 0, 'shear': 0, 'orientation': 0, 'hydrodynamics': 0, 'shell': 0, 'angle': 0, 'deformation': 0, 'dynamic': 0, 'interface': 0, 'circumvent': 0, 'change': 0}\n",
      "   Number of unique words: 50\n",
      "   Total word count: 0\n",
      "   Document count: 2\n",
      "\n",
      "‚ö†Ô∏è Node 26047 (Layer 1) has entropy=0:\n",
      "   Word distribution: {'concern': 0, 'numerical': 0, 'form': 0, 'extend': 0, 'solve': 0, 'real': 0, 'also': 0, 'accuracy': 0, 'good': 0, 'two': 0, 'example': 0, 'illustrate': 0, 'approach': 0, 'lagrangian': 0, 'deformation': 0, 'obtain': 0, 'element': 0, 'apply': 0, 'make': 0, 'work': 0, 'technique': 0, 'present': 0, 'object': 0, 'context': 0, 'problem': 0, 'method': 0, 'robustness': 0, 'virtual': 0, 'efficiency': 0, 'approximate': 0, 'rate': 0, 'optimal': 0, 'describe': 0, 'finite': 0, 'paper': 0, 'deformable': 0, 'volume': 0, 'use': 0, 'requirement': 0, 'compute': 0, 'time': 0, 'cost': 0, 'carry': 0, 'boundary': 0, 'function': 0, 'result': 0, 'cause': 0, 'regular': 0, 'characteristic': 0, 'adaptively': 0, 'difference': 0, 'improvement': 0, 'consider': 0, 'reflect': 0, 'handle': 0, 'consist': 0}\n",
      "   Number of unique words: 56\n",
      "   Total word count: 0\n",
      "   Document count: 2\n",
      "\n",
      "‚ö†Ô∏è Node 25528 (Layer 1) has entropy=0:\n",
      "   Word distribution: {'finite': 0, 'composite': 0, 'integral': 0, 'orientation': 0, 'high': 1, 'create': 0, 'therefore': 0, 'different': 0, 'context': 0, 'compute': 0, 'independent': 0, 'hence': 0, 'result': 0, 'element': 0, 'term': 0, 'standard': 0, 'propose': 0, 'require': 0, 'work': 0, 'numerical': 0, 'discretizations': 0, 'one': 0, 'number': 0, 'use': 0, 'method': 0, 'matrix': 0, 'problem': 0, 'present': 0, 'performance': 0, 'illustrate': 0, 'additionally': 0, 'technique': 0, 'isogeometric': 0, 'combination': 0, 'order': 0, 'magnitude': 0, 'assembly': 0, 'complexity': 0, 'methodology': 0, 'speed': 0, 'system': 0, 'equation': 0, 'paper': 0, 'stabilization': 0, 'robustness': 0, 'deformation': 0, 'analysis': 0, 'mode': 0, 'condition': 0, 'identify': 0, 'stiffness': 0, 'add': 0, 'consider': 0, 'lead': 0, 'demonstrate': 0, 'example': 0, 'small': 0, 'overcome': 0, 'apply': 0, 'side': 0, 'enhance': 0, 'large': 0, 'base': 0, 'support': 0, 'furthermore': 0, 'modify': 0, 'cell': 0, 'eigenvalue': 0, 'improve': 0, 'approach': 0, 'scheme': 0, 'solution': 0, 'global': 0, 'adapt': 0, 'volume': 0}\n",
      "   Number of unique words: 75\n",
      "   Total word count: 1\n",
      "   Document count: 2\n",
      "\n",
      "‚ö†Ô∏è Node 28585 (Layer 2) has entropy=0:\n",
      "   Word distribution: {'explicit': 0, 'mean': 0, 'parallel': 0, 'stochastic': 0, 'construct': 0, 'model': 0, 'train': 0, 'paper': 0, 'statistical': 0, 'track': 0, 'measurement': 0, 'calibrate': 0, 'simulation': 0, 'large': 0, 'previously': 0, 'scheme': 0, 'property': 0, 'sample': 0}\n",
      "   Number of unique words: 18\n",
      "   Total word count: 0\n",
      "   Document count: 1\n",
      "\n",
      "‚ö†Ô∏è Node 28414 (Layer 2) has entropy=0:\n",
      "   Word distribution: {'naturally': 0, 'bulk': 0, 'cell': 0, 'single': 0, 'chain': 0, 'energy': 0, 'study': 0, 'free': 0, 'describe': 0, 'related': 0, 'polymer': 0, 'approach': 0, 'incorporate': 0, 'density': 0, 'construct': 0, 'employ': 0, 'common': 0, 'model': 0, 'transition': 1, 'derive': 0, 'behaviour': 0, 'size': 0, 'stability': 0, 'variable': 0, 'form': 0, 'relate': 0, 'follow': 0, 'deformation': 0, 'material': 0, 'phasefield': 0, 'solution': 0, 'include': 0, 'effect': 0, 'natural': 0, 'process': 0, 'surface': 0, 'phase': 0, 'essential': 0, 'variety': 0, 'rate': 0, 'stochastic': 0, 'number': 0, 'onto': 0, 'boltzmann': 0, 'regime': 0, 'whereas': 0, 'divide': 0, 'product': 0, 'industrial': 0, 'show': 0, 'early': 0, 'space': 0, 'lattice': 0, 'independent': 0, 'scheme': 0, 'calculate': 0, 'print': 0, 'physical': 0, 'interaction': 0, 'three': 0, 'find': 0, 'degree': 0, 'stage': 0, 'curve': 0, 'observe': 0, 'insight': 0, 'result': 0, 'influence': 0}\n",
      "   Number of unique words: 68\n",
      "   Total word count: 1\n",
      "   Document count: 2\n",
      "\n",
      "‚ö†Ô∏è Node 28680 (Layer 2) has entropy=0:\n",
      "   Word distribution: {'investigation': 0, 'much': 0, 'reinforcement': 0, 'polymer': 0, 'counterpart': 0, 'firstorder': 0, 'homogenization': 0, 'lack': 0, 'enhancement': 0, 'assessment': 0, 'extension': 0, 'idea': 0, 'systematic': 0, 'limitation': 0, 'behavior': 0, 'composite': 0, 'filler': 0, 'grade': 0, 'comprehensive': 0, 'appropriate': 0, 'one': 0, 'interface': 0, 'conduct': 0, 'combination': 0, 'scale': 0, 'effect': 0, 'matrix': 0, 'inclusion': 0, 'length': 0, 'example': 0, 'alternative': 0, 'mean': 0, 'size': 0, 'analysis': 0, 'evaluation': 0, 'representative': 0, 'stiffness': 0, 'introduce': 0, 'technique': 0, 'demonstrate': 0, 'mechanical': 0, 'compare': 0, 'several': 0, 'experimental': 0, 'work': 0, 'base': 0, 'efficient': 0, 'scheme': 0, 'material': 0, 'standard': 0}\n",
      "   Number of unique words: 50\n",
      "   Total word count: 0\n",
      "   Document count: 1\n",
      "\n",
      "‚úÖ Chain 1 Finished and results saved to step2_d3_g005_e001_Âü∫‰∫ée01/depth_3_gamma_0.05_run_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed: 216.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All chains finish sampling and waiting for monitor process finish...\n",
      "‚úÖ R-hat monitor process finished !\n",
      "‚úÖ Overall convergence history saved to: step2_d3_g005_e001_Âü∫‰∫ée01/convergence_info.csv\n",
      "‚úÖ Chain 1 convergence info saved to: step2_d3_g005_e001_Âü∫‰∫ée01/depth_3_gamma_0.05_run_1/chain_convergence_info.csv\n",
      "‚úÖ Chain 2 convergence info saved to: step2_d3_g005_e001_Âü∫‰∫ée01/depth_3_gamma_0.05_run_2/chain_convergence_info.csv\n",
      "‚úÖ Chain 3 convergence info saved to: step2_d3_g005_e001_Âü∫‰∫ée01/depth_3_gamma_0.05_run_3/chain_convergence_info.csv\n",
      "‚úÖ All chains restored and continued running!\n"
     ]
    }
   ],
   "source": [
    "# ‰ªéÁé∞ÊúâÁªìÊûúÊÅ¢Â§çÂπ∂ÁªßÁª≠ËøêË°å\n",
    "results = run_multi_chain_hlda_with_restore(\n",
    "    corpus=corpus,\n",
    "    depth=3,\n",
    "    gamma=0.05,\n",
    "    eta=0.01,\n",
    "    alpha=0.1,\n",
    "    n_chains=3,\n",
    "    max_iterations=200,  # ÁªßÁª≠ËøêË°åÁöÑËø≠‰ª£Ê¨°Êï∞\n",
    "    general_dir=\"step2_d3_g005_e001_Âü∫‰∫ée01\",  # Êñ∞ÁöÑËæìÂá∫ÁõÆÂΩï\n",
    "    source_dir=\"machine3_step1_d3_g005_Êî∂Êïõ\",  # ÂåÖÂê´ÂéüÂßãÈìæÁªìÊûúÁöÑÁõÆÂΩï\n",
    "    restore_from_existing=True,  # ÂêØÁî®ÊÅ¢Â§çÊ®°Âºè\n",
    "    back_window=5,\n",
    "    check_interval=20,\n",
    "    burn_in=50,\n",
    "    r_hat_threshold=1.1\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All chains restored and continued running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681c90ab-cc11-4159-8122-e136d9daa85c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
