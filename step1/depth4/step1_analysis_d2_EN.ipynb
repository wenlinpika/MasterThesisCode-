{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18ef8259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from scipy.special import gammaln\n",
    "\n",
    "def calculate_renyi_entropy_vectorized(node_data, all_words, eta_prior=1.0, renyi_alpha=2.0):\n",
    "    \"\"\"\n",
    "    Vectorized Renyi entropy calculation.\n",
    "\n",
    "    Parameters:\n",
    "    node_data: DataFrame, node data containing 'word' and 'count' columns\n",
    "    all_words: list, full vocabulary list\n",
    "    eta_prior: float, Dirichlet prior smoothing parameter\n",
    "    renyi_alpha: float, order parameter of Renyi entropy\n",
    "\n",
    "    Returns:\n",
    "    tuple: (entropy, nonzero_word_count) Renyi entropy value and number of nonzero words\n",
    "    \"\"\"\n",
    "    if len(all_words) == 0:\n",
    "        return 0.0, 0\n",
    "\n",
    "    # Create mapping from vocabulary word to index\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "\n",
    "    # Initialize count vector\n",
    "    counts = np.zeros(len(all_words))\n",
    "\n",
    "    # Fill in actual counts\n",
    "    for _, row in node_data.iterrows():\n",
    "        word = row['word']\n",
    "        if pd.notna(word) and word in word_to_idx:\n",
    "            counts[word_to_idx[word]] = row['count']\n",
    "\n",
    "    # Count nonzero words (before smoothing)\n",
    "    nonzero_word_count = np.sum(counts > 0)\n",
    "\n",
    "    # Apply eta smoothing\n",
    "    smoothed_counts = counts + eta_prior\n",
    "\n",
    "    # Compute probability distribution\n",
    "    probabilities = smoothed_counts / np.sum(smoothed_counts)\n",
    "\n",
    "    # Compute Renyi entropy (using natural logarithm)\n",
    "    if renyi_alpha == 1.0:\n",
    "        # Shannon entropy (with smoothing all probabilities > 0, no small constant needed)\n",
    "        entropy = -np.sum(probabilities * np.log(probabilities))\n",
    "    else:\n",
    "        # General Renyi entropy\n",
    "        entropy = (1 / (1 - renyi_alpha)) * np.log(np.sum(probabilities ** renyi_alpha))\n",
    "\n",
    "    return entropy, int(nonzero_word_count)\n",
    "\n",
    "def process_all_iteration_files(base_path=\".\", eta_prior=1.0, renyi_alpha=2.0):\n",
    "    \"\"\"\n",
    "    Process each iteration_node_word_distributions.csv file individually and save results.\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_node_word_distributions.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "\n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        print(f\"\\nProcessing file: {file_path}\")\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Clean column names: remove single/double quotes and extra spaces\n",
    "            df.columns = [col.strip(\"'\\\" \") for col in df.columns]\n",
    "\n",
    "            if 'node_id' not in df.columns:\n",
    "                print(f\"Warning: {file_path} is missing 'node_id' column, skipping this file\")\n",
    "                continue\n",
    "\n",
    "            max_iteration = df['iteration'].max()\n",
    "            last_iteration_data = df[df['iteration'] == max_iteration]\n",
    "            all_words = list(last_iteration_data['word'].dropna().unique())\n",
    "\n",
    "            print(f\"Last iteration: {max_iteration}, vocabulary size: {len(all_words)}, node count: {last_iteration_data['node_id'].nunique()}\")\n",
    "\n",
    "            results = []\n",
    "            for node_id in last_iteration_data['node_id'].unique():\n",
    "                node_data = last_iteration_data[last_iteration_data['node_id'] == node_id]\n",
    "\n",
    "                entropy, nonzero_words = calculate_renyi_entropy_vectorized(\n",
    "                    node_data, all_words, eta_prior, renyi_alpha\n",
    "                )\n",
    "\n",
    "                # Calculate sparsity (proportion of nonzero words)\n",
    "                sparsity_ratio = nonzero_words / len(all_words) if len(all_words) > 0 else 0\n",
    "\n",
    "                results.append({\n",
    "                    'node_id': node_id,\n",
    "                    'renyi_entropy_corrected': entropy,\n",
    "                    'nonzero_word_count': nonzero_words,\n",
    "                    'total_vocabulary_size': len(all_words),\n",
    "                    'sparsity_ratio': sparsity_ratio,\n",
    "                    'eta_prior': eta_prior,\n",
    "                    'renyi_alpha': renyi_alpha,\n",
    "                    'iteration': max_iteration\n",
    "                })\n",
    "\n",
    "            # Save the new corrected_renyi_entropy.csv file\n",
    "            results_df = pd.DataFrame(results)\n",
    "            output_path = os.path.join(folder_path, 'corrected_renyi_entropy.csv')\n",
    "            results_df.to_csv(output_path, index=False)\n",
    "            print(f\"Saved corrected Renyi entropy results to: {output_path}\")\n",
    "\n",
    "            # Print some summary statistics\n",
    "            print(\"Node vocabulary sparsity statistics:\")\n",
    "            print(f\"  - Average nonzero word count: {results_df['nonzero_word_count'].mean():.1f}\")\n",
    "            print(f\"  - Nonzero word count range: {results_df['nonzero_word_count'].min()}-{results_df['nonzero_word_count'].max()}\")\n",
    "            print(f\"  - Average sparsity: {results_df['sparsity_ratio'].mean():.3f}\")\n",
    "            print(\"=\" * 50)\n",
    "\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "            print(\"Detailed traceback:\")\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b21f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Start batch computing corrected Renyi entropy...\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_1/iteration_node_word_distributions.csv\n",
      "Last iteration: 115, vocabulary size: 1490, node count: 228\n",
      "Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_1/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average nonzero word count: 71.7\n",
      "  - Nonzero word count range: 0-842\n",
      "  - Average sparsity: 0.048\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_2/iteration_node_word_distributions.csv\n",
      "Last iteration: 115, vocabulary size: 1490, node count: 235\n",
      "Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_2/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average nonzero word count: 72.6\n",
      "  - Nonzero word count range: 0-888\n",
      "  - Average sparsity: 0.049\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_3/iteration_node_word_distributions.csv\n",
      "Last iteration: 115, vocabulary size: 1490, node count: 246\n",
      "Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_3/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average nonzero word count: 78.5\n",
      "  - Nonzero word count range: 0-760\n",
      "  - Average sparsity: 0.053\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_1/iteration_node_word_distributions.csv\n",
      "Last iteration: 195, vocabulary size: 1490, node count: 268\n",
      "Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_1/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average nonzero word count: 74.6\n",
      "  - Nonzero word count range: 0-708\n",
      "  - Average sparsity: 0.050\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_2/iteration_node_word_distributions.csv\n",
      "Last iteration: 195, vocabulary size: 1490, node count: 270\n",
      "Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_2/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average nonzero word count: 75.0\n",
      "  - Nonzero word count range: 0-713\n",
      "  - Average sparsity: 0.050\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_3/iteration_node_word_distributions.csv\n",
      "Last iteration: 195, vocabulary size: 1490, node count: 247\n",
      "Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_3/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average nonzero word count: 76.9\n",
      "  - Nonzero word count range: 0-729\n",
      "  - Average sparsity: 0.052\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g01_收敛/depth_4_gamma_0.1_run_1/iteration_node_word_distributions.csv\n",
      "Last iteration: 135, vocabulary size: 1490, node count: 250\n",
      "Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step1/2/d4_g01_收敛/depth_4_gamma_0.1_run_1/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average nonzero word count: 74.2\n",
      "  - Nonzero word count range: 0-780\n",
      "  - Average sparsity: 0.050\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g01_收敛/depth_4_gamma_0.1_run_2/iteration_node_word_distributions.csv\n",
      "Last iteration: 135, vocabulary size: 1490, node count: 306\n",
      "Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step1/2/d4_g01_收敛/depth_4_gamma_0.1_run_2/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average nonzero word count: 70.5\n",
      "  - Nonzero word count range: 0-658\n",
      "  - Average sparsity: 0.047\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g01_收敛/depth_4_gamma_0.1_run_3/iteration_node_word_distributions.csv\n",
      "Last iteration: 135, vocabulary size: 1490, node count: 273\n",
      "Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step1/2/d4_g01_收敛/depth_4_gamma_0.1_run_3/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average nonzero word count: 70.6\n",
      "  - Nonzero word count range: 0-706\n",
      "  - Average sparsity: 0.047\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g0001_收敛/depth_4_gamma_0.001_run_1/iteration_node_word_distributions.csv\n",
      "Last iteration: 175, vocabulary size: 1490, node count: 258\n",
      "Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step1/2/d4_g0001_收敛/depth_4_gamma_0.001_run_1/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average nonzero word count: 75.9\n",
      "  - Nonzero word count range: 0-707\n",
      "  - Average sparsity: 0.051\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g0001_收敛/depth_4_gamma_0.001_run_2/iteration_node_word_distributions.csv\n",
      "Last iteration: 175, vocabulary size: 1490, node count: 240\n",
      "Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step1/2/d4_g0001_收敛/depth_4_gamma_0.001_run_2/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average nonzero word count: 78.5\n",
      "  - Nonzero word count range: 0-696\n",
      "  - Average sparsity: 0.053\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g0001_收敛/depth_4_gamma_0.001_run_3/iteration_node_word_distributions.csv\n",
      "Last iteration: 175, vocabulary size: 1490, node count: 233\n",
      "Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step1/2/d4_g0001_收敛/depth_4_gamma_0.001_run_3/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average nonzero word count: 72.2\n",
      "  - Nonzero word count range: 0-870\n",
      "  - Average sparsity: 0.048\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g005_收敛/depth_4_gamma_0.05_run_1/iteration_node_word_distributions.csv\n",
      "Last iteration: 315, vocabulary size: 1490, node count: 282\n",
      "Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step1/2/d4_g005_收敛/depth_4_gamma_0.05_run_1/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average nonzero word count: 72.8\n",
      "  - Nonzero word count range: 0-671\n",
      "  - Average sparsity: 0.049\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g005_收敛/depth_4_gamma_0.05_run_2/iteration_node_word_distributions.csv\n",
      "Last iteration: 315, vocabulary size: 1490, node count: 267\n",
      "Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step1/2/d4_g005_收敛/depth_4_gamma_0.05_run_2/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average nonzero word count: 79.6\n",
      "  - Nonzero word count range: 0-677\n",
      "  - Average sparsity: 0.053\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g005_收敛/depth_4_gamma_0.05_run_3/iteration_node_word_distributions.csv\n",
      "Last iteration: 315, vocabulary size: 1490, node count: 245\n",
      "Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step1/2/d4_g005_收敛/depth_4_gamma_0.05_run_3/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average nonzero word count: 79.9\n",
      "  - Nonzero word count range: 0-695\n",
      "  - Average sparsity: 0.054\n",
      "==================================================\n",
      "==================================================\n",
      "All processing completed!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step1/2\"  # root directory\n",
    "eta_prior = 0.1  # Dirichlet prior smoothing parameter\n",
    "renyi_alpha = 2.0  # Renyi entropy order parameter\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Start batch computing corrected Renyi entropy...\")\n",
    "print(\"=\" * 50)\n",
    "process_all_iteration_files(base_path, eta_prior, renyi_alpha)\n",
    "print(\"=\" * 50)\n",
    "print(\"All processing completed!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d81643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_node_document_counts(path_structures_df):\n",
    "    \"\"\"\n",
    "    Aggregate document counts from leaf nodes upward and compute node hierarchy.\n",
    "\n",
    "    Parameters:\n",
    "    path_structures_df: DataFrame, data from iteration_path_structures.csv (filtered to last iteration)\n",
    "\n",
    "    Returns:\n",
    "    dict: mapping {node_id: {'document_count': int, 'layer': int, 'parent_id': int, 'child_ids': list, 'child_count': int}}\n",
    "    \"\"\"\n",
    "    # Find all layer columns\n",
    "    layer_columns = [col for col in path_structures_df.columns if col.startswith('layer_') and col.endswith('_node_id')]\n",
    "    layer_columns.sort()\n",
    "    max_layer_idx = len(layer_columns) - 1\n",
    "\n",
    "    print(f\"[DEBUG] Found layer columns: {layer_columns}\")\n",
    "    print(f\"[DEBUG] Max layer index: {max_layer_idx}\")\n",
    "\n",
    "    # Initialize node info dictionary\n",
    "    node_info = {}\n",
    "\n",
    "    # Handle leaf nodes using leaf_node_id column\n",
    "    for _, row in path_structures_df.iterrows():\n",
    "        leaf_node = row.get('leaf_node_id')\n",
    "        if pd.notna(leaf_node):\n",
    "            if leaf_node not in node_info:\n",
    "                node_info[leaf_node] = {\n",
    "                    'document_count': 0,\n",
    "                    'layer': max_layer_idx,\n",
    "                    'parent_id': None,\n",
    "                    'child_ids': [],\n",
    "                    'child_count': 0\n",
    "                }\n",
    "            node_info[leaf_node]['document_count'] += row.get('document_count', 0)\n",
    "\n",
    "    # Build parent-child relationships and layer info\n",
    "    for _, row in path_structures_df.iterrows():\n",
    "        path_nodes = []\n",
    "        for layer_idx in range(max_layer_idx + 1):\n",
    "            layer_col = f'layer_{layer_idx}_node_id'\n",
    "            if layer_col in path_structures_df.columns and pd.notna(row.get(layer_col)):\n",
    "                path_nodes.append(row[layer_col])\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Set layer and parent-child for each node in the path\n",
    "        for i, node in enumerate(path_nodes):\n",
    "            if node not in node_info:\n",
    "                node_info[node] = {\n",
    "                    'document_count': 0,\n",
    "                    'layer': i,\n",
    "                    'parent_id': None,\n",
    "                    'child_ids': [],\n",
    "                    'child_count': 0\n",
    "                }\n",
    "            else:\n",
    "                node_info[node]['layer'] = i\n",
    "\n",
    "            if i > 0:\n",
    "                parent_node = path_nodes[i - 1]\n",
    "                node_info[node]['parent_id'] = parent_node\n",
    "\n",
    "                if parent_node not in node_info:\n",
    "                    node_info[parent_node] = {\n",
    "                        'document_count': 0,\n",
    "                        'layer': i - 1,\n",
    "                        'parent_id': None,\n",
    "                        'child_ids': [],\n",
    "                        'child_count': 0\n",
    "                    }\n",
    "\n",
    "                if node not in node_info[parent_node]['child_ids']:\n",
    "                    node_info[parent_node]['child_ids'].append(node)\n",
    "\n",
    "    # Aggregate document counts upward from second-last layer to root\n",
    "    for layer_idx in range(max_layer_idx - 1, -1, -1):\n",
    "        layer_col = f'layer_{layer_idx}_node_id'\n",
    "        if layer_col not in path_structures_df.columns:\n",
    "            continue\n",
    "\n",
    "        layer_nodes = path_structures_df[layer_col].dropna().unique()\n",
    "        for node in layer_nodes:\n",
    "            if node in node_info and node_info[node]['document_count'] == 0:\n",
    "                total_docs = path_structures_df[path_structures_df[layer_col] == node]['document_count'].sum()\n",
    "                node_info[node]['document_count'] = int(total_docs)\n",
    "\n",
    "    # Compute child counts\n",
    "    for node_id, info in node_info.items():\n",
    "        info['child_count'] = len(info.get('child_ids', []))\n",
    "\n",
    "    return node_info\n",
    "\n",
    "def add_document_counts_to_entropy_files(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    Add document counts and hierarchy info to corrected_renyi_entropy.csv files.\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_path_structures.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "\n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        print(f\"\\nProcessing path structure file: {file_path}\")\n",
    "\n",
    "        try:\n",
    "            # Read path structures file\n",
    "            df = pd.read_csv(file_path)\n",
    "            df.columns = [col.strip(\"'\\\" \") for col in df.columns]\n",
    "\n",
    "            # Get last iteration\n",
    "            max_iteration = df['iteration'].max()\n",
    "            last_iteration_data = df[df['iteration'] == max_iteration]\n",
    "\n",
    "            print(f\"Last iteration: {max_iteration}, number of paths: {len(last_iteration_data)}\")\n",
    "\n",
    "            # Compute node document counts and hierarchy\n",
    "            node_info = calculate_node_document_counts(last_iteration_data)\n",
    "\n",
    "            print(f\"Computed info for {len(node_info)} nodes\")\n",
    "\n",
    "            # Read corresponding corrected_renyi_entropy.csv\n",
    "            entropy_file = os.path.join(folder_path, 'corrected_renyi_entropy.csv')\n",
    "            if os.path.exists(entropy_file):\n",
    "                entropy_df = pd.read_csv(entropy_file)\n",
    "\n",
    "                # Add new columns: document_count, layer, parent_id, child_ids, child_count\n",
    "                entropy_df['document_count'] = entropy_df['node_id'].map(lambda x: node_info.get(x, {}).get('document_count', 0))\n",
    "                entropy_df['layer'] = entropy_df['node_id'].map(lambda x: node_info.get(x, {}).get('layer', -1))\n",
    "                entropy_df['parent_id'] = entropy_df['node_id'].map(lambda x: node_info.get(x, {}).get('parent_id', None))\n",
    "\n",
    "                # Format child_ids as bracketed list string (empty string if no children)\n",
    "                entropy_df['child_ids'] = entropy_df['node_id'].map(\n",
    "                    lambda x: '[' + ','.join(map(str, node_info.get(x, {}).get('child_ids', []))) + ']'\n",
    "                    if node_info.get(x, {}).get('child_ids') else ''\n",
    "                )\n",
    "\n",
    "                entropy_df['child_count'] = entropy_df['node_id'].map(lambda x: len(node_info.get(x, {}).get('child_ids', [])))\n",
    "\n",
    "                # Save updated file\n",
    "                entropy_df.to_csv(entropy_file, index=False)\n",
    "                print(f\"Updated {entropy_file} with columns: document_count, layer, parent_id, child_ids, child_count\")\n",
    "\n",
    "                # Print some statistics\n",
    "                print(\"Node hierarchy statistics:\")\n",
    "                print(f\"  - Layer distribution: {entropy_df['layer'].value_counts().sort_index().to_dict()}\")\n",
    "                print(f\"  - Document count range: {entropy_df['document_count'].min()} - {entropy_df['document_count'].max()}\")\n",
    "                print(f\"  - Root node count: {entropy_df[entropy_df['parent_id'].isna()].shape[0]}\")\n",
    "                print(f\"  - Leaf node count: {entropy_df[entropy_df['child_ids'] == ''].shape[0]}\")\n",
    "                print(f\"  - Child count distribution: {entropy_df['child_count'].value_counts().sort_index().to_dict()}\")\n",
    "            else:\n",
    "                print(f\"Warning: corresponding entropy file not found: {entropy_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "            print(\"Traceback:\")\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e230e0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Start adding document counts and hierarchy information to entropy files...\n",
      "==================================================\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_1/iteration_path_structures.csv\n",
      "Last iteration: 115, number of paths: 141\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] Max layer index: 3\n",
      "Computed info for 228 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_1/corrected_renyi_entropy.csv with columns: document_count, layer, parent_id, child_ids, child_count\n",
      "Node hierarchy statistics:\n",
      "  - Layer distribution: {0: 1, 1: 19, 2: 67, 3: 141}\n",
      "  - Document count range: 1 - 970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 141\n",
      "  - Child count distribution: {0: 141, 1: 37, 2: 28, 3: 8, 4: 9, 5: 1, 7: 1, 19: 2, 24: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_2/iteration_path_structures.csv\n",
      "Last iteration: 115, number of paths: 155\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] Max layer index: 3\n",
      "Computed info for 235 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_2/corrected_renyi_entropy.csv with columns: document_count, layer, parent_id, child_ids, child_count\n",
      "Node hierarchy statistics:\n",
      "  - Layer distribution: {0: 1, 1: 17, 2: 62, 3: 155}\n",
      "  - Document count range: 1 - 970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 155\n",
      "  - Child count distribution: {0: 155, 1: 30, 2: 27, 3: 8, 4: 6, 5: 4, 6: 2, 17: 1, 20: 1, 33: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_3/iteration_path_structures.csv\n",
      "Last iteration: 115, number of paths: 165\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] Max layer index: 3\n",
      "Computed info for 246 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_3/corrected_renyi_entropy.csv with columns: document_count, layer, parent_id, child_ids, child_count\n",
      "Node hierarchy statistics:\n",
      "  - Layer distribution: {0: 1, 1: 14, 2: 66, 3: 165}\n",
      "  - Document count range: 1 - 970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 165\n",
      "  - Child count distribution: {0: 165, 1: 32, 2: 19, 3: 12, 4: 6, 5: 5, 6: 1, 7: 1, 8: 1, 13: 1, 14: 1, 18: 1, 24: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_1/iteration_path_structures.csv\n",
      "Last iteration: 195, number of paths: 181\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] Max layer index: 3\n",
      "Computed info for 268 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_1/corrected_renyi_entropy.csv with columns: document_count, layer, parent_id, child_ids, child_count\n",
      "Node hierarchy statistics:\n",
      "  - Layer distribution: {0: 1, 1: 13, 2: 73, 3: 181}\n",
      "  - Document count range: 1 - 970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 181\n",
      "  - Child count distribution: {0: 181, 1: 32, 2: 19, 3: 19, 4: 6, 5: 6, 6: 1, 7: 1, 13: 1, 26: 1, 34: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_2/iteration_path_structures.csv\n",
      "Last iteration: 195, number of paths: 174\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] Max layer index: 3\n",
      "Computed info for 270 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_2/corrected_renyi_entropy.csv with columns: document_count, layer, parent_id, child_ids, child_count\n",
      "Node hierarchy statistics:\n",
      "  - Layer distribution: {0: 1, 1: 20, 2: 75, 3: 174}\n",
      "  - Document count range: 1 - 970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 174\n",
      "  - Child count distribution: {0: 174, 1: 34, 2: 34, 3: 16, 4: 4, 5: 4, 7: 1, 20: 1, 26: 1, 30: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_3/iteration_path_structures.csv\n",
      "Last iteration: 195, number of paths: 163\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] Max layer index: 3\n",
      "Computed info for 247 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_3/corrected_renyi_entropy.csv with columns: document_count, layer, parent_id, child_ids, child_count\n",
      "Node hierarchy statistics:\n",
      "  - Layer distribution: {0: 1, 1: 17, 2: 66, 3: 163}\n",
      "  - Document count range: 1 - 970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 163\n",
      "  - Child count distribution: {0: 163, 1: 32, 2: 21, 3: 19, 4: 5, 5: 1, 6: 3, 17: 1, 27: 1, 28: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step1/2/d4_g01_收敛/depth_4_gamma_0.1_run_1/iteration_path_structures.csv\n",
      "Last iteration: 135, number of paths: 171\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] Max layer index: 3\n",
      "Computed info for 250 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step1/2/d4_g01_收敛/depth_4_gamma_0.1_run_1/corrected_renyi_entropy.csv with columns: document_count, layer, parent_id, child_ids, child_count\n",
      "Node hierarchy statistics:\n",
      "  - Layer distribution: {0: 1, 1: 12, 2: 66, 3: 171}\n",
      "  - Document count range: 1 - 970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 171\n",
      "  - Child count distribution: {0: 171, 1: 26, 2: 22, 3: 13, 4: 9, 5: 4, 6: 2, 12: 1, 29: 1, 31: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step1/2/d4_g01_收敛/depth_4_gamma_0.1_run_2/iteration_path_structures.csv\n",
      "Last iteration: 135, number of paths: 206\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] Max layer index: 3\n",
      "Computed info for 306 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step1/2/d4_g01_收敛/depth_4_gamma_0.1_run_2/corrected_renyi_entropy.csv with columns: document_count, layer, parent_id, child_ids, child_count\n",
      "Node hierarchy statistics:\n",
      "  - Layer distribution: {0: 1, 1: 23, 2: 76, 3: 206}\n",
      "  - Document count range: 1 - 970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 206\n",
      "  - Child count distribution: {0: 206, 1: 33, 2: 37, 3: 10, 4: 10, 5: 6, 6: 1, 23: 1, 27: 1, 42: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step1/2/d4_g01_收敛/depth_4_gamma_0.1_run_3/iteration_path_structures.csv\n",
      "Last iteration: 135, number of paths: 188\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] Max layer index: 3\n",
      "Computed info for 273 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step1/2/d4_g01_收敛/depth_4_gamma_0.1_run_3/corrected_renyi_entropy.csv with columns: document_count, layer, parent_id, child_ids, child_count\n",
      "Node hierarchy statistics:\n",
      "  - Layer distribution: {0: 1, 1: 15, 2: 69, 3: 188}\n",
      "  - Document count range: 1 - 970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 188\n",
      "  - Child count distribution: {0: 188, 1: 27, 2: 25, 3: 14, 4: 11, 5: 5, 15: 1, 31: 1, 38: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step1/2/d4_g0001_收敛/depth_4_gamma_0.001_run_1/iteration_path_structures.csv\n",
      "Last iteration: 175, number of paths: 168\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] Max layer index: 3\n",
      "Computed info for 258 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step1/2/d4_g0001_收敛/depth_4_gamma_0.001_run_1/corrected_renyi_entropy.csv with columns: document_count, layer, parent_id, child_ids, child_count\n",
      "Node hierarchy statistics:\n",
      "  - Layer distribution: {0: 1, 1: 17, 2: 72, 3: 168}\n",
      "  - Document count range: 1 - 970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 168\n",
      "  - Child count distribution: {0: 168, 1: 34, 2: 31, 3: 9, 4: 9, 5: 3, 6: 1, 17: 1, 28: 1, 32: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step1/2/d4_g0001_收敛/depth_4_gamma_0.001_run_2/iteration_path_structures.csv\n",
      "Last iteration: 175, number of paths: 162\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] Max layer index: 3\n",
      "Computed info for 240 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step1/2/d4_g0001_收敛/depth_4_gamma_0.001_run_2/corrected_renyi_entropy.csv with columns: document_count, layer, parent_id, child_ids, child_count\n",
      "Node hierarchy statistics:\n",
      "  - Layer distribution: {0: 1, 1: 14, 2: 63, 3: 162}\n",
      "  - Document count range: 1 - 970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 162\n",
      "  - Child count distribution: {0: 162, 1: 27, 2: 26, 3: 14, 4: 2, 5: 4, 6: 1, 8: 1, 14: 1, 30: 1, 32: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step1/2/d4_g0001_收敛/depth_4_gamma_0.001_run_3/iteration_path_structures.csv\n",
      "Last iteration: 175, number of paths: 143\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] Max layer index: 3\n",
      "Computed info for 233 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step1/2/d4_g0001_收敛/depth_4_gamma_0.001_run_3/corrected_renyi_entropy.csv with columns: document_count, layer, parent_id, child_ids, child_count\n",
      "Node hierarchy statistics:\n",
      "  - Layer distribution: {0: 1, 1: 20, 2: 69, 3: 143}\n",
      "  - Document count range: 1 - 970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 143\n",
      "  - Child count distribution: {0: 143, 1: 31, 2: 35, 3: 9, 4: 7, 5: 5, 12: 1, 19: 1, 20: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step1/2/d4_g005_收敛/depth_4_gamma_0.05_run_1/iteration_path_structures.csv\n",
      "Last iteration: 315, number of paths: 187\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] Max layer index: 3\n",
      "Computed info for 282 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step1/2/d4_g005_收敛/depth_4_gamma_0.05_run_1/corrected_renyi_entropy.csv with columns: document_count, layer, parent_id, child_ids, child_count\n",
      "Node hierarchy statistics:\n",
      "  - Layer distribution: {0: 1, 1: 19, 2: 75, 3: 187}\n",
      "  - Document count range: 1 - 970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 187\n",
      "  - Child count distribution: {0: 187, 1: 38, 2: 30, 3: 16, 4: 6, 5: 1, 6: 1, 19: 1, 35: 1, 46: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step1/2/d4_g005_收敛/depth_4_gamma_0.05_run_2/iteration_path_structures.csv\n",
      "Last iteration: 315, number of paths: 181\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] Max layer index: 3\n",
      "Computed info for 267 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step1/2/d4_g005_收敛/depth_4_gamma_0.05_run_2/corrected_renyi_entropy.csv with columns: document_count, layer, parent_id, child_ids, child_count\n",
      "Node hierarchy statistics:\n",
      "  - Layer distribution: {0: 1, 1: 16, 2: 69, 3: 181}\n",
      "  - Document count range: 1 - 970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 181\n",
      "  - Child count distribution: {0: 181, 1: 28, 2: 26, 3: 15, 4: 9, 5: 2, 6: 2, 7: 1, 16: 1, 29: 1, 31: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step1/2/d4_g005_收敛/depth_4_gamma_0.05_run_3/iteration_path_structures.csv\n",
      "Last iteration: 315, number of paths: 169\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] Max layer index: 3\n",
      "Computed info for 245 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step1/2/d4_g005_收敛/depth_4_gamma_0.05_run_3/corrected_renyi_entropy.csv with columns: document_count, layer, parent_id, child_ids, child_count\n",
      "Node hierarchy statistics:\n",
      "  - Layer distribution: {0: 1, 1: 14, 2: 61, 3: 169}\n",
      "  - Document count range: 1 - 970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 169\n",
      "  - Child count distribution: {0: 169, 1: 23, 2: 27, 3: 12, 4: 7, 5: 3, 6: 1, 14: 1, 25: 1, 43: 1}\n",
      "==================================================\n",
      "Document counts and hierarchy information added.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Main function: add document counts and hierarchy info to entropy files\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step1/2\"  # root directory\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Start adding document counts and hierarchy information to entropy files...\")\n",
    "print(\"=\" * 50)\n",
    "add_document_counts_to_entropy_files(base_path)\n",
    "print(\"=\" * 50)\n",
    "print(\"Document counts and hierarchy information added.\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd6ab468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jensen_shannon_distances_with_weighted_entropy(base_path=\".\", eta=0.1):\n",
    "    \"\"\"\n",
    "    Calculate Jensen-Shannon distances between nodes in each layer and the document-count-weighted average Renyi entropy.\n",
    "    \n",
    "    Parameters:\n",
    "    base_path: str, root directory path\n",
    "    eta: float, Dirichlet smoothing parameter\n",
    "    \"\"\"\n",
    "    # Find all iteration_node_word_distributions.csv files\n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_node_word_distributions.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        print(f\"\\nProcessing file: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Read word distribution data\n",
    "            word_df = pd.read_csv(file_path)\n",
    "            word_df.columns = [col.strip(\"'\\\" \") for col in word_df.columns]\n",
    "            \n",
    "            # Get data from the last iteration\n",
    "            max_iteration = word_df['iteration'].max()\n",
    "            last_iteration_data = word_df[word_df['iteration'] == max_iteration]\n",
    "            \n",
    "            # Get the full vocabulary\n",
    "            all_words = sorted(list(last_iteration_data['word'].dropna().unique()))\n",
    "            print(f\"Vocabulary size: {len(all_words)}\")\n",
    "            \n",
    "            # Read the entropy file to get hierarchy information\n",
    "            entropy_file = os.path.join(folder_path, 'corrected_renyi_entropy.csv')\n",
    "            if not os.path.exists(entropy_file):\n",
    "                print(f\"Warning: Entropy file not found: {entropy_file}\")\n",
    "                continue\n",
    "                \n",
    "            entropy_df = pd.read_csv(entropy_file)\n",
    "            \n",
    "            # Group nodes by layer\n",
    "            layers = entropy_df.groupby('layer')['node_id'].apply(list).to_dict()\n",
    "            print(f\"Layer distribution: {[(layer, len(nodes)) for layer, nodes in layers.items()]}\")\n",
    "            \n",
    "            # Build probability distribution for each node\n",
    "            node_distributions = {}\n",
    "            \n",
    "            for node_id in entropy_df['node_id'].unique():\n",
    "                # Get the word distribution for this node\n",
    "                node_words = last_iteration_data[last_iteration_data['node_id'] == node_id]\n",
    "                \n",
    "                # Initialize count vector\n",
    "                counts = np.zeros(len(all_words))\n",
    "                word_to_idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "                \n",
    "                # Fill in actual counts\n",
    "                for _, row in node_words.iterrows():\n",
    "                    word = row['word']\n",
    "                    if pd.notna(word) and word in word_to_idx:\n",
    "                        counts[word_to_idx[word]] = row['count']\n",
    "                \n",
    "                # Add Dirichlet smoothing\n",
    "                smoothed_counts = counts + eta\n",
    "                \n",
    "                # Calculate probability distribution\n",
    "                probabilities = smoothed_counts / np.sum(smoothed_counts)\n",
    "                node_distributions[node_id] = probabilities\n",
    "            \n",
    "            # Calculate JS distances and weighted average entropy for nodes within each layer\n",
    "            all_js_distances = []\n",
    "            layer_avg_distances = []\n",
    "            \n",
    "            for layer, layer_nodes in layers.items():\n",
    "                print(f\"\\nCalculating JS distance and weighted average entropy for Layer {layer} ({len(layer_nodes)} nodes)\")\n",
    "                \n",
    "                layer_js_distances = []\n",
    "                n = len(layer_nodes)\n",
    "                \n",
    "                # Calculate JS distance for all pairs of nodes in this layer\n",
    "                for i, node1 in enumerate(layer_nodes):\n",
    "                    for j, node2 in enumerate(layer_nodes):\n",
    "                        if i < j:  # Only calculate for the upper triangle to avoid duplicates and self-comparison\n",
    "                            if node1 in node_distributions and node2 in node_distributions:\n",
    "                                p = node_distributions[node1]\n",
    "                                q = node_distributions[node2]\n",
    "                                \n",
    "                                # Calculate Jensen-Shannon distance\n",
    "                                js_distance = jensen_shannon_distance(p, q)\n",
    "                                \n",
    "                                layer_js_distances.append({\n",
    "                                    'layer': layer,\n",
    "                                    'node1_id': node1,\n",
    "                                    'node2_id': node2,\n",
    "                                    'js_distance': js_distance,\n",
    "                                    'node1_doc_count': entropy_df[entropy_df['node_id'] == node1]['document_count'].iloc[0] if len(entropy_df[entropy_df['node_id'] == node1]) > 0 else 0,\n",
    "                                    'node2_doc_count': entropy_df[entropy_df['node_id'] == node2]['document_count'].iloc[0] if len(entropy_df[entropy_df['node_id'] == node2]) > 0 else 0\n",
    "                                })\n",
    "                \n",
    "                all_js_distances.extend(layer_js_distances)\n",
    "                \n",
    "                # Calculate the average JS distance for the layer\n",
    "                avg_js_distance = 0.0\n",
    "                if layer_js_distances and n > 1:\n",
    "                    total_js_distance = sum(d['js_distance'] for d in layer_js_distances)\n",
    "                    max_pairs = n * (n - 1) // 2  # n*(n-1)/2\n",
    "                    avg_js_distance = total_js_distance / max_pairs\n",
    "                \n",
    "                # Calculate the document-count-weighted average Renyi entropy for the layer\n",
    "                layer_entropy_data = entropy_df[entropy_df['layer'] == layer]\n",
    "                total_docs = layer_entropy_data['document_count'].sum()\n",
    "                \n",
    "                if total_docs > 0:\n",
    "                    # Calculate weighted average entropy: sum(doc_count * entropy) / total_doc_count\n",
    "                    weighted_entropy = (layer_entropy_data['document_count'] * layer_entropy_data['renyi_entropy_corrected']).sum() / total_docs\n",
    "                else:\n",
    "                    weighted_entropy = 0.0\n",
    "                \n",
    "                layer_avg_distances.append({\n",
    "                    'layer': layer,\n",
    "                    'node_count': n,\n",
    "                    'total_pairs': len(layer_js_distances),\n",
    "                    'max_pairs': n * (n - 1) // 2 if n > 1 else 0,\n",
    "                    'sum_js_distance': sum(d['js_distance'] for d in layer_js_distances),\n",
    "                    'avg_js_distance': avg_js_distance,\n",
    "                    'total_documents': total_docs,\n",
    "                    'weighted_avg_renyi_entropy': weighted_entropy\n",
    "                })\n",
    "                \n",
    "                print(f\"  - Node count: {n}\")\n",
    "                print(f\"  - Calculated node pairs: {len(layer_js_distances)}\")\n",
    "                print(f\"  - Theoretical max node pairs: {n * (n - 1) // 2 if n > 1 else 0}\")\n",
    "                print(f\"  - Average JS distance: {avg_js_distance:.4f}\")\n",
    "                print(f\"  - Total documents: {total_docs}\")\n",
    "                print(f\"  - Document-weighted average Renyi entropy: {weighted_entropy:.4f}\")\n",
    "                print(\"=\" * 50)\n",
    "            \n",
    "            # Save detailed JS distance results\n",
    "            if all_js_distances:\n",
    "                js_df = pd.DataFrame(all_js_distances)\n",
    "                output_path = os.path.join(folder_path, 'jensen_shannon_distances.csv')\n",
    "                js_df.to_csv(output_path, index=False)\n",
    "                print(f\"\\nSaved detailed JS distance results to: {output_path}\")\n",
    "            \n",
    "            # Save average JS distance and weighted entropy results for each layer\n",
    "            if layer_avg_distances:\n",
    "                avg_df = pd.DataFrame(layer_avg_distances)\n",
    "                avg_output_path = os.path.join(folder_path, 'layer_average_js_distances.csv')\n",
    "                avg_df.to_csv(avg_output_path, index=False)\n",
    "                print(f\"Saved layer average JS distance and weighted entropy results to: {avg_output_path}\")\n",
    "                \n",
    "                # Overall statistics\n",
    "                print(f\"\\nOverall Statistics:\")\n",
    "                print(f\"  - Total layers: {len(layer_avg_distances)}\")\n",
    "                print(f\"  - Layer statistics:\")\n",
    "                for row in layer_avg_distances:\n",
    "                    print(f\"    Layer {row['layer']}: JS distance={row['avg_js_distance']:.4f}, Weighted entropy={row['weighted_avg_renyi_entropy']:.4f} (based on {row['node_count']} nodes, {row['total_documents']} documents)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "            print(\"Detailed traceback:\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "def jensen_shannon_distance(p, q):\n",
    "    \"\"\"\n",
    "    Calculate the Jensen-Shannon distance between two probability distributions.\n",
    "    \n",
    "    Parameters:\n",
    "    p, q: numpy arrays, probability distributions\n",
    "    \n",
    "    Returns:\n",
    "    float: Jensen-Shannon distance\n",
    "    \"\"\"\n",
    "    # Ensure probability distributions are normalized\n",
    "    p = p / np.sum(p)\n",
    "    q = q / np.sum(q)\n",
    "    \n",
    "    # Calculate the average distribution\n",
    "    m = 0.5 * (p + q)\n",
    "    \n",
    "    # Calculate KL divergence (using natural logarithm)\n",
    "    def kl_divergence(x, y):\n",
    "        # Avoid log(0)\n",
    "        mask = (x > 0) & (y > 0)\n",
    "        if np.sum(mask) == 0:\n",
    "            return 0.0\n",
    "        return np.sum(x[mask] * np.log(x[mask] / y[mask]))\n",
    "    \n",
    "    # Calculate Jensen-Shannon divergence\n",
    "    js_divergence = 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)\n",
    "    \n",
    "    # Convert to distance (square root)\n",
    "    js_distance = np.sqrt(js_divergence)\n",
    "    \n",
    "    return js_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7363e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Start calculating Jensen-Shannon distances and weighted average Renyi entropy...\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_1/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Layer distribution: [(0, 1), (1, 19), (2, 67), (3, 141)]\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 0 (1 nodes)\n",
      "  - Node count: 1\n",
      "  - Calculated node pairs: 0\n",
      "  - Theoretical max node pairs: 0\n",
      "  - Average JS distance: 0.0000\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 4.8932\n",
      "==================================================\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 1 (19 nodes)\n",
      "  - Node count: 19\n",
      "  - Calculated node pairs: 171\n",
      "  - Theoretical max node pairs: 171\n",
      "  - Average JS distance: 0.3923\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 5.0848\n",
      "==================================================\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 2 (67 nodes)\n",
      "  - Node count: 67\n",
      "  - Calculated node pairs: 2211\n",
      "  - Theoretical max node pairs: 2211\n",
      "  - Average JS distance: 0.4114\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 5.2341\n",
      "==================================================\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 3 (141 nodes)\n",
      "  - Node count: 141\n",
      "  - Calculated node pairs: 9870\n",
      "  - Theoretical max node pairs: 9870\n",
      "  - Average JS distance: 0.4564\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 5.2688\n",
      "==================================================\n",
      "\n",
      "Saved detailed JS distance results to: /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_1/jensen_shannon_distances.csv\n",
      "Saved layer average JS distance and weighted entropy results to: /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_1/layer_average_js_distances.csv\n",
      "\n",
      "Overall Statistics:\n",
      "  - Total layers: 4\n",
      "  - Layer statistics:\n",
      "    Layer 0: JS distance=0.0000, Weighted entropy=4.8932 (based on 1 nodes, 970 documents)\n",
      "    Layer 1: JS distance=0.3923, Weighted entropy=5.0848 (based on 19 nodes, 970 documents)\n",
      "    Layer 2: JS distance=0.4114, Weighted entropy=5.2341 (based on 67 nodes, 970 documents)\n",
      "    Layer 3: JS distance=0.4564, Weighted entropy=5.2688 (based on 141 nodes, 970 documents)\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_2/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Layer distribution: [(0, 1), (1, 17), (2, 62), (3, 155)]\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 0 (1 nodes)\n",
      "  - Node count: 1\n",
      "  - Calculated node pairs: 0\n",
      "  - Theoretical max node pairs: 0\n",
      "  - Average JS distance: 0.0000\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 4.9282\n",
      "==================================================\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 1 (17 nodes)\n",
      "  - Node count: 17\n",
      "  - Calculated node pairs: 136\n",
      "  - Theoretical max node pairs: 136\n",
      "  - Average JS distance: 0.4628\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 4.9661\n",
      "==================================================\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 2 (62 nodes)\n",
      "  - Node count: 62\n",
      "  - Calculated node pairs: 1891\n",
      "  - Theoretical max node pairs: 1891\n",
      "  - Average JS distance: 0.3820\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 5.3932\n",
      "==================================================\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 3 (155 nodes)\n",
      "  - Node count: 155\n",
      "  - Calculated node pairs: 11935\n",
      "  - Theoretical max node pairs: 11935\n",
      "  - Average JS distance: 0.4637\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 5.1939\n",
      "==================================================\n",
      "\n",
      "Saved detailed JS distance results to: /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_2/jensen_shannon_distances.csv\n",
      "Saved layer average JS distance and weighted entropy results to: /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_2/layer_average_js_distances.csv\n",
      "\n",
      "Overall Statistics:\n",
      "  - Total layers: 4\n",
      "  - Layer statistics:\n",
      "    Layer 0: JS distance=0.0000, Weighted entropy=4.9282 (based on 1 nodes, 970 documents)\n",
      "    Layer 1: JS distance=0.4628, Weighted entropy=4.9661 (based on 17 nodes, 970 documents)\n",
      "    Layer 2: JS distance=0.3820, Weighted entropy=5.3932 (based on 62 nodes, 970 documents)\n",
      "    Layer 3: JS distance=0.4637, Weighted entropy=5.1939 (based on 155 nodes, 970 documents)\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_3/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Layer distribution: [(0, 1), (1, 14), (2, 66), (3, 165)]\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 0 (1 nodes)\n",
      "  - Node count: 1\n",
      "  - Calculated node pairs: 0\n",
      "  - Theoretical max node pairs: 0\n",
      "  - Average JS distance: 0.0000\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 4.9502\n",
      "==================================================\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 1 (14 nodes)\n",
      "  - Node count: 14\n",
      "  - Calculated node pairs: 91\n",
      "  - Theoretical max node pairs: 91\n",
      "  - Average JS distance: 0.4780\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 4.8989\n",
      "==================================================\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 2 (66 nodes)\n",
      "  - Node count: 66\n",
      "  - Calculated node pairs: 2145\n",
      "  - Theoretical max node pairs: 2145\n",
      "  - Average JS distance: 0.4197\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 5.1741\n",
      "==================================================\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 3 (165 nodes)\n",
      "  - Node count: 165\n",
      "  - Calculated node pairs: 13530\n",
      "  - Theoretical max node pairs: 13530\n",
      "  - Average JS distance: 0.4738\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 5.1071\n",
      "==================================================\n",
      "\n",
      "Saved detailed JS distance results to: /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_3/jensen_shannon_distances.csv\n",
      "Saved layer average JS distance and weighted entropy results to: /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_3/layer_average_js_distances.csv\n",
      "\n",
      "Overall Statistics:\n",
      "  - Total layers: 4\n",
      "  - Layer statistics:\n",
      "    Layer 0: JS distance=0.0000, Weighted entropy=4.9502 (based on 1 nodes, 970 documents)\n",
      "    Layer 1: JS distance=0.4780, Weighted entropy=4.8989 (based on 14 nodes, 970 documents)\n",
      "    Layer 2: JS distance=0.4197, Weighted entropy=5.1741 (based on 66 nodes, 970 documents)\n",
      "    Layer 3: JS distance=0.4738, Weighted entropy=5.1071 (based on 165 nodes, 970 documents)\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_1/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Layer distribution: [(0, 1), (1, 13), (2, 73), (3, 181)]\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 0 (1 nodes)\n",
      "  - Node count: 1\n",
      "  - Calculated node pairs: 0\n",
      "  - Theoretical max node pairs: 0\n",
      "  - Average JS distance: 0.0000\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 4.7993\n",
      "==================================================\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 1 (13 nodes)\n",
      "  - Node count: 13\n",
      "  - Calculated node pairs: 78\n",
      "  - Theoretical max node pairs: 78\n",
      "  - Average JS distance: 0.4501\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 4.9592\n",
      "==================================================\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 2 (73 nodes)\n",
      "  - Node count: 73\n",
      "  - Calculated node pairs: 2628\n",
      "  - Theoretical max node pairs: 2628\n",
      "  - Average JS distance: 0.4139\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 4.9716\n",
      "==================================================\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 3 (181 nodes)\n",
      "  - Node count: 181\n",
      "  - Calculated node pairs: 16290\n",
      "  - Theoretical max node pairs: 16290\n",
      "  - Average JS distance: 0.4731\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 5.1366\n",
      "==================================================\n",
      "\n",
      "Saved detailed JS distance results to: /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_1/jensen_shannon_distances.csv\n",
      "Saved layer average JS distance and weighted entropy results to: /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_1/layer_average_js_distances.csv\n",
      "\n",
      "Overall Statistics:\n",
      "  - Total layers: 4\n",
      "  - Layer statistics:\n",
      "    Layer 0: JS distance=0.0000, Weighted entropy=4.7993 (based on 1 nodes, 970 documents)\n",
      "    Layer 1: JS distance=0.4501, Weighted entropy=4.9592 (based on 13 nodes, 970 documents)\n",
      "    Layer 2: JS distance=0.4139, Weighted entropy=4.9716 (based on 73 nodes, 970 documents)\n",
      "    Layer 3: JS distance=0.4731, Weighted entropy=5.1366 (based on 181 nodes, 970 documents)\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_2/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Layer distribution: [(0, 1), (1, 20), (2, 75), (3, 174)]\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 0 (1 nodes)\n",
      "  - Node count: 1\n",
      "  - Calculated node pairs: 0\n",
      "  - Theoretical max node pairs: 0\n",
      "  - Average JS distance: 0.0000\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 4.7787\n",
      "==================================================\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 1 (20 nodes)\n",
      "  - Node count: 20\n",
      "  - Calculated node pairs: 190\n",
      "  - Theoretical max node pairs: 190\n",
      "  - Average JS distance: 0.4341\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 4.9017\n",
      "==================================================\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 2 (75 nodes)\n",
      "  - Node count: 75\n",
      "  - Calculated node pairs: 2775\n",
      "  - Theoretical max node pairs: 2775\n",
      "  - Average JS distance: 0.4147\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 5.1521\n",
      "==================================================\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 3 (174 nodes)\n",
      "  - Node count: 174\n",
      "  - Calculated node pairs: 15051\n",
      "  - Theoretical max node pairs: 15051\n",
      "  - Average JS distance: 0.4707\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 5.1331\n",
      "==================================================\n",
      "\n",
      "Saved detailed JS distance results to: /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_2/jensen_shannon_distances.csv\n",
      "Saved layer average JS distance and weighted entropy results to: /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_2/layer_average_js_distances.csv\n",
      "\n",
      "Overall Statistics:\n",
      "  - Total layers: 4\n",
      "  - Layer statistics:\n",
      "    Layer 0: JS distance=0.0000, Weighted entropy=4.7787 (based on 1 nodes, 970 documents)\n",
      "    Layer 1: JS distance=0.4341, Weighted entropy=4.9017 (based on 20 nodes, 970 documents)\n",
      "    Layer 2: JS distance=0.4147, Weighted entropy=5.1521 (based on 75 nodes, 970 documents)\n",
      "    Layer 3: JS distance=0.4707, Weighted entropy=5.1331 (based on 174 nodes, 970 documents)\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_3/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Layer distribution: [(0, 1), (1, 17), (2, 66), (3, 163)]\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 0 (1 nodes)\n",
      "  - Node count: 1\n",
      "  - Calculated node pairs: 0\n",
      "  - Theoretical max node pairs: 0\n",
      "  - Average JS distance: 0.0000\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 4.8436\n",
      "==================================================\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 1 (17 nodes)\n",
      "  - Node count: 17\n",
      "  - Calculated node pairs: 136\n",
      "  - Theoretical max node pairs: 136\n",
      "  - Average JS distance: 0.4127\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 5.1444\n",
      "==================================================\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 2 (66 nodes)\n",
      "  - Node count: 66\n",
      "  - Calculated node pairs: 2145\n",
      "  - Theoretical max node pairs: 2145\n",
      "  - Average JS distance: 0.4072\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 5.1702\n",
      "==================================================\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 3 (163 nodes)\n",
      "  - Node count: 163\n",
      "  - Calculated node pairs: 13203\n",
      "  - Theoretical max node pairs: 13203\n",
      "  - Average JS distance: 0.4814\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 5.1070\n",
      "==================================================\n",
      "\n",
      "Saved detailed JS distance results to: /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_3/jensen_shannon_distances.csv\n",
      "Saved layer average JS distance and weighted entropy results to: /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_3/layer_average_js_distances.csv\n",
      "\n",
      "Overall Statistics:\n",
      "  - Total layers: 4\n",
      "  - Layer statistics:\n",
      "    Layer 0: JS distance=0.0000, Weighted entropy=4.8436 (based on 1 nodes, 970 documents)\n",
      "    Layer 1: JS distance=0.4127, Weighted entropy=5.1444 (based on 17 nodes, 970 documents)\n",
      "    Layer 2: JS distance=0.4072, Weighted entropy=5.1702 (based on 66 nodes, 970 documents)\n",
      "    Layer 3: JS distance=0.4814, Weighted entropy=5.1070 (based on 163 nodes, 970 documents)\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g01_收敛/depth_4_gamma_0.1_run_1/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Layer distribution: [(0, 1), (1, 12), (2, 66), (3, 171)]\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 0 (1 nodes)\n",
      "  - Node count: 1\n",
      "  - Calculated node pairs: 0\n",
      "  - Theoretical max node pairs: 0\n",
      "  - Average JS distance: 0.0000\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 4.9128\n",
      "==================================================\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 1 (12 nodes)\n",
      "  - Node count: 12\n",
      "  - Calculated node pairs: 66\n",
      "  - Theoretical max node pairs: 66\n",
      "  - Average JS distance: 0.4730\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 4.9742\n",
      "==================================================\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 2 (66 nodes)\n",
      "  - Node count: 66\n",
      "  - Calculated node pairs: 2145\n",
      "  - Theoretical max node pairs: 2145\n",
      "  - Average JS distance: 0.4212\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 5.2418\n",
      "==================================================\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 3 (171 nodes)\n",
      "  - Node count: 171\n",
      "  - Calculated node pairs: 14535\n",
      "  - Theoretical max node pairs: 14535\n",
      "  - Average JS distance: 0.4584\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 5.2135\n",
      "==================================================\n",
      "\n",
      "Saved detailed JS distance results to: /Volumes/My Passport/收敛结果/step1/2/d4_g01_收敛/depth_4_gamma_0.1_run_1/jensen_shannon_distances.csv\n",
      "Saved layer average JS distance and weighted entropy results to: /Volumes/My Passport/收敛结果/step1/2/d4_g01_收敛/depth_4_gamma_0.1_run_1/layer_average_js_distances.csv\n",
      "\n",
      "Overall Statistics:\n",
      "  - Total layers: 4\n",
      "  - Layer statistics:\n",
      "    Layer 0: JS distance=0.0000, Weighted entropy=4.9128 (based on 1 nodes, 970 documents)\n",
      "    Layer 1: JS distance=0.4730, Weighted entropy=4.9742 (based on 12 nodes, 970 documents)\n",
      "    Layer 2: JS distance=0.4212, Weighted entropy=5.2418 (based on 66 nodes, 970 documents)\n",
      "    Layer 3: JS distance=0.4584, Weighted entropy=5.2135 (based on 171 nodes, 970 documents)\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g01_收敛/depth_4_gamma_0.1_run_2/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Layer distribution: [(0, 1), (1, 23), (2, 76), (3, 206)]\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 0 (1 nodes)\n",
      "  - Node count: 1\n",
      "  - Calculated node pairs: 0\n",
      "  - Theoretical max node pairs: 0\n",
      "  - Average JS distance: 0.0000\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 4.8075\n",
      "==================================================\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 1 (23 nodes)\n",
      "  - Node count: 23\n",
      "  - Calculated node pairs: 253\n",
      "  - Theoretical max node pairs: 253\n",
      "  - Average JS distance: 0.4177\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 4.8900\n",
      "==================================================\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 2 (76 nodes)\n",
      "  - Node count: 76\n",
      "  - Calculated node pairs: 2850\n",
      "  - Theoretical max node pairs: 2850\n",
      "  - Average JS distance: 0.3820\n",
      "  - Total documents: 970\n",
      "  - Document-weighted average Renyi entropy: 5.1692\n",
      "==================================================\n",
      "\n",
      "Calculating JS distance and weighted average entropy for Layer 3 (206 nodes)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd \n",
    "# Main function: Calculate Jensen-Shannon distance and weighted average Renyi entropy\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step1/2\"  # Root directory\n",
    "eta = 0.1  # Dirichlet smoothing parameter\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Start calculating Jensen-Shannon distances and weighted average Renyi entropy...\")\n",
    "print(\"=\" * 50)\n",
    "calculate_jensen_shannon_distances_with_weighted_entropy(base_path, eta)\n",
    "print(\"=\" * 50)\n",
    "print(\"Jensen-Shannon distance and weighted average Renyi entropy calculation completed!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a7e2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_layer_statistics_by_gamma(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    Aggregate layer-wise JS distance and weighted entropy statistics by gamma value, \n",
    "    and generate a summary table at the same level as the run folder.\n",
    "    \"\"\"\n",
    "    # Find all layer_average_js_distances.csv files\n",
    "    pattern = os.path.join(base_path, \"**\", \"layer_average_js_distances.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    # Store all data and grouping information\n",
    "    all_data = []\n",
    "    gamma_experiment_groups = {}  # Used to store the parent directory for each gamma_experiment combination\n",
    "    \n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        parent_folder = os.path.dirname(folder_path)  # parent directory of the run folder\n",
    "        \n",
    "        # Extract gamma value from the folder name\n",
    "        if 'gamma_0.001' in folder_name:\n",
    "            if '2chains' in parent_folder: # Assuming '2条链' is '2chains' in the path\n",
    "                gamma = 0.001\n",
    "                experiment_type = '2chains'\n",
    "            else:\n",
    "                gamma = 0.001\n",
    "                experiment_type = 'single'\n",
    "        elif 'gamma_0.005' in folder_name:\n",
    "            gamma = 0.005\n",
    "            experiment_type = 'single'\n",
    "        elif 'gamma_0.01' in folder_name:\n",
    "            gamma = 0.01\n",
    "            experiment_type = 'single'\n",
    "        elif 'gamma_0.05' in folder_name:\n",
    "            gamma = 0.05\n",
    "            experiment_type = 'single'\n",
    "        elif 'gamma_0.1' in folder_name:\n",
    "            gamma = 0.1\n",
    "            experiment_type = 'single'\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Extract run number\n",
    "        run_match = folder_name.split('_run_')\n",
    "        if len(run_match) > 1:\n",
    "            run_id = run_match[1]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Record the parent directory for the gamma_experiment combination\n",
    "        group_key = f\"{gamma}_{experiment_type}\"\n",
    "        if group_key not in gamma_experiment_groups:\n",
    "            gamma_experiment_groups[group_key] = parent_folder\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                all_data.append({\n",
    "                    'gamma': gamma,\n",
    "                    'experiment_type': experiment_type,\n",
    "                    'run_id': run_id,\n",
    "                    'layer': row['layer'],\n",
    "                    'node_count': row['node_count'],\n",
    "                    'avg_js_distance': row['avg_js_distance'],\n",
    "                    'weighted_avg_renyi_entropy': row['weighted_avg_renyi_entropy'],\n",
    "                    'total_documents': row['total_documents'],\n",
    "                    'parent_folder': parent_folder\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_path}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    summary_df = pd.DataFrame(all_data)\n",
    "    \n",
    "    if summary_df.empty:\n",
    "        print(\"No valid data found\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"Summary Statistics for Each GAMMA Value by Layer\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Group by gamma, experiment_type, and parent_folder to generate summary files\n",
    "    for (gamma, experiment_type), group_data in summary_df.groupby(['gamma', 'experiment_type']):\n",
    "        parent_folder = group_data['parent_folder'].iloc[0]\n",
    "        \n",
    "        print(f\"\\nProcessing Gamma={gamma:.3f}, Experiment Type={'2 chains' if experiment_type == '2chains' else 'single chain'}\")\n",
    "        print(f\"Output directory: {parent_folder}\")\n",
    "        \n",
    "        # Calculate summary statistics for each layer\n",
    "        layer_summary = group_data.groupby('layer').agg({\n",
    "            'avg_js_distance': ['mean', 'std', 'count'],\n",
    "            'weighted_avg_renyi_entropy': ['mean', 'std', 'count'],\n",
    "            'node_count': ['mean', 'std'],\n",
    "            'total_documents': 'mean',\n",
    "            'run_id': lambda x: ', '.join(sorted(x.unique()))\n",
    "        }).round(4)\n",
    "        \n",
    "        # Flatten column names\n",
    "        layer_summary.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in layer_summary.columns]\n",
    "        layer_summary = layer_summary.reset_index()\n",
    "        \n",
    "        # Rename columns to be more descriptive\n",
    "        column_mapping = {\n",
    "            'avg_js_distance_mean': 'avg_js_distance_mean',\n",
    "            'avg_js_distance_std': 'avg_js_distance_std', \n",
    "            'avg_js_distance_count': 'run_count',\n",
    "            'weighted_avg_renyi_entropy_mean': 'weighted_avg_renyi_entropy_mean',\n",
    "            'weighted_avg_renyi_entropy_std': 'weighted_avg_renyi_entropy_std',\n",
    "            'weighted_avg_renyi_entropy_count': 'entropy_run_count',\n",
    "            'node_count_mean': 'avg_node_count',\n",
    "            'node_count_std': 'node_count_std',\n",
    "            'total_documents_mean': 'avg_total_documents',\n",
    "            'run_id_<lambda>': 'included_runs'\n",
    "        }\n",
    "        \n",
    "        for old_name, new_name in column_mapping.items():\n",
    "            if old_name in layer_summary.columns:\n",
    "                layer_summary = layer_summary.rename(columns={old_name: new_name})\n",
    "        \n",
    "        # Add gamma and experiment_type information\n",
    "        layer_summary.insert(0, 'gamma', gamma)\n",
    "        layer_summary.insert(1, 'experiment_type', experiment_type)\n",
    "        \n",
    "        # Save the summary results to the same level as the run folder\n",
    "        if experiment_type == '2chains':\n",
    "            output_filename = f'gamma_{gamma:.3f}_2chains_layer_summary.csv'\n",
    "        else:\n",
    "            output_filename = f'gamma_{gamma:.3f}_single_layer_summary.csv'\n",
    "        \n",
    "        output_path = os.path.join(parent_folder, output_filename)\n",
    "        layer_summary.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"  Saved summary file: {output_path}\")\n",
    "        print(f\"  Included runs: {layer_summary['included_runs'].iloc[0] if 'included_runs' in layer_summary.columns else 'N/A'}\")\n",
    "        print(f\"  Number of layers: {len(layer_summary)}\")\n",
    "        \n",
    "        # Display brief statistics\n",
    "        for _, row in layer_summary.iterrows():\n",
    "            layer_num = int(row['layer'])\n",
    "            js_mean = row['avg_js_distance_mean']\n",
    "            js_std = row['avg_js_distance_std'] if 'avg_js_distance_std' in row else 0\n",
    "            entropy_mean = row['weighted_avg_renyi_entropy_mean']\n",
    "            entropy_std = row['weighted_avg_renyi_entropy_std'] if 'weighted_avg_renyi_entropy_std' in row else 0\n",
    "            node_count = row['avg_node_count']\n",
    "            run_count = int(row['run_count']) if 'run_count' in row else 0\n",
    "            \n",
    "            print(f\"    Layer {layer_num}: JS={js_mean:.4f}(±{js_std:.4f}), Entropy={entropy_mean:.4f}(±{entropy_std:.4f}), Nodes={node_count:.1f}, runs={run_count}\")\n",
    "    \n",
    "    # Generate overall comparison file (saved under base_path)\n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"Generating overall comparison file\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Only analyze the cross-gamma comparison for single-chain experiments\n",
    "    single_chain_data = summary_df[summary_df['experiment_type'] == 'single']\n",
    "    \n",
    "    if not single_chain_data.empty:\n",
    "        overall_summary = single_chain_data.groupby(['gamma', 'layer']).agg({\n",
    "            'avg_js_distance': ['mean', 'std'],\n",
    "            'weighted_avg_renyi_entropy': ['mean', 'std'],\n",
    "            'node_count': ['mean', 'std'],\n",
    "            'run_id': 'count'\n",
    "        }).round(4)\n",
    "        \n",
    "        # Flatten column names\n",
    "        overall_summary.columns = ['_'.join(col).strip() for col in overall_summary.columns]\n",
    "        overall_summary = overall_summary.reset_index()\n",
    "        \n",
    "        overall_output_path = os.path.join(base_path, 'gamma_layer_comparison.csv')\n",
    "        overall_summary.to_csv(overall_output_path, index=False)\n",
    "        print(f\"Overall comparison file saved to: {overall_output_path}\")\n",
    "        \n",
    "        # Display cross-gamma comparison\n",
    "        for layer in sorted(single_chain_data['layer'].unique()):\n",
    "            print(f\"\\nLayer {int(layer)} Cross-Gamma Comparison:\")\n",
    "            print(\"Gamma      JS Distance(±std)   Weighted Entropy(±std)   Node Count(±std)   Runs\")\n",
    "            print(\"-\" * 75)\n",
    "            \n",
    "            layer_data = overall_summary[overall_summary['layer'] == layer]\n",
    "            for _, row in layer_data.iterrows():\n",
    "                gamma = row['gamma']\n",
    "                js_mean = row['avg_js_distance_mean']\n",
    "                js_std = row['avg_js_distance_std']\n",
    "                entropy_mean = row['weighted_avg_renyi_entropy_mean']\n",
    "                entropy_std = row['weighted_avg_renyi_entropy_std']\n",
    "                node_mean = row['node_count_mean']\n",
    "                node_std = row['node_count_std']\n",
    "                run_count = int(row['run_id_count'])\n",
    "                \n",
    "                print(f\"{gamma:6.3f}    {js_mean:6.4f}(±{js_std:5.4f})   {entropy_mean:6.4f}(±{entropy_std:5.4f})   {node_mean:6.1f}(±{node_std:4.1f})   {run_count:4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5a49f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Start aggregating layer statistics for each Gamma value...\n",
      "======================================================================\n",
      "======================================================================\n",
      "Summary Statistics for Each GAMMA Value by Layer\n",
      "======================================================================\n",
      "\n",
      "Processing Gamma=0.001, Experiment Type=single chain\n",
      "Output directory: /Volumes/My Passport/收敛结果/step1/3/d3_g0001_2条链收敛\n",
      "  Saved summary file: /Volumes/My Passport/收敛结果/step1/3/d3_g0001_2条链收敛/gamma_0.001_single_layer_summary.csv\n",
      "  Included runs: 1, 2, 3\n",
      "  Number of layers: 3\n",
      "    Layer 0: JS=0.0000(±0.0000), Entropy=4.9490(±0.0399), Nodes=1.0, runs=5\n",
      "    Layer 1: JS=0.4870(±0.0201), Entropy=5.0720(±0.1288), Nodes=42.2, runs=5\n",
      "    Layer 2: JS=0.5069(±0.0121), Entropy=5.0823(±0.1317), Nodes=145.8, runs=5\n",
      "\n",
      "Processing Gamma=0.005, Experiment Type=single chain\n",
      "Output directory: /Volumes/My Passport/收敛结果/step1/3/d3_g0005_收敛\n",
      "  Saved summary file: /Volumes/My Passport/收敛结果/step1/3/d3_g0005_收敛/gamma_0.005_single_layer_summary.csv\n",
      "  Included runs: 1, 2, 3\n",
      "  Number of layers: 3\n",
      "    Layer 0: JS=0.0000(±0.0000), Entropy=4.9424(±0.0206), Nodes=1.0, runs=3\n",
      "    Layer 1: JS=0.4915(±0.0111), Entropy=5.1457(±0.0513), Nodes=44.0, runs=3\n",
      "    Layer 2: JS=0.4944(±0.0076), Entropy=5.1035(±0.0213), Nodes=153.3, runs=3\n",
      "\n",
      "Processing Gamma=0.010, Experiment Type=single chain\n",
      "Output directory: /Volumes/My Passport/收敛结果/step1/3/d3_g001_收敛\n",
      "  Saved summary file: /Volumes/My Passport/收敛结果/step1/3/d3_g001_收敛/gamma_0.010_single_layer_summary.csv\n",
      "  Included runs: 1, 2, 3\n",
      "  Number of layers: 3\n",
      "    Layer 0: JS=0.0000(±0.0000), Entropy=4.9376(±0.0301), Nodes=1.0, runs=3\n",
      "    Layer 1: JS=0.4784(±0.0240), Entropy=5.1824(±0.1588), Nodes=41.3, runs=3\n",
      "    Layer 2: JS=0.5006(±0.0078), Entropy=5.0835(±0.0670), Nodes=152.7, runs=3\n",
      "\n",
      "Processing Gamma=0.050, Experiment Type=single chain\n",
      "Output directory: /Volumes/My Passport/收敛结果/step1/3/d3_g005_收敛\n",
      "  Saved summary file: /Volumes/My Passport/收敛结果/step1/3/d3_g005_收敛/gamma_0.050_single_layer_summary.csv\n",
      "  Included runs: 1, 2, 3\n",
      "  Number of layers: 3\n",
      "    Layer 0: JS=0.0000(±0.0000), Entropy=4.9197(±0.0219), Nodes=1.0, runs=3\n",
      "    Layer 1: JS=0.4579(±0.0178), Entropy=5.2068(±0.1233), Nodes=41.7, runs=3\n",
      "    Layer 2: JS=0.4982(±0.0058), Entropy=5.0438(±0.0160), Nodes=174.3, runs=3\n",
      "\n",
      "Processing Gamma=0.100, Experiment Type=single chain\n",
      "Output directory: /Volumes/My Passport/收敛结果/step1/3/d3_g01_收敛\n",
      "  Saved summary file: /Volumes/My Passport/收敛结果/step1/3/d3_g01_收敛/gamma_0.100_single_layer_summary.csv\n",
      "  Included runs: 1, 2, 3\n",
      "  Number of layers: 3\n",
      "    Layer 0: JS=0.0000(±0.0000), Entropy=4.8972(±0.0389), Nodes=1.0, runs=3\n",
      "    Layer 1: JS=0.4700(±0.0183), Entropy=5.0821(±0.0651), Nodes=42.7, runs=3\n",
      "    Layer 2: JS=0.4913(±0.0041), Entropy=5.0317(±0.0376), Nodes=183.3, runs=3\n",
      "\n",
      "======================================================================\n",
      "Generating overall comparison file\n",
      "======================================================================\n",
      "Overall comparison file saved to: /Volumes/My Passport/收敛结果/step1/3/gamma_layer_comparison.csv\n",
      "\n",
      "Layer 0 Cross-Gamma Comparison:\n",
      "Gamma      JS Distance(±std)   Weighted Entropy(±std)   Node Count(±std)   Runs\n",
      "---------------------------------------------------------------------------\n",
      " 0.001    0.0000(±0.0000)   4.9490(±0.0399)      1.0(± 0.0)      5\n",
      " 0.005    0.0000(±0.0000)   4.9424(±0.0206)      1.0(± 0.0)      3\n",
      " 0.010    0.0000(±0.0000)   4.9376(±0.0301)      1.0(± 0.0)      3\n",
      " 0.050    0.0000(±0.0000)   4.9197(±0.0219)      1.0(± 0.0)      3\n",
      " 0.100    0.0000(±0.0000)   4.8972(±0.0389)      1.0(± 0.0)      3\n",
      "\n",
      "Layer 1 Cross-Gamma Comparison:\n",
      "Gamma      JS Distance(±std)   Weighted Entropy(±std)   Node Count(±std)   Runs\n",
      "---------------------------------------------------------------------------\n",
      " 0.001    0.4870(±0.0201)   5.0720(±0.1288)     42.2(± 3.4)      5\n",
      " 0.005    0.4915(±0.0111)   5.1457(±0.0513)     44.0(± 3.5)      3\n",
      " 0.010    0.4784(±0.0240)   5.1824(±0.1588)     41.3(± 3.8)      3\n",
      " 0.050    0.4579(±0.0178)   5.2068(±0.1233)     41.7(± 2.1)      3\n",
      " 0.100    0.4700(±0.0183)   5.0821(±0.0651)     42.7(± 2.5)      3\n",
      "\n",
      "Layer 2 Cross-Gamma Comparison:\n",
      "Gamma      JS Distance(±std)   Weighted Entropy(±std)   Node Count(±std)   Runs\n",
      "---------------------------------------------------------------------------\n",
      " 0.001    0.5069(±0.0121)   5.0823(±0.1317)    145.8(±10.5)      5\n",
      " 0.005    0.4944(±0.0076)   5.1035(±0.0213)    153.3(±12.1)      3\n",
      " 0.010    0.5006(±0.0078)   5.0835(±0.0670)    152.7(± 4.2)      3\n",
      " 0.050    0.4982(±0.0058)   5.0438(±0.0160)    174.3(±11.1)      3\n",
      " 0.100    0.4913(±0.0041)   5.0317(±0.0376)    183.3(± 6.1)      3\n",
      "======================================================================\n",
      "Summary analysis completed!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Execute summary analysis\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step1/2\"\n",
    "print(\"=\" * 70)\n",
    "print(\"Start aggregating layer statistics for each Gamma value...\")\n",
    "print(\"=\" * 70)\n",
    "aggregate_layer_statistics_by_gamma(base_path)\n",
    "print(\"=\" * 70)\n",
    "print(\"Summary analysis completed!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ed95526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated mean values per layer for each parameter set: all_params_layer_mean.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step1/3\"\n",
    "pattern = os.path.join(base_path, \"**\", \"result_layers.csv\")\n",
    "files = glob.glob(pattern, recursive=True)\n",
    "\n",
    "all_rows = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "    # Add parameter information\n",
    "    for col in ['depth', 'gamma', 'eta', 'alpha']:\n",
    "        if col not in df.columns:\n",
    "            # Extract parameters from the file path\n",
    "            folder = os.path.dirname(file)\n",
    "            if f\"{col}_\" in folder:\n",
    "                try:\n",
    "                    value = float(folder.split(f\"{col}_\")[1].split(\"_\")[0])\n",
    "                except:\n",
    "                    value = None\n",
    "                df[col] = value\n",
    "            else:\n",
    "                df[col] = None\n",
    "    all_rows.append(df)\n",
    "\n",
    "merged = pd.concat(all_rows, ignore_index=True)\n",
    "\n",
    "# Group by parameter set and layer to calculate mean and standard deviation\n",
    "group_cols = ['depth', 'gamma', 'eta', 'alpha', 'layer']\n",
    "summary = merged.groupby(group_cols).agg({\n",
    "    'entropy_wavg': ['mean', 'std'],\n",
    "    'distinctiveness_wavg_jsd': ['mean', 'std'],\n",
    "    'nodes_in_layer': ['mean', 'std'],\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten the multi-level column names\n",
    "summary.columns = ['_'.join(col).strip('_') for col in summary.columns]\n",
    "\n",
    "summary.to_csv(os.path.join(base_path, \"all_params_layer_mean.csv\"), index=False)\n",
    "print(\"Generated mean values per layer for each parameter set: all_params_layer_mean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56b2646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_parent_child_jsd(base_path=\".\", eta=0.1):\n",
    "    \"\"\"\n",
    "    Calculate Jensen-Shannon distances between parent and child nodes across layers.\n",
    "    \n",
    "    Parameters:\n",
    "    base_path: str, root directory path\n",
    "    eta: float, Dirichlet smoothing parameter\n",
    "    \"\"\"\n",
    "    # Find all iteration_node_word_distributions.csv files\n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_node_word_distributions.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        print(f\"\\nProcessing file: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Read word distribution data\n",
    "            word_df = pd.read_csv(file_path)\n",
    "            word_df.columns = [col.strip(\"'\\\" \") for col in word_df.columns]\n",
    "            \n",
    "            # Get data from the last iteration\n",
    "            max_iteration = word_df['iteration'].max()\n",
    "            last_iteration_data = word_df[word_df['iteration'] == max_iteration]\n",
    "            \n",
    "            # Get the full vocabulary\n",
    "            all_words = sorted(list(last_iteration_data['word'].dropna().unique()))\n",
    "            print(f\"Vocabulary size: {len(all_words)}\")\n",
    "            \n",
    "            # Read the entropy file to get hierarchy information\n",
    "            entropy_file = os.path.join(folder_path, 'corrected_renyi_entropy.csv')\n",
    "            if not os.path.exists(entropy_file):\n",
    "                print(f\"Warning: Entropy file not found: {entropy_file}\")\n",
    "                continue\n",
    "                \n",
    "            entropy_df = pd.read_csv(entropy_file)\n",
    "            \n",
    "            # Build probability distribution for each node\n",
    "            node_distributions = {}\n",
    "            \n",
    "            for node_id in entropy_df['node_id'].unique():\n",
    "                # Get the word distribution for this node\n",
    "                node_words = last_iteration_data[last_iteration_data['node_id'] == node_id]\n",
    "                \n",
    "                # Initialize count vector\n",
    "                counts = np.zeros(len(all_words))\n",
    "                word_to_idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "                \n",
    "                # Fill in actual counts\n",
    "                for _, row in node_words.iterrows():\n",
    "                    word = row['word']\n",
    "                    if pd.notna(word) and word in word_to_idx:\n",
    "                        counts[word_to_idx[word]] = row['count']\n",
    "                \n",
    "                # Add Dirichlet smoothing\n",
    "                smoothed_counts = counts + eta\n",
    "                \n",
    "                # Calculate probability distribution\n",
    "                probabilities = smoothed_counts / np.sum(smoothed_counts)\n",
    "                node_distributions[node_id] = probabilities\n",
    "            \n",
    "            # Calculate parent-child JSD\n",
    "            parent_child_distances = []\n",
    "            layer_transitions = []\n",
    "            \n",
    "            for _, row in entropy_df.iterrows():\n",
    "                child_id = row['node_id']\n",
    "                parent_id = row['parent_id']\n",
    "                \n",
    "                if pd.notna(parent_id) and parent_id in node_distributions and child_id in node_distributions:\n",
    "                    parent_id = int(parent_id)\n",
    "                    \n",
    "                    # Get parent layer info\n",
    "                    parent_info = entropy_df[entropy_df['node_id'] == parent_id].iloc[0]\n",
    "                    parent_layer = parent_info['layer']\n",
    "                    parent_doc_count = parent_info['document_count']\n",
    "                    \n",
    "                    child_layer = row['layer']\n",
    "                    child_doc_count = row['document_count']\n",
    "                    \n",
    "                    # Calculate JSD\n",
    "                    p_parent = node_distributions[parent_id]\n",
    "                    p_child = node_distributions[child_id]\n",
    "                    js_distance = jensen_shannon_distance(p_parent, p_child)\n",
    "                    \n",
    "                    parent_child_distances.append({\n",
    "                        'parent_id': parent_id,\n",
    "                        'parent_layer': parent_layer,\n",
    "                        'child_id': child_id,\n",
    "                        'child_layer': child_layer,\n",
    "                        'js_distance': js_distance,\n",
    "                        'parent_doc_count': parent_doc_count,\n",
    "                        'child_doc_count': child_doc_count\n",
    "                    })\n",
    "            \n",
    "            print(f\"Calculated {len(parent_child_distances)} parent-child JSD pairs\")\n",
    "            \n",
    "            # Calculate layer transition statistics\n",
    "            if parent_child_distances:\n",
    "                pc_df = pd.DataFrame(parent_child_distances)\n",
    "                \n",
    "                # Group by layer transition (parent_layer -> child_layer)\n",
    "                transition_stats = pc_df.groupby(['parent_layer', 'child_layer']).agg({\n",
    "                    'js_distance': ['mean', 'std', 'count'],\n",
    "                    'parent_doc_count': 'sum',\n",
    "                    'child_doc_count': 'sum'\n",
    "                }).round(4)\n",
    "                \n",
    "                # Flatten column names\n",
    "                transition_stats.columns = ['_'.join(col).strip() for col in transition_stats.columns]\n",
    "                transition_stats = transition_stats.reset_index()\n",
    "                \n",
    "                # Rename columns\n",
    "                transition_stats = transition_stats.rename(columns={\n",
    "                    'js_distance_mean': 'avg_js_distance',\n",
    "                    'js_distance_std': 'js_distance_std',\n",
    "                    'js_distance_count': 'pair_count',\n",
    "                    'parent_doc_count_sum': 'total_parent_docs',\n",
    "                    'child_doc_count_sum': 'total_child_docs'\n",
    "                })\n",
    "                \n",
    "                layer_transitions = transition_stats.to_dict('records')\n",
    "            \n",
    "            # Save detailed parent-child JSD results\n",
    "            if parent_child_distances:\n",
    "                pc_df = pd.DataFrame(parent_child_distances)\n",
    "                output_path = os.path.join(folder_path, 'parent_child_js_distances.csv')\n",
    "                pc_df.to_csv(output_path, index=False)\n",
    "                print(f\"Saved detailed parent-child JS distance results to: {output_path}\")\n",
    "            \n",
    "            # Save layer transition summary\n",
    "            if layer_transitions:\n",
    "                lt_df = pd.DataFrame(layer_transitions)\n",
    "                lt_output_path = os.path.join(folder_path, 'layer_transition_js_distances.csv')\n",
    "                lt_df.to_csv(lt_output_path, index=False)\n",
    "                print(f\"Saved layer transition JS distance summary to: {lt_output_path}\")\n",
    "                \n",
    "                # Print transition statistics\n",
    "                print(f\"\\nLayer Transition Statistics:\")\n",
    "                for transition in layer_transitions:\n",
    "                    parent_layer = int(transition['parent_layer'])\n",
    "                    child_layer = int(transition['child_layer'])\n",
    "                    avg_jsd = transition['avg_js_distance']\n",
    "                    std_jsd = transition['js_distance_std']\n",
    "                    pair_count = int(transition['pair_count'])\n",
    "                    print(f\"  Layer {parent_layer} -> Layer {child_layer}: JSD={avg_jsd:.4f}(±{std_jsd:.4f}), pairs={pair_count}\")\n",
    "            \n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "            print(\"Detailed traceback:\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "def jensen_shannon_distance(p, q):\n",
    "    \"\"\"\n",
    "    Calculate the Jensen-Shannon distance between two probability distributions.\n",
    "    \n",
    "    Parameters:\n",
    "    p, q: numpy arrays, probability distributions\n",
    "    \n",
    "    Returns:\n",
    "    float: Jensen-Shannon distance\n",
    "    \"\"\"\n",
    "    # Ensure probability distributions are normalized\n",
    "    p = p / np.sum(p)\n",
    "    q = q / np.sum(q)\n",
    "    \n",
    "    # Calculate the average distribution\n",
    "    m = 0.5 * (p + q)\n",
    "    \n",
    "    # Calculate KL divergence (using natural logarithm)\n",
    "    def kl_divergence(x, y):\n",
    "        # Avoid log(0)\n",
    "        mask = (x > 0) & (y > 0)\n",
    "        if np.sum(mask) == 0:\n",
    "            return 0.0\n",
    "        return np.sum(x[mask] * np.log(x[mask] / y[mask]))\n",
    "    \n",
    "    # Calculate Jensen-Shannon divergence\n",
    "    js_divergence = 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)\n",
    "    \n",
    "    # Convert to distance (square root)\n",
    "    js_distance = np.sqrt(js_divergence)\n",
    "    \n",
    "    return js_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67ece9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Start calculating parent-child Jensen-Shannon distances...\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_1/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 227 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_1/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_1/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6029(±0.0416), pairs=19\n",
      "  Layer 1 -> Layer 2: JSD=0.4641(±0.1583), pairs=67\n",
      "  Layer 2 -> Layer 3: JSD=0.4623(±0.1051), pairs=141\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_2/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 234 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_2/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_2/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6059(±0.0346), pairs=17\n",
      "  Layer 1 -> Layer 2: JSD=0.4757(±0.1237), pairs=62\n",
      "  Layer 2 -> Layer 3: JSD=0.4726(±0.1037), pairs=155\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_3/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 245 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_3/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/2/d4_g0005_收敛/depth_4_gamma_0.005_run_3/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6099(±0.0351), pairs=14\n",
      "  Layer 1 -> Layer 2: JSD=0.5127(±0.1429), pairs=66\n",
      "  Layer 2 -> Layer 3: JSD=0.4860(±0.1067), pairs=165\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_1/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 267 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_1/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_1/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6083(±0.0513), pairs=13\n",
      "  Layer 1 -> Layer 2: JSD=0.5050(±0.1651), pairs=73\n",
      "  Layer 2 -> Layer 3: JSD=0.4787(±0.1140), pairs=181\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_2/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 269 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_2/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_2/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6167(±0.0369), pairs=20\n",
      "  Layer 1 -> Layer 2: JSD=0.5062(±0.1337), pairs=75\n",
      "  Layer 2 -> Layer 3: JSD=0.4777(±0.1075), pairs=174\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_3/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 246 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_3/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_3/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6079(±0.0418), pairs=17\n",
      "  Layer 1 -> Layer 2: JSD=0.4756(±0.1506), pairs=66\n",
      "  Layer 2 -> Layer 3: JSD=0.4777(±0.1140), pairs=163\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g01_收敛/depth_4_gamma_0.1_run_1/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 249 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/2/d4_g01_收敛/depth_4_gamma_0.1_run_1/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/2/d4_g01_收敛/depth_4_gamma_0.1_run_1/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.5971(±0.0527), pairs=12\n",
      "  Layer 1 -> Layer 2: JSD=0.5202(±0.1115), pairs=66\n",
      "  Layer 2 -> Layer 3: JSD=0.4776(±0.0994), pairs=171\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g01_收敛/depth_4_gamma_0.1_run_2/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 305 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/2/d4_g01_收敛/depth_4_gamma_0.1_run_2/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/2/d4_g01_收敛/depth_4_gamma_0.1_run_2/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6144(±0.0443), pairs=23\n",
      "  Layer 1 -> Layer 2: JSD=0.4678(±0.1619), pairs=76\n",
      "  Layer 2 -> Layer 3: JSD=0.4788(±0.1274), pairs=206\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g01_收敛/depth_4_gamma_0.1_run_3/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 272 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/2/d4_g01_收敛/depth_4_gamma_0.1_run_3/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/2/d4_g01_收敛/depth_4_gamma_0.1_run_3/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.5996(±0.0520), pairs=15\n",
      "  Layer 1 -> Layer 2: JSD=0.5067(±0.1517), pairs=69\n",
      "  Layer 2 -> Layer 3: JSD=0.4740(±0.1082), pairs=188\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g0001_收敛/depth_4_gamma_0.001_run_1/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 257 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/2/d4_g0001_收敛/depth_4_gamma_0.001_run_1/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/2/d4_g0001_收敛/depth_4_gamma_0.001_run_1/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6024(±0.0523), pairs=17\n",
      "  Layer 1 -> Layer 2: JSD=0.5184(±0.1279), pairs=72\n",
      "  Layer 2 -> Layer 3: JSD=0.4812(±0.1166), pairs=168\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g0001_收敛/depth_4_gamma_0.001_run_2/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 239 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/2/d4_g0001_收敛/depth_4_gamma_0.001_run_2/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/2/d4_g0001_收敛/depth_4_gamma_0.001_run_2/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6064(±0.0561), pairs=14\n",
      "  Layer 1 -> Layer 2: JSD=0.5031(±0.1490), pairs=63\n",
      "  Layer 2 -> Layer 3: JSD=0.4923(±0.1032), pairs=162\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g0001_收敛/depth_4_gamma_0.001_run_3/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 232 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/2/d4_g0001_收敛/depth_4_gamma_0.001_run_3/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/2/d4_g0001_收敛/depth_4_gamma_0.001_run_3/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6203(±0.0296), pairs=20\n",
      "  Layer 1 -> Layer 2: JSD=0.4774(±0.1163), pairs=69\n",
      "  Layer 2 -> Layer 3: JSD=0.4555(±0.0940), pairs=143\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g005_收敛/depth_4_gamma_0.05_run_1/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 281 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/2/d4_g005_收敛/depth_4_gamma_0.05_run_1/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/2/d4_g005_收敛/depth_4_gamma_0.05_run_1/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6239(±0.0403), pairs=19\n",
      "  Layer 1 -> Layer 2: JSD=0.5048(±0.1325), pairs=75\n",
      "  Layer 2 -> Layer 3: JSD=0.4685(±0.1342), pairs=187\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g005_收敛/depth_4_gamma_0.05_run_2/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 266 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/2/d4_g005_收敛/depth_4_gamma_0.05_run_2/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/2/d4_g005_收敛/depth_4_gamma_0.05_run_2/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6119(±0.0607), pairs=16\n",
      "  Layer 1 -> Layer 2: JSD=0.5028(±0.1517), pairs=69\n",
      "  Layer 2 -> Layer 3: JSD=0.4966(±0.1074), pairs=181\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/2/d4_g005_收敛/depth_4_gamma_0.05_run_3/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 244 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/2/d4_g005_收敛/depth_4_gamma_0.05_run_3/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/2/d4_g005_收敛/depth_4_gamma_0.05_run_3/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6157(±0.0459), pairs=14\n",
      "  Layer 1 -> Layer 2: JSD=0.4881(±0.1593), pairs=61\n",
      "  Layer 2 -> Layer 3: JSD=0.4937(±0.1193), pairs=169\n",
      "==================================================\n",
      "==================================================\n",
      "Parent-child Jensen-Shannon distance calculation completed!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Main function: Calculate parent-child Jensen-Shannon distances\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step1/2\"  # Root directory\n",
    "eta = 0.1  # Dirichlet smoothing parameter\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Start calculating parent-child Jensen-Shannon distances...\")\n",
    "print(\"=\" * 50)\n",
    "calculate_parent_child_jsd(base_path, eta)\n",
    "print(\"=\" * 50)\n",
    "print(\"Parent-child Jensen-Shannon distance calculation completed!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71448e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_parent_child_jsd_by_gamma(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    Aggregate parent-child JS distance data by gamma value across all runs,\n",
    "    with proper weighted averages by individual parent-child pairs' child document counts relative to total 970 documents.\n",
    "    \"\"\"\n",
    "    # Find all parent_child_js_distances.csv files (detailed data)\n",
    "    pattern = os.path.join(base_path, \"**\", \"parent_child_js_distances.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    all_detailed_data = []\n",
    "    \n",
    "    # 假设总文档数为970（可以从数据中动态获取）\n",
    "    TOTAL_DOCUMENTS = 970\n",
    "    \n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        parent_folder = os.path.dirname(folder_path)\n",
    "        \n",
    "        # Extract gamma value from the folder name\n",
    "        if 'gamma_0.001' in folder_name:\n",
    "            if '2chains' in parent_folder:\n",
    "                gamma = 0.001\n",
    "                experiment_type = '2chains'\n",
    "            else:\n",
    "                gamma = 0.001\n",
    "                experiment_type = 'single'\n",
    "        elif 'gamma_0.005' in folder_name:\n",
    "            gamma = 0.005\n",
    "            experiment_type = 'single'\n",
    "        elif 'gamma_0.01' in folder_name:\n",
    "            gamma = 0.01\n",
    "            experiment_type = 'single'\n",
    "        elif 'gamma_0.05' in folder_name:\n",
    "            gamma = 0.05\n",
    "            experiment_type = 'single'\n",
    "        elif 'gamma_0.1' in folder_name:\n",
    "            gamma = 0.1\n",
    "            experiment_type = 'single'\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Extract run number\n",
    "        run_match = folder_name.split('_run_')\n",
    "        if len(run_match) > 1:\n",
    "            run_id = run_match[1]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Add run and gamma information to each detailed parent-child pair\n",
    "            for _, row in df.iterrows():\n",
    "                all_detailed_data.append({\n",
    "                    'gamma': gamma,\n",
    "                    'experiment_type': experiment_type,\n",
    "                    'run_id': run_id,\n",
    "                    'parent_id': row['parent_id'],\n",
    "                    'parent_layer': row['parent_layer'],\n",
    "                    'child_id': row['child_id'],\n",
    "                    'child_layer': row['child_layer'],\n",
    "                    'layer_transition': f\"{int(row['parent_layer'])}->{int(row['child_layer'])}\",\n",
    "                    'js_distance': row['js_distance'],\n",
    "                    'parent_doc_count': row['parent_doc_count'],\n",
    "                    'child_doc_count': row['child_doc_count']\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_path}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    detailed_df = pd.DataFrame(all_detailed_data)\n",
    "    \n",
    "    if detailed_df.empty:\n",
    "        print(\"No valid detailed parent-child JSD data found\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"Parent-Child JSD Data Collection by GAMMA Value (weighted by child_doc_count/970)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Calculate properly weighted averages by gamma and layer transition\n",
    "    weighted_summary = []\n",
    "    \n",
    "    for (gamma, experiment_type), gamma_group in detailed_df.groupby(['gamma', 'experiment_type']):\n",
    "        for transition, transition_group in gamma_group.groupby('layer_transition'):\n",
    "            # 计算基于全局970文档的加权平均\n",
    "            # 每个child的权重 = child_doc_count / 970\n",
    "            weights = transition_group['child_doc_count'] / TOTAL_DOCUMENTS\n",
    "            \n",
    "            # 加权平均JSD = sum(js_distance * weight) / sum(weight)\n",
    "            # 但这等价于：sum(js_distance * child_doc_count) / sum(child_doc_count)\n",
    "            # 因为970是常数，会被约掉\n",
    "            \n",
    "            # 方法1：使用归一化权重\n",
    "            total_weight = weights.sum()\n",
    "            if total_weight > 0:\n",
    "                weighted_avg_jsd_normalized = (transition_group['js_distance'] * weights).sum() / total_weight\n",
    "            else:\n",
    "                weighted_avg_jsd_normalized = 0.0\n",
    "            \n",
    "            # 方法2：直接使用文档数作为权重（与方法1结果相同）\n",
    "            total_child_docs = transition_group['child_doc_count'].sum()\n",
    "            if total_child_docs > 0:\n",
    "                weighted_avg_jsd_direct = (transition_group['js_distance'] * transition_group['child_doc_count']).sum() / total_child_docs\n",
    "            else:\n",
    "                weighted_avg_jsd_direct = 0.0\n",
    "            \n",
    "            # Calculate simple average for comparison\n",
    "            simple_avg_jsd = transition_group['js_distance'].mean()\n",
    "            \n",
    "            # Get additional statistics\n",
    "            parent_layer = transition_group['parent_layer'].iloc[0]\n",
    "            child_layer = transition_group['child_layer'].iloc[0]\n",
    "            pair_count = len(transition_group)\n",
    "            run_count = len(transition_group['run_id'].unique())\n",
    "            runs_included = ', '.join(sorted(transition_group['run_id'].unique()))\n",
    "            \n",
    "            # 计算权重占比（相对于970）\n",
    "            weight_proportion = total_child_docs / TOTAL_DOCUMENTS\n",
    "            \n",
    "            weighted_summary.append({\n",
    "                'gamma': gamma,\n",
    "                'experiment_type': experiment_type,\n",
    "                'layer_transition': transition,\n",
    "                'parent_layer': parent_layer,\n",
    "                'child_layer': child_layer,\n",
    "                'weighted_avg_jsd': weighted_avg_jsd_direct,  # 两种方法结果相同\n",
    "                'simple_avg_jsd': simple_avg_jsd,\n",
    "                'total_child_docs': total_child_docs,\n",
    "                'weight_proportion_of_970': weight_proportion,  # 权重占970的比例\n",
    "                'total_pairs': pair_count,\n",
    "                'run_count': run_count,\n",
    "                'runs_included': runs_included,\n",
    "                'jsd_std': transition_group['js_distance'].std(),\n",
    "                'child_doc_count_mean': transition_group['child_doc_count'].mean(),\n",
    "                'child_doc_count_std': transition_group['child_doc_count'].std()\n",
    "            })\n",
    "    \n",
    "    # Save detailed individual pair data\n",
    "    detailed_output_path = os.path.join(base_path, 'all_runs_detailed_parent_child_jsd_by_gamma.csv')\n",
    "    detailed_df.to_csv(detailed_output_path, index=False)\n",
    "    print(f\"Saved complete detailed parent-child JSD data to: {detailed_output_path}\")\n",
    "    \n",
    "    # Save weighted summary\n",
    "    weighted_df = pd.DataFrame(weighted_summary)\n",
    "    weighted_output_path = os.path.join(base_path, 'properly_weighted_parent_child_jsd_by_gamma.csv')\n",
    "    weighted_df.to_csv(weighted_output_path, index=False)\n",
    "    print(f\"Saved properly weighted parent-child JSD summary to: {weighted_output_path}\")\n",
    "    \n",
    "    # Print summary statistics for each gamma\n",
    "    for gamma in sorted(detailed_df['gamma'].unique()):\n",
    "        gamma_data = detailed_df[detailed_df['gamma'] == gamma]\n",
    "        experiment_types = gamma_data['experiment_type'].unique()\n",
    "        \n",
    "        for exp_type in experiment_types:\n",
    "            exp_data = gamma_data[gamma_data['experiment_type'] == exp_type]\n",
    "            run_count = len(exp_data['run_id'].unique())\n",
    "            transition_count = len(exp_data['layer_transition'].unique())\n",
    "            total_pairs = len(exp_data)\n",
    "            \n",
    "            print(f\"\\nGamma {gamma:.3f} ({'2 chains' if exp_type == '2chains' else 'single chain'}):\")\n",
    "            print(f\"  - Runs: {run_count}\")\n",
    "            print(f\"  - Layer transitions: {transition_count}\")\n",
    "            print(f\"  - Total parent-child pairs: {total_pairs}\")\n",
    "            \n",
    "            # Show transition summary with properly weighted averages\n",
    "            print(f\"  - Layer transition summary (weighted by child_doc_count/970):\")\n",
    "            gamma_weighted = weighted_df[(weighted_df['gamma'] == gamma) & (weighted_df['experiment_type'] == exp_type)]\n",
    "            \n",
    "            for _, row in gamma_weighted.iterrows():\n",
    "                transition = row['layer_transition']\n",
    "                weighted_jsd = row['weighted_avg_jsd']\n",
    "                simple_jsd = row['simple_avg_jsd']\n",
    "                total_child_docs = int(row['total_child_docs'])\n",
    "                weight_prop = row['weight_proportion_of_970']\n",
    "                total_pairs_trans = int(row['total_pairs'])\n",
    "                run_count_trans = int(row['run_count'])\n",
    "                jsd_std = row['jsd_std']\n",
    "                \n",
    "                print(f\"    {transition}: Weighted JSD={weighted_jsd:.4f}, Simple JSD={simple_jsd:.4f}(±{jsd_std:.4f})\")\n",
    "                print(f\"      Child docs={total_child_docs}, Weight prop={weight_prop:.3f}, pairs={total_pairs_trans}, runs={run_count_trans}\")\n",
    "                \n",
    "                # Show the difference between weighted and simple averages\n",
    "                diff = abs(weighted_jsd - simple_jsd)\n",
    "                if diff > 0.0001:  # Only show if there's a meaningful difference\n",
    "                    print(f\"      -> Difference: {diff:.4f} (Weighted vs Simple)\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"Data collection with properly weighted averages (relative to 970 total documents) completed!\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d22af9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Start collecting parent-child JSD data with weighted averages for each Gamma value...\n",
      "======================================================================\n",
      "======================================================================\n",
      "Parent-Child JSD Data Collection by GAMMA Value (weighted by child_doc_count/970)\n",
      "======================================================================\n",
      "Saved complete detailed parent-child JSD data to: /Volumes/My Passport/收敛结果/step1/2/all_runs_detailed_parent_child_jsd_by_gamma.csv\n",
      "Saved properly weighted parent-child JSD summary to: /Volumes/My Passport/收敛结果/step1/2/properly_weighted_parent_child_jsd_by_gamma.csv\n",
      "\n",
      "Gamma 0.001 (single chain):\n",
      "  - Runs: 3\n",
      "  - Layer transitions: 3\n",
      "  - Total parent-child pairs: 728\n",
      "  - Layer transition summary (weighted by child_doc_count/970):\n",
      "    0->1: Weighted JSD=0.4798, Simple JSD=0.6105(±0.0457)\n",
      "      Child docs=2910, Weight prop=3.000, pairs=51, runs=3\n",
      "      -> Difference: 0.1308 (Weighted vs Simple)\n",
      "    1->2: Weighted JSD=0.6074, Simple JSD=0.4998(±0.1316)\n",
      "      Child docs=2910, Weight prop=3.000, pairs=204, runs=3\n",
      "      -> Difference: 0.1076 (Weighted vs Simple)\n",
      "    2->3: Weighted JSD=0.5523, Simple JSD=0.4772(±0.1064)\n",
      "      Child docs=2910, Weight prop=3.000, pairs=473, runs=3\n",
      "      -> Difference: 0.0751 (Weighted vs Simple)\n",
      "\n",
      "Gamma 0.005 (single chain):\n",
      "  - Runs: 3\n",
      "  - Layer transitions: 3\n",
      "  - Total parent-child pairs: 706\n",
      "  - Layer transition summary (weighted by child_doc_count/970):\n",
      "    0->1: Weighted JSD=0.5121, Simple JSD=0.6059(±0.0369)\n",
      "      Child docs=2910, Weight prop=3.000, pairs=50, runs=3\n",
      "      -> Difference: 0.0938 (Weighted vs Simple)\n",
      "    1->2: Weighted JSD=0.5833, Simple JSD=0.4842(±0.1436)\n",
      "      Child docs=2910, Weight prop=3.000, pairs=195, runs=3\n",
      "      -> Difference: 0.0991 (Weighted vs Simple)\n",
      "    2->3: Weighted JSD=0.5566, Simple JSD=0.4742(±0.1054)\n",
      "      Child docs=2910, Weight prop=3.000, pairs=461, runs=3\n",
      "      -> Difference: 0.0823 (Weighted vs Simple)\n",
      "\n",
      "Gamma 0.010 (single chain):\n",
      "  - Runs: 3\n",
      "  - Layer transitions: 3\n",
      "  - Total parent-child pairs: 782\n",
      "  - Layer transition summary (weighted by child_doc_count/970):\n",
      "    0->1: Weighted JSD=0.4986, Simple JSD=0.6115(±0.0419)\n",
      "      Child docs=2910, Weight prop=3.000, pairs=50, runs=3\n",
      "      -> Difference: 0.1129 (Weighted vs Simple)\n",
      "    1->2: Weighted JSD=0.5841, Simple JSD=0.4964(±0.1501)\n",
      "      Child docs=2910, Weight prop=3.000, pairs=214, runs=3\n",
      "      -> Difference: 0.0878 (Weighted vs Simple)\n",
      "    2->3: Weighted JSD=0.5531, Simple JSD=0.4781(±0.1117)\n",
      "      Child docs=2910, Weight prop=3.000, pairs=518, runs=3\n",
      "      -> Difference: 0.0750 (Weighted vs Simple)\n",
      "\n",
      "Gamma 0.050 (single chain):\n",
      "  - Runs: 3\n",
      "  - Layer transitions: 3\n",
      "  - Total parent-child pairs: 791\n",
      "  - Layer transition summary (weighted by child_doc_count/970):\n",
      "    0->1: Weighted JSD=0.4803, Simple JSD=0.6176(±0.0486)\n",
      "      Child docs=2910, Weight prop=3.000, pairs=49, runs=3\n",
      "      -> Difference: 0.1373 (Weighted vs Simple)\n",
      "    1->2: Weighted JSD=0.5639, Simple JSD=0.4992(±0.1468)\n",
      "      Child docs=2910, Weight prop=3.000, pairs=205, runs=3\n",
      "      -> Difference: 0.0648 (Weighted vs Simple)\n",
      "    2->3: Weighted JSD=0.5589, Simple JSD=0.4859(±0.1215)\n",
      "      Child docs=2910, Weight prop=3.000, pairs=537, runs=3\n",
      "      -> Difference: 0.0730 (Weighted vs Simple)\n",
      "\n",
      "Gamma 0.100 (single chain):\n",
      "  - Runs: 3\n",
      "  - Layer transitions: 3\n",
      "  - Total parent-child pairs: 826\n",
      "  - Layer transition summary (weighted by child_doc_count/970):\n",
      "    0->1: Weighted JSD=0.4628, Simple JSD=0.6058(±0.0484)\n",
      "      Child docs=2910, Weight prop=3.000, pairs=50, runs=3\n",
      "      -> Difference: 0.1430 (Weighted vs Simple)\n",
      "    1->2: Weighted JSD=0.5834, Simple JSD=0.4969(±0.1455)\n",
      "      Child docs=2910, Weight prop=3.000, pairs=211, runs=3\n",
      "      -> Difference: 0.0865 (Weighted vs Simple)\n",
      "    2->3: Weighted JSD=0.5536, Simple JSD=0.4768(±0.1130)\n",
      "      Child docs=2910, Weight prop=3.000, pairs=565, runs=3\n",
      "      -> Difference: 0.0767 (Weighted vs Simple)\n",
      "\n",
      "======================================================================\n",
      "Data collection with properly weighted averages (relative to 970 total documents) completed!\n",
      "======================================================================\n",
      "======================================================================\n",
      "Parent-child JSD data collection with weighted averages completed!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Execute parent-child JSD data collection with weighted averages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step1/2\"\n",
    "print(\"=\" * 70)\n",
    "print(\"Start collecting parent-child JSD data with weighted averages for each Gamma value...\")\n",
    "print(\"=\" * 70)\n",
    "aggregate_parent_child_jsd_by_gamma(base_path)\n",
    "print(\"=\" * 70)\n",
    "print(\"Parent-child JSD data collection with weighted averages completed!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
