{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ef8259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from scipy.special import gammaln\n",
    "\n",
    "def calculate_renyi_entropy_vectorized(node_data, all_words, alpha_prior=1.0, renyi_alpha=2.0):\n",
    "    \"\"\"\n",
    "    向量化版本的Renyi熵计算\n",
    "    \n",
    "    Parameters:\n",
    "    node_data: DataFrame, 包含word和count列的节点数据\n",
    "    all_words: list, 全量词汇表\n",
    "    alpha_prior: float, Dirichlet先验平滑参数  \n",
    "    renyi_alpha: float, Renyi熵的阶数参数\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (entropy, nonzero_word_count) Renyi熵值和非零词汇数量\n",
    "    \"\"\"\n",
    "    if len(all_words) == 0:\n",
    "        return 0.0, 0\n",
    "    \n",
    "    # 创建词汇到索引的映射\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "    \n",
    "    # 初始化计数向量\n",
    "    counts = np.zeros(len(all_words))\n",
    "    \n",
    "    # 填充实际计数\n",
    "    for _, row in node_data.iterrows():\n",
    "        word = row['word']\n",
    "        if pd.notna(word) and word in word_to_idx:\n",
    "            counts[word_to_idx[word]] = row['count']\n",
    "    \n",
    "    # 统计非零词汇数量（平滑前）\n",
    "    nonzero_word_count = np.sum(counts > 0)\n",
    "    \n",
    "    # 添加alpha平滑\n",
    "    smoothed_counts = counts + alpha_prior\n",
    "    \n",
    "    # 计算概率分布\n",
    "    probabilities = smoothed_counts / np.sum(smoothed_counts)\n",
    "    \n",
    "    # 计算Renyi熵\n",
    "    if renyi_alpha == 1.0:\n",
    "        # Shannon熵（由于alpha平滑，所有概率都>0，无需添加小常数）\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "    else:\n",
    "        # 一般Renyi熵\n",
    "        entropy = (1 / (1 - renyi_alpha)) * np.log2(np.sum(probabilities ** renyi_alpha))\n",
    "    \n",
    "    return entropy, int(nonzero_word_count)\n",
    "\n",
    "def process_all_iteration_files(base_path=\".\", alpha_prior=1.0, renyi_alpha=2.0):\n",
    "    \"\"\"\n",
    "    针对每个iteration_node_word_distributions.csv单独处理并保存结果\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_node_word_distributions.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        print(f\"\\n处理文件: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # 清理列名，去除单引号、双引号和空格\n",
    "            df.columns = [col.strip(\"'\\\" \") for col in df.columns]\n",
    "            \n",
    "            if 'node_id' not in df.columns:\n",
    "                print(f\"警告：{file_path} 缺少 node_id 列，跳过该文件\")\n",
    "                continue\n",
    "                \n",
    "            max_iteration = df['iteration'].max()\n",
    "            last_iteration_data = df[df['iteration'] == max_iteration]\n",
    "            all_words = list(last_iteration_data['word'].dropna().unique())\n",
    "            \n",
    "            print(f\"最后一轮iteration: {max_iteration}, 词汇表大小: {len(all_words)}, 节点数: {last_iteration_data['node_id'].nunique()}\")\n",
    "            \n",
    "            results = []\n",
    "            for node_id in last_iteration_data['node_id'].unique():\n",
    "                node_data = last_iteration_data[last_iteration_data['node_id'] == node_id]\n",
    "                \n",
    "                entropy, nonzero_words = calculate_renyi_entropy_vectorized(\n",
    "                    node_data, all_words, alpha_prior, renyi_alpha\n",
    "                )\n",
    "                \n",
    "                # 计算稀疏度（非零词汇占比）\n",
    "                sparsity_ratio = nonzero_words / len(all_words) if len(all_words) > 0 else 0\n",
    "                \n",
    "                results.append({\n",
    "                    'node_id': node_id,\n",
    "                    'renyi_entropy_corrected': entropy,\n",
    "                    'nonzero_word_count': nonzero_words,\n",
    "                    'total_vocabulary_size': len(all_words),\n",
    "                    'sparsity_ratio': sparsity_ratio,\n",
    "                    'alpha_prior': alpha_prior,\n",
    "                    'renyi_alpha': renyi_alpha,\n",
    "                    'iteration': max_iteration\n",
    "                })\n",
    "            \n",
    "            # 保存新的corrected_renyi_entropy.csv文件\n",
    "            results_df = pd.DataFrame(results)\n",
    "            output_path = os.path.join(folder_path, 'corrected_renyi_entropy.csv')\n",
    "            results_df.to_csv(output_path, index=False)\n",
    "            print(f\"保存修正的Renyi熵结果到: {output_path}\")\n",
    "            \n",
    "            # 输出一些统计信息\n",
    "            print(f\"节点词汇稀疏性统计:\")\n",
    "            print(f\"  - 平均非零词汇数: {results_df['nonzero_word_count'].mean():.1f}\")\n",
    "            print(f\"  - 非零词汇数范围: {results_df['nonzero_word_count'].min()}-{results_df['nonzero_word_count'].max()}\")\n",
    "            print(f\"  - 平均稀疏度: {results_df['sparsity_ratio'].mean():.3f}\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "                \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"处理文件 {file_path} 时出错: {str(e)}\")\n",
    "            print(\"详细错误信息:\")\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b21f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "开始批量计算修正的Renyi熵...\n",
      "==================================================\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g0005_收敛/depth_4_gamma_0.005_run_1/iteration_node_word_distributions.csv\n",
      "最后一轮iteration: 115, 词汇表大小: 1490, 节点数: 228\n",
      "保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/2/d4_g0005_收敛/depth_4_gamma_0.005_run_1/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 71.7\n",
      "  - 非零词汇数范围: 0-842\n",
      "  - 平均稀疏度: 0.048\n",
      "==================================================\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g0005_收敛/depth_4_gamma_0.005_run_2/iteration_node_word_distributions.csv\n",
      "最后一轮iteration: 115, 词汇表大小: 1490, 节点数: 235\n",
      "保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/2/d4_g0005_收敛/depth_4_gamma_0.005_run_2/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 72.6\n",
      "  - 非零词汇数范围: 0-888\n",
      "  - 平均稀疏度: 0.049\n",
      "==================================================\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g0005_收敛/depth_4_gamma_0.005_run_3/iteration_node_word_distributions.csv\n",
      "最后一轮iteration: 115, 词汇表大小: 1490, 节点数: 246\n",
      "保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/2/d4_g0005_收敛/depth_4_gamma_0.005_run_3/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 78.5\n",
      "  - 非零词汇数范围: 0-760\n",
      "  - 平均稀疏度: 0.053\n",
      "==================================================\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_1/iteration_node_word_distributions.csv\n",
      "最后一轮iteration: 195, 词汇表大小: 1490, 节点数: 268\n",
      "保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_1/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 74.6\n",
      "  - 非零词汇数范围: 0-708\n",
      "  - 平均稀疏度: 0.050\n",
      "==================================================\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_2/iteration_node_word_distributions.csv\n",
      "最后一轮iteration: 195, 词汇表大小: 1490, 节点数: 270\n",
      "保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_2/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 75.0\n",
      "  - 非零词汇数范围: 0-713\n",
      "  - 平均稀疏度: 0.050\n",
      "==================================================\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_3/iteration_node_word_distributions.csv\n",
      "最后一轮iteration: 195, 词汇表大小: 1490, 节点数: 247\n",
      "保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_3/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 76.9\n",
      "  - 非零词汇数范围: 0-729\n",
      "  - 平均稀疏度: 0.052\n",
      "==================================================\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g01_收敛/depth_4_gamma_0.1_run_1/iteration_node_word_distributions.csv\n",
      "最后一轮iteration: 135, 词汇表大小: 1490, 节点数: 250\n",
      "保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/2/d4_g01_收敛/depth_4_gamma_0.1_run_1/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 74.2\n",
      "  - 非零词汇数范围: 0-780\n",
      "  - 平均稀疏度: 0.050\n",
      "==================================================\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g01_收敛/depth_4_gamma_0.1_run_2/iteration_node_word_distributions.csv\n",
      "最后一轮iteration: 135, 词汇表大小: 1490, 节点数: 306\n",
      "保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/2/d4_g01_收敛/depth_4_gamma_0.1_run_2/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 70.5\n",
      "  - 非零词汇数范围: 0-658\n",
      "  - 平均稀疏度: 0.047\n",
      "==================================================\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g01_收敛/depth_4_gamma_0.1_run_3/iteration_node_word_distributions.csv\n",
      "最后一轮iteration: 135, 词汇表大小: 1490, 节点数: 273\n",
      "保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/2/d4_g01_收敛/depth_4_gamma_0.1_run_3/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 70.6\n",
      "  - 非零词汇数范围: 0-706\n",
      "  - 平均稀疏度: 0.047\n",
      "==================================================\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g0001_收敛/depth_4_gamma_0.001_run_1/iteration_node_word_distributions.csv\n",
      "最后一轮iteration: 175, 词汇表大小: 1490, 节点数: 258\n",
      "保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/2/d4_g0001_收敛/depth_4_gamma_0.001_run_1/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 75.9\n",
      "  - 非零词汇数范围: 0-707\n",
      "  - 平均稀疏度: 0.051\n",
      "==================================================\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g0001_收敛/depth_4_gamma_0.001_run_2/iteration_node_word_distributions.csv\n",
      "最后一轮iteration: 175, 词汇表大小: 1490, 节点数: 240\n",
      "保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/2/d4_g0001_收敛/depth_4_gamma_0.001_run_2/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 78.5\n",
      "  - 非零词汇数范围: 0-696\n",
      "  - 平均稀疏度: 0.053\n",
      "==================================================\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g0001_收敛/depth_4_gamma_0.001_run_3/iteration_node_word_distributions.csv\n",
      "最后一轮iteration: 175, 词汇表大小: 1490, 节点数: 233\n",
      "保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/2/d4_g0001_收敛/depth_4_gamma_0.001_run_3/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 72.2\n",
      "  - 非零词汇数范围: 0-870\n",
      "  - 平均稀疏度: 0.048\n",
      "==================================================\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g005_收敛/depth_4_gamma_0.05_run_1/iteration_node_word_distributions.csv\n",
      "最后一轮iteration: 315, 词汇表大小: 1490, 节点数: 282\n",
      "保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/2/d4_g005_收敛/depth_4_gamma_0.05_run_1/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 72.8\n",
      "  - 非零词汇数范围: 0-671\n",
      "  - 平均稀疏度: 0.049\n",
      "==================================================\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g005_收敛/depth_4_gamma_0.05_run_2/iteration_node_word_distributions.csv\n",
      "最后一轮iteration: 315, 词汇表大小: 1490, 节点数: 267\n",
      "保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/2/d4_g005_收敛/depth_4_gamma_0.05_run_2/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 79.6\n",
      "  - 非零词汇数范围: 0-677\n",
      "  - 平均稀疏度: 0.053\n",
      "==================================================\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g005_收敛/depth_4_gamma_0.05_run_3/iteration_node_word_distributions.csv\n",
      "最后一轮iteration: 315, 词汇表大小: 1490, 节点数: 245\n",
      "保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/2/d4_g005_收敛/depth_4_gamma_0.05_run_3/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 79.9\n",
      "  - 非零词汇数范围: 0-695\n",
      "  - 平均稀疏度: 0.054\n",
      "==================================================\n",
      "==================================================\n",
      "全部处理完成！\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 设置参数\n",
    "base_path = \"/Volumes/My Passport/收敛结果/2\"  # 根目录\n",
    "alpha_prior = 0.1  # Dirichlet先验平滑参数\n",
    "renyi_alpha = 2.0  # Renyi熵阶数参数\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"开始批量计算修正的Renyi熵...\")\n",
    "print(\"=\" * 50)\n",
    "process_all_iteration_files(base_path, alpha_prior, renyi_alpha)\n",
    "print(\"=\" * 50)\n",
    "print(\"全部处理完成！\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d81643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_node_document_counts(path_structures_df):\n",
    "    \"\"\"\n",
    "    从叶子节点向上聚合，计算每个节点的文档数和层级关系\n",
    "    \n",
    "    Parameters:\n",
    "    path_structures_df: DataFrame, iteration_path_structures.csv的数据（已经过滤为最后一轮）\n",
    "    \n",
    "    Returns:\n",
    "    dict: {node_id: {'document_count': int, 'layer': int, 'parent_id': int, 'child_ids': list}} 映射\n",
    "    \"\"\"\n",
    "    # 获取所有layer列 - 修正正则表达式\n",
    "    layer_columns = [col for col in path_structures_df.columns if col.startswith('layer_') and col.endswith('_node_id')]\n",
    "    layer_columns.sort()  # 确保按顺序排列\n",
    "    max_layer_idx = len(layer_columns) - 1\n",
    "    \n",
    "    print(f\"[DEBUG] 发现层级列: {layer_columns}\")\n",
    "    print(f\"[DEBUG] 最大层级索引: {max_layer_idx}\")\n",
    "    \n",
    "    # 初始化节点信息字典\n",
    "    node_info = {}\n",
    "    \n",
    "    # 处理叶子节点 - 直接使用leaf_node_id列\n",
    "    for _, row in path_structures_df.iterrows():\n",
    "        leaf_node = row['leaf_node_id']\n",
    "        if pd.notna(leaf_node):\n",
    "            if leaf_node not in node_info:\n",
    "                node_info[leaf_node] = {\n",
    "                    'document_count': 0,\n",
    "                    'layer': max_layer_idx,\n",
    "                    'parent_id': None,\n",
    "                    'child_ids': [],\n",
    "                    'child_count': 0\n",
    "                }\n",
    "            node_info[leaf_node]['document_count'] += row['document_count']\n",
    "    \n",
    "    # 建立父子关系和层级信息\n",
    "    for _, row in path_structures_df.iterrows():\n",
    "        path_nodes = []\n",
    "        for layer_idx in range(max_layer_idx + 1):\n",
    "            layer_col = f'layer_{layer_idx}_node_id'\n",
    "            if layer_col in path_structures_df.columns and pd.notna(row[layer_col]):\n",
    "                path_nodes.append(row[layer_col])\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # 为路径中的每个节点建立层级和父子关系\n",
    "        for i, node in enumerate(path_nodes):\n",
    "            if node not in node_info:\n",
    "                node_info[node] = {\n",
    "                    'document_count': 0,\n",
    "                    'layer': i,\n",
    "                    'parent_id': None,\n",
    "                    'child_ids': [],\n",
    "                    'child_count': 0\n",
    "                }\n",
    "            else:\n",
    "                # 更新层级信息（确保一致性）\n",
    "                node_info[node]['layer'] = i\n",
    "            \n",
    "            # 设置父节点关系\n",
    "            if i > 0:  # 不是根节点\n",
    "                parent_node = path_nodes[i-1]\n",
    "                node_info[node]['parent_id'] = parent_node\n",
    "                \n",
    "                # 在父节点的子节点列表中添加当前节点\n",
    "                if parent_node not in node_info:\n",
    "                    node_info[parent_node] = {\n",
    "                        'document_count': 0,\n",
    "                        'layer': i-1,\n",
    "                        'parent_id': None,\n",
    "                        'child_ids': [],\n",
    "                        'child_count': 0\n",
    "                    }\n",
    "                \n",
    "                if node not in node_info[parent_node]['child_ids']:\n",
    "                    node_info[parent_node]['child_ids'].append(node)\n",
    "    \n",
    "    # 从倒数第二层开始向上聚合文档数\n",
    "    for layer_idx in range(max_layer_idx - 1, -1, -1):  # 从倒数第二层到第0层\n",
    "        layer_col = f'layer_{layer_idx}_node_id'\n",
    "        \n",
    "        if layer_col not in path_structures_df.columns:\n",
    "            continue\n",
    "            \n",
    "        # 获取这一层的所有唯一节点\n",
    "        layer_nodes = path_structures_df[layer_col].dropna().unique()\n",
    "        \n",
    "        for node in layer_nodes:\n",
    "            if node in node_info and node_info[node]['document_count'] == 0:\n",
    "                # 计算文档数\n",
    "                total_docs = path_structures_df[path_structures_df[layer_col] == node]['document_count'].sum()\n",
    "                node_info[node]['document_count'] = total_docs\n",
    "\n",
    "    # 计算每个节点的子节点数量\n",
    "    for node_id, info in node_info.items():\n",
    "        info['child_count'] = len(info['child_ids'])\n",
    "    \n",
    "    return node_info\n",
    "\n",
    "def add_document_counts_to_entropy_files(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    将文档数和层级信息添加到corrected_renyi_entropy.csv文件中\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_path_structures.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        print(f\"\\n处理路径结构文件: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            # 读取path_structures文件\n",
    "            df = pd.read_csv(file_path)\n",
    "            df.columns = [col.strip(\"'\\\" \") for col in df.columns]\n",
    "            \n",
    "            # 获取最后一轮数据\n",
    "            max_iteration = df['iteration'].max()\n",
    "            last_iteration_data = df[df['iteration'] == max_iteration]\n",
    "            \n",
    "            print(f\"最后一轮iteration: {max_iteration}, 路径数: {len(last_iteration_data)}\")\n",
    "            \n",
    "            # 计算每个节点的文档数和层级关系\n",
    "            node_info = calculate_node_document_counts(last_iteration_data)\n",
    "            \n",
    "            print(f\"计算得到 {len(node_info)} 个节点的信息\")\n",
    "            \n",
    "            # 读取对应的corrected_renyi_entropy.csv\n",
    "            entropy_file = os.path.join(folder_path, 'corrected_renyi_entropy.csv')\n",
    "            if os.path.exists(entropy_file):\n",
    "                entropy_df = pd.read_csv(entropy_file)\n",
    "                \n",
    "                # 添加新列 - 修正child_ids格式和child_count计算\n",
    "                entropy_df['document_count'] = entropy_df['node_id'].map(lambda x: node_info.get(x, {}).get('document_count', 0))\n",
    "                entropy_df['layer'] = entropy_df['node_id'].map(lambda x: node_info.get(x, {}).get('layer', -1))\n",
    "                entropy_df['parent_id'] = entropy_df['node_id'].map(lambda x: node_info.get(x, {}).get('parent_id', None))\n",
    "                \n",
    "                # 修正child_ids格式：使用方括号而不是逗号\n",
    "                entropy_df['child_ids'] = entropy_df['node_id'].map(\n",
    "                    lambda x: '[' + ','.join(map(str, node_info.get(x, {}).get('child_ids', []))) + ']' \n",
    "                    if node_info.get(x, {}).get('child_ids') else ''\n",
    "                )\n",
    "                \n",
    "                # 修正child_count：直接使用列表长度\n",
    "                entropy_df['child_count'] = entropy_df['node_id'].map(lambda x: len(node_info.get(x, {}).get('child_ids', [])))\n",
    "\n",
    "                # 保存更新后的文件\n",
    "                entropy_df.to_csv(entropy_file, index=False)\n",
    "                print(f\"已更新 {entropy_file}，添加了document_count, layer, parent_id, child_ids, child_count列\")\n",
    "                \n",
    "                # 显示一些统计信息\n",
    "                print(f\"节点层级统计:\")\n",
    "                print(f\"  - 层级分布: {entropy_df['layer'].value_counts().sort_index().to_dict()}\")\n",
    "                print(f\"  - 文档数范围: {entropy_df['document_count'].min()}-{entropy_df['document_count'].max()}\")\n",
    "                print(f\"  - 根节点数: {entropy_df[entropy_df['parent_id'].isna()].shape[0]}\")\n",
    "                print(f\"  - 叶子节点数: {entropy_df[entropy_df['child_ids'] == ''].shape[0]}\")\n",
    "                print(f\"  - 子节点数分布: {entropy_df['child_count'].value_counts().sort_index().to_dict()}\")\n",
    "            else:\n",
    "                print(f\"警告：未找到对应的entropy文件 {entropy_file}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"处理文件 {file_path} 时出错: {str(e)}\")\n",
    "            print(\"详细错误信息:\")\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e230e0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "开始添加文档数和层级信息到entropy文件...\n",
      "==================================================\n",
      "\n",
      "处理路径结构文件: /Volumes/My Passport/收敛结果/2/d4_g0005_收敛/depth_4_gamma_0.005_run_1/iteration_path_structures.csv\n",
      "最后一轮iteration: 115, 路径数: 141\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] 最大层级索引: 3\n",
      "计算得到 228 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/2/d4_g0005_收敛/depth_4_gamma_0.005_run_1/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 19, 2: 67, 3: 141}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 141\n",
      "  - 子节点数分布: {0: 141, 1: 37, 2: 28, 3: 8, 4: 9, 5: 1, 7: 1, 19: 2, 24: 1}\n",
      "\n",
      "处理路径结构文件: /Volumes/My Passport/收敛结果/2/d4_g0005_收敛/depth_4_gamma_0.005_run_2/iteration_path_structures.csv\n",
      "最后一轮iteration: 115, 路径数: 155\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] 最大层级索引: 3\n",
      "计算得到 235 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/2/d4_g0005_收敛/depth_4_gamma_0.005_run_2/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 17, 2: 62, 3: 155}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 155\n",
      "  - 子节点数分布: {0: 155, 1: 30, 2: 27, 3: 8, 4: 6, 5: 4, 6: 2, 17: 1, 20: 1, 33: 1}\n",
      "\n",
      "处理路径结构文件: /Volumes/My Passport/收敛结果/2/d4_g0005_收敛/depth_4_gamma_0.005_run_3/iteration_path_structures.csv\n",
      "最后一轮iteration: 115, 路径数: 165\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] 最大层级索引: 3\n",
      "计算得到 246 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/2/d4_g0005_收敛/depth_4_gamma_0.005_run_3/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 14, 2: 66, 3: 165}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 165\n",
      "  - 子节点数分布: {0: 165, 1: 32, 2: 19, 3: 12, 4: 6, 5: 5, 6: 1, 7: 1, 8: 1, 13: 1, 14: 1, 18: 1, 24: 1}\n",
      "\n",
      "处理路径结构文件: /Volumes/My Passport/收敛结果/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_1/iteration_path_structures.csv\n",
      "最后一轮iteration: 195, 路径数: 181\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] 最大层级索引: 3\n",
      "计算得到 268 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_1/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 13, 2: 73, 3: 181}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 181\n",
      "  - 子节点数分布: {0: 181, 1: 32, 2: 19, 3: 19, 4: 6, 5: 6, 6: 1, 7: 1, 13: 1, 26: 1, 34: 1}\n",
      "\n",
      "处理路径结构文件: /Volumes/My Passport/收敛结果/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_2/iteration_path_structures.csv\n",
      "最后一轮iteration: 195, 路径数: 174\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] 最大层级索引: 3\n",
      "计算得到 270 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_2/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 20, 2: 75, 3: 174}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 174\n",
      "  - 子节点数分布: {0: 174, 1: 34, 2: 34, 3: 16, 4: 4, 5: 4, 7: 1, 20: 1, 26: 1, 30: 1}\n",
      "\n",
      "处理路径结构文件: /Volumes/My Passport/收敛结果/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_3/iteration_path_structures.csv\n",
      "最后一轮iteration: 195, 路径数: 163\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] 最大层级索引: 3\n",
      "计算得到 247 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_3/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 17, 2: 66, 3: 163}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 163\n",
      "  - 子节点数分布: {0: 163, 1: 32, 2: 21, 3: 19, 4: 5, 5: 1, 6: 3, 17: 1, 27: 1, 28: 1}\n",
      "\n",
      "处理路径结构文件: /Volumes/My Passport/收敛结果/2/d4_g01_收敛/depth_4_gamma_0.1_run_1/iteration_path_structures.csv\n",
      "最后一轮iteration: 135, 路径数: 171\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] 最大层级索引: 3\n",
      "计算得到 250 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/2/d4_g01_收敛/depth_4_gamma_0.1_run_1/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 12, 2: 66, 3: 171}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 171\n",
      "  - 子节点数分布: {0: 171, 1: 26, 2: 22, 3: 13, 4: 9, 5: 4, 6: 2, 12: 1, 29: 1, 31: 1}\n",
      "\n",
      "处理路径结构文件: /Volumes/My Passport/收敛结果/2/d4_g01_收敛/depth_4_gamma_0.1_run_2/iteration_path_structures.csv\n",
      "最后一轮iteration: 135, 路径数: 206\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] 最大层级索引: 3\n",
      "计算得到 306 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/2/d4_g01_收敛/depth_4_gamma_0.1_run_2/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 23, 2: 76, 3: 206}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 206\n",
      "  - 子节点数分布: {0: 206, 1: 33, 2: 37, 3: 10, 4: 10, 5: 6, 6: 1, 23: 1, 27: 1, 42: 1}\n",
      "\n",
      "处理路径结构文件: /Volumes/My Passport/收敛结果/2/d4_g01_收敛/depth_4_gamma_0.1_run_3/iteration_path_structures.csv\n",
      "最后一轮iteration: 135, 路径数: 188\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] 最大层级索引: 3\n",
      "计算得到 273 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/2/d4_g01_收敛/depth_4_gamma_0.1_run_3/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 15, 2: 69, 3: 188}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 188\n",
      "  - 子节点数分布: {0: 188, 1: 27, 2: 25, 3: 14, 4: 11, 5: 5, 15: 1, 31: 1, 38: 1}\n",
      "\n",
      "处理路径结构文件: /Volumes/My Passport/收敛结果/2/d4_g0001_收敛/depth_4_gamma_0.001_run_1/iteration_path_structures.csv\n",
      "最后一轮iteration: 175, 路径数: 168\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] 最大层级索引: 3\n",
      "计算得到 258 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/2/d4_g0001_收敛/depth_4_gamma_0.001_run_1/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 17, 2: 72, 3: 168}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 168\n",
      "  - 子节点数分布: {0: 168, 1: 34, 2: 31, 3: 9, 4: 9, 5: 3, 6: 1, 17: 1, 28: 1, 32: 1}\n",
      "\n",
      "处理路径结构文件: /Volumes/My Passport/收敛结果/2/d4_g0001_收敛/depth_4_gamma_0.001_run_2/iteration_path_structures.csv\n",
      "最后一轮iteration: 175, 路径数: 162\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] 最大层级索引: 3\n",
      "计算得到 240 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/2/d4_g0001_收敛/depth_4_gamma_0.001_run_2/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 14, 2: 63, 3: 162}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 162\n",
      "  - 子节点数分布: {0: 162, 1: 27, 2: 26, 3: 14, 4: 2, 5: 4, 6: 1, 8: 1, 14: 1, 30: 1, 32: 1}\n",
      "\n",
      "处理路径结构文件: /Volumes/My Passport/收敛结果/2/d4_g0001_收敛/depth_4_gamma_0.001_run_3/iteration_path_structures.csv\n",
      "最后一轮iteration: 175, 路径数: 143\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] 最大层级索引: 3\n",
      "计算得到 233 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/2/d4_g0001_收敛/depth_4_gamma_0.001_run_3/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 20, 2: 69, 3: 143}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 143\n",
      "  - 子节点数分布: {0: 143, 1: 31, 2: 35, 3: 9, 4: 7, 5: 5, 12: 1, 19: 1, 20: 1}\n",
      "\n",
      "处理路径结构文件: /Volumes/My Passport/收敛结果/2/d4_g005_收敛/depth_4_gamma_0.05_run_1/iteration_path_structures.csv\n",
      "最后一轮iteration: 315, 路径数: 187\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] 最大层级索引: 3\n",
      "计算得到 282 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/2/d4_g005_收敛/depth_4_gamma_0.05_run_1/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 19, 2: 75, 3: 187}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 187\n",
      "  - 子节点数分布: {0: 187, 1: 38, 2: 30, 3: 16, 4: 6, 5: 1, 6: 1, 19: 1, 35: 1, 46: 1}\n",
      "\n",
      "处理路径结构文件: /Volumes/My Passport/收敛结果/2/d4_g005_收敛/depth_4_gamma_0.05_run_2/iteration_path_structures.csv\n",
      "最后一轮iteration: 315, 路径数: 181\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] 最大层级索引: 3\n",
      "计算得到 267 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/2/d4_g005_收敛/depth_4_gamma_0.05_run_2/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 16, 2: 69, 3: 181}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 181\n",
      "  - 子节点数分布: {0: 181, 1: 28, 2: 26, 3: 15, 4: 9, 5: 2, 6: 2, 7: 1, 16: 1, 29: 1, 31: 1}\n",
      "\n",
      "处理路径结构文件: /Volumes/My Passport/收敛结果/2/d4_g005_收敛/depth_4_gamma_0.05_run_3/iteration_path_structures.csv\n",
      "最后一轮iteration: 315, 路径数: 169\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'layer_3_node_id']\n",
      "[DEBUG] 最大层级索引: 3\n",
      "计算得到 245 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/2/d4_g005_收敛/depth_4_gamma_0.05_run_3/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 14, 2: 61, 3: 169}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 169\n",
      "  - 子节点数分布: {0: 169, 1: 23, 2: 27, 3: 12, 4: 7, 5: 3, 6: 1, 14: 1, 25: 1, 43: 1}\n",
      "==================================================\n",
      "文档数和层级信息添加完成！\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 主函数：添加文档数和层级信息到entropy文件\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd \n",
    "\n",
    "base_path = \"/Volumes/My Passport/收敛结果/2\"  # 根目录\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"开始添加文档数和层级信息到entropy文件...\")\n",
    "print(\"=\" * 50)\n",
    "add_document_counts_to_entropy_files(base_path)\n",
    "print(\"=\" * 50)\n",
    "print(\"文档数和层级信息添加完成！\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd6ab468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jensen_shannon_distances_with_weighted_entropy(base_path=\".\", eta=0.1):\n",
    "    \"\"\"\n",
    "    计算每层节点之间的Jensen-Shannon距离和文档数加权平均Renyi熵\n",
    "    \n",
    "    Parameters:\n",
    "    base_path: str, 根目录路径\n",
    "    eta: float, Dirichlet平滑参数\n",
    "    \"\"\"\n",
    "    # 查找所有iteration_node_word_distributions.csv文件\n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_node_word_distributions.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        print(f\"\\n处理文件: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            # 读取词分布数据\n",
    "            word_df = pd.read_csv(file_path)\n",
    "            word_df.columns = [col.strip(\"'\\\" \") for col in word_df.columns]\n",
    "            \n",
    "            # 获取最后一轮数据\n",
    "            max_iteration = word_df['iteration'].max()\n",
    "            last_iteration_data = word_df[word_df['iteration'] == max_iteration]\n",
    "            \n",
    "            # 获取全量词汇表\n",
    "            all_words = sorted(list(last_iteration_data['word'].dropna().unique()))\n",
    "            print(f\"全量词汇表大小: {len(all_words)}\")\n",
    "            \n",
    "            # 读取entropy文件获取层级信息\n",
    "            entropy_file = os.path.join(folder_path, 'corrected_renyi_entropy.csv')\n",
    "            if not os.path.exists(entropy_file):\n",
    "                print(f\"警告：未找到entropy文件 {entropy_file}\")\n",
    "                continue\n",
    "                \n",
    "            entropy_df = pd.read_csv(entropy_file)\n",
    "            \n",
    "            # 按层级分组节点\n",
    "            layers = entropy_df.groupby('layer')['node_id'].apply(list).to_dict()\n",
    "            print(f\"层级分布: {[(layer, len(nodes)) for layer, nodes in layers.items()]}\")\n",
    "            \n",
    "            # 为每个节点构建概率分布\n",
    "            node_distributions = {}\n",
    "            \n",
    "            for node_id in entropy_df['node_id'].unique():\n",
    "                # 获取该节点的词分布\n",
    "                node_words = last_iteration_data[last_iteration_data['node_id'] == node_id]\n",
    "                \n",
    "                # 初始化计数向量\n",
    "                counts = np.zeros(len(all_words))\n",
    "                word_to_idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "                \n",
    "                # 填充实际计数\n",
    "                for _, row in node_words.iterrows():\n",
    "                    word = row['word']\n",
    "                    if pd.notna(word) and word in word_to_idx:\n",
    "                        counts[word_to_idx[word]] = row['count']\n",
    "                \n",
    "                # 添加Dirichlet平滑\n",
    "                smoothed_counts = counts + eta\n",
    "                \n",
    "                # 计算概率分布\n",
    "                probabilities = smoothed_counts / np.sum(smoothed_counts)\n",
    "                node_distributions[node_id] = probabilities\n",
    "            \n",
    "            # 计算每层内节点的JS距离和加权平均熵\n",
    "            all_js_distances = []\n",
    "            layer_avg_distances = []\n",
    "            \n",
    "            for layer, layer_nodes in layers.items():\n",
    "                print(f\"\\n计算Layer {layer}的JS距离和加权平均熵 ({len(layer_nodes)} 个节点)\")\n",
    "                \n",
    "                layer_js_distances = []\n",
    "                n = len(layer_nodes)\n",
    "                \n",
    "                # 计算该层内所有节点对的JS距离\n",
    "                for i, node1 in enumerate(layer_nodes):\n",
    "                    for j, node2 in enumerate(layer_nodes):\n",
    "                        if i < j:  # 只计算上三角矩阵，避免重复和自己与自己\n",
    "                            if node1 in node_distributions and node2 in node_distributions:\n",
    "                                p = node_distributions[node1]\n",
    "                                q = node_distributions[node2]\n",
    "                                \n",
    "                                # 计算Jensen-Shannon距离\n",
    "                                js_distance = jensen_shannon_distance(p, q)\n",
    "                                \n",
    "                                layer_js_distances.append({\n",
    "                                    'layer': layer,\n",
    "                                    'node1_id': node1,\n",
    "                                    'node2_id': node2,\n",
    "                                    'js_distance': js_distance,\n",
    "                                    'node1_doc_count': entropy_df[entropy_df['node_id'] == node1]['document_count'].iloc[0] if len(entropy_df[entropy_df['node_id'] == node1]) > 0 else 0,\n",
    "                                    'node2_doc_count': entropy_df[entropy_df['node_id'] == node2]['document_count'].iloc[0] if len(entropy_df[entropy_df['node_id'] == node2]) > 0 else 0\n",
    "                                })\n",
    "                \n",
    "                all_js_distances.extend(layer_js_distances)\n",
    "                \n",
    "                # 计算该层的平均JS距离\n",
    "                avg_js_distance = 0.0\n",
    "                if layer_js_distances and n > 1:\n",
    "                    total_js_distance = sum(d['js_distance'] for d in layer_js_distances)\n",
    "                    max_pairs = n * (n - 1) // 2  # n(n-1)/2\n",
    "                    avg_js_distance = total_js_distance / max_pairs\n",
    "                \n",
    "                # 计算该层的文档数加权平均Renyi熵\n",
    "                layer_entropy_data = entropy_df[entropy_df['layer'] == layer]\n",
    "                total_docs = layer_entropy_data['document_count'].sum()\n",
    "                \n",
    "                if total_docs > 0:\n",
    "                    # 计算加权平均熵：sum(文档数 * 熵) / 总文档数\n",
    "                    weighted_entropy = (layer_entropy_data['document_count'] * layer_entropy_data['renyi_entropy_corrected']).sum() / total_docs\n",
    "                else:\n",
    "                    weighted_entropy = 0.0\n",
    "                \n",
    "                layer_avg_distances.append({\n",
    "                    'layer': layer,\n",
    "                    'node_count': n,\n",
    "                    'total_pairs': len(layer_js_distances),\n",
    "                    'max_pairs': n * (n - 1) // 2 if n > 1 else 0,\n",
    "                    'sum_js_distance': sum(d['js_distance'] for d in layer_js_distances),\n",
    "                    'avg_js_distance': avg_js_distance,\n",
    "                    'total_documents': total_docs,\n",
    "                    'weighted_avg_renyi_entropy': weighted_entropy\n",
    "                })\n",
    "                \n",
    "                print(f\"  - 节点数: {n}\")\n",
    "                print(f\"  - 计算的节点对数: {len(layer_js_distances)}\")\n",
    "                print(f\"  - 理论最大节点对数: {n * (n - 1) // 2 if n > 1 else 0}\")\n",
    "                print(f\"  - 平均JS距离: {avg_js_distance:.4f}\")\n",
    "                print(f\"  - 总文档数: {total_docs}\")\n",
    "                print(f\"  - 文档数加权平均Renyi熵: {weighted_entropy:.4f}\")\n",
    "                print(\"=\" * 50)\n",
    "            \n",
    "            # 保存详细的JS距离结果\n",
    "            if all_js_distances:\n",
    "                js_df = pd.DataFrame(all_js_distances)\n",
    "                output_path = os.path.join(folder_path, 'jensen_shannon_distances.csv')\n",
    "                js_df.to_csv(output_path, index=False)\n",
    "                print(f\"\\n保存详细JS距离结果到: {output_path}\")\n",
    "            \n",
    "            # 保存每层平均JS距离和加权熵结果\n",
    "            if layer_avg_distances:\n",
    "                avg_df = pd.DataFrame(layer_avg_distances)\n",
    "                avg_output_path = os.path.join(folder_path, 'layer_average_js_distances.csv')\n",
    "                avg_df.to_csv(avg_output_path, index=False)\n",
    "                print(f\"保存每层平均JS距离和加权熵结果到: {avg_output_path}\")\n",
    "                \n",
    "                # 总体统计\n",
    "                print(f\"\\n总体统计:\")\n",
    "                print(f\"  - 总层数: {len(layer_avg_distances)}\")\n",
    "                print(f\"  - 各层统计:\")\n",
    "                for row in layer_avg_distances:\n",
    "                    print(f\"    Layer {row['layer']}: JS距离={row['avg_js_distance']:.4f}, 加权熵={row['weighted_avg_renyi_entropy']:.4f} (基于{row['node_count']}个节点, {row['total_documents']}个文档)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"处理文件 {file_path} 时出错: {str(e)}\")\n",
    "            print(\"详细错误信息:\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "def jensen_shannon_distance(p, q):\n",
    "    \"\"\"\n",
    "    计算两个概率分布之间的Jensen-Shannon距离\n",
    "    \n",
    "    Parameters:\n",
    "    p, q: numpy arrays, 概率分布\n",
    "    \n",
    "    Returns:\n",
    "    float: Jensen-Shannon距离\n",
    "    \"\"\"\n",
    "    # 确保概率分布归一化\n",
    "    p = p / np.sum(p)\n",
    "    q = q / np.sum(q)\n",
    "    \n",
    "    # 计算平均分布\n",
    "    m = 0.5 * (p + q)\n",
    "    \n",
    "    # 计算KL散度（使用自然对数）\n",
    "    def kl_divergence(x, y):\n",
    "        # 避免log(0)的情况\n",
    "        mask = (x > 0) & (y > 0)\n",
    "        if np.sum(mask) == 0:\n",
    "            return 0.0\n",
    "        return np.sum(x[mask] * np.log(x[mask] / y[mask]))\n",
    "    \n",
    "    # 计算Jensen-Shannon散度\n",
    "    js_divergence = 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)\n",
    "    \n",
    "    # 转换为距离（取平方根）\n",
    "    js_distance = np.sqrt(js_divergence)\n",
    "    \n",
    "    return js_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7363e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "开始计算Jensen-Shannon距离和加权平均Renyi熵...\n",
      "==================================================\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g0005_收敛/depth_4_gamma_0.005_run_1/iteration_node_word_distributions.csv\n",
      "全量词汇表大小: 1490\n",
      "层级分布: [(0, 1), (1, 19), (2, 67), (3, 141)]\n",
      "\n",
      "计算Layer 0的JS距离和加权平均熵 (1 个节点)\n",
      "  - 节点数: 1\n",
      "  - 计算的节点对数: 0\n",
      "  - 理论最大节点对数: 0\n",
      "  - 平均JS距离: 0.0000\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.0594\n",
      "==================================================\n",
      "\n",
      "计算Layer 1的JS距离和加权平均熵 (19 个节点)\n",
      "  - 节点数: 19\n",
      "  - 计算的节点对数: 171\n",
      "  - 理论最大节点对数: 171\n",
      "  - 平均JS距离: 0.3923\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.3358\n",
      "==================================================\n",
      "\n",
      "计算Layer 2的JS距离和加权平均熵 (67 个节点)\n",
      "  - 节点数: 67\n",
      "  - 计算的节点对数: 2211\n",
      "  - 理论最大节点对数: 2211\n",
      "  - 平均JS距离: 0.4114\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.5513\n",
      "==================================================\n",
      "\n",
      "计算Layer 3的JS距离和加权平均熵 (141 个节点)\n",
      "  - 节点数: 141\n",
      "  - 计算的节点对数: 9870\n",
      "  - 理论最大节点对数: 9870\n",
      "  - 平均JS距离: 0.4564\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.6013\n",
      "==================================================\n",
      "\n",
      "保存详细JS距离结果到: /Volumes/My Passport/收敛结果/2/d4_g0005_收敛/depth_4_gamma_0.005_run_1/jensen_shannon_distances.csv\n",
      "保存每层平均JS距离和加权熵结果到: /Volumes/My Passport/收敛结果/2/d4_g0005_收敛/depth_4_gamma_0.005_run_1/layer_average_js_distances.csv\n",
      "\n",
      "总体统计:\n",
      "  - 总层数: 4\n",
      "  - 各层统计:\n",
      "    Layer 0: JS距离=0.0000, 加权熵=7.0594 (基于1个节点, 970个文档)\n",
      "    Layer 1: JS距离=0.3923, 加权熵=7.3358 (基于19个节点, 970个文档)\n",
      "    Layer 2: JS距离=0.4114, 加权熵=7.5513 (基于67个节点, 970个文档)\n",
      "    Layer 3: JS距离=0.4564, 加权熵=7.6013 (基于141个节点, 970个文档)\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g0005_收敛/depth_4_gamma_0.005_run_2/iteration_node_word_distributions.csv\n",
      "全量词汇表大小: 1490\n",
      "层级分布: [(0, 1), (1, 17), (2, 62), (3, 155)]\n",
      "\n",
      "计算Layer 0的JS距离和加权平均熵 (1 个节点)\n",
      "  - 节点数: 1\n",
      "  - 计算的节点对数: 0\n",
      "  - 理论最大节点对数: 0\n",
      "  - 平均JS距离: 0.0000\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.1099\n",
      "==================================================\n",
      "\n",
      "计算Layer 1的JS距离和加权平均熵 (17 个节点)\n",
      "  - 节点数: 17\n",
      "  - 计算的节点对数: 136\n",
      "  - 理论最大节点对数: 136\n",
      "  - 平均JS距离: 0.4628\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.1645\n",
      "==================================================\n",
      "\n",
      "计算Layer 2的JS距离和加权平均熵 (62 个节点)\n",
      "  - 节点数: 62\n",
      "  - 计算的节点对数: 1891\n",
      "  - 理论最大节点对数: 1891\n",
      "  - 平均JS距离: 0.3820\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.7808\n",
      "==================================================\n",
      "\n",
      "计算Layer 3的JS距离和加权平均熵 (155 个节点)\n",
      "  - 节点数: 155\n",
      "  - 计算的节点对数: 11935\n",
      "  - 理论最大节点对数: 11935\n",
      "  - 平均JS距离: 0.4637\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.4932\n",
      "==================================================\n",
      "\n",
      "保存详细JS距离结果到: /Volumes/My Passport/收敛结果/2/d4_g0005_收敛/depth_4_gamma_0.005_run_2/jensen_shannon_distances.csv\n",
      "保存每层平均JS距离和加权熵结果到: /Volumes/My Passport/收敛结果/2/d4_g0005_收敛/depth_4_gamma_0.005_run_2/layer_average_js_distances.csv\n",
      "\n",
      "总体统计:\n",
      "  - 总层数: 4\n",
      "  - 各层统计:\n",
      "    Layer 0: JS距离=0.0000, 加权熵=7.1099 (基于1个节点, 970个文档)\n",
      "    Layer 1: JS距离=0.4628, 加权熵=7.1645 (基于17个节点, 970个文档)\n",
      "    Layer 2: JS距离=0.3820, 加权熵=7.7808 (基于62个节点, 970个文档)\n",
      "    Layer 3: JS距离=0.4637, 加权熵=7.4932 (基于155个节点, 970个文档)\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g0005_收敛/depth_4_gamma_0.005_run_3/iteration_node_word_distributions.csv\n",
      "全量词汇表大小: 1490\n",
      "层级分布: [(0, 1), (1, 14), (2, 66), (3, 165)]\n",
      "\n",
      "计算Layer 0的JS距离和加权平均熵 (1 个节点)\n",
      "  - 节点数: 1\n",
      "  - 计算的节点对数: 0\n",
      "  - 理论最大节点对数: 0\n",
      "  - 平均JS距离: 0.0000\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.1416\n",
      "==================================================\n",
      "\n",
      "计算Layer 1的JS距离和加权平均熵 (14 个节点)\n",
      "  - 节点数: 14\n",
      "  - 计算的节点对数: 91\n",
      "  - 理论最大节点对数: 91\n",
      "  - 平均JS距离: 0.4780\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.0676\n",
      "==================================================\n",
      "\n",
      "计算Layer 2的JS距离和加权平均熵 (66 个节点)\n",
      "  - 节点数: 66\n",
      "  - 计算的节点对数: 2145\n",
      "  - 理论最大节点对数: 2145\n",
      "  - 平均JS距离: 0.4197\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.4647\n",
      "==================================================\n",
      "\n",
      "计算Layer 3的JS距离和加权平均熵 (165 个节点)\n",
      "  - 节点数: 165\n",
      "  - 计算的节点对数: 13530\n",
      "  - 理论最大节点对数: 13530\n",
      "  - 平均JS距离: 0.4738\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.3679\n",
      "==================================================\n",
      "\n",
      "保存详细JS距离结果到: /Volumes/My Passport/收敛结果/2/d4_g0005_收敛/depth_4_gamma_0.005_run_3/jensen_shannon_distances.csv\n",
      "保存每层平均JS距离和加权熵结果到: /Volumes/My Passport/收敛结果/2/d4_g0005_收敛/depth_4_gamma_0.005_run_3/layer_average_js_distances.csv\n",
      "\n",
      "总体统计:\n",
      "  - 总层数: 4\n",
      "  - 各层统计:\n",
      "    Layer 0: JS距离=0.0000, 加权熵=7.1416 (基于1个节点, 970个文档)\n",
      "    Layer 1: JS距离=0.4780, 加权熵=7.0676 (基于14个节点, 970个文档)\n",
      "    Layer 2: JS距离=0.4197, 加权熵=7.4647 (基于66个节点, 970个文档)\n",
      "    Layer 3: JS距离=0.4738, 加权熵=7.3679 (基于165个节点, 970个文档)\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_1/iteration_node_word_distributions.csv\n",
      "全量词汇表大小: 1490\n",
      "层级分布: [(0, 1), (1, 13), (2, 73), (3, 181)]\n",
      "\n",
      "计算Layer 0的JS距离和加权平均熵 (1 个节点)\n",
      "  - 节点数: 1\n",
      "  - 计算的节点对数: 0\n",
      "  - 理论最大节点对数: 0\n",
      "  - 平均JS距离: 0.0000\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 6.9239\n",
      "==================================================\n",
      "\n",
      "计算Layer 1的JS距离和加权平均熵 (13 个节点)\n",
      "  - 节点数: 13\n",
      "  - 计算的节点对数: 78\n",
      "  - 理论最大节点对数: 78\n",
      "  - 平均JS距离: 0.4501\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.1546\n",
      "==================================================\n",
      "\n",
      "计算Layer 2的JS距离和加权平均熵 (73 个节点)\n",
      "  - 节点数: 73\n",
      "  - 计算的节点对数: 2628\n",
      "  - 理论最大节点对数: 2628\n",
      "  - 平均JS距离: 0.4139\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.1725\n",
      "==================================================\n",
      "\n",
      "计算Layer 3的JS距离和加权平均熵 (181 个节点)\n",
      "  - 节点数: 181\n",
      "  - 计算的节点对数: 16290\n",
      "  - 理论最大节点对数: 16290\n",
      "  - 平均JS距离: 0.4731\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.4106\n",
      "==================================================\n",
      "\n",
      "保存详细JS距离结果到: /Volumes/My Passport/收敛结果/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_1/jensen_shannon_distances.csv\n",
      "保存每层平均JS距离和加权熵结果到: /Volumes/My Passport/收敛结果/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_1/layer_average_js_distances.csv\n",
      "\n",
      "总体统计:\n",
      "  - 总层数: 4\n",
      "  - 各层统计:\n",
      "    Layer 0: JS距离=0.0000, 加权熵=6.9239 (基于1个节点, 970个文档)\n",
      "    Layer 1: JS距离=0.4501, 加权熵=7.1546 (基于13个节点, 970个文档)\n",
      "    Layer 2: JS距离=0.4139, 加权熵=7.1725 (基于73个节点, 970个文档)\n",
      "    Layer 3: JS距离=0.4731, 加权熵=7.4106 (基于181个节点, 970个文档)\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_2/iteration_node_word_distributions.csv\n",
      "全量词汇表大小: 1490\n",
      "层级分布: [(0, 1), (1, 20), (2, 75), (3, 174)]\n",
      "\n",
      "计算Layer 0的JS距离和加权平均熵 (1 个节点)\n",
      "  - 节点数: 1\n",
      "  - 计算的节点对数: 0\n",
      "  - 理论最大节点对数: 0\n",
      "  - 平均JS距离: 0.0000\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 6.8942\n",
      "==================================================\n",
      "\n",
      "计算Layer 1的JS距离和加权平均熵 (20 个节点)\n",
      "  - 节点数: 20\n",
      "  - 计算的节点对数: 190\n",
      "  - 理论最大节点对数: 190\n",
      "  - 平均JS距离: 0.4341\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.0717\n",
      "==================================================\n",
      "\n",
      "计算Layer 2的JS距离和加权平均熵 (75 个节点)\n",
      "  - 节点数: 75\n",
      "  - 计算的节点对数: 2775\n",
      "  - 理论最大节点对数: 2775\n",
      "  - 平均JS距离: 0.4147\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.4329\n",
      "==================================================\n",
      "\n",
      "计算Layer 3的JS距离和加权平均熵 (174 个节点)\n",
      "  - 节点数: 174\n",
      "  - 计算的节点对数: 15051\n",
      "  - 理论最大节点对数: 15051\n",
      "  - 平均JS距离: 0.4707\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.4056\n",
      "==================================================\n",
      "\n",
      "保存详细JS距离结果到: /Volumes/My Passport/收敛结果/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_2/jensen_shannon_distances.csv\n",
      "保存每层平均JS距离和加权熵结果到: /Volumes/My Passport/收敛结果/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_2/layer_average_js_distances.csv\n",
      "\n",
      "总体统计:\n",
      "  - 总层数: 4\n",
      "  - 各层统计:\n",
      "    Layer 0: JS距离=0.0000, 加权熵=6.8942 (基于1个节点, 970个文档)\n",
      "    Layer 1: JS距离=0.4341, 加权熵=7.0717 (基于20个节点, 970个文档)\n",
      "    Layer 2: JS距离=0.4147, 加权熵=7.4329 (基于75个节点, 970个文档)\n",
      "    Layer 3: JS距离=0.4707, 加权熵=7.4056 (基于174个节点, 970个文档)\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_3/iteration_node_word_distributions.csv\n",
      "全量词汇表大小: 1490\n",
      "层级分布: [(0, 1), (1, 17), (2, 66), (3, 163)]\n",
      "\n",
      "计算Layer 0的JS距离和加权平均熵 (1 个节点)\n",
      "  - 节点数: 1\n",
      "  - 计算的节点对数: 0\n",
      "  - 理论最大节点对数: 0\n",
      "  - 平均JS距离: 0.0000\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 6.9879\n",
      "==================================================\n",
      "\n",
      "计算Layer 1的JS距离和加权平均熵 (17 个节点)\n",
      "  - 节点数: 17\n",
      "  - 计算的节点对数: 136\n",
      "  - 理论最大节点对数: 136\n",
      "  - 平均JS距离: 0.4127\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.4218\n",
      "==================================================\n",
      "\n",
      "计算Layer 2的JS距离和加权平均熵 (66 个节点)\n",
      "  - 节点数: 66\n",
      "  - 计算的节点对数: 2145\n",
      "  - 理论最大节点对数: 2145\n",
      "  - 平均JS距离: 0.4072\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.4590\n",
      "==================================================\n",
      "\n",
      "计算Layer 3的JS距离和加权平均熵 (163 个节点)\n",
      "  - 节点数: 163\n",
      "  - 计算的节点对数: 13203\n",
      "  - 理论最大节点对数: 13203\n",
      "  - 平均JS距离: 0.4814\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.3679\n",
      "==================================================\n",
      "\n",
      "保存详细JS距离结果到: /Volumes/My Passport/收敛结果/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_3/jensen_shannon_distances.csv\n",
      "保存每层平均JS距离和加权熵结果到: /Volumes/My Passport/收敛结果/2/d4_g001_v2_收敛/depth_4_gamma_0.01_run_3/layer_average_js_distances.csv\n",
      "\n",
      "总体统计:\n",
      "  - 总层数: 4\n",
      "  - 各层统计:\n",
      "    Layer 0: JS距离=0.0000, 加权熵=6.9879 (基于1个节点, 970个文档)\n",
      "    Layer 1: JS距离=0.4127, 加权熵=7.4218 (基于17个节点, 970个文档)\n",
      "    Layer 2: JS距离=0.4072, 加权熵=7.4590 (基于66个节点, 970个文档)\n",
      "    Layer 3: JS距离=0.4814, 加权熵=7.3679 (基于163个节点, 970个文档)\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g01_收敛/depth_4_gamma_0.1_run_1/iteration_node_word_distributions.csv\n",
      "全量词汇表大小: 1490\n",
      "层级分布: [(0, 1), (1, 12), (2, 66), (3, 171)]\n",
      "\n",
      "计算Layer 0的JS距离和加权平均熵 (1 个节点)\n",
      "  - 节点数: 1\n",
      "  - 计算的节点对数: 0\n",
      "  - 理论最大节点对数: 0\n",
      "  - 平均JS距离: 0.0000\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.0877\n",
      "==================================================\n",
      "\n",
      "计算Layer 1的JS距离和加权平均熵 (12 个节点)\n",
      "  - 节点数: 12\n",
      "  - 计算的节点对数: 66\n",
      "  - 理论最大节点对数: 66\n",
      "  - 平均JS距离: 0.4730\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.1762\n",
      "==================================================\n",
      "\n",
      "计算Layer 2的JS距离和加权平均熵 (66 个节点)\n",
      "  - 节点数: 66\n",
      "  - 计算的节点对数: 2145\n",
      "  - 理论最大节点对数: 2145\n",
      "  - 平均JS距离: 0.4212\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.5624\n",
      "==================================================\n",
      "\n",
      "计算Layer 3的JS距离和加权平均熵 (171 个节点)\n",
      "  - 节点数: 171\n",
      "  - 计算的节点对数: 14535\n",
      "  - 理论最大节点对数: 14535\n",
      "  - 平均JS距离: 0.4584\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.5215\n",
      "==================================================\n",
      "\n",
      "保存详细JS距离结果到: /Volumes/My Passport/收敛结果/2/d4_g01_收敛/depth_4_gamma_0.1_run_1/jensen_shannon_distances.csv\n",
      "保存每层平均JS距离和加权熵结果到: /Volumes/My Passport/收敛结果/2/d4_g01_收敛/depth_4_gamma_0.1_run_1/layer_average_js_distances.csv\n",
      "\n",
      "总体统计:\n",
      "  - 总层数: 4\n",
      "  - 各层统计:\n",
      "    Layer 0: JS距离=0.0000, 加权熵=7.0877 (基于1个节点, 970个文档)\n",
      "    Layer 1: JS距离=0.4730, 加权熵=7.1762 (基于12个节点, 970个文档)\n",
      "    Layer 2: JS距离=0.4212, 加权熵=7.5624 (基于66个节点, 970个文档)\n",
      "    Layer 3: JS距离=0.4584, 加权熵=7.5215 (基于171个节点, 970个文档)\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g01_收敛/depth_4_gamma_0.1_run_2/iteration_node_word_distributions.csv\n",
      "全量词汇表大小: 1490\n",
      "层级分布: [(0, 1), (1, 23), (2, 76), (3, 206)]\n",
      "\n",
      "计算Layer 0的JS距离和加权平均熵 (1 个节点)\n",
      "  - 节点数: 1\n",
      "  - 计算的节点对数: 0\n",
      "  - 理论最大节点对数: 0\n",
      "  - 平均JS距离: 0.0000\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 6.9358\n",
      "==================================================\n",
      "\n",
      "计算Layer 1的JS距离和加权平均熵 (23 个节点)\n",
      "  - 节点数: 23\n",
      "  - 计算的节点对数: 253\n",
      "  - 理论最大节点对数: 253\n",
      "  - 平均JS距离: 0.4177\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.0548\n",
      "==================================================\n",
      "\n",
      "计算Layer 2的JS距离和加权平均熵 (76 个节点)\n",
      "  - 节点数: 76\n",
      "  - 计算的节点对数: 2850\n",
      "  - 理论最大节点对数: 2850\n",
      "  - 平均JS距离: 0.3820\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.4576\n",
      "==================================================\n",
      "\n",
      "计算Layer 3的JS距离和加权平均熵 (206 个节点)\n",
      "  - 节点数: 206\n",
      "  - 计算的节点对数: 21115\n",
      "  - 理论最大节点对数: 21115\n",
      "  - 平均JS距离: 0.4675\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.3184\n",
      "==================================================\n",
      "\n",
      "保存详细JS距离结果到: /Volumes/My Passport/收敛结果/2/d4_g01_收敛/depth_4_gamma_0.1_run_2/jensen_shannon_distances.csv\n",
      "保存每层平均JS距离和加权熵结果到: /Volumes/My Passport/收敛结果/2/d4_g01_收敛/depth_4_gamma_0.1_run_2/layer_average_js_distances.csv\n",
      "\n",
      "总体统计:\n",
      "  - 总层数: 4\n",
      "  - 各层统计:\n",
      "    Layer 0: JS距离=0.0000, 加权熵=6.9358 (基于1个节点, 970个文档)\n",
      "    Layer 1: JS距离=0.4177, 加权熵=7.0548 (基于23个节点, 970个文档)\n",
      "    Layer 2: JS距离=0.3820, 加权熵=7.4576 (基于76个节点, 970个文档)\n",
      "    Layer 3: JS距离=0.4675, 加权熵=7.3184 (基于206个节点, 970个文档)\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g01_收敛/depth_4_gamma_0.1_run_3/iteration_node_word_distributions.csv\n",
      "全量词汇表大小: 1490\n",
      "层级分布: [(0, 1), (1, 15), (2, 69), (3, 188)]\n",
      "\n",
      "计算Layer 0的JS距离和加权平均熵 (1 个节点)\n",
      "  - 节点数: 1\n",
      "  - 计算的节点对数: 0\n",
      "  - 理论最大节点对数: 0\n",
      "  - 平均JS距离: 0.0000\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 6.9906\n",
      "==================================================\n",
      "\n",
      "计算Layer 1的JS距离和加权平均熵 (15 个节点)\n",
      "  - 节点数: 15\n",
      "  - 计算的节点对数: 105\n",
      "  - 理论最大节点对数: 105\n",
      "  - 平均JS距离: 0.4487\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.0271\n",
      "==================================================\n",
      "\n",
      "计算Layer 2的JS距离和加权平均熵 (69 个节点)\n",
      "  - 节点数: 69\n",
      "  - 计算的节点对数: 2346\n",
      "  - 理论最大节点对数: 2346\n",
      "  - 平均JS距离: 0.4027\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.5340\n",
      "==================================================\n",
      "\n",
      "计算Layer 3的JS距离和加权平均熵 (188 个节点)\n",
      "  - 节点数: 188\n",
      "  - 计算的节点对数: 17578\n",
      "  - 理论最大节点对数: 17578\n",
      "  - 平均JS距离: 0.4635\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.3310\n",
      "==================================================\n",
      "\n",
      "保存详细JS距离结果到: /Volumes/My Passport/收敛结果/2/d4_g01_收敛/depth_4_gamma_0.1_run_3/jensen_shannon_distances.csv\n",
      "保存每层平均JS距离和加权熵结果到: /Volumes/My Passport/收敛结果/2/d4_g01_收敛/depth_4_gamma_0.1_run_3/layer_average_js_distances.csv\n",
      "\n",
      "总体统计:\n",
      "  - 总层数: 4\n",
      "  - 各层统计:\n",
      "    Layer 0: JS距离=0.0000, 加权熵=6.9906 (基于1个节点, 970个文档)\n",
      "    Layer 1: JS距离=0.4487, 加权熵=7.0271 (基于15个节点, 970个文档)\n",
      "    Layer 2: JS距离=0.4027, 加权熵=7.5340 (基于69个节点, 970个文档)\n",
      "    Layer 3: JS距离=0.4635, 加权熵=7.3310 (基于188个节点, 970个文档)\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g0001_收敛/depth_4_gamma_0.001_run_1/iteration_node_word_distributions.csv\n",
      "全量词汇表大小: 1490\n",
      "层级分布: [(0, 1), (1, 17), (2, 72), (3, 168)]\n",
      "\n",
      "计算Layer 0的JS距离和加权平均熵 (1 个节点)\n",
      "  - 节点数: 1\n",
      "  - 计算的节点对数: 0\n",
      "  - 理论最大节点对数: 0\n",
      "  - 平均JS距离: 0.0000\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.0352\n",
      "==================================================\n",
      "\n",
      "计算Layer 1的JS距离和加权平均熵 (17 个节点)\n",
      "  - 节点数: 17\n",
      "  - 计算的节点对数: 136\n",
      "  - 理论最大节点对数: 136\n",
      "  - 平均JS距离: 0.4620\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 6.8899\n",
      "==================================================\n",
      "\n",
      "计算Layer 2的JS距离和加权平均熵 (72 个节点)\n",
      "  - 节点数: 72\n",
      "  - 计算的节点对数: 2556\n",
      "  - 理论最大节点对数: 2556\n",
      "  - 平均JS距离: 0.4188\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.5078\n",
      "==================================================\n",
      "\n",
      "计算Layer 3的JS距离和加权平均熵 (168 个节点)\n",
      "  - 节点数: 168\n",
      "  - 计算的节点对数: 14028\n",
      "  - 理论最大节点对数: 14028\n",
      "  - 平均JS距离: 0.4723\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.2848\n",
      "==================================================\n",
      "\n",
      "保存详细JS距离结果到: /Volumes/My Passport/收敛结果/2/d4_g0001_收敛/depth_4_gamma_0.001_run_1/jensen_shannon_distances.csv\n",
      "保存每层平均JS距离和加权熵结果到: /Volumes/My Passport/收敛结果/2/d4_g0001_收敛/depth_4_gamma_0.001_run_1/layer_average_js_distances.csv\n",
      "\n",
      "总体统计:\n",
      "  - 总层数: 4\n",
      "  - 各层统计:\n",
      "    Layer 0: JS距离=0.0000, 加权熵=7.0352 (基于1个节点, 970个文档)\n",
      "    Layer 1: JS距离=0.4620, 加权熵=6.8899 (基于17个节点, 970个文档)\n",
      "    Layer 2: JS距离=0.4188, 加权熵=7.5078 (基于72个节点, 970个文档)\n",
      "    Layer 3: JS距离=0.4723, 加权熵=7.2848 (基于168个节点, 970个文档)\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g0001_收敛/depth_4_gamma_0.001_run_2/iteration_node_word_distributions.csv\n",
      "全量词汇表大小: 1490\n",
      "层级分布: [(0, 1), (1, 14), (2, 63), (3, 162)]\n",
      "\n",
      "计算Layer 0的JS距离和加权平均熵 (1 个节点)\n",
      "  - 节点数: 1\n",
      "  - 计算的节点对数: 0\n",
      "  - 理论最大节点对数: 0\n",
      "  - 平均JS距离: 0.0000\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.0390\n",
      "==================================================\n",
      "\n",
      "计算Layer 1的JS距离和加权平均熵 (14 个节点)\n",
      "  - 节点数: 14\n",
      "  - 计算的节点对数: 91\n",
      "  - 理论最大节点对数: 91\n",
      "  - 平均JS距离: 0.4081\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.0456\n",
      "==================================================\n",
      "\n",
      "计算Layer 2的JS距离和加权平均熵 (63 个节点)\n",
      "  - 节点数: 63\n",
      "  - 计算的节点对数: 1953\n",
      "  - 理论最大节点对数: 1953\n",
      "  - 平均JS距离: 0.4178\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.4971\n",
      "==================================================\n",
      "\n",
      "计算Layer 3的JS距离和加权平均熵 (162 个节点)\n",
      "  - 节点数: 162\n",
      "  - 计算的节点对数: 13041\n",
      "  - 理论最大节点对数: 13041\n",
      "  - 平均JS距离: 0.4827\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.3566\n",
      "==================================================\n",
      "\n",
      "保存详细JS距离结果到: /Volumes/My Passport/收敛结果/2/d4_g0001_收敛/depth_4_gamma_0.001_run_2/jensen_shannon_distances.csv\n",
      "保存每层平均JS距离和加权熵结果到: /Volumes/My Passport/收敛结果/2/d4_g0001_收敛/depth_4_gamma_0.001_run_2/layer_average_js_distances.csv\n",
      "\n",
      "总体统计:\n",
      "  - 总层数: 4\n",
      "  - 各层统计:\n",
      "    Layer 0: JS距离=0.0000, 加权熵=7.0390 (基于1个节点, 970个文档)\n",
      "    Layer 1: JS距离=0.4081, 加权熵=7.0456 (基于14个节点, 970个文档)\n",
      "    Layer 2: JS距离=0.4178, 加权熵=7.4971 (基于63个节点, 970个文档)\n",
      "    Layer 3: JS距离=0.4827, 加权熵=7.3566 (基于162个节点, 970个文档)\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g0001_收敛/depth_4_gamma_0.001_run_3/iteration_node_word_distributions.csv\n",
      "全量词汇表大小: 1490\n",
      "层级分布: [(0, 1), (1, 20), (2, 69), (3, 143)]\n",
      "\n",
      "计算Layer 0的JS距离和加权平均熵 (1 个节点)\n",
      "  - 节点数: 1\n",
      "  - 计算的节点对数: 0\n",
      "  - 理论最大节点对数: 0\n",
      "  - 平均JS距离: 0.0000\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 6.9883\n",
      "==================================================\n",
      "\n",
      "计算Layer 1的JS距离和加权平均熵 (20 个节点)\n",
      "  - 节点数: 20\n",
      "  - 计算的节点对数: 190\n",
      "  - 理论最大节点对数: 190\n",
      "  - 平均JS距离: 0.4392\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.4793\n",
      "==================================================\n",
      "\n",
      "计算Layer 2的JS距离和加权平均熵 (69 个节点)\n",
      "  - 节点数: 69\n",
      "  - 计算的节点对数: 2346\n",
      "  - 理论最大节点对数: 2346\n",
      "  - 平均JS距离: 0.4023\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.7637\n",
      "==================================================\n",
      "\n",
      "计算Layer 3的JS距离和加权平均熵 (143 个节点)\n",
      "  - 节点数: 143\n",
      "  - 计算的节点对数: 10153\n",
      "  - 理论最大节点对数: 10153\n",
      "  - 平均JS距离: 0.4616\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.5887\n",
      "==================================================\n",
      "\n",
      "保存详细JS距离结果到: /Volumes/My Passport/收敛结果/2/d4_g0001_收敛/depth_4_gamma_0.001_run_3/jensen_shannon_distances.csv\n",
      "保存每层平均JS距离和加权熵结果到: /Volumes/My Passport/收敛结果/2/d4_g0001_收敛/depth_4_gamma_0.001_run_3/layer_average_js_distances.csv\n",
      "\n",
      "总体统计:\n",
      "  - 总层数: 4\n",
      "  - 各层统计:\n",
      "    Layer 0: JS距离=0.0000, 加权熵=6.9883 (基于1个节点, 970个文档)\n",
      "    Layer 1: JS距离=0.4392, 加权熵=7.4793 (基于20个节点, 970个文档)\n",
      "    Layer 2: JS距离=0.4023, 加权熵=7.7637 (基于69个节点, 970个文档)\n",
      "    Layer 3: JS距离=0.4616, 加权熵=7.5887 (基于143个节点, 970个文档)\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g005_收敛/depth_4_gamma_0.05_run_1/iteration_node_word_distributions.csv\n",
      "全量词汇表大小: 1490\n",
      "层级分布: [(0, 1), (1, 19), (2, 75), (3, 187)]\n",
      "\n",
      "计算Layer 0的JS距离和加权平均熵 (1 个节点)\n",
      "  - 节点数: 1\n",
      "  - 计算的节点对数: 0\n",
      "  - 理论最大节点对数: 0\n",
      "  - 平均JS距离: 0.0000\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 6.8588\n",
      "==================================================\n",
      "\n",
      "计算Layer 1的JS距离和加权平均熵 (19 个节点)\n",
      "  - 节点数: 19\n",
      "  - 计算的节点对数: 171\n",
      "  - 理论最大节点对数: 171\n",
      "  - 平均JS距离: 0.4770\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 6.9780\n",
      "==================================================\n",
      "\n",
      "计算Layer 2的JS距离和加权平均熵 (75 个节点)\n",
      "  - 节点数: 75\n",
      "  - 计算的节点对数: 2775\n",
      "  - 理论最大节点对数: 2775\n",
      "  - 平均JS距离: 0.3457\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.6785\n",
      "==================================================\n",
      "\n",
      "计算Layer 3的JS距离和加权平均熵 (187 个节点)\n",
      "  - 节点数: 187\n",
      "  - 计算的节点对数: 17391\n",
      "  - 理论最大节点对数: 17391\n",
      "  - 平均JS距离: 0.4788\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.2640\n",
      "==================================================\n",
      "\n",
      "保存详细JS距离结果到: /Volumes/My Passport/收敛结果/2/d4_g005_收敛/depth_4_gamma_0.05_run_1/jensen_shannon_distances.csv\n",
      "保存每层平均JS距离和加权熵结果到: /Volumes/My Passport/收敛结果/2/d4_g005_收敛/depth_4_gamma_0.05_run_1/layer_average_js_distances.csv\n",
      "\n",
      "总体统计:\n",
      "  - 总层数: 4\n",
      "  - 各层统计:\n",
      "    Layer 0: JS距离=0.0000, 加权熵=6.8588 (基于1个节点, 970个文档)\n",
      "    Layer 1: JS距离=0.4770, 加权熵=6.9780 (基于19个节点, 970个文档)\n",
      "    Layer 2: JS距离=0.3457, 加权熵=7.6785 (基于75个节点, 970个文档)\n",
      "    Layer 3: JS距离=0.4788, 加权熵=7.2640 (基于187个节点, 970个文档)\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g005_收敛/depth_4_gamma_0.05_run_2/iteration_node_word_distributions.csv\n",
      "全量词汇表大小: 1490\n",
      "层级分布: [(0, 1), (1, 16), (2, 69), (3, 181)]\n",
      "\n",
      "计算Layer 0的JS距离和加权平均熵 (1 个节点)\n",
      "  - 节点数: 1\n",
      "  - 计算的节点对数: 0\n",
      "  - 理论最大节点对数: 0\n",
      "  - 平均JS距离: 0.0000\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 6.9557\n",
      "==================================================\n",
      "\n",
      "计算Layer 1的JS距离和加权平均熵 (16 个节点)\n",
      "  - 节点数: 16\n",
      "  - 计算的节点对数: 120\n",
      "  - 理论最大节点对数: 120\n",
      "  - 平均JS距离: 0.4728\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.0416\n",
      "==================================================\n",
      "\n",
      "计算Layer 2的JS距离和加权平均熵 (69 个节点)\n",
      "  - 节点数: 69\n",
      "  - 计算的节点对数: 2346\n",
      "  - 理论最大节点对数: 2346\n",
      "  - 平均JS距离: 0.4003\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.3958\n",
      "==================================================\n",
      "\n",
      "计算Layer 3的JS距离和加权平均熵 (181 个节点)\n",
      "  - 节点数: 181\n",
      "  - 计算的节点对数: 16290\n",
      "  - 理论最大节点对数: 16290\n",
      "  - 平均JS距离: 0.4935\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.2475\n",
      "==================================================\n",
      "\n",
      "保存详细JS距离结果到: /Volumes/My Passport/收敛结果/2/d4_g005_收敛/depth_4_gamma_0.05_run_2/jensen_shannon_distances.csv\n",
      "保存每层平均JS距离和加权熵结果到: /Volumes/My Passport/收敛结果/2/d4_g005_收敛/depth_4_gamma_0.05_run_2/layer_average_js_distances.csv\n",
      "\n",
      "总体统计:\n",
      "  - 总层数: 4\n",
      "  - 各层统计:\n",
      "    Layer 0: JS距离=0.0000, 加权熵=6.9557 (基于1个节点, 970个文档)\n",
      "    Layer 1: JS距离=0.4728, 加权熵=7.0416 (基于16个节点, 970个文档)\n",
      "    Layer 2: JS距离=0.4003, 加权熵=7.3958 (基于69个节点, 970个文档)\n",
      "    Layer 3: JS距离=0.4935, 加权熵=7.2475 (基于181个节点, 970个文档)\n",
      "\n",
      "处理文件: /Volumes/My Passport/收敛结果/2/d4_g005_收敛/depth_4_gamma_0.05_run_3/iteration_node_word_distributions.csv\n",
      "全量词汇表大小: 1490\n",
      "层级分布: [(0, 1), (1, 14), (2, 61), (3, 169)]\n",
      "\n",
      "计算Layer 0的JS距离和加权平均熵 (1 个节点)\n",
      "  - 节点数: 1\n",
      "  - 计算的节点对数: 0\n",
      "  - 理论最大节点对数: 0\n",
      "  - 平均JS距离: 0.0000\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 6.9417\n",
      "==================================================\n",
      "\n",
      "计算Layer 1的JS距离和加权平均熵 (14 个节点)\n",
      "  - 节点数: 14\n",
      "  - 计算的节点对数: 91\n",
      "  - 理论最大节点对数: 91\n",
      "  - 平均JS距离: 0.4509\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.0599\n",
      "==================================================\n",
      "\n",
      "计算Layer 2的JS距离和加权平均熵 (61 个节点)\n",
      "  - 节点数: 61\n",
      "  - 计算的节点对数: 1830\n",
      "  - 理论最大节点对数: 1830\n",
      "  - 平均JS距离: 0.4013\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.6373\n",
      "==================================================\n",
      "\n",
      "计算Layer 3的JS距离和加权平均熵 (169 个节点)\n",
      "  - 节点数: 169\n",
      "  - 计算的节点对数: 14196\n",
      "  - 理论最大节点对数: 14196\n",
      "  - 平均JS距离: 0.4866\n",
      "  - 总文档数: 970\n",
      "  - 文档数加权平均Renyi熵: 7.2758\n",
      "==================================================\n",
      "\n",
      "保存详细JS距离结果到: /Volumes/My Passport/收敛结果/2/d4_g005_收敛/depth_4_gamma_0.05_run_3/jensen_shannon_distances.csv\n",
      "保存每层平均JS距离和加权熵结果到: /Volumes/My Passport/收敛结果/2/d4_g005_收敛/depth_4_gamma_0.05_run_3/layer_average_js_distances.csv\n",
      "\n",
      "总体统计:\n",
      "  - 总层数: 4\n",
      "  - 各层统计:\n",
      "    Layer 0: JS距离=0.0000, 加权熵=6.9417 (基于1个节点, 970个文档)\n",
      "    Layer 1: JS距离=0.4509, 加权熵=7.0599 (基于14个节点, 970个文档)\n",
      "    Layer 2: JS距离=0.4013, 加权熵=7.6373 (基于61个节点, 970个文档)\n",
      "    Layer 3: JS距离=0.4866, 加权熵=7.2758 (基于169个节点, 970个文档)\n",
      "==================================================\n",
      "Jensen-Shannon距离和加权平均Renyi熵计算完成！\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd \n",
    "# 主函数：计算Jensen-Shannon距离和加权平均Renyi熵\n",
    "base_path = \"/Volumes/My Passport/收敛结果/2\"  # 根目录\n",
    "eta = 0.1  # Dirichlet平滑参数\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"开始计算Jensen-Shannon距离和加权平均Renyi熵...\")\n",
    "print(\"=\" * 50)\n",
    "calculate_jensen_shannon_distances_with_weighted_entropy(base_path, eta)\n",
    "print(\"=\" * 50)\n",
    "print(\"Jensen-Shannon距离和加权平均Renyi熵计算完成！\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18a7e2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_layer_statistics_by_gamma(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    按gamma值汇总各层的JS距离和加权熵统计，在与run文件夹同级位置生成汇总表\n",
    "    \"\"\"\n",
    "    # 查找所有layer_average_js_distances.csv文件\n",
    "    pattern = os.path.join(base_path, \"**\", \"layer_average_js_distances.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    # 存储所有数据和分组信息\n",
    "    all_data = []\n",
    "    gamma_experiment_groups = {}  # 用于存储每个gamma_experiment组合的父目录\n",
    "    \n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        parent_folder = os.path.dirname(folder_path)  # run文件夹的父目录\n",
    "        \n",
    "        # 从文件夹名称提取gamma值\n",
    "        if 'gamma_0.001' in folder_name:\n",
    "            if '2条链' in parent_folder:\n",
    "                gamma = 0.001\n",
    "                experiment_type = '2chains'\n",
    "            else:\n",
    "                gamma = 0.001\n",
    "                experiment_type = 'single'\n",
    "        elif 'gamma_0.005' in folder_name:\n",
    "            gamma = 0.005\n",
    "            experiment_type = 'single'\n",
    "        elif 'gamma_0.01' in folder_name:\n",
    "            gamma = 0.01\n",
    "            experiment_type = 'single'\n",
    "        elif 'gamma_0.05' in folder_name:\n",
    "            gamma = 0.05\n",
    "            experiment_type = 'single'\n",
    "        elif 'gamma_0.1' in folder_name:\n",
    "            gamma = 0.1\n",
    "            experiment_type = 'single'\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # 提取run编号\n",
    "        run_match = folder_name.split('_run_')\n",
    "        if len(run_match) > 1:\n",
    "            run_id = run_match[1]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # 记录gamma_experiment组合的父目录\n",
    "        group_key = f\"{gamma}_{experiment_type}\"\n",
    "        if group_key not in gamma_experiment_groups:\n",
    "            gamma_experiment_groups[group_key] = parent_folder\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                all_data.append({\n",
    "                    'gamma': gamma,\n",
    "                    'experiment_type': experiment_type,\n",
    "                    'run_id': run_id,\n",
    "                    'layer': row['layer'],\n",
    "                    'node_count': row['node_count'],\n",
    "                    'avg_js_distance': row['avg_js_distance'],\n",
    "                    'weighted_avg_renyi_entropy': row['weighted_avg_renyi_entropy'],\n",
    "                    'total_documents': row['total_documents'],\n",
    "                    'parent_folder': parent_folder\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"读取文件 {file_path} 时出错: {e}\")\n",
    "    \n",
    "    # 转换为DataFrame\n",
    "    summary_df = pd.DataFrame(all_data)\n",
    "    \n",
    "    if summary_df.empty:\n",
    "        print(\"未找到有效数据\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"各GAMMA值的层级汇总统计\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 按gamma、experiment_type和parent_folder分组，生成汇总文件\n",
    "    for (gamma, experiment_type), group_data in summary_df.groupby(['gamma', 'experiment_type']):\n",
    "        parent_folder = group_data['parent_folder'].iloc[0]\n",
    "        \n",
    "        print(f\"\\n处理 Gamma={gamma:.3f}, 实验类型={'2条链' if experiment_type == '2chains' else '单链'}\")\n",
    "        print(f\"输出目录: {parent_folder}\")\n",
    "        \n",
    "        # 计算各层的汇总统计\n",
    "        layer_summary = group_data.groupby('layer').agg({\n",
    "            'avg_js_distance': ['mean', 'std', 'count'],\n",
    "            'weighted_avg_renyi_entropy': ['mean', 'std', 'count'],\n",
    "            'node_count': ['mean', 'std'],\n",
    "            'total_documents': 'mean',\n",
    "            'run_id': lambda x: ', '.join(sorted(x.unique()))\n",
    "        }).round(4)\n",
    "        \n",
    "        # 平铺列名\n",
    "        layer_summary.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in layer_summary.columns]\n",
    "        layer_summary = layer_summary.reset_index()\n",
    "        \n",
    "        # 重命名列，使其更清晰\n",
    "        column_mapping = {\n",
    "            'avg_js_distance_mean': 'avg_js_distance_mean',\n",
    "            'avg_js_distance_std': 'avg_js_distance_std', \n",
    "            'avg_js_distance_count': 'run_count',\n",
    "            'weighted_avg_renyi_entropy_mean': 'weighted_avg_renyi_entropy_mean',\n",
    "            'weighted_avg_renyi_entropy_std': 'weighted_avg_renyi_entropy_std',\n",
    "            'weighted_avg_renyi_entropy_count': 'entropy_run_count',\n",
    "            'node_count_mean': 'avg_node_count',\n",
    "            'node_count_std': 'node_count_std',\n",
    "            'total_documents_mean': 'avg_total_documents',\n",
    "            'run_id_<lambda>': 'included_runs'\n",
    "        }\n",
    "        \n",
    "        for old_name, new_name in column_mapping.items():\n",
    "            if old_name in layer_summary.columns:\n",
    "                layer_summary = layer_summary.rename(columns={old_name: new_name})\n",
    "        \n",
    "        # 添加gamma和experiment_type信息\n",
    "        layer_summary.insert(0, 'gamma', gamma)\n",
    "        layer_summary.insert(1, 'experiment_type', experiment_type)\n",
    "        \n",
    "        # 保存汇总结果到与run文件夹同级的位置\n",
    "        if experiment_type == '2chains':\n",
    "            output_filename = f'gamma_{gamma:.3f}_2chains_layer_summary.csv'\n",
    "        else:\n",
    "            output_filename = f'gamma_{gamma:.3f}_single_layer_summary.csv'\n",
    "        \n",
    "        output_path = os.path.join(parent_folder, output_filename)\n",
    "        layer_summary.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"  保存汇总文件: {output_path}\")\n",
    "        print(f\"  包含运行: {layer_summary['included_runs'].iloc[0] if 'included_runs' in layer_summary.columns else 'N/A'}\")\n",
    "        print(f\"  层数: {len(layer_summary)}\")\n",
    "        \n",
    "        # 显示简要统计\n",
    "        for _, row in layer_summary.iterrows():\n",
    "            layer_num = int(row['layer'])\n",
    "            js_mean = row['avg_js_distance_mean']\n",
    "            js_std = row['avg_js_distance_std'] if 'avg_js_distance_std' in row else 0\n",
    "            entropy_mean = row['weighted_avg_renyi_entropy_mean']\n",
    "            entropy_std = row['weighted_avg_renyi_entropy_std'] if 'weighted_avg_renyi_entropy_std' in row else 0\n",
    "            node_count = row['avg_node_count']\n",
    "            run_count = int(row['run_count']) if 'run_count' in row else 0\n",
    "            \n",
    "            print(f\"    Layer {layer_num}: JS={js_mean:.4f}(±{js_std:.4f}), 熵={entropy_mean:.4f}(±{entropy_std:.4f}), 节点={node_count:.1f}, runs={run_count}\")\n",
    "    \n",
    "    # 生成总体对比文件（保存在base_path下）\n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"生成总体对比文件\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 只分析单链实验的跨gamma对比\n",
    "    single_chain_data = summary_df[summary_df['experiment_type'] == 'single']\n",
    "    \n",
    "    if not single_chain_data.empty:\n",
    "        overall_summary = single_chain_data.groupby(['gamma', 'layer']).agg({\n",
    "            'avg_js_distance': ['mean', 'std'],\n",
    "            'weighted_avg_renyi_entropy': ['mean', 'std'],\n",
    "            'node_count': ['mean', 'std'],\n",
    "            'run_id': 'count'\n",
    "        }).round(4)\n",
    "        \n",
    "        # 平铺列名\n",
    "        overall_summary.columns = ['_'.join(col).strip() for col in overall_summary.columns]\n",
    "        overall_summary = overall_summary.reset_index()\n",
    "        \n",
    "        overall_output_path = os.path.join(base_path, 'gamma_layer_comparison.csv')\n",
    "        overall_summary.to_csv(overall_output_path, index=False)\n",
    "        print(f\"总体对比文件保存到: {overall_output_path}\")\n",
    "        \n",
    "        # 显示跨gamma对比\n",
    "        for layer in sorted(single_chain_data['layer'].unique()):\n",
    "            print(f\"\\nLayer {int(layer)} 跨Gamma对比:\")\n",
    "            print(\"Gamma值    JS距离(±std)      加权熵(±std)      节点数(±std)   运行数\")\n",
    "            print(\"-\" * 75)\n",
    "            \n",
    "            layer_data = overall_summary[overall_summary['layer'] == layer]\n",
    "            for _, row in layer_data.iterrows():\n",
    "                gamma = row['gamma']\n",
    "                js_mean = row['avg_js_distance_mean']\n",
    "                js_std = row['avg_js_distance_std']\n",
    "                entropy_mean = row['weighted_avg_renyi_entropy_mean']\n",
    "                entropy_std = row['weighted_avg_renyi_entropy_std']\n",
    "                node_mean = row['node_count_mean']\n",
    "                node_std = row['node_count_std']\n",
    "                run_count = int(row['run_id_count'])\n",
    "                \n",
    "                print(f\"{gamma:6.3f}    {js_mean:6.4f}(±{js_std:5.4f})   {entropy_mean:6.4f}(±{entropy_std:5.4f})   {node_mean:6.1f}(±{node_std:4.1f})   {run_count:4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a5a49f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "开始汇总各Gamma值的层级统计...\n",
      "======================================================================\n",
      "======================================================================\n",
      "各GAMMA值的层级汇总统计\n",
      "======================================================================\n",
      "\n",
      "处理 Gamma=0.001, 实验类型=单链\n",
      "输出目录: /Volumes/My Passport/收敛结果/2/d4_g0001_收敛\n",
      "  保存汇总文件: /Volumes/My Passport/收敛结果/2/d4_g0001_收敛/gamma_0.001_single_layer_summary.csv\n",
      "  包含运行: 1, 2, 3\n",
      "  层数: 4\n",
      "    Layer 0: JS=0.0000(±0.0000), 熵=7.0208(±0.0282), 节点=1.0, runs=3\n",
      "    Layer 1: JS=0.4364(±0.0271), 熵=7.1383(±0.3054), 节点=17.0, runs=3\n",
      "    Layer 2: JS=0.4130(±0.0092), 熵=7.5895(±0.1510), 节点=68.0, runs=3\n",
      "    Layer 3: JS=0.4722(±0.0105), 熵=7.4100(±0.1589), 节点=157.7, runs=3\n",
      "\n",
      "处理 Gamma=0.005, 实验类型=单链\n",
      "输出目录: /Volumes/My Passport/收敛结果/2/d4_g0005_收敛\n",
      "  保存汇总文件: /Volumes/My Passport/收敛结果/2/d4_g0005_收敛/gamma_0.005_single_layer_summary.csv\n",
      "  包含运行: 1, 2, 3\n",
      "  层数: 4\n",
      "    Layer 0: JS=0.0000(±0.0000), 熵=7.1037(±0.0415), 节点=1.0, runs=3\n",
      "    Layer 1: JS=0.4444(±0.0457), 熵=7.1893(±0.1358), 节点=16.7, runs=3\n",
      "    Layer 2: JS=0.4044(±0.0198), 熵=7.5989(±0.1633), 节点=65.0, runs=3\n",
      "    Layer 3: JS=0.4646(±0.0087), 熵=7.4875(±0.1168), 节点=153.7, runs=3\n",
      "\n",
      "处理 Gamma=0.010, 实验类型=单链\n",
      "输出目录: /Volumes/My Passport/收敛结果/2/d4_g001_v2_收敛\n",
      "  保存汇总文件: /Volumes/My Passport/收敛结果/2/d4_g001_v2_收敛/gamma_0.010_single_layer_summary.csv\n",
      "  包含运行: 1, 2, 3\n",
      "  层数: 4\n",
      "    Layer 0: JS=0.0000(±0.0000), 熵=6.9353(±0.0479), 节点=1.0, runs=3\n",
      "    Layer 1: JS=0.4323(±0.0188), 熵=7.2160(±0.1830), 节点=16.7, runs=3\n",
      "    Layer 2: JS=0.4119(±0.0041), 熵=7.3548(±0.1584), 节点=71.3, runs=3\n",
      "    Layer 3: JS=0.4750(±0.0056), 熵=7.3947(±0.0233), 节点=172.7, runs=3\n",
      "\n",
      "处理 Gamma=0.050, 实验类型=单链\n",
      "输出目录: /Volumes/My Passport/收敛结果/2/d4_g005_收敛\n",
      "  保存汇总文件: /Volumes/My Passport/收敛结果/2/d4_g005_收敛/gamma_0.050_single_layer_summary.csv\n",
      "  包含运行: 1, 2, 3\n",
      "  层数: 4\n",
      "    Layer 0: JS=0.0000(±0.0000), 熵=6.9188(±0.0524), 节点=1.0, runs=3\n",
      "    Layer 1: JS=0.4669(±0.0140), 熵=7.0265(±0.0430), 节点=16.3, runs=3\n",
      "    Layer 2: JS=0.3824(±0.0318), 熵=7.5705(±0.1527), 节点=68.3, runs=3\n",
      "    Layer 3: JS=0.4863(±0.0073), 熵=7.2624(±0.0142), 节点=179.0, runs=3\n",
      "\n",
      "处理 Gamma=0.100, 实验类型=单链\n",
      "输出目录: /Volumes/My Passport/收敛结果/2/d4_g01_收敛\n",
      "  保存汇总文件: /Volumes/My Passport/收敛结果/2/d4_g01_收敛/gamma_0.100_single_layer_summary.csv\n",
      "  包含运行: 1, 2, 3\n",
      "  层数: 4\n",
      "    Layer 0: JS=0.0000(±0.0000), 熵=7.0047(±0.0770), 节点=1.0, runs=3\n",
      "    Layer 1: JS=0.4465(±0.0277), 熵=7.0861(±0.0793), 节点=16.7, runs=3\n",
      "    Layer 2: JS=0.4020(±0.0196), 熵=7.5180(±0.0542), 节点=70.3, runs=3\n",
      "    Layer 3: JS=0.4631(±0.0046), 熵=7.3903(±0.1138), 节点=188.3, runs=3\n",
      "\n",
      "======================================================================\n",
      "生成总体对比文件\n",
      "======================================================================\n",
      "总体对比文件保存到: /Volumes/My Passport/收敛结果/2/gamma_layer_comparison.csv\n",
      "\n",
      "Layer 0 跨Gamma对比:\n",
      "Gamma值    JS距离(±std)      加权熵(±std)      节点数(±std)   运行数\n",
      "---------------------------------------------------------------------------\n",
      " 0.001    0.0000(±0.0000)   7.0208(±0.0282)      1.0(± 0.0)      3\n",
      " 0.005    0.0000(±0.0000)   7.1037(±0.0415)      1.0(± 0.0)      3\n",
      " 0.010    0.0000(±0.0000)   6.9353(±0.0479)      1.0(± 0.0)      3\n",
      " 0.050    0.0000(±0.0000)   6.9188(±0.0524)      1.0(± 0.0)      3\n",
      " 0.100    0.0000(±0.0000)   7.0047(±0.0770)      1.0(± 0.0)      3\n",
      "\n",
      "Layer 1 跨Gamma对比:\n",
      "Gamma值    JS距离(±std)      加权熵(±std)      节点数(±std)   运行数\n",
      "---------------------------------------------------------------------------\n",
      " 0.001    0.4364(±0.0271)   7.1383(±0.3054)     17.0(± 3.0)      3\n",
      " 0.005    0.4444(±0.0457)   7.1893(±0.1358)     16.7(± 2.5)      3\n",
      " 0.010    0.4323(±0.0188)   7.2160(±0.1830)     16.7(± 3.5)      3\n",
      " 0.050    0.4669(±0.0140)   7.0265(±0.0430)     16.3(± 2.5)      3\n",
      " 0.100    0.4465(±0.0277)   7.0861(±0.0793)     16.7(± 5.7)      3\n",
      "\n",
      "Layer 2 跨Gamma对比:\n",
      "Gamma值    JS距离(±std)      加权熵(±std)      节点数(±std)   运行数\n",
      "---------------------------------------------------------------------------\n",
      " 0.001    0.4130(±0.0092)   7.5895(±0.1510)     68.0(± 4.6)      3\n",
      " 0.005    0.4044(±0.0198)   7.5989(±0.1633)     65.0(± 2.6)      3\n",
      " 0.010    0.4119(±0.0041)   7.3548(±0.1584)     71.3(± 4.7)      3\n",
      " 0.050    0.3824(±0.0318)   7.5705(±0.1527)     68.3(± 7.0)      3\n",
      " 0.100    0.4020(±0.0196)   7.5180(±0.0542)     70.3(± 5.1)      3\n",
      "\n",
      "Layer 3 跨Gamma对比:\n",
      "Gamma值    JS距离(±std)      加权熵(±std)      节点数(±std)   运行数\n",
      "---------------------------------------------------------------------------\n",
      " 0.001    0.4722(±0.0105)   7.4100(±0.1589)    157.7(±13.1)      3\n",
      " 0.005    0.4646(±0.0087)   7.4875(±0.1168)    153.7(±12.1)      3\n",
      " 0.010    0.4750(±0.0056)   7.3947(±0.0233)    172.7(± 9.1)      3\n",
      " 0.050    0.4863(±0.0073)   7.2624(±0.0142)    179.0(± 9.2)      3\n",
      " 0.100    0.4631(±0.0046)   7.3903(±0.1138)    188.3(±17.5)      3\n",
      "======================================================================\n",
      "汇总分析完成！\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# 执行汇总分析\n",
    "base_path = \"/Volumes/My Passport/收敛结果/2\"\n",
    "print(\"=\" * 70)\n",
    "print(\"开始汇总各Gamma值的层级统计...\")\n",
    "print(\"=\" * 70)\n",
    "aggregate_layer_statistics_by_gamma(base_path)\n",
    "print(\"=\" * 70)\n",
    "print(\"汇总分析完成！\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ed95526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已生成每组参数每层的均值表 all_params_layer_mean.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "base_path = \"/Volumes/My Passport/收敛结果/4\"\n",
    "pattern = os.path.join(base_path, \"**\", \"result_layers.csv\")\n",
    "files = glob.glob(pattern, recursive=True)\n",
    "\n",
    "all_rows = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "    # 补充参数信息\n",
    "    for col in ['depth', 'gamma', 'eta', 'alpha']:\n",
    "        if col not in df.columns:\n",
    "            # 从路径中提取参数\n",
    "            folder = os.path.dirname(file)\n",
    "            if f\"{col}_\" in folder:\n",
    "                try:\n",
    "                    value = float(folder.split(f\"{col}_\")[1].split(\"_\")[0])\n",
    "                except:\n",
    "                    value = None\n",
    "                df[col] = value\n",
    "            else:\n",
    "                df[col] = None\n",
    "    all_rows.append(df)\n",
    "\n",
    "merged = pd.concat(all_rows, ignore_index=True)\n",
    "\n",
    "# 按参数组和layer分组，计算均值和标准差\n",
    "group_cols = ['depth', 'gamma', 'eta', 'alpha', 'layer']\n",
    "summary = merged.groupby(group_cols).agg({\n",
    "    'entropy_wavg': ['mean', 'std'],\n",
    "    'distinctiveness_wavg_jsd': ['mean', 'std'],\n",
    "    'nodes_in_layer': ['mean', 'std'],\n",
    "}).reset_index()\n",
    "\n",
    "# 展开多级列名\n",
    "summary.columns = ['_'.join(col).strip('_') for col in summary.columns]\n",
    "\n",
    "summary.to_csv(os.path.join(base_path, \"all_params_layer_mean.csv\"), index=False)\n",
    "print(\"已生成每组参数每层的均值表 all_params_layer_mean.csv\")\n",
    "# // filepath: /Volumes/My Passport/收敛结果/3/step1_analysis.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
