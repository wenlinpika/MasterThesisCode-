{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0485e089",
   "metadata": {},
   "source": [
    "## Smmary\n",
    "#### When $depth=3$, Fix $\\alpha=0.1, \\eta=0.1$, we get model result from 5 combinations of different gamma = [0.001,0.005,0.01,0.05,0.1], each with 3 multiple chains\n",
    "\n",
    "#### 1.For each run/chain, we calculate the Renyi Entropy with eta smooth for each node in the last iteration by **calculate_renyi_entropy_vectorized()**.\n",
    "- save to: corrected_renyi_entropy.csv in each run folder like: <u>/Volumes/My Passport/收敛结果/step1/depth3/d3_g001_收敛/depth_3_gamma_0.01_run_1/corrected_renyi_entropy.csv </u>\n",
    "\n",
    "- we add some extra information eg. (node's layer, document count) to corrected_renyi_entropy.csv, in order to calculate weighted Renyi entropy.\n",
    "\n",
    "#### 2. For each run/chain, we calculate_jensen_shannon_distances_with_weighted_entropy() within each layer for node pairs\n",
    "- save to: <u>/Volumes/My Passport/收敛结果/step1/depth3/d3_g001_收敛/depth_3_gamma_0.01_run_1/jensen_shannon_distances.csv</u>\n",
    "\n",
    "#### 3. For each run/chain, we get average within layer of jensen_shannon_distance and weighted average layer Renyi Entropy by **calculate_jensen_shannon_distances_with_weighted_entropy()**\n",
    "- both save to: <u>/Volumes/My Passport/收敛结果/step1/depth3/d3_g001_收敛/depth_3_gamma_0.01_run_1/layer_average_js_distances.csv</u>\n",
    "\n",
    "#### 4. For all runs/chains in depth=3, we also have without eta entropy caclulated in each chain convergence ipynb file\n",
    "- save to <u>/Volumes/My Passport/收敛结果/step1/depth3/d3_g0001_收敛/depth_3_gamma_0.001_run_2/result_layers.csv </u>. \n",
    "#### Then we average each run's layer and get the mean entropy and mean JSD for each layer in each gamma\n",
    "- save to: <u>/Volumes/My Passport/收敛结果/step1/depth3/all_params_layer_mean.csv</u>\n",
    "\n",
    "#### 5.For all runs/chains in depth=3, we aggrate average layer-level entropy and JSD with eta for all chains in 5 different gamma folder in depth=3\n",
    "- save to: <u>/Volumes/My Passport/收敛结果/step1/depth3/gamma_layer_comparison.csv</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18ef8259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from scipy.special import gammaln\n",
    "\n",
    "def calculate_renyi_entropy_vectorized(node_data, all_words, eta_prior=1.0, renyi_alpha=2.0):\n",
    "    \"\"\"\n",
    "    Vectorized Renyi entropy calculation.\n",
    "\n",
    "    Parameters:\n",
    "    node_data: DataFrame, node data containing 'word' and 'count' columns\n",
    "    all_words: list, full vocabulary list\n",
    "    eta_prior: float, Dirichlet prior smoothing parameter\n",
    "    renyi_alpha: float, order parameter of Renyi entropy\n",
    "\n",
    "    Returns:\n",
    "    tuple: (entropy, nonzero_word_count) Renyi entropy value and number of nonzero words\n",
    "    \"\"\"\n",
    "    if len(all_words) == 0:\n",
    "        return 0.0, 0\n",
    "\n",
    "    # Create mapping from vocabulary word to index\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "\n",
    "    # Initialize count vector\n",
    "    counts = np.zeros(len(all_words))\n",
    "\n",
    "    # Fill in actual counts\n",
    "    for _, row in node_data.iterrows():\n",
    "        word = row['word']\n",
    "        if pd.notna(word) and word in word_to_idx:\n",
    "            counts[word_to_idx[word]] = row['count']\n",
    "\n",
    "    # Count nonzero words (before smoothing)\n",
    "    nonzero_word_count = np.sum(counts > 0)\n",
    "\n",
    "    # Apply eta smoothing\n",
    "    smoothed_counts = counts + eta_prior\n",
    "\n",
    "    # Compute probability distribution\n",
    "    probabilities = smoothed_counts / np.sum(smoothed_counts)\n",
    "\n",
    "    # Compute Renyi entropy (using natural logarithm)\n",
    "    if renyi_alpha == 1.0:\n",
    "        # Shannon entropy (with smoothing all probabilities > 0, no small constant needed)\n",
    "        entropy = -np.sum(probabilities * np.log(probabilities))\n",
    "    else:\n",
    "        # General Renyi entropy\n",
    "        entropy = (1 / (1 - renyi_alpha)) * np.log(np.sum(probabilities ** renyi_alpha))\n",
    "\n",
    "    return entropy, int(nonzero_word_count)\n",
    "\n",
    "def process_all_iteration_files(base_path=\".\", eta_prior=1.0, renyi_alpha=2.0):\n",
    "    \"\"\"\n",
    "    Process each iteration_node_word_distributions.csv file individually and save results.\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_node_word_distributions.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "\n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        print(f\"\\nProcessing file: {file_path}\")\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Clean column names: remove single/double quotes and extra spaces\n",
    "        df.columns = [col.strip(\"'\\\" \") for col in df.columns]\n",
    "\n",
    "        if 'node_id' not in df.columns:\n",
    "            print(f\"Warning: {file_path} is missing 'node_id' column, skipping this file\")\n",
    "            continue\n",
    "\n",
    "        max_iteration = df['iteration'].max()\n",
    "        last_iteration_data = df[df['iteration'] == max_iteration]\n",
    "        all_words = list(last_iteration_data['word'].dropna().unique())\n",
    "\n",
    "        print(f\"Last iteration: {max_iteration}, vocabulary size: {len(all_words)}, node count: {last_iteration_data['node_id'].nunique()}\")\n",
    "\n",
    "        results = []\n",
    "        for node_id in last_iteration_data['node_id'].unique():\n",
    "            node_data = last_iteration_data[last_iteration_data['node_id'] == node_id]\n",
    "\n",
    "            entropy, nonzero_words = calculate_renyi_entropy_vectorized(\n",
    "                node_data, all_words, eta_prior, renyi_alpha\n",
    "            )\n",
    "\n",
    "            # Calculate sparsity (proportion of nonzero words)\n",
    "            sparsity_ratio = nonzero_words / len(all_words) if len(all_words) > 0 else 0\n",
    "\n",
    "            results.append({\n",
    "                'node_id': node_id,\n",
    "                'renyi_entropy_corrected': entropy,\n",
    "                'nonzero_word_count': nonzero_words,\n",
    "                'total_vocabulary_size': len(all_words),\n",
    "                'sparsity_ratio': sparsity_ratio,\n",
    "                'eta_prior': eta_prior,\n",
    "                'renyi_alpha': renyi_alpha,\n",
    "                'iteration': max_iteration\n",
    "            })\n",
    "\n",
    "        # Save the new corrected_renyi_entropy.csv file\n",
    "        results_df = pd.DataFrame(results)\n",
    "        output_path = os.path.join(folder_path, 'corrected_renyi_entropy.csv')\n",
    "        results_df.to_csv(output_path, index=False)\n",
    "        print(f\"Saved corrected Renyi entropy results to: {output_path}\")\n",
    "\n",
    "        # Print some summary statistics\n",
    "        print(\"Node vocabulary sparsity statistics:\")\n",
    "        print(f\"  - Average nonzero word count: {results_df['nonzero_word_count'].mean():.1f}\")\n",
    "        print(f\"  - Nonzero word count range: {results_df['nonzero_word_count'].min()}-{results_df['nonzero_word_count'].max()}\")\n",
    "        print(f\"  - Average sparsity: {results_df['sparsity_ratio'].mean():.3f}\")\n",
    "        print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b21f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Start batch computing corrected Renyi entropy...\n",
      "==================================================\n",
      "==================================================\n",
      "All processing completed!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step1/depth3\"  # root directory\n",
    "eta_prior = 0.1  # Dirichlet prior smoothing parameter\n",
    "renyi_alpha = 2.0  # Renyi entropy order parameter\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Start batch computing corrected Renyi entropy...\")\n",
    "print(\"=\" * 50)\n",
    "process_all_iteration_files(base_path, eta_prior, renyi_alpha)\n",
    "print(\"=\" * 50)\n",
    "print(\"All processing completed!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8d81643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_node_document_counts(path_structures_df):\n",
    "    \"\"\"\n",
    "    Aggregate document counts from leaf nodes upward and compute node hierarchy.\n",
    "\n",
    "    Parameters:\n",
    "    path_structures_df: DataFrame, data from iteration_path_structures.csv (filtered to last iteration)\n",
    "\n",
    "    Returns:\n",
    "    dict: mapping {node_id: {'document_count': int, 'layer': int, 'parent_id': int, 'child_ids': list, 'child_count': int}}\n",
    "    \"\"\"\n",
    "    # Find all layer columns\n",
    "    layer_columns = [col for col in path_structures_df.columns if col.startswith('layer_') and col.endswith('_node_id')]\n",
    "    layer_columns.sort()\n",
    "    max_layer_idx = len(layer_columns) - 1\n",
    "\n",
    "    print(f\"[DEBUG] Found layer columns: {layer_columns}\")\n",
    "    print(f\"[DEBUG] Max layer index: {max_layer_idx}\")\n",
    "\n",
    "    # Initialize node info dictionary\n",
    "    node_info = {}\n",
    "\n",
    "    # Handle leaf nodes using leaf_node_id column\n",
    "    for _, row in path_structures_df.iterrows():\n",
    "        leaf_node = row.get('leaf_node_id')\n",
    "        if pd.notna(leaf_node):\n",
    "            if leaf_node not in node_info:\n",
    "                node_info[leaf_node] = {\n",
    "                    'document_count': 0,\n",
    "                    'layer': max_layer_idx,\n",
    "                    'parent_id': None,\n",
    "                    'child_ids': [],\n",
    "                    'child_count': 0\n",
    "                }\n",
    "            node_info[leaf_node]['document_count'] += row.get('document_count', 0)\n",
    "\n",
    "    # Build parent-child relationships and layer info\n",
    "    for _, row in path_structures_df.iterrows():\n",
    "        path_nodes = []\n",
    "        for layer_idx in range(max_layer_idx + 1):\n",
    "            layer_col = f'layer_{layer_idx}_node_id'\n",
    "            if layer_col in path_structures_df.columns and pd.notna(row.get(layer_col)):\n",
    "                path_nodes.append(row[layer_col])\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Set layer and parent-child for each node in the path\n",
    "        for i, node in enumerate(path_nodes):\n",
    "            if node not in node_info:\n",
    "                node_info[node] = {\n",
    "                    'document_count': 0,\n",
    "                    'layer': i,\n",
    "                    'parent_id': None,\n",
    "                    'child_ids': [],\n",
    "                    'child_count': 0\n",
    "                }\n",
    "            else:\n",
    "                node_info[node]['layer'] = i\n",
    "\n",
    "            if i > 0:\n",
    "                parent_node = path_nodes[i - 1]\n",
    "                node_info[node]['parent_id'] = parent_node\n",
    "\n",
    "                if parent_node not in node_info:\n",
    "                    node_info[parent_node] = {\n",
    "                        'document_count': 0,\n",
    "                        'layer': i - 1,\n",
    "                        'parent_id': None,\n",
    "                        'child_ids': [],\n",
    "                        'child_count': 0\n",
    "                    }\n",
    "\n",
    "                if node not in node_info[parent_node]['child_ids']:\n",
    "                    node_info[parent_node]['child_ids'].append(node)\n",
    "\n",
    "    # Aggregate document counts upward from second-last layer to root\n",
    "    for layer_idx in range(max_layer_idx - 1, -1, -1):\n",
    "        layer_col = f'layer_{layer_idx}_node_id'\n",
    "        if layer_col not in path_structures_df.columns:\n",
    "            continue\n",
    "\n",
    "        layer_nodes = path_structures_df[layer_col].dropna().unique()\n",
    "        for node in layer_nodes:\n",
    "            if node in node_info and node_info[node]['document_count'] == 0:\n",
    "                total_docs = path_structures_df[path_structures_df[layer_col] == node]['document_count'].sum()\n",
    "                node_info[node]['document_count'] = int(total_docs)\n",
    "\n",
    "    # Compute child counts\n",
    "    for node_id, info in node_info.items():\n",
    "        info['child_count'] = len(info.get('child_ids', []))\n",
    "\n",
    "    return node_info\n",
    "\n",
    "def add_document_counts_to_entropy_files(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    Add document counts and hierarchy info to corrected_renyi_entropy.csv files.\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_path_structures.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "\n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        print(f\"\\nProcessing path structure file: {file_path}\")\n",
    "\n",
    "        try:\n",
    "            # Read path structures file\n",
    "            df = pd.read_csv(file_path)\n",
    "            df.columns = [col.strip(\"'\\\" \") for col in df.columns]\n",
    "\n",
    "            # Get last iteration\n",
    "            max_iteration = df['iteration'].max()\n",
    "            last_iteration_data = df[df['iteration'] == max_iteration]\n",
    "\n",
    "            print(f\"Last iteration: {max_iteration}, number of paths: {len(last_iteration_data)}\")\n",
    "\n",
    "            # Compute node document counts and hierarchy\n",
    "            node_info = calculate_node_document_counts(last_iteration_data)\n",
    "\n",
    "            print(f\"Computed info for {len(node_info)} nodes\")\n",
    "\n",
    "            # Read corresponding corrected_renyi_entropy.csv\n",
    "            entropy_file = os.path.join(folder_path, 'corrected_renyi_entropy.csv')\n",
    "            if os.path.exists(entropy_file):\n",
    "                entropy_df = pd.read_csv(entropy_file)\n",
    "\n",
    "                # Add new columns: document_count, layer, parent_id, child_ids, child_count\n",
    "                entropy_df['document_count'] = entropy_df['node_id'].map(lambda x: node_info.get(x, {}).get('document_count', 0))\n",
    "                entropy_df['layer'] = entropy_df['node_id'].map(lambda x: node_info.get(x, {}).get('layer', -1))\n",
    "                entropy_df['parent_id'] = entropy_df['node_id'].map(lambda x: node_info.get(x, {}).get('parent_id', None))\n",
    "\n",
    "                # Format child_ids as bracketed list string (empty string if no children)\n",
    "                entropy_df['child_ids'] = entropy_df['node_id'].map(\n",
    "                    lambda x: '[' + ','.join(map(str, node_info.get(x, {}).get('child_ids', []))) + ']'\n",
    "                    if node_info.get(x, {}).get('child_ids') else ''\n",
    "                )\n",
    "\n",
    "                entropy_df['child_count'] = entropy_df['node_id'].map(lambda x: len(node_info.get(x, {}).get('child_ids', [])))\n",
    "\n",
    "                # Save updated file\n",
    "                entropy_df.to_csv(entropy_file, index=False)\n",
    "                print(f\"Updated {entropy_file} with columns: document_count, layer, parent_id, child_ids, child_count\")\n",
    "\n",
    "                # Print some statistics\n",
    "                print(\"Node hierarchy statistics:\")\n",
    "                print(f\"  - Layer distribution: {entropy_df['layer'].value_counts().sort_index().to_dict()}\")\n",
    "                print(f\"  - Document count range: {entropy_df['document_count'].min()} - {entropy_df['document_count'].max()}\")\n",
    "                print(f\"  - Root node count: {entropy_df[entropy_df['parent_id'].isna()].shape[0]}\")\n",
    "                print(f\"  - Leaf node count: {entropy_df[entropy_df['child_ids'] == ''].shape[0]}\")\n",
    "                print(f\"  - Child count distribution: {entropy_df['child_count'].value_counts().sort_index().to_dict()}\")\n",
    "            else:\n",
    "                print(f\"Warning: corresponding entropy file not found: {entropy_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "            print(\"Traceback:\")\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e230e0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Start adding document counts and hierarchy information to entropy files...\n",
      "==================================================\n",
      "==================================================\n",
      "Document counts and hierarchy information added.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Main function: add document counts and hierarchy info to entropy files\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step1/depth3\"  # root directory\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Start adding document counts and hierarchy information to entropy files...\")\n",
    "print(\"=\" * 50)\n",
    "add_document_counts_to_entropy_files(base_path)\n",
    "print(\"=\" * 50)\n",
    "print(\"Document counts and hierarchy information added.\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6ab468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jensen_shannon_distances_with_weighted_entropy(base_path=\".\", eta=0.1):\n",
    "    \"\"\"\n",
    "    Calculate Jensen-Shannon distances between nodes in each layer and the document-count-weighted average Renyi entropy.\n",
    "    \n",
    "    Parameters:\n",
    "    base_path: str, root directory path\n",
    "    eta: float, Dirichlet smoothing parameter\n",
    "    \"\"\"\n",
    "    # Find all iteration_node_word_distributions.csv files\n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_node_word_distributions.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        print(f\"\\nProcessing file: {file_path}\")\n",
    "\n",
    "        # Read word distribution data\n",
    "        word_df = pd.read_csv(file_path)\n",
    "        word_df.columns = [col.strip(\"'\\\" \") for col in word_df.columns]\n",
    "        \n",
    "        # Get data from the last iteration\n",
    "        max_iteration = word_df['iteration'].max()\n",
    "        last_iteration_data = word_df[word_df['iteration'] == max_iteration]\n",
    "        \n",
    "        # Get the full vocabulary\n",
    "        all_words = sorted(list(last_iteration_data['word'].dropna().unique()))\n",
    "        print(f\"Vocabulary size: {len(all_words)}\")\n",
    "        \n",
    "        # Read the entropy file to get hierarchy information\n",
    "        entropy_file = os.path.join(folder_path, 'corrected_renyi_entropy.csv')\n",
    "        if not os.path.exists(entropy_file):\n",
    "            print(f\"Warning: Entropy file not found: {entropy_file}\")\n",
    "            continue\n",
    "            \n",
    "        entropy_df = pd.read_csv(entropy_file)\n",
    "        \n",
    "        # Group nodes by layer\n",
    "        layers = entropy_df.groupby('layer')['node_id'].apply(list).to_dict()\n",
    "        print(f\"Layer distribution: {[(layer, len(nodes)) for layer, nodes in layers.items()]}\")\n",
    "        \n",
    "        # Build probability distribution for each node\n",
    "        node_distributions = {}\n",
    "        \n",
    "        for node_id in entropy_df['node_id'].unique():\n",
    "            # Get the word distribution for this node\n",
    "            node_words = last_iteration_data[last_iteration_data['node_id'] == node_id]\n",
    "            \n",
    "            # Initialize count vector\n",
    "            counts = np.zeros(len(all_words))\n",
    "            word_to_idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "            \n",
    "            # Fill in actual counts\n",
    "            for _, row in node_words.iterrows():\n",
    "                word = row['word']\n",
    "                if pd.notna(word) and word in word_to_idx:\n",
    "                    counts[word_to_idx[word]] = row['count']\n",
    "            \n",
    "            # Add Dirichlet smoothing\n",
    "            smoothed_counts = counts + eta\n",
    "            \n",
    "            # Calculate probability distribution\n",
    "            probabilities = smoothed_counts / np.sum(smoothed_counts)\n",
    "            node_distributions[node_id] = probabilities\n",
    "        \n",
    "        # Calculate JS distances and weighted average entropy for nodes within each layer\n",
    "        all_js_distances = []\n",
    "        layer_avg_distances = []\n",
    "        \n",
    "        for layer, layer_nodes in layers.items():\n",
    "            print(f\"\\nCalculating JS distance and weighted average entropy for Layer {layer} ({len(layer_nodes)} nodes)\")\n",
    "            \n",
    "            layer_js_distances = []\n",
    "            n = len(layer_nodes)\n",
    "            \n",
    "            # Calculate JS distance for all pairs of nodes in this layer\n",
    "            for i, node1 in enumerate(layer_nodes):\n",
    "                for j, node2 in enumerate(layer_nodes):\n",
    "                    if i < j:  # Only calculate for the upper triangle to avoid duplicates and self-comparison\n",
    "                        if node1 in node_distributions and node2 in node_distributions:\n",
    "                            p = node_distributions[node1]\n",
    "                            q = node_distributions[node2]\n",
    "                            \n",
    "                            # Calculate Jensen-Shannon distance\n",
    "                            js_distance = jensen_shannon_distance(p, q)\n",
    "                            \n",
    "                            layer_js_distances.append({\n",
    "                                'layer': layer,\n",
    "                                'node1_id': node1,\n",
    "                                'node2_id': node2,\n",
    "                                'js_distance': js_distance,\n",
    "                                'node1_doc_count': entropy_df[entropy_df['node_id'] == node1]['document_count'].iloc[0] if len(entropy_df[entropy_df['node_id'] == node1]) > 0 else 0,\n",
    "                                'node2_doc_count': entropy_df[entropy_df['node_id'] == node2]['document_count'].iloc[0] if len(entropy_df[entropy_df['node_id'] == node2]) > 0 else 0\n",
    "                            })\n",
    "            \n",
    "            all_js_distances.extend(layer_js_distances)\n",
    "            \n",
    "            # Calculate the average JS distance for the layer\n",
    "            avg_js_distance = 0.0\n",
    "            if layer_js_distances and n > 1:\n",
    "                total_js_distance = sum(d['js_distance'] for d in layer_js_distances)\n",
    "                max_pairs = n * (n - 1) // 2  # n*(n-1)/2\n",
    "                avg_js_distance = total_js_distance / max_pairs\n",
    "            \n",
    "            # Calculate the document-count-weighted average Renyi entropy for the layer\n",
    "            layer_entropy_data = entropy_df[entropy_df['layer'] == layer]\n",
    "            total_docs = layer_entropy_data['document_count'].sum()\n",
    "            \n",
    "            if total_docs > 0:\n",
    "                # Calculate weighted average entropy: sum(doc_count * entropy) / total_doc_count\n",
    "                weighted_entropy = (layer_entropy_data['document_count'] * layer_entropy_data['renyi_entropy_corrected']).sum() / total_docs\n",
    "            else:\n",
    "                weighted_entropy = 0.0\n",
    "            \n",
    "            layer_avg_distances.append({\n",
    "                'layer': layer,\n",
    "                'node_count': n,\n",
    "                'total_pairs': len(layer_js_distances),\n",
    "                'max_pairs': n * (n - 1) // 2 if n > 1 else 0,\n",
    "                'sum_js_distance': sum(d['js_distance'] for d in layer_js_distances),\n",
    "                'avg_js_distance': avg_js_distance,\n",
    "                'total_documents': total_docs,\n",
    "                'weighted_avg_renyi_entropy': weighted_entropy\n",
    "            })\n",
    "            \n",
    "            print(f\"  - Node count: {n}\")\n",
    "            print(f\"  - Calculated node pairs: {len(layer_js_distances)}\")\n",
    "            print(f\"  - Theoretical max node pairs: {n * (n - 1) // 2 if n > 1 else 0}\")\n",
    "            print(f\"  - Average JS distance: {avg_js_distance:.4f}\")\n",
    "            print(f\"  - Total documents: {total_docs}\")\n",
    "            print(f\"  - Document-weighted average Renyi entropy: {weighted_entropy:.4f}\")\n",
    "            print(\"=\" * 50)\n",
    "        \n",
    "        # Save detailed JS distance results\n",
    "        if all_js_distances:\n",
    "            js_df = pd.DataFrame(all_js_distances)\n",
    "            output_path = os.path.join(folder_path, 'jensen_shannon_distances.csv')\n",
    "            js_df.to_csv(output_path, index=False)\n",
    "            print(f\"\\nSaved detailed JS distance results to: {output_path}\")\n",
    "        \n",
    "        # Save average JS distance and weighted entropy results for each layer\n",
    "        if layer_avg_distances:\n",
    "            avg_df = pd.DataFrame(layer_avg_distances)\n",
    "            avg_output_path = os.path.join(folder_path, 'layer_average_js_distances.csv')\n",
    "            avg_df.to_csv(avg_output_path, index=False)\n",
    "            print(f\"Saved layer average JS distance and weighted entropy results to: {avg_output_path}\")\n",
    "            \n",
    "            # Overall statistics\n",
    "            print(f\"\\nOverall Statistics:\")\n",
    "            print(f\"  - Total layers: {len(layer_avg_distances)}\")\n",
    "            print(f\"  - Layer statistics:\")\n",
    "            for row in layer_avg_distances:\n",
    "                print(f\" Layer {row['layer']}: JS distance={row['avg_js_distance']:.4f}, Weighted entropy={row['weighted_avg_renyi_entropy']:.4f} (based on {row['node_count']} nodes, {row['total_documents']} documents)\")\n",
    "\n",
    "\n",
    "def jensen_shannon_distance(p, q):\n",
    "    \"\"\"\n",
    "    Calculate the Jensen-Shannon distance between two probability distributions.\n",
    "    \n",
    "    Parameters:\n",
    "    p, q: numpy arrays, probability distributions\n",
    "    \n",
    "    Returns:\n",
    "    float: Jensen-Shannon distance\n",
    "    \"\"\"\n",
    "    # Ensure probability distributions are normalized\n",
    "    p = p / np.sum(p)\n",
    "    q = q / np.sum(q)\n",
    "    \n",
    "    # Calculate the average distribution\n",
    "    m = 0.5 * (p + q)\n",
    "    \n",
    "    # Calculate KL divergence (using natural logarithm)\n",
    "    def kl_divergence(x, y):\n",
    "        # Avoid log(0)\n",
    "        mask = (x > 0) & (y > 0)\n",
    "        if np.sum(mask) == 0:\n",
    "            return 0.0\n",
    "        return np.sum(x[mask] * np.log(x[mask] / y[mask]))\n",
    "    \n",
    "    # Calculate Jensen-Shannon divergence\n",
    "    js_divergence = 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)\n",
    "    \n",
    "    # Convert to distance (square root)\n",
    "    js_distance = np.sqrt(js_divergence)\n",
    "    \n",
    "    return js_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7363e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Start calculating Jensen-Shannon distances and weighted average Renyi entropy...\n",
      "==================================================\n",
      "==================================================\n",
      "Jensen-Shannon distance and weighted average Renyi entropy calculation completed!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd \n",
    "# Main function: Calculate Jensen-Shannon distance and weighted average Renyi entropy\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step1/depth3\"  # Root directory\n",
    "eta = 0.1  # Dirichlet smoothing parameter\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Start calculating Jensen-Shannon distances and weighted average Renyi entropy...\")\n",
    "print(\"=\" * 50)\n",
    "calculate_jensen_shannon_distances_with_weighted_entropy(base_path, eta)\n",
    "print(\"=\" * 50)\n",
    "print(\"Jensen-Shannon distance and weighted average Renyi entropy calculation completed!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18a7e2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_layer_statistics_by_gamma(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    Aggregate layer-wise JS distance and weighted entropy statistics by gamma value, \n",
    "    and generate a summary table at the same level as the run folder.\n",
    "    \"\"\"\n",
    "    # Find all layer_average_js_distances.csv files\n",
    "    pattern = os.path.join(base_path, \"**\", \"layer_average_js_distances.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    # Store all data and grouping information\n",
    "    all_data = []\n",
    "    gamma_experiment_groups = {}  # Used to store the parent directory for each gamma_experiment combination\n",
    "    \n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        parent_folder = os.path.dirname(folder_path)  # parent directory of the run folder\n",
    "        \n",
    "        # Extract gamma value from the folder name\n",
    "        if 'gamma_0.001' in folder_name:\n",
    "            if '2chains' in parent_folder: # Assuming '2条链' is '2chains' in the path\n",
    "                gamma = 0.001\n",
    "                experiment_type = '2chains'\n",
    "            else:\n",
    "                gamma = 0.001\n",
    "                experiment_type = 'single'\n",
    "        elif 'gamma_0.005' in folder_name:\n",
    "            gamma = 0.005\n",
    "            experiment_type = 'single'\n",
    "        elif 'gamma_0.01' in folder_name:\n",
    "            gamma = 0.01\n",
    "            experiment_type = 'single'\n",
    "        elif 'gamma_0.05' in folder_name:\n",
    "            gamma = 0.05\n",
    "            experiment_type = 'single'\n",
    "        elif 'gamma_0.1' in folder_name:\n",
    "            gamma = 0.1\n",
    "            experiment_type = 'single'\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Extract run number\n",
    "        run_match = folder_name.split('_run_')\n",
    "        if len(run_match) > 1:\n",
    "            run_id = run_match[1]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Record the parent directory for the gamma_experiment combination\n",
    "        group_key = f\"{gamma}_{experiment_type}\"\n",
    "        if group_key not in gamma_experiment_groups:\n",
    "            gamma_experiment_groups[group_key] = parent_folder\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                all_data.append({\n",
    "                    'gamma': gamma,\n",
    "                    'experiment_type': experiment_type,\n",
    "                    'run_id': run_id,\n",
    "                    'layer': row['layer'],\n",
    "                    'node_count': row['node_count'],\n",
    "                    'avg_js_distance': row['avg_js_distance'],\n",
    "                    'weighted_avg_renyi_entropy': row['weighted_avg_renyi_entropy'],\n",
    "                    'total_documents': row['total_documents'],\n",
    "                    'parent_folder': parent_folder\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_path}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    summary_df = pd.DataFrame(all_data)\n",
    "    \n",
    "    if summary_df.empty:\n",
    "        print(\"No valid data found\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"Summary Statistics for Each GAMMA Value by Layer\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Group by gamma, experiment_type, and parent_folder to generate summary files\n",
    "    for (gamma, experiment_type), group_data in summary_df.groupby(['gamma', 'experiment_type']):\n",
    "        parent_folder = group_data['parent_folder'].iloc[0]\n",
    "        \n",
    "        print(f\"\\nProcessing Gamma={gamma:.3f}, Experiment Type={'2 chains' if experiment_type == '2chains' else 'single chain'}\")\n",
    "        print(f\"Output directory: {parent_folder}\")\n",
    "        \n",
    "        # Calculate summary statistics for each layer\n",
    "        layer_summary = group_data.groupby('layer').agg({\n",
    "            'avg_js_distance': ['mean', 'std', 'count'],\n",
    "            'weighted_avg_renyi_entropy': ['mean', 'std', 'count'],\n",
    "            'node_count': ['mean', 'std'],\n",
    "            'total_documents': 'mean',\n",
    "            'run_id': lambda x: ', '.join(sorted(x.unique()))\n",
    "        }).round(4)\n",
    "        \n",
    "        # Flatten column names\n",
    "        layer_summary.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in layer_summary.columns]\n",
    "        layer_summary = layer_summary.reset_index()\n",
    "        \n",
    "        # Rename columns to be more descriptive\n",
    "        column_mapping = {\n",
    "            'avg_js_distance_mean': 'avg_js_distance_mean',\n",
    "            'avg_js_distance_std': 'avg_js_distance_std', \n",
    "            'avg_js_distance_count': 'run_count',\n",
    "            'weighted_avg_renyi_entropy_mean': 'weighted_avg_renyi_entropy_mean',\n",
    "            'weighted_avg_renyi_entropy_std': 'weighted_avg_renyi_entropy_std',\n",
    "            'weighted_avg_renyi_entropy_count': 'entropy_run_count',\n",
    "            'node_count_mean': 'avg_node_count',\n",
    "            'node_count_std': 'node_count_std',\n",
    "            'total_documents_mean': 'avg_total_documents',\n",
    "            'run_id_<lambda>': 'included_runs'\n",
    "        }\n",
    "        \n",
    "        for old_name, new_name in column_mapping.items():\n",
    "            if old_name in layer_summary.columns:\n",
    "                layer_summary = layer_summary.rename(columns={old_name: new_name})\n",
    "        \n",
    "        # Add gamma and experiment_type information\n",
    "        layer_summary.insert(0, 'gamma', gamma)\n",
    "        layer_summary.insert(1, 'experiment_type', experiment_type)\n",
    "        \n",
    "        # Save the summary results to the same level as the run folder\n",
    "        if experiment_type == '2chains':\n",
    "            output_filename = f'gamma_{gamma:.3f}_2chains_layer_summary.csv'\n",
    "        else:\n",
    "            output_filename = f'gamma_{gamma:.3f}_single_layer_summary.csv'\n",
    "        \n",
    "        output_path = os.path.join(parent_folder, output_filename)\n",
    "        layer_summary.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"  Saved summary file: {output_path}\")\n",
    "        print(f\"  Included runs: {layer_summary['included_runs'].iloc[0] if 'included_runs' in layer_summary.columns else 'N/A'}\")\n",
    "        print(f\"  Number of layers: {len(layer_summary)}\")\n",
    "        \n",
    "        # Display brief statistics\n",
    "        for _, row in layer_summary.iterrows():\n",
    "            layer_num = int(row['layer'])\n",
    "            js_mean = row['avg_js_distance_mean']\n",
    "            js_std = row['avg_js_distance_std'] if 'avg_js_distance_std' in row else 0\n",
    "            entropy_mean = row['weighted_avg_renyi_entropy_mean']\n",
    "            entropy_std = row['weighted_avg_renyi_entropy_std'] if 'weighted_avg_renyi_entropy_std' in row else 0\n",
    "            node_count = row['avg_node_count']\n",
    "            run_count = int(row['run_count']) if 'run_count' in row else 0\n",
    "            \n",
    "            print(f\"    Layer {layer_num}: JS={js_mean:.4f}(±{js_std:.4f}), Entropy={entropy_mean:.4f}(±{entropy_std:.4f}), Nodes={node_count:.1f}, runs={run_count}\")\n",
    "    \n",
    "    # Generate overall comparison file (saved under base_path)\n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"Generating overall comparison file\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Only analyze the cross-gamma comparison for single-chain experiments\n",
    "    single_chain_data = summary_df[summary_df['experiment_type'] == 'single']\n",
    "    \n",
    "    if not single_chain_data.empty:\n",
    "        overall_summary = single_chain_data.groupby(['gamma', 'layer']).agg({\n",
    "            'avg_js_distance': ['mean', 'std'],\n",
    "            'weighted_avg_renyi_entropy': ['mean', 'std'],\n",
    "            'node_count': ['mean', 'std'],\n",
    "            'run_id': 'count'\n",
    "        }).round(4)\n",
    "        \n",
    "        # Flatten column names\n",
    "        overall_summary.columns = ['_'.join(col).strip() for col in overall_summary.columns]\n",
    "        overall_summary = overall_summary.reset_index()\n",
    "        \n",
    "        overall_output_path = os.path.join(base_path, 'gamma_layer_comparison.csv')\n",
    "        overall_summary.to_csv(overall_output_path, index=False)\n",
    "        print(f\"Overall comparison file saved to: {overall_output_path}\")\n",
    "        \n",
    "        # Display cross-gamma comparison\n",
    "        for layer in sorted(single_chain_data['layer'].unique()):\n",
    "            print(f\"\\nLayer {int(layer)} Cross-Gamma Comparison:\")\n",
    "            print(\"Gamma      JS Distance(±std)   Weighted Entropy(±std)   Node Count(±std)   Runs\")\n",
    "            print(\"-\" * 75)\n",
    "            \n",
    "            layer_data = overall_summary[overall_summary['layer'] == layer]\n",
    "            for _, row in layer_data.iterrows():\n",
    "                gamma = row['gamma']\n",
    "                js_mean = row['avg_js_distance_mean']\n",
    "                js_std = row['avg_js_distance_std']\n",
    "                entropy_mean = row['weighted_avg_renyi_entropy_mean']\n",
    "                entropy_std = row['weighted_avg_renyi_entropy_std']\n",
    "                node_mean = row['node_count_mean']\n",
    "                node_std = row['node_count_std']\n",
    "                run_count = int(row['run_id_count'])\n",
    "                \n",
    "                print(f\"{gamma:6.3f}    {js_mean:6.4f}(±{js_std:5.4f})   {entropy_mean:6.4f}(±{entropy_std:5.4f})   {node_mean:6.1f}(±{node_std:4.1f})   {run_count:4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a5a49f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Start aggregating layer statistics for each Gamma value...\n",
      "======================================================================\n",
      "======================================================================\n",
      "Summary Statistics for Each GAMMA Value by Layer\n",
      "======================================================================\n",
      "\n",
      "Processing Gamma=0.001, Experiment Type=single chain\n",
      "Output directory: /Volumes/My Passport/收敛结果/step1/depth3/d3_g0001_2条链收敛\n",
      "  Saved summary file: /Volumes/My Passport/收敛结果/step1/depth3/d3_g0001_2条链收敛/gamma_0.001_single_layer_summary.csv\n",
      "  Included runs: 1, 2, 3\n",
      "  Number of layers: 3\n",
      "    Layer 0: JS=0.0000(±0.0000), Entropy=4.9490(±0.0399), Nodes=1.0, runs=5\n",
      "    Layer 1: JS=0.4870(±0.0201), Entropy=5.0720(±0.1288), Nodes=42.2, runs=5\n",
      "    Layer 2: JS=0.5069(±0.0121), Entropy=5.0823(±0.1317), Nodes=145.8, runs=5\n",
      "\n",
      "Processing Gamma=0.005, Experiment Type=single chain\n",
      "Output directory: /Volumes/My Passport/收敛结果/step1/depth3/d3_g0005_收敛\n",
      "  Saved summary file: /Volumes/My Passport/收敛结果/step1/depth3/d3_g0005_收敛/gamma_0.005_single_layer_summary.csv\n",
      "  Included runs: 1, 2, 3\n",
      "  Number of layers: 3\n",
      "    Layer 0: JS=0.0000(±0.0000), Entropy=4.9424(±0.0206), Nodes=1.0, runs=3\n",
      "    Layer 1: JS=0.4915(±0.0111), Entropy=5.1457(±0.0513), Nodes=44.0, runs=3\n",
      "    Layer 2: JS=0.4944(±0.0076), Entropy=5.1035(±0.0213), Nodes=153.3, runs=3\n",
      "\n",
      "Processing Gamma=0.010, Experiment Type=single chain\n",
      "Output directory: /Volumes/My Passport/收敛结果/step1/depth3/d3_g001_收敛\n",
      "  Saved summary file: /Volumes/My Passport/收敛结果/step1/depth3/d3_g001_收敛/gamma_0.010_single_layer_summary.csv\n",
      "  Included runs: 1, 2, 3\n",
      "  Number of layers: 3\n",
      "    Layer 0: JS=0.0000(±0.0000), Entropy=4.9376(±0.0301), Nodes=1.0, runs=3\n",
      "    Layer 1: JS=0.4784(±0.0240), Entropy=5.1824(±0.1588), Nodes=41.3, runs=3\n",
      "    Layer 2: JS=0.5006(±0.0078), Entropy=5.0835(±0.0670), Nodes=152.7, runs=3\n",
      "\n",
      "Processing Gamma=0.050, Experiment Type=single chain\n",
      "Output directory: /Volumes/My Passport/收敛结果/step1/depth3/d3_g005_收敛\n",
      "  Saved summary file: /Volumes/My Passport/收敛结果/step1/depth3/d3_g005_收敛/gamma_0.050_single_layer_summary.csv\n",
      "  Included runs: 1, 2, 3\n",
      "  Number of layers: 3\n",
      "    Layer 0: JS=0.0000(±0.0000), Entropy=4.9197(±0.0219), Nodes=1.0, runs=3\n",
      "    Layer 1: JS=0.4579(±0.0178), Entropy=5.2068(±0.1233), Nodes=41.7, runs=3\n",
      "    Layer 2: JS=0.4982(±0.0058), Entropy=5.0438(±0.0160), Nodes=174.3, runs=3\n",
      "\n",
      "Processing Gamma=0.100, Experiment Type=single chain\n",
      "Output directory: /Volumes/My Passport/收敛结果/step1/depth3/d3_g01_收敛\n",
      "  Saved summary file: /Volumes/My Passport/收敛结果/step1/depth3/d3_g01_收敛/gamma_0.100_single_layer_summary.csv\n",
      "  Included runs: 1, 2, 3\n",
      "  Number of layers: 3\n",
      "    Layer 0: JS=0.0000(±0.0000), Entropy=4.8972(±0.0389), Nodes=1.0, runs=3\n",
      "    Layer 1: JS=0.4700(±0.0183), Entropy=5.0821(±0.0651), Nodes=42.7, runs=3\n",
      "    Layer 2: JS=0.4913(±0.0041), Entropy=5.0317(±0.0376), Nodes=183.3, runs=3\n",
      "\n",
      "======================================================================\n",
      "Generating overall comparison file\n",
      "======================================================================\n",
      "Overall comparison file saved to: /Volumes/My Passport/收敛结果/step1/depth3/gamma_layer_comparison.csv\n",
      "\n",
      "Layer 0 Cross-Gamma Comparison:\n",
      "Gamma      JS Distance(±std)   Weighted Entropy(±std)   Node Count(±std)   Runs\n",
      "---------------------------------------------------------------------------\n",
      " 0.001    0.0000(±0.0000)   4.9490(±0.0399)      1.0(± 0.0)      5\n",
      " 0.005    0.0000(±0.0000)   4.9424(±0.0206)      1.0(± 0.0)      3\n",
      " 0.010    0.0000(±0.0000)   4.9376(±0.0301)      1.0(± 0.0)      3\n",
      " 0.050    0.0000(±0.0000)   4.9197(±0.0219)      1.0(± 0.0)      3\n",
      " 0.100    0.0000(±0.0000)   4.8972(±0.0389)      1.0(± 0.0)      3\n",
      "\n",
      "Layer 1 Cross-Gamma Comparison:\n",
      "Gamma      JS Distance(±std)   Weighted Entropy(±std)   Node Count(±std)   Runs\n",
      "---------------------------------------------------------------------------\n",
      " 0.001    0.4870(±0.0201)   5.0720(±0.1288)     42.2(± 3.4)      5\n",
      " 0.005    0.4915(±0.0111)   5.1457(±0.0513)     44.0(± 3.5)      3\n",
      " 0.010    0.4784(±0.0240)   5.1824(±0.1588)     41.3(± 3.8)      3\n",
      " 0.050    0.4579(±0.0178)   5.2068(±0.1233)     41.7(± 2.1)      3\n",
      " 0.100    0.4700(±0.0183)   5.0821(±0.0651)     42.7(± 2.5)      3\n",
      "\n",
      "Layer 2 Cross-Gamma Comparison:\n",
      "Gamma      JS Distance(±std)   Weighted Entropy(±std)   Node Count(±std)   Runs\n",
      "---------------------------------------------------------------------------\n",
      " 0.001    0.5069(±0.0121)   5.0823(±0.1317)    145.8(±10.5)      5\n",
      " 0.005    0.4944(±0.0076)   5.1035(±0.0213)    153.3(±12.1)      3\n",
      " 0.010    0.5006(±0.0078)   5.0835(±0.0670)    152.7(± 4.2)      3\n",
      " 0.050    0.4982(±0.0058)   5.0438(±0.0160)    174.3(±11.1)      3\n",
      " 0.100    0.4913(±0.0041)   5.0317(±0.0376)    183.3(± 6.1)      3\n",
      "======================================================================\n",
      "Summary analysis completed!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Execute summary analysis\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step1/depth3\"\n",
    "print(\"=\" * 70)\n",
    "print(\"Start aggregating layer statistics for each Gamma value...\")\n",
    "print(\"=\" * 70)\n",
    "aggregate_layer_statistics_by_gamma(base_path)\n",
    "print(\"=\" * 70)\n",
    "print(\"Summary analysis completed!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ed95526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated mean values per layer for each parameter set: all_params_layer_mean.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step1/depth3\"\n",
    "pattern = os.path.join(base_path, \"**\", \"result_layers.csv\")\n",
    "files = glob.glob(pattern, recursive=True)\n",
    "\n",
    "all_rows = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "    # Add parameter information\n",
    "    for col in ['depth', 'gamma', 'eta', 'alpha']:\n",
    "        if col not in df.columns:\n",
    "            # Extract parameters from the file path\n",
    "            folder = os.path.dirname(file)\n",
    "            if f\"{col}_\" in folder:\n",
    "                try:\n",
    "                    value = float(folder.split(f\"{col}_\")[1].split(\"_\")[0])\n",
    "                except:\n",
    "                    value = None\n",
    "                df[col] = value\n",
    "            else:\n",
    "                df[col] = None\n",
    "    all_rows.append(df)\n",
    "\n",
    "merged = pd.concat(all_rows, ignore_index=True)\n",
    "\n",
    "# Group by parameter set and layer to calculate mean and standard deviation\n",
    "group_cols = ['depth', 'gamma', 'eta', 'alpha', 'layer']\n",
    "summary = merged.groupby(group_cols).agg({\n",
    "    'entropy_wavg': ['mean', 'std'],\n",
    "    'distinctiveness_wavg_jsd': ['mean', 'std'],\n",
    "    'nodes_in_layer': ['mean', 'std'],\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten the multi-level column names\n",
    "summary.columns = ['_'.join(col).strip('_') for col in summary.columns]\n",
    "\n",
    "summary.to_csv(os.path.join(base_path, \"all_params_layer_mean.csv\"), index=False)\n",
    "print(\"Generated mean values per layer for each parameter set: all_params_layer_mean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628ef831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_parent_child_jsd(base_path=\".\", eta=0.1):\n",
    "    \"\"\"\n",
    "    Calculate Jensen-Shannon distances between parent and child nodes across layers.\n",
    "    \n",
    "    Parameters:\n",
    "    base_path: str, root directory path\n",
    "    eta: float, Dirichlet smoothing parameter\n",
    "    \"\"\"\n",
    "    # Find all iteration_node_word_distributions.csv files\n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_node_word_distributions.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        print(f\"\\nProcessing file: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Read word distribution data\n",
    "            word_df = pd.read_csv(file_path)\n",
    "            word_df.columns = [col.strip(\"'\\\" \") for col in word_df.columns]\n",
    "            \n",
    "            # Get data from the last iteration\n",
    "            max_iteration = word_df['iteration'].max()\n",
    "            last_iteration_data = word_df[word_df['iteration'] == max_iteration]\n",
    "            \n",
    "            # Get the full vocabulary\n",
    "            all_words = sorted(list(last_iteration_data['word'].dropna().unique()))\n",
    "            print(f\"Vocabulary size: {len(all_words)}\")\n",
    "            \n",
    "            # Read the entropy file to get hierarchy information\n",
    "            entropy_file = os.path.join(folder_path, 'corrected_renyi_entropy.csv')\n",
    "            if not os.path.exists(entropy_file):\n",
    "                print(f\"Warning: Entropy file not found: {entropy_file}\")\n",
    "                continue\n",
    "                \n",
    "            entropy_df = pd.read_csv(entropy_file)\n",
    "            \n",
    "            # Build probability distribution for each node\n",
    "            node_distributions = {}\n",
    "            \n",
    "            for node_id in entropy_df['node_id'].unique():\n",
    "                # Get the word distribution for this node\n",
    "                node_words = last_iteration_data[last_iteration_data['node_id'] == node_id]\n",
    "                \n",
    "                # Initialize count vector\n",
    "                counts = np.zeros(len(all_words))\n",
    "                word_to_idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "                \n",
    "                # Fill in actual counts\n",
    "                for _, row in node_words.iterrows():\n",
    "                    word = row['word']\n",
    "                    if pd.notna(word) and word in word_to_idx:\n",
    "                        counts[word_to_idx[word]] = row['count']\n",
    "                \n",
    "                # Add Dirichlet smoothing\n",
    "                smoothed_counts = counts + eta\n",
    "                \n",
    "                # Calculate probability distribution\n",
    "                probabilities = smoothed_counts / np.sum(smoothed_counts)\n",
    "                node_distributions[node_id] = probabilities\n",
    "            \n",
    "            # Calculate parent-child JSD\n",
    "            parent_child_distances = []\n",
    "            layer_transitions = []\n",
    "            \n",
    "            for _, row in entropy_df.iterrows():\n",
    "                child_id = row['node_id']\n",
    "                parent_id = row['parent_id']\n",
    "                \n",
    "                if pd.notna(parent_id) and parent_id in node_distributions and child_id in node_distributions:\n",
    "                    parent_id = int(parent_id)\n",
    "                    \n",
    "                    # Get parent layer info\n",
    "                    parent_info = entropy_df[entropy_df['node_id'] == parent_id].iloc[0]\n",
    "                    parent_layer = parent_info['layer']\n",
    "                    parent_doc_count = parent_info['document_count']\n",
    "                    \n",
    "                    child_layer = row['layer']\n",
    "                    child_doc_count = row['document_count']\n",
    "                    \n",
    "                    # Calculate JSD\n",
    "                    p_parent = node_distributions[parent_id]\n",
    "                    p_child = node_distributions[child_id]\n",
    "                    js_distance = jensen_shannon_distance(p_parent, p_child)\n",
    "                    \n",
    "                    parent_child_distances.append({\n",
    "                        'parent_id': parent_id,\n",
    "                        'parent_layer': parent_layer,\n",
    "                        'child_id': child_id,\n",
    "                        'child_layer': child_layer,\n",
    "                        'js_distance': js_distance,\n",
    "                        'parent_doc_count': parent_doc_count,\n",
    "                        'child_doc_count': child_doc_count\n",
    "                    })\n",
    "            \n",
    "            print(f\"Calculated {len(parent_child_distances)} parent-child JSD pairs\")\n",
    "            \n",
    "            # Calculate layer transition statistics\n",
    "            if parent_child_distances:\n",
    "                pc_df = pd.DataFrame(parent_child_distances)\n",
    "                \n",
    "                # Group by layer transition (parent_layer -> child_layer)\n",
    "                transition_stats = pc_df.groupby(['parent_layer', 'child_layer']).agg({\n",
    "                    'js_distance': ['mean', 'std', 'count'],\n",
    "                    'parent_doc_count': 'sum',\n",
    "                    'child_doc_count': 'sum'\n",
    "                }).round(4)\n",
    "                \n",
    "                # Flatten column names\n",
    "                transition_stats.columns = ['_'.join(col).strip() for col in transition_stats.columns]\n",
    "                transition_stats = transition_stats.reset_index()\n",
    "                \n",
    "                # Rename columns\n",
    "                transition_stats = transition_stats.rename(columns={\n",
    "                    'js_distance_mean': 'avg_js_distance',\n",
    "                    'js_distance_std': 'js_distance_std',\n",
    "                    'js_distance_count': 'pair_count',\n",
    "                    'parent_doc_count_sum': 'total_parent_docs',\n",
    "                    'child_doc_count_sum': 'total_child_docs'\n",
    "                })\n",
    "                \n",
    "                layer_transitions = transition_stats.to_dict('records')\n",
    "            \n",
    "            # Save detailed parent-child JSD results\n",
    "            if parent_child_distances:\n",
    "                pc_df = pd.DataFrame(parent_child_distances)\n",
    "                output_path = os.path.join(folder_path, 'parent_child_js_distances.csv')\n",
    "                pc_df.to_csv(output_path, index=False)\n",
    "                print(f\"Saved detailed parent-child JS distance results to: {output_path}\")\n",
    "            \n",
    "            # Save layer transition summary\n",
    "            if layer_transitions:\n",
    "                lt_df = pd.DataFrame(layer_transitions)\n",
    "                lt_output_path = os.path.join(folder_path, 'layer_transition_js_distances.csv')\n",
    "                lt_df.to_csv(lt_output_path, index=False)\n",
    "                print(f\"Saved layer transition JS distance summary to: {lt_output_path}\")\n",
    "                \n",
    "                # Print transition statistics\n",
    "                print(f\"\\nLayer Transition Statistics:\")\n",
    "                for transition in layer_transitions:\n",
    "                    parent_layer = int(transition['parent_layer'])\n",
    "                    child_layer = int(transition['child_layer'])\n",
    "                    avg_jsd = transition['avg_js_distance']\n",
    "                    std_jsd = transition['js_distance_std']\n",
    "                    pair_count = int(transition['pair_count'])\n",
    "                    print(f\"  Layer {parent_layer} -> Layer {child_layer}: JSD={avg_jsd:.4f}(±{std_jsd:.4f}), pairs={pair_count}\")\n",
    "            \n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "            print(\"Detailed traceback:\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "def jensen_shannon_distance(p, q):\n",
    "    \"\"\"\n",
    "    Calculate the Jensen-Shannon distance between two probability distributions.\n",
    "    \n",
    "    Parameters:\n",
    "    p, q: numpy arrays, probability distributions\n",
    "    \n",
    "    Returns:\n",
    "    float: Jensen-Shannon distance\n",
    "    \"\"\"\n",
    "    # Ensure probability distributions are normalized\n",
    "    p = p / np.sum(p)\n",
    "    q = q / np.sum(q)\n",
    "    \n",
    "    # Calculate the average distribution\n",
    "    m = 0.5 * (p + q)\n",
    "    \n",
    "    # Calculate KL divergence (using natural logarithm)\n",
    "    def kl_divergence(x, y):\n",
    "        # Avoid log(0)\n",
    "        mask = (x > 0) & (y > 0)\n",
    "        if np.sum(mask) == 0:\n",
    "            return 0.0\n",
    "        return np.sum(x[mask] * np.log(x[mask] / y[mask]))\n",
    "    \n",
    "    # Calculate Jensen-Shannon divergence\n",
    "    js_divergence = 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)\n",
    "    \n",
    "    # Convert to distance (square root)\n",
    "    js_distance = np.sqrt(js_divergence)\n",
    "    \n",
    "    return js_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa192f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Start calculating parent-child Jensen-Shannon distances...\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/3/d3_g005_收敛/depth_3_gamma_0.05_run_1/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 204 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/3/d3_g005_收敛/depth_3_gamma_0.05_run_1/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/3/d3_g005_收敛/depth_3_gamma_0.05_run_1/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6144(±0.0317), pairs=40\n",
      "  Layer 1 -> Layer 2: JSD=0.5217(±0.0825), pairs=164\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/3/d3_g005_收敛/depth_3_gamma_0.05_run_2/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 230 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/3/d3_g005_收敛/depth_3_gamma_0.05_run_2/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/3/d3_g005_收敛/depth_3_gamma_0.05_run_2/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6192(±0.0278), pairs=44\n",
      "  Layer 1 -> Layer 2: JSD=0.4986(±0.0999), pairs=186\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/3/d3_g005_收敛/depth_3_gamma_0.05_run_3/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 214 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/3/d3_g005_收敛/depth_3_gamma_0.05_run_3/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/3/d3_g005_收敛/depth_3_gamma_0.05_run_3/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6176(±0.0254), pairs=41\n",
      "  Layer 1 -> Layer 2: JSD=0.5232(±0.0983), pairs=173\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/3/d3_g001_收敛/depth_3_gamma_0.01_run_1/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 191 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/3/d3_g001_收敛/depth_3_gamma_0.01_run_1/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/3/d3_g001_收敛/depth_3_gamma_0.01_run_1/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6168(±0.0235), pairs=43\n",
      "  Layer 1 -> Layer 2: JSD=0.5184(±0.0844), pairs=148\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/3/d3_g001_收敛/depth_3_gamma_0.01_run_2/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 198 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/3/d3_g001_收敛/depth_3_gamma_0.01_run_2/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/3/d3_g001_收敛/depth_3_gamma_0.01_run_2/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6172(±0.0203), pairs=44\n",
      "  Layer 1 -> Layer 2: JSD=0.4999(±0.0893), pairs=154\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/3/d3_g001_收敛/depth_3_gamma_0.01_run_3/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 193 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/3/d3_g001_收敛/depth_3_gamma_0.01_run_3/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/3/d3_g001_收敛/depth_3_gamma_0.01_run_3/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6120(±0.0376), pairs=37\n",
      "  Layer 1 -> Layer 2: JSD=0.5347(±0.0920), pairs=156\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/3/d3_g0001_2条链收敛/depth_3_gamma_0.001_run_2/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 210 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/3/d3_g0001_2条链收敛/depth_3_gamma_0.001_run_2/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/3/d3_g0001_2条链收敛/depth_3_gamma_0.001_run_2/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6302(±0.0330), pairs=47\n",
      "  Layer 1 -> Layer 2: JSD=0.5341(±0.0831), pairs=163\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/3/d3_g0001_2条链收敛/depth_3_gamma_0.001_run_3/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 191 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/3/d3_g0001_2条链收敛/depth_3_gamma_0.001_run_3/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/3/d3_g0001_2条链收敛/depth_3_gamma_0.001_run_3/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6094(±0.0321), pairs=44\n",
      "  Layer 1 -> Layer 2: JSD=0.5285(±0.1054), pairs=147\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/3/d3_g01_收敛/depth_3_gamma_0.1_run_1/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 218 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/3/d3_g01_收敛/depth_3_gamma_0.1_run_1/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/3/d3_g01_收敛/depth_3_gamma_0.1_run_1/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6212(±0.0227), pairs=40\n",
      "  Layer 1 -> Layer 2: JSD=0.5097(±0.0942), pairs=178\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/3/d3_g01_收敛/depth_3_gamma_0.1_run_2/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 227 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/3/d3_g01_收敛/depth_3_gamma_0.1_run_2/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/3/d3_g01_收敛/depth_3_gamma_0.1_run_2/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6161(±0.0309), pairs=45\n",
      "  Layer 1 -> Layer 2: JSD=0.5166(±0.0931), pairs=182\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/3/d3_g01_收敛/depth_3_gamma_0.1_run_3/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 233 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/3/d3_g01_收敛/depth_3_gamma_0.1_run_3/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/3/d3_g01_收敛/depth_3_gamma_0.1_run_3/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6260(±0.0359), pairs=43\n",
      "  Layer 1 -> Layer 2: JSD=0.5303(±0.0951), pairs=190\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/3/d3_g0005_收敛/depth_3_gamma_0.005_run_1/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 184 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/3/d3_g0005_收敛/depth_3_gamma_0.005_run_1/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/3/d3_g0005_收敛/depth_3_gamma_0.005_run_1/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6217(±0.0256), pairs=40\n",
      "  Layer 1 -> Layer 2: JSD=0.5212(±0.0825), pairs=144\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/3/d3_g0005_收敛/depth_3_gamma_0.005_run_2/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 195 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/3/d3_g0005_收敛/depth_3_gamma_0.005_run_2/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/3/d3_g0005_收敛/depth_3_gamma_0.005_run_2/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6213(±0.0215), pairs=46\n",
      "  Layer 1 -> Layer 2: JSD=0.5175(±0.0790), pairs=149\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/3/d3_g0005_收敛/depth_3_gamma_0.005_run_3/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 213 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/3/d3_g0005_收敛/depth_3_gamma_0.005_run_3/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/3/d3_g0005_收敛/depth_3_gamma_0.005_run_3/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6205(±0.0259), pairs=46\n",
      "  Layer 1 -> Layer 2: JSD=0.5167(±0.0893), pairs=167\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/3/d3_g0001_收敛/depth_3_gamma_0.001_run_1/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 177 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/3/d3_g0001_收敛/depth_3_gamma_0.001_run_1/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/3/d3_g0001_收敛/depth_3_gamma_0.001_run_1/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6130(±0.0276), pairs=41\n",
      "  Layer 1 -> Layer 2: JSD=0.5128(±0.0717), pairs=136\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/3/d3_g0001_收敛/depth_3_gamma_0.001_run_2/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 177 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/3/d3_g0001_收敛/depth_3_gamma_0.001_run_2/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/3/d3_g0001_收敛/depth_3_gamma_0.001_run_2/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6146(±0.0150), pairs=38\n",
      "  Layer 1 -> Layer 2: JSD=0.5163(±0.0842), pairs=139\n",
      "==================================================\n",
      "\n",
      "Processing file: /Volumes/My Passport/收敛结果/step1/3/d3_g0001_收敛/depth_3_gamma_0.001_run_3/iteration_node_word_distributions.csv\n",
      "Vocabulary size: 1490\n",
      "Calculated 185 parent-child JSD pairs\n",
      "Saved detailed parent-child JS distance results to: /Volumes/My Passport/收敛结果/step1/3/d3_g0001_收敛/depth_3_gamma_0.001_run_3/parent_child_js_distances.csv\n",
      "Saved layer transition JS distance summary to: /Volumes/My Passport/收敛结果/step1/3/d3_g0001_收敛/depth_3_gamma_0.001_run_3/layer_transition_js_distances.csv\n",
      "\n",
      "Layer Transition Statistics:\n",
      "  Layer 0 -> Layer 1: JSD=0.6103(±0.0290), pairs=41\n",
      "  Layer 1 -> Layer 2: JSD=0.5216(±0.0741), pairs=144\n",
      "==================================================\n",
      "==================================================\n",
      "Parent-child Jensen-Shannon distance calculation completed!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Main function: Calculate parent-child Jensen-Shannon distances\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step1/depth3\"  # Root directory\n",
    "eta = 0.1  # Dirichlet smoothing parameter\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Start calculating parent-child Jensen-Shannon distances...\")\n",
    "print(\"=\" * 50)\n",
    "calculate_parent_child_jsd(base_path, eta)\n",
    "print(\"=\" * 50)\n",
    "print(\"Parent-child Jensen-Shannon distance calculation completed!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9144ef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_parent_child_jsd_by_gamma(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    Aggregate parent-child JS distance data by gamma value across all runs,\n",
    "    with proper weighted averages by individual parent-child pairs' child document counts relative to total 970 documents.\n",
    "    \"\"\"\n",
    "    # Find all parent_child_js_distances.csv files (detailed data)\n",
    "    pattern = os.path.join(base_path, \"**\", \"parent_child_js_distances.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    all_detailed_data = []\n",
    "    \n",
    "    # 假设总文档数为970（可以从数据中动态获取）\n",
    "    TOTAL_DOCUMENTS = 970\n",
    "    \n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        parent_folder = os.path.dirname(folder_path)\n",
    "        \n",
    "        # Extract gamma value from the folder name\n",
    "        if 'gamma_0.001' in folder_name:\n",
    "            if '2chains' in parent_folder:\n",
    "                gamma = 0.001\n",
    "                experiment_type = '2chains'\n",
    "            else:\n",
    "                gamma = 0.001\n",
    "                experiment_type = 'single'\n",
    "        elif 'gamma_0.005' in folder_name:\n",
    "            gamma = 0.005\n",
    "            experiment_type = 'single'\n",
    "        elif 'gamma_0.01' in folder_name:\n",
    "            gamma = 0.01\n",
    "            experiment_type = 'single'\n",
    "        elif 'gamma_0.05' in folder_name:\n",
    "            gamma = 0.05\n",
    "            experiment_type = 'single'\n",
    "        elif 'gamma_0.1' in folder_name:\n",
    "            gamma = 0.1\n",
    "            experiment_type = 'single'\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Extract run number\n",
    "        run_match = folder_name.split('_run_')\n",
    "        if len(run_match) > 1:\n",
    "            run_id = run_match[1]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Add run and gamma information to each detailed parent-child pair\n",
    "            for _, row in df.iterrows():\n",
    "                all_detailed_data.append({\n",
    "                    'gamma': gamma,\n",
    "                    'experiment_type': experiment_type,\n",
    "                    'run_id': run_id,\n",
    "                    'parent_id': row['parent_id'],\n",
    "                    'parent_layer': row['parent_layer'],\n",
    "                    'child_id': row['child_id'],\n",
    "                    'child_layer': row['child_layer'],\n",
    "                    'layer_transition': f\"{int(row['parent_layer'])}->{int(row['child_layer'])}\",\n",
    "                    'js_distance': row['js_distance'],\n",
    "                    'parent_doc_count': row['parent_doc_count'],\n",
    "                    'child_doc_count': row['child_doc_count']\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_path}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    detailed_df = pd.DataFrame(all_detailed_data)\n",
    "    \n",
    "    if detailed_df.empty:\n",
    "        print(\"No valid detailed parent-child JSD data found\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"Parent-Child JSD Data Collection by GAMMA Value (weighted by child_doc_count/970)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Calculate properly weighted averages by gamma and layer transition\n",
    "    weighted_summary = []\n",
    "    \n",
    "    for (gamma, experiment_type), gamma_group in detailed_df.groupby(['gamma', 'experiment_type']):\n",
    "        for transition, transition_group in gamma_group.groupby('layer_transition'):\n",
    "            # 计算基于全局970文档的加权平均\n",
    "            # 每个child的权重 = child_doc_count / 970\n",
    "            weights = transition_group['child_doc_count'] / TOTAL_DOCUMENTS\n",
    "            \n",
    "            # 加权平均JSD = sum(js_distance * weight) / sum(weight)\n",
    "            # 但这等价于：sum(js_distance * child_doc_count) / sum(child_doc_count)\n",
    "            # 因为970是常数，会被约掉\n",
    "            \n",
    "            # 方法1：使用归一化权重\n",
    "            total_weight = weights.sum()\n",
    "            if total_weight > 0:\n",
    "                weighted_avg_jsd_normalized = (transition_group['js_distance'] * weights).sum() / total_weight\n",
    "            else:\n",
    "                weighted_avg_jsd_normalized = 0.0\n",
    "            \n",
    "            # 方法2：直接使用文档数作为权重（与方法1结果相同）\n",
    "            total_child_docs = transition_group['child_doc_count'].sum()\n",
    "            if total_child_docs > 0:\n",
    "                weighted_avg_jsd_direct = (transition_group['js_distance'] * transition_group['child_doc_count']).sum() / total_child_docs\n",
    "            else:\n",
    "                weighted_avg_jsd_direct = 0.0\n",
    "            \n",
    "            # Calculate simple average for comparison\n",
    "            simple_avg_jsd = transition_group['js_distance'].mean()\n",
    "            \n",
    "            # Get additional statistics\n",
    "            parent_layer = transition_group['parent_layer'].iloc[0]\n",
    "            child_layer = transition_group['child_layer'].iloc[0]\n",
    "            pair_count = len(transition_group)\n",
    "            run_count = len(transition_group['run_id'].unique())\n",
    "            runs_included = ', '.join(sorted(transition_group['run_id'].unique()))\n",
    "            \n",
    "            # 计算权重占比（相对于970）\n",
    "            weight_proportion = total_child_docs / TOTAL_DOCUMENTS\n",
    "            \n",
    "            weighted_summary.append({\n",
    "                'gamma': gamma,\n",
    "                'experiment_type': experiment_type,\n",
    "                'layer_transition': transition,\n",
    "                'parent_layer': parent_layer,\n",
    "                'child_layer': child_layer,\n",
    "                'weighted_avg_jsd': weighted_avg_jsd_direct,  # 两种方法结果相同\n",
    "                'simple_avg_jsd': simple_avg_jsd,\n",
    "                'total_child_docs': total_child_docs,\n",
    "                'weight_proportion_of_970': weight_proportion,  # 权重占970的比例\n",
    "                'total_pairs': pair_count,\n",
    "                'run_count': run_count,\n",
    "                'runs_included': runs_included,\n",
    "                'jsd_std': transition_group['js_distance'].std(),\n",
    "                'child_doc_count_mean': transition_group['child_doc_count'].mean(),\n",
    "                'child_doc_count_std': transition_group['child_doc_count'].std()\n",
    "            })\n",
    "    \n",
    "    # Save detailed individual pair data\n",
    "    detailed_output_path = os.path.join(base_path, 'all_runs_detailed_parent_child_jsd_by_gamma.csv')\n",
    "    detailed_df.to_csv(detailed_output_path, index=False)\n",
    "    print(f\"Saved complete detailed parent-child JSD data to: {detailed_output_path}\")\n",
    "    \n",
    "    # Save weighted summary\n",
    "    weighted_df = pd.DataFrame(weighted_summary)\n",
    "    weighted_output_path = os.path.join(base_path, 'properly_weighted_parent_child_jsd_by_gamma.csv')\n",
    "    weighted_df.to_csv(weighted_output_path, index=False)\n",
    "    print(f\"Saved properly weighted parent-child JSD summary to: {weighted_output_path}\")\n",
    "    \n",
    "    # Print summary statistics for each gamma\n",
    "    for gamma in sorted(detailed_df['gamma'].unique()):\n",
    "        gamma_data = detailed_df[detailed_df['gamma'] == gamma]\n",
    "        experiment_types = gamma_data['experiment_type'].unique()\n",
    "        \n",
    "        for exp_type in experiment_types:\n",
    "            exp_data = gamma_data[gamma_data['experiment_type'] == exp_type]\n",
    "            run_count = len(exp_data['run_id'].unique())\n",
    "            transition_count = len(exp_data['layer_transition'].unique())\n",
    "            total_pairs = len(exp_data)\n",
    "            \n",
    "            print(f\"\\nGamma {gamma:.3f} ({'2 chains' if exp_type == '2chains' else 'single chain'}):\")\n",
    "            print(f\"  - Runs: {run_count}\")\n",
    "            print(f\"  - Layer transitions: {transition_count}\")\n",
    "            print(f\"  - Total parent-child pairs: {total_pairs}\")\n",
    "            \n",
    "            # Show transition summary with properly weighted averages\n",
    "            print(f\"  - Layer transition summary (weighted by child_doc_count/970):\")\n",
    "            gamma_weighted = weighted_df[(weighted_df['gamma'] == gamma) & (weighted_df['experiment_type'] == exp_type)]\n",
    "            \n",
    "            for _, row in gamma_weighted.iterrows():\n",
    "                transition = row['layer_transition']\n",
    "                weighted_jsd = row['weighted_avg_jsd']\n",
    "                simple_jsd = row['simple_avg_jsd']\n",
    "                total_child_docs = int(row['total_child_docs'])\n",
    "                weight_prop = row['weight_proportion_of_970']\n",
    "                total_pairs_trans = int(row['total_pairs'])\n",
    "                run_count_trans = int(row['run_count'])\n",
    "                jsd_std = row['jsd_std']\n",
    "                \n",
    "                print(f\"    {transition}: Weighted JSD={weighted_jsd:.4f}, Simple JSD={simple_jsd:.4f}(±{jsd_std:.4f})\")\n",
    "                print(f\"      Child docs={total_child_docs}, Weight prop={weight_prop:.3f}, pairs={total_pairs_trans}, runs={run_count_trans}\")\n",
    "                \n",
    "                # Show the difference between weighted and simple averages\n",
    "                diff = abs(weighted_jsd - simple_jsd)\n",
    "                if diff > 0.0001:  # Only show if there's a meaningful difference\n",
    "                    print(f\"      -> Difference: {diff:.4f} (Weighted vs Simple)\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"Data collection with properly weighted averages (relative to 970 total documents) completed!\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be0991e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Start collecting parent-child JSD data with weighted averages for each Gamma value...\n",
      "======================================================================\n",
      "======================================================================\n",
      "Parent-Child JSD Data Collection by GAMMA Value (weighted by child_doc_count/970)\n",
      "======================================================================\n",
      "Saved complete detailed parent-child JSD data to: /Volumes/My Passport/收敛结果/step1/depth3/all_runs_detailed_parent_child_jsd_by_gamma.csv\n",
      "Saved properly weighted parent-child JSD summary to: /Volumes/My Passport/收敛结果/step1/depth3/properly_weighted_parent_child_jsd_by_gamma.csv\n",
      "\n",
      "Gamma 0.001 (single chain):\n",
      "  - Runs: 3\n",
      "  - Layer transitions: 2\n",
      "  - Total parent-child pairs: 940\n",
      "  - Layer transition summary (weighted by child_doc_count/970):\n",
      "    0->1: Weighted JSD=0.5491, Simple JSD=0.6158(±0.0293)\n",
      "      Child docs=4850, Weight prop=5.000, pairs=211, runs=3\n",
      "      -> Difference: 0.0667 (Weighted vs Simple)\n",
      "    1->2: Weighted JSD=0.5717, Simple JSD=0.5231(±0.0849)\n",
      "      Child docs=4850, Weight prop=5.000, pairs=729, runs=3\n",
      "      -> Difference: 0.0485 (Weighted vs Simple)\n",
      "\n",
      "Gamma 0.005 (single chain):\n",
      "  - Runs: 3\n",
      "  - Layer transitions: 2\n",
      "  - Total parent-child pairs: 592\n",
      "  - Layer transition summary (weighted by child_doc_count/970):\n",
      "    0->1: Weighted JSD=0.5624, Simple JSD=0.6211(±0.0242)\n",
      "      Child docs=2910, Weight prop=3.000, pairs=132, runs=3\n",
      "      -> Difference: 0.0588 (Weighted vs Simple)\n",
      "    1->2: Weighted JSD=0.5774, Simple JSD=0.5184(±0.0838)\n",
      "      Child docs=2910, Weight prop=3.000, pairs=460, runs=3\n",
      "      -> Difference: 0.0590 (Weighted vs Simple)\n",
      "\n",
      "Gamma 0.010 (single chain):\n",
      "  - Runs: 3\n",
      "  - Layer transitions: 2\n",
      "  - Total parent-child pairs: 582\n",
      "  - Layer transition summary (weighted by child_doc_count/970):\n",
      "    0->1: Weighted JSD=0.5365, Simple JSD=0.6155(±0.0274)\n",
      "      Child docs=2910, Weight prop=3.000, pairs=124, runs=3\n",
      "      -> Difference: 0.0790 (Weighted vs Simple)\n",
      "    1->2: Weighted JSD=0.5730, Simple JSD=0.5177(±0.0897)\n",
      "      Child docs=2910, Weight prop=3.000, pairs=458, runs=3\n",
      "      -> Difference: 0.0553 (Weighted vs Simple)\n",
      "\n",
      "Gamma 0.050 (single chain):\n",
      "  - Runs: 3\n",
      "  - Layer transitions: 2\n",
      "  - Total parent-child pairs: 648\n",
      "  - Layer transition summary (weighted by child_doc_count/970):\n",
      "    0->1: Weighted JSD=0.5437, Simple JSD=0.6171(±0.0282)\n",
      "      Child docs=2910, Weight prop=3.000, pairs=125, runs=3\n",
      "      -> Difference: 0.0734 (Weighted vs Simple)\n",
      "    1->2: Weighted JSD=0.5719, Simple JSD=0.5140(±0.0947)\n",
      "      Child docs=2910, Weight prop=3.000, pairs=523, runs=3\n",
      "      -> Difference: 0.0579 (Weighted vs Simple)\n",
      "\n",
      "Gamma 0.100 (single chain):\n",
      "  - Runs: 3\n",
      "  - Layer transitions: 2\n",
      "  - Total parent-child pairs: 678\n",
      "  - Layer transition summary (weighted by child_doc_count/970):\n",
      "    0->1: Weighted JSD=0.5371, Simple JSD=0.6210(±0.0306)\n",
      "      Child docs=2910, Weight prop=3.000, pairs=128, runs=3\n",
      "      -> Difference: 0.0839 (Weighted vs Simple)\n",
      "    1->2: Weighted JSD=0.5779, Simple JSD=0.5191(±0.0944)\n",
      "      Child docs=2910, Weight prop=3.000, pairs=550, runs=3\n",
      "      -> Difference: 0.0588 (Weighted vs Simple)\n",
      "\n",
      "======================================================================\n",
      "Data collection with properly weighted averages (relative to 970 total documents) completed!\n",
      "======================================================================\n",
      "======================================================================\n",
      "Parent-child JSD data collection with weighted averages completed!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Execute parent-child JSD data collection with weighted averages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step1/depth3\"\n",
    "print(\"=\" * 70)\n",
    "print(\"Start collecting parent-child JSD data with weighted averages for each Gamma value...\")\n",
    "print(\"=\" * 70)\n",
    "aggregate_parent_child_jsd_by_gamma(base_path)\n",
    "print(\"=\" * 70)\n",
    "print(\"Parent-child JSD data collection with weighted averages completed!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
