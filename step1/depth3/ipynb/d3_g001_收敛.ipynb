{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2044b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 0. set-up part:  import necessary libraries and set up environment \"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from threading import Thread\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "import json\n",
    "import cProfile\n",
    "\n",
    "# download nltk data once time\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "#  chinese character support in matplotlib\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS' 'SimHei' 'DejaVu Sans']  \n",
    "plt.rcParams['axes.unicode_minus'] = False  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a654762",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 1.1 Data Preprocessing: load data, clean text, lemmatization, remove low-frequency words\"\"\"\n",
    "\n",
    "# Map POS tags to WordNet formatÔºå Penn Treebank annotation: fine-grained (45 tags), WordNet annotation: coarse-grained (4 tags: a, v, n, r)\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'  # ÂΩ¢ÂÆπËØç\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'  # Âä®ËØç\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n'  # ÂêçËØç\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'  # ÂâØËØç\n",
    "    else:\n",
    "        return 'n'  # ÈªòËÆ§ÂêçËØç\n",
    "\n",
    "# Text cleaning and lemmatization preprocessing function\n",
    "def clean_and_lemmatize(text):\n",
    "    if pd.isnull(text):\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove non-alphabetic characters using regex\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmatized = [lemmatizer.lemmatize(w, get_wordnet_pos(pos)) for w, pos in pos_tags]\n",
    "    return lemmatized  \n",
    "\n",
    "#-----------------Load data----------------\n",
    "data = pd.read_excel('./data/raw/papers_CM.xlsx', usecols=['PaperID', 'Abstract', 'Keywords', 'Year'])\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# clean and lemmatize the abstracts\n",
    "data['Lemmatized_Tokens'] = data['Abstract'].apply(clean_and_lemmatize)\n",
    "\n",
    "# count word frequencies\n",
    "all_tokens = [word for tokens in data['Lemmatized_Tokens'] for word in tokens]\n",
    "word_counts = Counter(all_tokens)\n",
    "\n",
    "# set a minimum frequency threshold for valid words\n",
    "min_freq = 10\n",
    "valid_words = set([word for word, freq in word_counts.items() if freq >= min_freq])\n",
    "\n",
    "# remove rare words based on frequency threshold\n",
    "def remove_rare_words(tokens):\n",
    "    return [word for word in tokens if word in valid_words]\n",
    "\n",
    "data['Filtered_Tokens'] = data['Lemmatized_Tokens'].apply(remove_rare_words)\n",
    "\n",
    "# join tokens back into cleaned abstracts\n",
    "data['Cleaned_Abstract'] = data['Filtered_Tokens'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "# create a cleaned DataFrame with relevant columns\n",
    "cleaned_data = data[['PaperID', 'Year', 'Cleaned_Abstract']]\n",
    "cleaned_data = cleaned_data[~(cleaned_data['PaperID'] == 57188)] # this paper has no abstract\n",
    "cleaned_data = cleaned_data.reset_index(drop=True) \n",
    "cleaned_data.insert(0, 'Document_ID', range(len(cleaned_data))) \n",
    "abstract_list = cleaned_data['Cleaned_Abstract'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "corpus = {doc_id: abstract_list for doc_id, abstract_list in enumerate(abstract_list)}\n",
    "# cleaned_data.to_csv('./data/processed/cleaned_data.xlsx', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c0e112a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Corpus word frequency information:\n",
      "total words: 83,202\n",
      "num of unique words: 1,490\n",
      "average num of words for each doc: 55.84\n",
      "standard deviation of word frequency among docs: 104.60\n",
      "\n",
      "üèÜ Top 20 High-frequency words:\n",
      " Rank          Word  Frequency  Percentage  Cumulative_Percentage  Document_Count  Document_Percentage Frequency_Category\n",
      "    1        method       1598      1.9206                 1.9206             654              67.4227               High\n",
      "    2         model       1554      1.8677                 3.7883             579              59.6907               High\n",
      "    3       element       1112      1.3365                 5.1248             513              52.8866               High\n",
      "    4           use        999      1.2007                 6.3255             572              58.9691               High\n",
      "    5       propose        823      0.9892                 7.3147             510              52.5773             Medium\n",
      "    6     numerical        799      0.9603                 8.2750             535              55.1546             Medium\n",
      "    7       problem        774      0.9303                 9.2053             442              45.5670             Medium\n",
      "    8        finite        712      0.8557                10.0610             447              46.0825             Medium\n",
      "    9      material        695      0.8353                10.8963             339              34.9485             Medium\n",
      "   10       present        642      0.7716                11.6679             468              48.2474             Medium\n",
      "   11      approach        608      0.7308                12.3987             368              37.9381             Medium\n",
      "   12   formulation        575      0.6911                13.0898             305              31.4433             Medium\n",
      "   13          base        554      0.6658                13.7556             410              42.2680             Medium\n",
      "   14        result        526      0.6322                14.3878             402              41.4433             Medium\n",
      "   15 computational        486      0.5841                14.9719             333              34.3299             Medium\n",
      "   16      analysis        442      0.5312                15.5031             256              26.3918             Medium\n",
      "   17      solution        415      0.4988                16.0019             285              29.3814             Medium\n",
      "   18    simulation        405      0.4868                16.4887             261              26.9072             Medium\n",
      "   19      equation        404      0.4856                16.9743             251              25.8763             Medium\n",
      "   20          mesh        396      0.4760                17.4503             194              20.0000             Medium\n",
      "\n",
      "üìà Word Rank Information:\n",
      "Frequency_Category\n",
      "Low       1264\n",
      "Medium     222\n",
      "High         4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìã create doc-words matrix, shape of matrix is : (970, 1003)\n",
      "\n",
      "üìä Corpus detailed statistics info:\n",
      "==================================================\n",
      "\n",
      "BASIC_STATS:\n",
      "  total_documents: 970\n",
      "  total_words: 83202\n",
      "  unique_words: 1490\n",
      "  vocabulary_richness: 0.0179\n",
      "  average_doc_length: 85.7753\n",
      "  median_doc_length: 81.0000\n",
      "  min_doc_length: 4\n",
      "  max_doc_length: 216\n",
      "  std_doc_length: 26.3635\n",
      "\n",
      "FREQUENCY_DISTRIBUTION:\n",
      "  words_appearing_once: 0\n",
      "  words_appearing_2_5_times: 0\n",
      "  words_appearing_6_20_times: 608\n",
      "  words_appearing_more_than_20: 882\n",
      "\n",
      "TOP_WORDS:\n",
      "  Top words:\n",
      "    method: 1598\n",
      "    model: 1554\n",
      "    element: 1112\n",
      "    use: 999\n",
      "    propose: 823\n",
      "    numerical: 799\n",
      "    problem: 774\n",
      "    finite: 712\n",
      "    material: 695\n",
      "    present: 642\n",
      "    approach: 608\n",
      "    formulation: 575\n",
      "    base: 554\n",
      "    result: 526\n",
      "    computational: 486\n",
      "    analysis: 442\n",
      "    solution: 415\n",
      "    simulation: 405\n",
      "    equation: 404\n",
      "    mesh: 396\n",
      "\n",
      "üíæ save statistics tables...\n",
      "‚úÖ save <- corpus_word_frequency_table.csv\n",
      "‚úÖ  save <- document_word_matrix.csv\n",
      "‚úÖ save <- corpus_statistics_summary.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 1.2 Corpus Analysis: create corpus word frequency table, document-word matrix, and generate corpus statistics \"\"\"\n",
    "\n",
    "# Create corpus word frequency table\n",
    "def create_corpus_word_frequency_table(corpus):\n",
    "    # count all words in the corpus\n",
    "    all_words = [word for doc in corpus.values() for word in doc]\n",
    "    word_freq_counter = Counter(all_words)\n",
    "    \n",
    "    # create a DataFrame for word frequencies\n",
    "    word_freq_df = pd.DataFrame([\n",
    "        {'Word': word, 'Frequency': freq}\n",
    "        for word, freq in word_freq_counter.items()\n",
    "    ]).sort_values('Frequency', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # add statistics like total words, unique words, and percentages\n",
    "    total_words = len(all_words)\n",
    "    unique_words = len(word_freq_counter)\n",
    "    \n",
    "    word_freq_df['Percentage'] = (word_freq_df['Frequency'] / total_words * 100).round(4)\n",
    "    word_freq_df['Cumulative_Percentage'] = word_freq_df['Percentage'].cumsum().round(4)\n",
    "    \n",
    "    # add word distribution across documents like document count and percentage\n",
    "    word_doc_count = {}\n",
    "    for word in word_freq_counter.keys():\n",
    "        doc_count = sum(1 for doc in corpus.values() if word in doc)\n",
    "        word_doc_count[word] = doc_count\n",
    "    \n",
    "    word_freq_df['Document_Count'] = word_freq_df['Word'].map(word_doc_count)\n",
    "    word_freq_df['Document_Percentage'] = (word_freq_df['Document_Count'] / len(corpus) * 100).round(4)\n",
    "    \n",
    "    # categorize frequency into High, Medium, Low, Very Low\n",
    "    def categorize_frequency(freq, total):\n",
    "        if freq >= total * 0.01:  # >1%\n",
    "            return 'High'\n",
    "        elif freq >= total * 0.001:  # 0.1%-1%\n",
    "            return 'Medium'\n",
    "        elif freq >= total * 0.0001:  # 0.01%-0.1%\n",
    "            return 'Low'\n",
    "        else:\n",
    "            return 'Very Low'\n",
    "    \n",
    "    word_freq_df['Frequency_Category'] = word_freq_df['Frequency'].apply(\n",
    "        lambda x: categorize_frequency(x, total_words)\n",
    "    )\n",
    "    \n",
    "    # add ranking of words based on frequency\n",
    "    word_freq_df['Rank'] = range(1, len(word_freq_df) + 1)\n",
    "    \n",
    "    # return DataFrame with selected columns in a specific order\n",
    "    word_freq_df = word_freq_df[[\n",
    "        'Rank', 'Word', 'Frequency', 'Percentage', 'Cumulative_Percentage',\n",
    "        'Document_Count', 'Document_Percentage', 'Frequency_Category'\n",
    "    ]]\n",
    "    \n",
    "    return word_freq_df\n",
    "\n",
    "# Create document-word matrix\n",
    "def create_document_word_matrix(corpus, cleaned_data=None):\n",
    "    # get all unique words (sorted by frequency, keeping only the top 1000 most common words)\n",
    "    all_words = [word for doc in corpus.values() for word in doc]\n",
    "    word_counter = Counter(all_words)\n",
    "    top_words = [word for word, _ in word_counter.most_common(1000)]\n",
    "    \n",
    "    # create a document-word matrix, where each row corresponds to a document and each column corresponds to a word\n",
    "    # the value is the frequency of that word in the document\n",
    "    doc_word_matrix = []\n",
    "    \n",
    "    for doc_id, doc in corpus.items():\n",
    "        doc_counter = Counter(doc)\n",
    "        row = [doc_counter.get(word, 0) for word in top_words]\n",
    "        doc_word_matrix.append(row)\n",
    "    \n",
    "    # create DataFrame from the document-word matrix\n",
    "    df = pd.DataFrame(doc_word_matrix, columns=top_words)\n",
    "    \n",
    "    # insert Document_ID and other metadata if available\n",
    "    if cleaned_data is not None and len(cleaned_data) == len(corpus):\n",
    "        df.insert(0, 'Document_ID', range(len(corpus)))\n",
    "        df.insert(1, 'PaperID', cleaned_data['PaperID'].values)\n",
    "        df.insert(2, 'Year', cleaned_data['Year'].values)\n",
    "    else:\n",
    "        df.insert(0, 'Document_ID', range(len(corpus)))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate corpus statistics like total documents, total words, unique words, vocabulary richness, average document length, etc.\n",
    "def generate_corpus_statistics(corpus, cleaned_data=None):\n",
    "\n",
    "    # word frequency statistics\n",
    "    total_docs = len(corpus)\n",
    "    all_words = [word for doc in corpus.values() for word in doc]\n",
    "    total_words = len(all_words)\n",
    "    unique_words = len(set(all_words))\n",
    "    \n",
    "    # document length statistics\n",
    "    doc_lengths = [len(doc) for doc in corpus.values()]\n",
    "    \n",
    "    # count word frequencies and return statistics\n",
    "    word_counter = Counter(all_words)\n",
    "    \n",
    "    statistics = {\n",
    "        'basic_stats': {\n",
    "            'total_documents': total_docs,\n",
    "            'total_words': total_words,\n",
    "            'unique_words': unique_words,\n",
    "            'vocabulary_richness': unique_words / total_words,\n",
    "            'average_doc_length': np.mean(doc_lengths),\n",
    "            'median_doc_length': np.median(doc_lengths),\n",
    "            'min_doc_length': min(doc_lengths),\n",
    "            'max_doc_length': max(doc_lengths),\n",
    "            'std_doc_length': np.std(doc_lengths)\n",
    "        },\n",
    "        'frequency_distribution': {\n",
    "            'words_appearing_once': sum(1 for freq in word_counter.values() if freq == 1),\n",
    "            'words_appearing_2_5_times': sum(1 for freq in word_counter.values() if 2 <= freq <= 5),\n",
    "            'words_appearing_6_20_times': sum(1 for freq in word_counter.values() if 6 <= freq <= 20),\n",
    "            'words_appearing_more_than_20': sum(1 for freq in word_counter.values() if freq > 20),\n",
    "        },\n",
    "        'top_words': word_counter.most_common(20)\n",
    "    }\n",
    "    \n",
    "    return statistics\n",
    "\n",
    "#-------------------create word frequency summary-----------------\n",
    "word_freq_table = create_corpus_word_frequency_table(corpus)\n",
    "\n",
    "print(f\"\\nüìä Corpus word frequency information:\")\n",
    "print(f\"total words: {word_freq_table['Frequency'].sum():,}\")\n",
    "print(f\"num of unique words: {len(word_freq_table):,}\")\n",
    "print(f\"average num of words for each doc: {word_freq_table['Frequency'].mean():.2f}\")\n",
    "print(f\"standard deviation of word frequency among docs: {word_freq_table['Frequency'].std():.2f}\")\n",
    "\n",
    "print(f\"\\nüèÜ Top 20 High-frequency words:\")\n",
    "print(word_freq_table.head(20).to_string(index=False))\n",
    "\n",
    "print(f\"\\nüìà Word Rank Information:\")\n",
    "freq_category_stats = word_freq_table['Frequency_Category'].value_counts()\n",
    "print(freq_category_stats)\n",
    "\n",
    "#-------------------create document-word matrix and generate corpus statistics----------------\n",
    "# create document-word matrix (optional, suitable for smaller datasets)\n",
    "\n",
    "doc_word_matrix = create_document_word_matrix(corpus, cleaned_data)\n",
    "print(f\"\\nüìã create doc-words matrix, shape of matrix is : {doc_word_matrix.shape}\")\n",
    "\n",
    "# generate detailed statistics\n",
    "corpus_stats = generate_corpus_statistics(corpus, cleaned_data)\n",
    "\n",
    "print(f\"\\nüìä Corpus detailed statistics info:\")\n",
    "print(\"=\" * 50)\n",
    "for category, stats in corpus_stats.items():\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    if isinstance(stats, dict):\n",
    "        for key, value in stats.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"  {key}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"  {key}: {value}\")\n",
    "    elif isinstance(stats, list):\n",
    "        print(\"  Top words:\")\n",
    "        for word, freq in stats:\n",
    "            print(f\"    {word}: {freq}\")\n",
    "\n",
    "# ---------------------save statistics tables-------------------\n",
    "print(f\"\\nüíæ save statistics tables...\")\n",
    "\n",
    "# word_freq_table.to_csv('corpus_word_frequency_table.csv', index=False, encoding='utf-8')\n",
    "print(\"‚úÖ save <- corpus_word_frequency_table.csv\")\n",
    "\n",
    "if 'doc_word_matrix' in locals():\n",
    "    # doc_word_matrix.to_csv('document_word_matrix.csv', index=False, encoding='utf-8')\n",
    "    print(\"‚úÖ  save <- document_word_matrix.csv\")\n",
    "\n",
    "# ‰øùÂ≠òÁªüËÆ°ÊëòË¶Å\n",
    "stats_df = pd.DataFrame([\n",
    "    {'Metric': key, 'Value': value} \n",
    "    for category_stats in corpus_stats.values() \n",
    "    if isinstance(category_stats, dict)\n",
    "    for key, value in category_stats.items()\n",
    "])\n",
    "\n",
    "# stats_df.to_csv('corpus_statistics_summary.csv', index=False, encoding='utf-8')\n",
    "print(\"‚úÖ save <- corpus_statistics_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6702e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"2. Core logic function: Chain Rule Process (CRP)\"\"\"\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    Tree node class for hierarchical topic modeling using nested Chinese Restaurant Process (nCRP).\n",
    "    Each node represents a topic in the hierarchy, with the tree structure representing\n",
    "    the nested relationship between general and specific topics.\n",
    "    \"\"\"\n",
    "    last_node_id = 0\n",
    "    total_node = 0\n",
    "    node_with_id = {}\n",
    "\n",
    "    def __init__(self, parent=None, layer=0):\n",
    "        self.node_id = Node.last_node_id # Unique identifier for this node\n",
    "        Node.last_node_id += 1\n",
    "        Node.total_node += 1\n",
    "        self.layer = layer # layer (int): Depth level in the hierarchy (0=root, 1=first level, etc.)\n",
    "        self.children = [] # children (list): List of child Node objects\n",
    "        self.parent = parent # parent (Node): Reference to parent node (None for root)\n",
    "        self.docs_list = [] # record index of documents reaching this node\n",
    "        Node.node_with_id[self.node_id] = self \n",
    "\n",
    "    def add_child(self):\n",
    "        child = Node(parent=self, layer=self.layer+1)\n",
    "        self.children.append(child)\n",
    "        return child\n",
    "    \n",
    "    def remove_child(self, child):\n",
    "        self.children.remove(child)\n",
    "        Node.node_with_id[child.node_id] = None \n",
    "        child.parent = None \n",
    "        Node.total_node -= 1\n",
    "\n",
    "def nCRP(corpus, depth, gamma):\n",
    "    \"\"\"\n",
    "    Nested Chinese Restaurant Process (nCRP) for initializing hierarchical topic structure.\n",
    "    \n",
    "    This function implements the generative process for creating a tree structure where:\n",
    "    1. Each document follows a path from root to a leaf node\n",
    "    2. At each level, documents choose to either create a new topic or join an existing one\n",
    "    3. Words in documents are randomly assigned to topics along their path\n",
    "    \"\"\"\n",
    "    # Initialize the root Node class each time nCRP is called\n",
    "    Node.last_node_id = 0\n",
    "    Node.total_node = 0\n",
    "    Node.node_with_id = {}\n",
    "    \n",
    "    \"\"\"\n",
    "     Args:\n",
    "        corpus (dict): Document collection {doc_id: [word1, word2, ...]}\n",
    "        depth (int): Maximum depth of the topic hierarchy (number of levels)\n",
    "        gamma (float): Concentration parameter controlling topic creation probability\n",
    "                      Higher gamma -> more likely to create new topics\n",
    "                      \n",
    "    Returns: [root_node, path_list, doc_path, doc_word_allocation] where:\n",
    "        - root_node (Node): Root node of the topic tree\n",
    "        - path_list (dict): {leaf_node_id: [node0, node1, ...]} - Complete paths from root to each leaf\n",
    "        - doc_path (dict): {doc_id: leaf_node_id} - Maps each document to its assigned leaf node\n",
    "        - doc_word_allocation (dict): {doc_id: [layer0, layer1, ...]} - Word-to-layer assignments\n",
    "    \"\"\"\n",
    "\n",
    "    root_node = Node()\n",
    "    path_list = {} # {leaf_node_id: [node0, node2,...]} - record each path from root to leaf nodes\n",
    "    doc_word_allocation = {} # {doc_id: {word: layer}} - record word allocation for each document\n",
    "    doc_path = {} # [leaf_node_id, leaf_node_id, ...] - record only the leaf node id of the pathÔºå list indexed by doc_id\n",
    "    \n",
    "    for c, doc in corpus.items(): # c is the index, d is the document\n",
    "        # print(doc)\n",
    "        # all docs starts from the root node\n",
    "        path = [root_node]\n",
    "        root_node.docs_list.append(c)\n",
    "\n",
    "        for i in range(1, depth):\n",
    "            # chose node based on CRP\n",
    "            parent_node = path[i-1]\n",
    "\n",
    "            CRP_probs = [gamma/(gamma + len(parent_node.docs_list) - 1)] # choose a new node: gamma/(gamma + n - 1)\n",
    "            for child in parent_node.children:\n",
    "                CRP_probs.append(len(child.docs_list)/(gamma + len(parent_node.docs_list) - 1))\n",
    "\n",
    "            chosen_index = np.random.choice(len(CRP_probs), p=CRP_probs) \n",
    "            if chosen_index == 0: # create a new node\n",
    "                current_node = parent_node.add_child()\n",
    "            else: # chose an existing node\n",
    "                current_node = parent_node.children[chosen_index - 1]\n",
    "\n",
    "            path.append(current_node)\n",
    "            current_node.docs_list.append(c)\n",
    "\n",
    "        if path[-1].node_id not in path_list.keys():\n",
    "            path_list[path[-1].node_id] = path\n",
    "        doc_path[c] = path[-1].node_id\n",
    "\n",
    "        # assign the words location to the document\n",
    "        word_allocation = []\n",
    "        for word in doc:\n",
    "            allocate_layer = np.random.randint(0,depth) # randomly allocate word to a layer\n",
    "            word_allocation.append(allocate_layer)\n",
    "\n",
    "        doc_word_allocation[c] = word_allocation\n",
    "    return [root_node, path_list, doc_path, doc_word_allocation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feb2366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"3. Core logic functions: Gibbs sampling, and relevant sub-function: word distribution, likelihood calculation, etc.\"\"\"\n",
    "def _create_default_dict_int():\n",
    "    return defaultdict(int)\n",
    "\n",
    "def aggregate_words(corpus, doc_word_allocation):\n",
    "    \"\"\" \n",
    "    Convert document word-layer assignments into hierarchical node word count.\n",
    "\n",
    "    Args:\n",
    "        corpus: {doc_id: [word1, word2, ...]} - Document collection\n",
    "        doc_word_allocation : {doc_id: [layer0, layer1, ...]} - Word-to-layer assignments\n",
    "    \n",
    "    Returns:\n",
    "        doc_node_allocation: {doc_id: {layer: {word: count}}} - Nested word counts by document and layer\n",
    "    \"\"\"\n",
    "    doc_node_allocation = {}\n",
    "\n",
    "    for doc_id, doc in corpus.items(): # c is the index, d is the document\n",
    "        allocation = doc_word_allocation[doc_id]\n",
    "        \n",
    "        # Use the named function here\n",
    "        layer_counts = defaultdict(_create_default_dict_int)\n",
    "        \n",
    "        for word, layer in zip(doc, allocation):\n",
    "            layer_counts[layer][word] += 1\n",
    "\n",
    "        doc_node_allocation[doc_id] = layer_counts\n",
    "    return doc_node_allocation\n",
    "\n",
    "def node_word_distribution(doc_node_allocation, doc_path, path_list, exclude_docs=None):\n",
    "    \"\"\"\n",
    "    Calculate global node-word distribution by aggregating word counts from all documents.\n",
    "    \n",
    "    Args:\n",
    "        doc_node_allocation (dict): {doc_id: {layer: {word: count}}} - Document word counts by layer\n",
    "        doc_path (dict): {doc_id: leaf_id} - Document to leaf node mapping\n",
    "        path_list (dict): {leaf_id: [Node1, Node2, ...]} - Complete paths from root to leaf\n",
    "        exclude_docs (set, optional): Document IDs to exclude from calculation\n",
    "    \n",
    "    Returns:\n",
    "        node_word_dist: {node_id: {word: count}} - Word count distribution for each node\n",
    "    \"\"\"\n",
    "    node_word_dist = defaultdict(lambda: defaultdict(int))\n",
    "    exclude_docs = set() if exclude_docs is None else set(exclude_docs)\n",
    "    \n",
    "    for doc_id, leaf_id in doc_path.items():\n",
    "        if doc_id in exclude_docs:\n",
    "            continue\n",
    "            \n",
    "        path = path_list[leaf_id]\n",
    "        \n",
    "        doc_allocation = doc_node_allocation[doc_id]\n",
    "        \n",
    "        for node in path:\n",
    "            node_layer = node.layer\n",
    "            if node_layer in doc_allocation:\n",
    "                for word, count in doc_allocation[node_layer].items():\n",
    "                    node_word_dist[node.node_id][word] += count\n",
    "    \n",
    "    return node_word_dist\n",
    "\n",
    "def calc_node_likelihood(compare_dist, target_dist, eta, len_W):\n",
    "    \"\"\"\n",
    "    Calculate likelihood of generating target word distribution from comparison node \n",
    "    to any tree node using Dirichlet-multinomial model, providing a probability estimate to choose the best path.\n",
    "   \n",
    "    Args:\n",
    "        compare_dist: {word: count} - Current word distribution of the comparison node\n",
    "        target_dist: {word: count} - Target word distribution to be generated  \n",
    "        eta (int): Dirichlet prior parameter (smoothing parameter)\n",
    "        len_W (int): Vocabulary size\n",
    "    \n",
    "    Returns:\n",
    "        float: Likelihood probability (or -inf for numerical overflow)\n",
    "    \"\"\"\n",
    "    sum_A = sum(compare_dist.values()) + eta * len_W\n",
    "    sum_B = sum(compare_dist.values()) + sum(target_dist.values()) + eta * len_W\n",
    "    \n",
    "    lgamma_sum_A = math.lgamma(sum_A)\n",
    "    lgamma_sum_B = math.lgamma(sum_B)\n",
    "    \n",
    "    lgamma_prod_A = 0.0\n",
    "    lgamma_prod_B = 0.0\n",
    "    \n",
    "    for word, count in target_dist.items():\n",
    "        comp_val = compare_dist.get(word, 0)\n",
    "        A = comp_val + eta\n",
    "        B = comp_val + count + eta\n",
    "        \n",
    "        lgamma_prod_A += math.lgamma(A)\n",
    "        lgamma_prod_B += math.lgamma(B)\n",
    "\n",
    "    try:\n",
    "        log_likelihood = (lgamma_sum_A - lgamma_prod_A) + (lgamma_prod_B - lgamma_sum_B)\n",
    "        return math.exp(log_likelihood)\n",
    "    except (OverflowError, ValueError):\n",
    "        return float('-inf')  # ËøîÂõûË¥üÊó†Á©∑Ë°®Á§∫ÊûÅÂ∞èÊ¶ÇÁéá    \n",
    "\n",
    "def create_new_path(base_node, doc_id, depth):\n",
    "    new_path = []\n",
    "    current = base_node\n",
    "    while current:\n",
    "        new_path.insert(0, current)\n",
    "        current = current.parent\n",
    "    \n",
    "    current = base_node\n",
    "    for _ in range(base_node.layer, depth-1):\n",
    "        new_node = current.add_child()\n",
    "        new_path.append(new_node)\n",
    "        current = new_node\n",
    "        \n",
    "    return current.node_id, new_path\n",
    "\n",
    "def exclude_doc_from_node_dist(global_node_word_dist, doc_node_allocation, doc_path, path_list, doc_id):\n",
    "    doc_path_lst = path_list[doc_path[doc_id]]\n",
    "    for node in doc_path_lst:\n",
    "        if node.node_id in global_node_word_dist:\n",
    "            for word, count in doc_node_allocation[doc_id].get(node.layer, {}).items():\n",
    "                global_node_word_dist[node.node_id][word] -= count\n",
    "                if global_node_word_dist[node.node_id][word] <= 0:\n",
    "                    del global_node_word_dist[node.node_id][word]\n",
    "    return global_node_word_dist\n",
    "\n",
    "def add_doc_to_node_dist(global_node_word_dist, doc_node_allocation, doc_path, path_list, doc_id):\n",
    "    \"\"\"Â∞ÜÊñáÊ°£ÁöÑËØçÂàÜÂ∏ÉÊ∑ªÂä†Âà∞ÂÖ®Â±ÄËØçÂàÜÂ∏É‰∏≠\"\"\"\n",
    "    doc_path_lst = path_list[doc_path[doc_id]]\n",
    "    for node in doc_path_lst:\n",
    "        node_layer = node.layer\n",
    "        if node_layer in doc_node_allocation[doc_id]:\n",
    "            for word, count in doc_node_allocation[doc_id][node_layer].items():\n",
    "                if node.node_id not in global_node_word_dist:\n",
    "                    global_node_word_dist[node.node_id] = defaultdict(int)\n",
    "                global_node_word_dist[node.node_id][word] += count\n",
    "    return global_node_word_dist\n",
    "\n",
    "def Gibbs_sampling(corpus, root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation, global_node_word_dist,\n",
    "                   gamma, eta, alpha, depth, iteration, iter_start=None, iter_end=None):\n",
    "    W = set(itertools.chain.from_iterable(corpus.values()))\n",
    "    all_jump_records_this_iteration = []\n",
    "    all_detail_records_this_iteration = []\n",
    "\n",
    "    for doc_id, doc in corpus.items():\n",
    "        # print('doc_id', doc_id)\n",
    "        current_path = path_list[doc_path[doc_id]]\n",
    "        current_node_allocation = doc_node_allocation[doc_id]\n",
    "        node_word_dist = exclude_doc_from_node_dist(global_node_word_dist, doc_node_allocation, doc_path, path_list, doc_id)\n",
    "        # print('g2',global_node_word_dist[0])\n",
    "\n",
    "        # isolate doc_c from current situation\n",
    "        nodes_to_remove = []\n",
    "        for node in current_path[::-1]:\n",
    "            node.docs_list.remove(doc_id)\n",
    "            if len(node.docs_list) == 0 and node != root_node:\n",
    "                nodes_to_remove.append(node)\n",
    "        \n",
    "        if nodes_to_remove:\n",
    "            del path_list[doc_path[doc_id]]\n",
    "            for node in nodes_to_remove:\n",
    "                if node.parent:  # ÂÜçÊ¨°Ê£ÄÊü•‰ª•Á°Æ‰øùÂÆâÂÖ®\n",
    "                    node.parent.remove_child(node)\n",
    "\n",
    "        # sample path\n",
    "        path_prior_lst = {}\n",
    "        path_likelihood_lst = {}\n",
    "\n",
    "        target_prior = {}\n",
    "        target_likelihood = {}\n",
    "        for path_id, path in path_list.items():\n",
    "            node_prior_lst = [1.0]\n",
    "            node_likelihood_lst = []\n",
    "            \n",
    "            for node in path:\n",
    "                if node.parent:\n",
    "                    prior = (len(node.docs_list)) / (gamma + len(node.parent.docs_list)) \n",
    "                    node_prior_lst.append(prior)\n",
    "                \n",
    "                if node.layer in current_node_allocation:\n",
    "                    likelihood = calc_node_likelihood(node_word_dist[node.node_id], current_node_allocation[node.layer], eta, len(W))\n",
    "                    node_likelihood_lst.append(likelihood)\n",
    "                else:\n",
    "                    node_likelihood_lst.append(0)\n",
    "\n",
    "            target_prior[path_id] = node_prior_lst\n",
    "            target_likelihood[path_id] = node_likelihood_lst\n",
    "\n",
    "            path_prior = reduce(operator.mul, (x for x in node_prior_lst if x != 0), 1)\n",
    "            path_likelihood = reduce(operator.mul, (x for x in node_likelihood_lst if x != 0), 1)\n",
    "            path_prior_lst[path_id] = path_prior\n",
    "            path_likelihood_lst[path_id] = path_likelihood\n",
    "\n",
    "        multiplied_path_dict = {key: path_prior_lst[key] * path_likelihood_lst[key] for key in path_likelihood_lst}\n",
    "        \n",
    "        # create new path,\n",
    "        new_path_prior_lst = {}\n",
    "        new_path_likelihood_lst = {}\n",
    "\n",
    "        new_target_prior = {}\n",
    "        new_target_likelihood = {}\n",
    "\n",
    "        for node_id, node in Node.node_with_id.items():\n",
    "            if node != None and node.layer < depth-1:\n",
    "                new_node_prior_lst = [gamma / (gamma + len(node.docs_list))]\n",
    "                \n",
    "                if 0 in current_node_allocation:\n",
    "                    new_node_likelihood_lst = [calc_node_likelihood(node_word_dist[0], current_node_allocation[0], eta, len(W))]\n",
    "                else:\n",
    "                    new_node_likelihood_lst = [0]\n",
    "                \n",
    "                temp_node = node\n",
    "                while temp_node.parent:\n",
    "                    new_node_prior_lst.insert(0, (len(temp_node.docs_list)) / (gamma + len(temp_node.parent.docs_list)))             \n",
    "                \n",
    "                    if temp_node.layer in current_node_allocation:\n",
    "                        new_node_likelihood_lst.insert(1, calc_node_likelihood(node_word_dist[temp_node.node_id], current_node_allocation[temp_node.layer], eta, len(W)))\n",
    "                    else:\n",
    "                        new_node_likelihood_lst.insert(1, 0)\n",
    "\n",
    "                    temp_node = temp_node.parent\n",
    "\n",
    "                for layer in range(node.layer+1, depth):\n",
    "                    if layer in current_node_allocation:\n",
    "                        new_node_likelihood_lst.append(calc_node_likelihood({}, current_node_allocation[layer], eta, len(W)))\n",
    "\n",
    "                new_target_prior[node_id] = new_node_prior_lst\n",
    "                new_target_likelihood[node_id] = new_node_likelihood_lst\n",
    "                new_path_prior = reduce(operator.mul, (x for x in new_node_prior_lst if x != 0), 1)\n",
    "                new_path_likelihood = reduce(operator.mul, (x for x in new_node_likelihood_lst if x != 0), 1)\n",
    "                new_path_prior_lst[node_id] = new_path_prior\n",
    "                new_path_likelihood_lst[node_id] = new_path_likelihood\n",
    "        \n",
    "        multiplied_new_path_dict = {f'create{key}': new_path_prior_lst[key] * new_path_likelihood_lst[key] for key in new_path_likelihood_lst}\n",
    "        all_probs = {**multiplied_path_dict, **multiplied_new_path_dict}\n",
    "\n",
    "        total_prob = sum(v for v in all_probs.values())\n",
    "        if total_prob > 0:\n",
    "            normalized_probs = {k: v/total_prob for k, v in all_probs.items()}\n",
    "        else:\n",
    "            normalized_probs = {k: 1.0/len(all_probs) for k in all_probs}\n",
    "        # print(normalized_probs)\n",
    "        \n",
    "        chosen_path = np.random.choice(list(normalized_probs.keys()),p=list(normalized_probs.values()))\n",
    "        if chosen_path.startswith('create'):\n",
    "            base_node = Node.node_with_id[int(chosen_path[6:])]\n",
    "            leaf_id, added_path = create_new_path(base_node, doc_id, depth)\n",
    "            path_list.update({leaf_id:added_path})\n",
    "            doc_path[doc_id] = leaf_id\n",
    "        else:\n",
    "            leaf_id = int(chosen_path)\n",
    "            added_path = path_list[int(chosen_path)]\n",
    "            doc_path[doc_id] = int(chosen_path)\n",
    "\n",
    "        \"\"\"check doc jump\"\"\"\n",
    "        current_doc_jump_record = {} # ‰∏∫ÂΩìÂâçÊñáÊ°£ÂàõÂª∫\n",
    "        current_doc_detail_record = {} # ‰∏∫ÂΩìÂâçÊñáÊ°£ÂàõÂª∫\n",
    "\n",
    "        if iter_start <= iteration <= iter_end and str(current_path[-1].node_id) != chosen_path:\n",
    "            # print(f\"Iteration {iteration}: Document {doc_id} changed path from {current_path[-1].node_id} to {added_path[-1].node_id}\")\n",
    "            \n",
    "            current_doc_detail_record = {\n",
    "                'iteration': iteration,\n",
    "                'doc_id': doc_id,\n",
    "                'deleted_path_list': {leaf_id: [node.node_id for node in path_nodes] for leaf_id, path_nodes in path_list.items()}, # Ê≥®ÊÑèÊ∑±Êã∑Ë¥ùÁöÑÊÄßËÉΩÂΩ±Âìç\n",
    "                'doc_path': {k:v for k, v in doc_path.items()},\n",
    "                'doc_word_allocation': list(doc_word_allocation[doc_id]),\n",
    "                'doc_node_allocation': {layer: {word: count for word, count in word_counts.items()} \n",
    "                                        for layer, word_counts in doc_node_allocation[doc_id].items()}\n",
    "            }\n",
    "\n",
    "            if chosen_path.startswith('create'):\n",
    "                # print(f\"!!! Created new path: {[nd.node_id for nd in added_path]}\")\n",
    "                chosen_path_prob = normalized_probs[chosen_path]\n",
    "                values = list(normalized_probs.values())\n",
    "                sorted_values = sorted(values, reverse=True)\n",
    "                rank = sorted_values.index(chosen_path_prob) + 1 \n",
    "                # print(f\"Path creation rank: {rank} out of {len(values)}\")\n",
    "\n",
    "                current_doc_jump_record = {\n",
    "                    'iteration': iteration,\n",
    "                    'doc_id': doc_id,\n",
    "                    'old_leaf_id': current_path[-1].node_id,\n",
    "                    'old_path': [n.node_id for n in current_path],\n",
    "                    'new_leaf_id': added_path[-1].node_id,\n",
    "                    'new_path': [n.node_id for n in added_path],\n",
    "                    'origal_probs':all_probs,\n",
    "                    'normalized_probs': normalized_probs,\n",
    "                    'chosen_path_prob': chosen_path_prob,\n",
    "                    'rank': f'{rank} out of {len(values)}',\n",
    "                    'create_path': True\n",
    "                }\n",
    "\n",
    "            else:\n",
    "                # print(f\"Reused existing path: {[nd.node_id for nd in added_path]}\")\n",
    "                chosen_path_prob = normalized_probs[int(chosen_path)]\n",
    "                values = list(normalized_probs.values())\n",
    "                sorted_values = sorted(values, reverse=True)\n",
    "                rank = sorted_values.index(chosen_path_prob) + 1  \n",
    "                # print(f\"Path reuse rank: {rank} out of {len(values)}\")\n",
    "                # print('---------------------------------------')\n",
    "\n",
    "                current_doc_jump_record = {\n",
    "                    'iteration': iteration,\n",
    "                    'doc_id': doc_id,\n",
    "                    'old_leaf_id': current_path[-1].node_id,\n",
    "                    'old_path_ids': [n.node_id for n in current_path],\n",
    "                    'new_leaf_id': added_path[-1].node_id,\n",
    "                    'new_path_ids': [n.node_id for n in added_path],\n",
    "                    'origal_probs':all_probs,\n",
    "                    'normalized_probs': normalized_probs,\n",
    "                    'chosen_path_prob': chosen_path_prob,\n",
    "                    'rank': f'{rank} out of {len(values)}',\n",
    "                    'create_path': False\n",
    "                }\n",
    "\n",
    "            if current_doc_jump_record: # Á°Æ‰øùËÆ∞ÂΩï‰∏ç‰∏∫Á©∫\n",
    "                all_jump_records_this_iteration.append(current_doc_jump_record)\n",
    "            if current_doc_detail_record: # Á°Æ‰øùËÆ∞ÂΩï‰∏ç‰∏∫Á©∫\n",
    "                all_detail_records_this_iteration.append(current_doc_detail_record)\n",
    "        \n",
    "        for node in added_path:\n",
    "                node.docs_list.append(doc_id)\n",
    "        \n",
    "        \"\"\"sample topic\"\"\"\n",
    "        # node_word_dist_update = node_word_distribution(doc_node_allocation, doc_path, path_list, exclude_docs=None)\n",
    "        node_word_dist_update = add_doc_to_node_dist(global_node_word_dist, doc_node_allocation, doc_path, path_list, doc_id)\n",
    "        # print('g3',global_node_word_dist[0])\n",
    "        \n",
    "        doc_word2node = doc_word_allocation[doc_id]\n",
    "        current_path = path_list[doc_path[doc_id]]\n",
    "        update_doc_word2node = []\n",
    "        for word, old_layer in zip(doc, doc_word2node):\n",
    "            doc_node_allocation[doc_id][old_layer][word] -= 1\n",
    "            node_word_dist_update[current_path[old_layer].node_id][word] -= 1\n",
    "            topic_probs = {}\n",
    "\n",
    "            for layer, topic in enumerate(current_path):\n",
    "                word_in_topic = node_word_dist_update[topic.node_id].get(word,0)\n",
    "                topic_in_doc = sum(doc_node_allocation[doc_id].get(layer,{}).values())\n",
    "\n",
    "                word_prop = (word_in_topic+eta)/(sum(node_word_dist_update[topic.node_id].values())+len(W)*eta)\n",
    "                topic_prop = (topic_in_doc+alpha)/(len(doc)+depth*alpha)\n",
    "                topic_probs[layer] = word_prop*topic_prop\n",
    "            \n",
    "            total_topic_prob = sum(v for v in topic_probs.values() if v > 0)\n",
    "            if total_topic_prob > 0:\n",
    "                normalized_topic_probs = {k: v/total_topic_prob for k, v in topic_probs.items()}\n",
    "            else:\n",
    "                normalized_topic_probs = {k: 1.0/len(topic_probs) for k in topic_probs}\n",
    "\n",
    "            chosen_layer = np.random.choice(list(normalized_topic_probs.keys()),p=list(normalized_topic_probs.values()))\n",
    "            update_doc_word2node.append(chosen_layer)\n",
    "            doc_node_allocation[doc_id][chosen_layer][word] += 1\n",
    "            node_word_dist_update[current_path[chosen_layer].node_id][word] += 1\n",
    "\n",
    "        doc_word_allocation[doc_id] = update_doc_word2node\n",
    "        # print('g4',global_node_word_dist[0])\n",
    "\n",
    "    return all_jump_records_this_iteration, all_detail_records_this_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caa69ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recorder:\n",
    "    \"\"\"GibbsÈááÊ†∑Ëø≠‰ª£ËÆ∞ÂΩïÂô®\"\"\"\n",
    "    \n",
    "    def __init__(self, corpus, depth, eta, alpha):\n",
    "        self.corpus = corpus\n",
    "        self.depth = depth\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.iteration_records = []\n",
    "        self.vocab = set(itertools.chain.from_iterable(corpus.values()))\n",
    "        \n",
    "    def record_iteration(self, iteration_num, root_node, path_list, doc_path, \n",
    "                        doc_word_allocation, doc_node_allocation, global_node_word_dist, jump_record=[], detail_record=[],\n",
    "                        newly_created_paths=None, iter_start_for_detailed_log=float('inf')): # Ê∑ªÂä†Êñ∞ÂèÇÊï∞Âπ∂ËÆæÈªòËÆ§ÂÄº\n",
    "        \"\"\"ËÆ∞ÂΩïÂçïÊ¨°Ëø≠‰ª£ÁöÑÊâÄÊúâ‰ø°ÊÅØ\"\"\"\n",
    "        \n",
    "        should_record_word_node_details_to_file = (iteration_num >= iter_start_for_detailed_log)\n",
    "\n",
    "        # ËÆ°ÁÆóÊîπÂèòË∑ØÂæÑÁöÑÊñáÊ°£Êï∞\n",
    "        changed_docs_count = 0\n",
    "        if iteration_num > 0:  # ÂØπÁ¨¨‰∏ÄËΩÆ‰πãÂêéÁöÑËø≠‰ª£ËøõË°åÁªüËÆ°\n",
    "            previous_doc_paths = {doc['document_id']: doc['leaf_node_id'] \n",
    "                                for doc in self.iteration_records[-1]['doc_path_assignments']}\n",
    "            changed_docs_count = sum(1 for doc_id, leaf_id in doc_path.items() \n",
    "                                if doc_id in previous_doc_paths and previous_doc_paths[doc_id] != leaf_id)\n",
    "        \n",
    "        # 1. ËÆ∞ÂΩïÊñáÊ°£Ë∑ØÂæÑÂàÜÈÖç\n",
    "        # file_name: gibbs_iteration_records_document_paths - \n",
    "        doc_path_records = []\n",
    "        for doc_id, leaf_id in doc_path.items():\n",
    "            doc_path_records.append({\n",
    "                'iteration': iteration_num,\n",
    "                'document_id': doc_id,\n",
    "                'leaf_node_id': leaf_id,\n",
    "                'path_created_this_iteration': leaf_id in (newly_created_paths or [])\n",
    "            })\n",
    "        \n",
    "        # 2. ËÆ∞ÂΩïË∑ØÂæÑ/Ê†ëÁªìÊûÑ \n",
    "        # path_list: {leaf_id: [Node1, Node2, ...]}\n",
    "        # doc_path: {doc_id: leaf_id}\n",
    "        # file_name:gibbs_iteration_records_path_structures - \n",
    "        path_structure_records = []\n",
    "        for leaf_id, path_nodes in path_list.items():\n",
    "            node_ids = [node.node_id for node in path_nodes]\n",
    "            \n",
    "            docs_in_this_path = [doc_id for doc_id, assigned_leaf_id in doc_path.items()\n",
    "                                if assigned_leaf_id == leaf_id]\n",
    "            \n",
    "            path_record = {\n",
    "                'iteration': iteration_num,\n",
    "                'leaf_node_id': leaf_id,\n",
    "                'document_count': len(docs_in_this_path),\n",
    "                'documents_in_path': docs_in_this_path,\n",
    "                'path_created_this_iteration': leaf_id in (newly_created_paths or [])\n",
    "            }\n",
    "\n",
    "            for i in range(self.depth):\n",
    "                layer_key = f'layer_{i}_node_id'\n",
    "                if i < len(node_ids):\n",
    "                    path_record[layer_key] = node_ids[i]\n",
    "                else:\n",
    "                    path_record[layer_key] = None \n",
    "            \n",
    "            path_structure_records.append(path_record),\n",
    "        \n",
    "        # 3. ËÆ∞ÂΩïËØçËØ≠ÂàÜÈÖç (‰ªÖÂú®ËØ¶ÁªÜËÆ∞ÂΩïÁ™óÂè£ÂÜÖÂ°´ÂÖÖ)\n",
    "        # file_name: gibbs_iteration_records_word_allocations - \n",
    "        # doc_word_allocation: {doc_id: [layer_for_word1, layer_for_word2,...]}\n",
    "        word_allocation_records = []\n",
    "        # attention: should_record_word_node_details_to_file = (iteration_num >= iter_start_for_detailed_log)\n",
    "        if should_record_word_node_details_to_file:\n",
    "            for doc_id, word_assignments in doc_word_allocation.items():\n",
    "                doc_words = self.corpus[doc_id]\n",
    "                for word_idx, (word, layer) in enumerate(zip(doc_words, word_assignments)):\n",
    "                    word_allocation_records.append({\n",
    "                        'iteration': iteration_num,\n",
    "                        'document_id': doc_id,\n",
    "                        'word_index': word_idx,\n",
    "                        'word': word,\n",
    "                        'assigned_layer': layer,\n",
    "                        'leaf_node_id': doc_path[doc_id],\n",
    "                        'assigned_node_id': path_list[doc_path[doc_id]][layer].node_id\n",
    "                    })\n",
    "        \n",
    "        # 4. ËÆ°ÁÆóËäÇÁÇπËØçÂàÜÂ∏É (ÂßãÁªàÈúÄË¶Å‰∏∫log-likelihoodËÆ°ÁÆó)\n",
    "        # file_name: gibbs_iteration_records_node_word_distributions - \n",
    "        # doc_node_allocation: ?\n",
    "        # node_word_dist = self._calculate_node_word_distribution(\n",
    "        #     doc_node_allocation, doc_path, path_list\n",
    "        # )\n",
    "        # node_word_dist: # {node_id: {word: count}}\n",
    "        #   ËÆ∞ÂΩïËäÇÁÇπËØçÂàÜÂ∏É (‰ªÖÂú®ËØ¶ÁªÜËÆ∞ÂΩïÁ™óÂè£ÂÜÖÂ°´ÂÖÖ)\n",
    "        node_word_records = []\n",
    "        # attention: should_record_word_node_details_to_file = (iteration_num >= iter_start_for_detailed_log)\n",
    "        if should_record_word_node_details_to_file:\n",
    "            for node_id, current_word_dist in global_node_word_dist.items(): # ‰ΩøÁî®ËÆ°ÁÆóÂæóÂà∞ÁöÑ node_word_dist\n",
    "                for word, count in current_word_dist.items():\n",
    "                    node_word_records.append({\n",
    "                        'iteration': iteration_num,\n",
    "                        'node_id': node_id,\n",
    "                        'word': word,\n",
    "                        'count': count\n",
    "                    })\n",
    "        \n",
    "        # 5. ËÆ°ÁÆóÁîüÊàêÊ¶ÇÁéá/log-likelihood (ÂßãÁªà‰ΩøÁî®ËÆ°ÁÆóÂá∫ÁöÑ node_word_dist)\n",
    "        # file_name: gibbs_iteration_records_iteration_summaries - \n",
    "        log_likelihood = self._calculate_log_likelihood(\n",
    "            doc_path, path_list, doc_word_allocation, global_node_word_dist\n",
    "        )\n",
    "        \n",
    "        # 6. ËÆ∞ÂΩïÊï¥‰ΩìÁªüËÆ°‰ø°ÊÅØ,\n",
    "        # file_name: gibbs_iteration_records_iteration_summaries -\n",
    "        iteration_summary = {\n",
    "            'iteration': iteration_num,\n",
    "            'total_paths': len(path_list),\n",
    "            'total_documents': len(doc_path),\n",
    "            'log_likelihood': log_likelihood,\n",
    "            'changed_docs_count': changed_docs_count, \n",
    "            'newly_created_paths': len(newly_created_paths or []),\n",
    "            'avg_path_size': np.mean([len([doc_id for doc_id, assigned_leaf_id in doc_path.items() \n",
    "                                            if assigned_leaf_id == leaf_id]) for leaf_id in path_list.keys()]) if path_list else 0,\n",
    "            'max_path_size': max([len([doc_id for doc_id, assigned_leaf_id in doc_path.items() \n",
    "                                        if assigned_leaf_id == leaf_id]) for leaf_id in path_list.keys()]) if path_list else 0,\n",
    "            'min_path_size': min([len([doc_id for doc_id, assigned_leaf_id in doc_path.items() \n",
    "                                        if assigned_leaf_id == leaf_id]) for leaf_id in path_list.keys()]) if path_list else 0,\n",
    "        #     'doc_path': doc_path,\n",
    "        #     'doc_node_allocation':doc_node_allocation\n",
    "        }\n",
    "\n",
    "        # 7.ËÆ∞ÂΩïË∑≥Ë∑ÉËÆ∞ÂΩïÂíåËØ¶ÁªÜËÆ∞ÂΩï\n",
    "        # file_name: gibbs_iteration_records_jump_records - \n",
    "        # jump_record: [{iteration, doc_id, old_leaf_id, old_path, new\n",
    "        iteration_jump_record = []\n",
    "        if jump_record:\n",
    "            iteration_jump_record = jump_record\n",
    "        \n",
    "        iteration_detail_record = []\n",
    "        if detail_record:\n",
    "            iteration_detail_record = detail_record\n",
    "            \n",
    "        self.iteration_records.append({\n",
    "            'iteration': iteration_num,\n",
    "            'doc_path_assignments': doc_path_records,\n",
    "            'path_structures': path_structure_records,\n",
    "            'word_allocations': word_allocation_records, # Â¶ÇÊûú‰∏çÊª°Ë∂≥Êù°‰ª∂ÔºåÂàô‰∏∫Á©∫ÂàóË°®\n",
    "            'node_word_distributions': node_word_records, # Â¶ÇÊûú‰∏çÊª°Ë∂≥Êù°‰ª∂ÔºåÂàô‰∏∫Á©∫ÂàóË°®\n",
    "            'iteration_summary': iteration_summary,\n",
    "            'jump_records_list': iteration_jump_record,\n",
    "            'detail_records_list': iteration_detail_record\n",
    "        })\n",
    "        \n",
    "        return iteration_summary\n",
    "    \n",
    "    # def _calculate_node_word_distribution(self, doc_node_allocation, doc_path, path_list):\n",
    "    #     \"\"\"ËÆ°ÁÆóËäÇÁÇπËØçÂàÜÂ∏É\"\"\"\n",
    "    #     node_word_dist = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "    #     for doc_id, leaf_id in doc_path.items():\n",
    "    #         path = path_list[leaf_id]\n",
    "    #         doc_allocation = doc_node_allocation[doc_id]\n",
    "            \n",
    "    #         for node in path:\n",
    "    #             node_layer = node.layer\n",
    "    #             if node_layer in doc_allocation:\n",
    "    #                 for word, count in doc_allocation[node_layer].items():\n",
    "    #                     node_word_dist[node.node_id][word] += count\n",
    "        \n",
    "    #     return node_word_dist\n",
    "\n",
    "    def _calculate_log_likelihood(self, doc_path, path_list, \n",
    "                                    doc_word_allocation, # {doc_id: [layer_for_word1, layer_for_word2,...]}\n",
    "                                    node_word_dist):     # {node_id: {word: count}}\n",
    "        \"\"\"\n",
    "        ËÆ°ÁÆóÂú®ÂΩìÂâçÊñáÊ°£ËØçËØ≠Â±ÇÁ∫ßÂàÜÈÖçÂíåË∑ØÂæÑ‰∏ãÔºåÈáçÊñ∞ÁîüÊàêÊâÄÊúâÊñáÊ°£ËØçËØ≠ÁöÑÂØπÊï∞Ê¶ÇÁéá„ÄÇ\n",
    "        log P(Words | WordLayerAssignments, Paths, eta)\n",
    "        \"\"\"\n",
    "        total_log_likelihood = 0.0\n",
    "        vocab_size = len(self.vocab)\n",
    "        \n",
    "        # È¢ÑËÆ°ÁÆóÊØè‰∏™ËäÇÁÇπÁöÑÊÄªËØçÊï∞\n",
    "        node_total_words_map = defaultdict(int)\n",
    "        for node_id, dist in node_word_dist.items():\n",
    "            node_total_words_map[node_id] = sum(dist.values())\n",
    "\n",
    "        for doc_id, words_in_doc in self.corpus.items():\n",
    "            if not words_in_doc:\n",
    "                continue\n",
    "\n",
    "            current_doc_word_layer_assignments = doc_word_allocation[doc_id]\n",
    "            current_doc_path_nodes = path_list[doc_path[doc_id]] # List of Node objects\n",
    "\n",
    "            for i, word in enumerate(words_in_doc):\n",
    "                assigned_layer = current_doc_word_layer_assignments[i]\n",
    "\n",
    "                # P(word | assigned_node, eta)\n",
    "                # = (count_of_this_word_in_node + eta) / (total_words_in_node + vocab_size * eta)\n",
    "                if assigned_layer >= len(current_doc_path_nodes):\n",
    "                    # print(f\\Warning: Likelihood calc - doc_id {doc_id}, word '{word}' assigned_layer {assigned_layer} out of bounds for path length {len(current_doc_path_nodes)}.\\)\n",
    "                    total_log_likelihood += -float('inf') # Penalize for inconsistent assignment\n",
    "                    continue\n",
    "\n",
    "                assigned_node_object = current_doc_path_nodes[assigned_layer]\n",
    "                assigned_node_id = assigned_node_object.node_id\n",
    "                \n",
    "                count_word_in_assigned_node = node_word_dist.get(assigned_node_id, {}).get(word, 0)\n",
    "                total_words_in_assigned_node = node_total_words_map.get(assigned_node_id, 0)\n",
    "\n",
    "                denominator_val = total_words_in_assigned_node + vocab_size * self.eta\n",
    "                if denominator_val <= 0: \n",
    "                    log_prob_word_generation = -float('inf') \n",
    "                else:\n",
    "                    numerator_val = count_word_in_assigned_node + self.eta\n",
    "                    if numerator_val <=0: # Should not happen if eta > 0\n",
    "                        log_prob_word_generation = -float('inf')\n",
    "                    else:\n",
    "                        log_prob_word_generation = math.log(numerator_val) - math.log(denominator_val)\n",
    "                \n",
    "                total_log_likelihood += log_prob_word_generation\n",
    "                \n",
    "        return total_log_likelihood\n",
    "    \n",
    "    def save_to_files(self, base_filename=\"iteration\", last_n_iterations=20):\n",
    "        \"\"\"‰øùÂ≠òÊâÄÊúâËÆ∞ÂΩïÂà∞CSVÊñá‰ª∂\"\"\"\n",
    "        \n",
    "        if not self.iteration_records:\n",
    "            print(\"No iteration records to save.\")\n",
    "            return\n",
    "        \n",
    "        # 1. ‰øùÂ≠òÊñáÊ°£Ë∑ØÂæÑÂàÜÈÖçËÆ∞ÂΩï (‰øùÊåÅ‰∏çÂèò)\n",
    "        all_doc_paths = []\n",
    "        for record in self.iteration_records:\n",
    "            all_doc_paths.extend(record['doc_path_assignments'])\n",
    "        \n",
    "        doc_path_df = pd.DataFrame(all_doc_paths)\n",
    "        doc_path_df.to_csv(f'{base_filename}_document_paths.csv', index=False, encoding='utf-8')\n",
    "        print(f\"‚úÖ doc-path allocation is saved : {base_filename}_document_paths.csv\")\n",
    "    \n",
    "        # 2. ‰øùÂ≠òË∑ØÂæÑÁªìÊûÑËÆ∞ÂΩï - Â§ÑÁêÜÂàóË°®Â≠óÊÆµ\n",
    "        # ÂÅáËÆæ path_record (Êù•Ëá™ record['path_structures']) Â∑≤ÁªèÂåÖÂê´‰∫Ü layer_X_node_id Âàó\n",
    "        all_path_structures = []\n",
    "        for record in self.iteration_records:\n",
    "            for path_record_item in record['path_structures']: # path_record_item ÊòØÂåÖÂê´ÂàÜÂ±Ç‰ø°ÊÅØÁöÑÂ≠óÂÖ∏\n",
    "                path_record_copy = path_record_item.copy()\n",
    "                all_path_structures.append(path_record_copy)\n",
    "        \n",
    "        path_structure_df = pd.DataFrame(all_path_structures)\n",
    "        path_structure_df.to_csv(f'{base_filename}_path_structures.csv', index=False, encoding='utf-8')\n",
    "        print(f\"‚úÖ structure of tree path is saved: {base_filename}_path_structures.csv\")\n",
    "        \n",
    "        # 5. ‰øùÂ≠òËø≠‰ª£ÊëòË¶Å (‰øùÊåÅ‰∏çÂèò)\n",
    "        iteration_summaries = [record['iteration_summary'] for record in self.iteration_records]\n",
    "        summary_df = pd.DataFrame(iteration_summaries)\n",
    "        summary_df.to_csv(f'{base_filename}_iteration_summaries.csv', index=False, encoding='utf-8')\n",
    "        print(f\"‚úÖ info recoreded in iteration is saved: {base_filename}_iteration_summaries.csv\")\n",
    "        \n",
    "        # 6. È¢ùÂ§ñ‰øùÂ≠ò‰∏Ä‰∏™ËØ¶ÁªÜÁöÑË∑ØÂæÑ-ÊñáÊ°£Êò†Â∞ÑË°® - ‰øÆÊîπÊ≠§Â§Ñ‰ª•ÈÄÇÂ∫îÂàÜÂ±ÇÁªìÊûÑ   \n",
    "        path_document_mapping = []\n",
    "        for record in self.iteration_records:\n",
    "            for path_record_item in record['path_structures']: # path_record_item ÊòØÂåÖÂê´ÂàÜÂ±Ç‰ø°ÊÅØÁöÑÂ≠óÂÖ∏\n",
    "                iteration_num = path_record_item['iteration']\n",
    "                leaf_id = path_record_item['leaf_node_id']\n",
    "\n",
    "                for doc_id_in_path in path_record_item['documents_in_path']:\n",
    "                    mapping_entry = {\n",
    "                        'iteration': iteration_num,\n",
    "                        'leaf_node_id': leaf_id,\n",
    "                        'document_id': doc_id_in_path,\n",
    "                    }\n",
    "                    # Ê∑ªÂä†ÂàÜÂ±ÇËäÇÁÇπIDÔºåÁõ¥Âà∞Ê®°ÂûãÂÆö‰πâÁöÑÊ∑±Â∫¶ self.depth\n",
    "                    # Á°Æ‰øù self.depth Âú® __init__ ‰∏≠Ë¢´ËÆæÁΩÆ\n",
    "                    if hasattr(self, 'depth'):\n",
    "                        for i in range(self.depth):\n",
    "                            layer_key = f'layer_{i}_node_id'\n",
    "                            mapping_entry[layer_key] = path_record_item.get(layer_key) # ‰ªé path_record_item Ëé∑Âèñ\n",
    "                    else:\n",
    "                        # Â¶ÇÊûú self.depth ‰∏çÂèØÁî®ÔºåÂèØ‰ª•Â∞ùËØï‰ªé path_record_item ÁöÑÈîÆÊé®Êñ≠Ôºå‰ΩÜËøô‰∏çÂ§™ÁêÜÊÉ≥\n",
    "                        # ÊàñËÄÖÂè™ËÆ∞ÂΩïÂÆûÈôÖÂ≠òÂú®ÁöÑÂ±ÇÁ∫ß\n",
    "                        pass # Ê†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂ§ÑÁêÜ self.depth ‰∏çÂèØÁî®ÁöÑÊÉÖÂÜµ\n",
    "                    \n",
    "                    path_document_mapping.append(mapping_entry)\n",
    "        \n",
    "        path_doc_mapping_df = pd.DataFrame(path_document_mapping)\n",
    "        path_doc_mapping_df.to_csv(f'{base_filename}_path_document_mapping.csv', index=False, encoding='utf-8')\n",
    "        print(f\"‚úÖ doc-path mapping is saved: {base_filename}_path_document_mapping.csv\")\n",
    "        \n",
    "        filtered_records_for_detail = self.iteration_records[-last_n_iterations:]\n",
    "        \n",
    "        # 3. ‰øùÂ≠òËØçËØ≠ÂàÜÈÖçËÆ∞ÂΩï (‰øùÊåÅ‰∏çÂèò)\n",
    "        all_word_allocations = []\n",
    "        for record in filtered_records_for_detail:\n",
    "            all_word_allocations.extend(record['word_allocations'])\n",
    "        \n",
    "        word_allocation_df = pd.DataFrame(all_word_allocations)\n",
    "        word_allocation_df.to_csv(f'{base_filename}_word_allocations.csv', index=False, encoding='utf-8')\n",
    "        print(f\"‚úÖ doc's words allocation to node is saved: {base_filename}_word_allocations.csv\")\n",
    "        \n",
    "        # 4. ‰øùÂ≠òËäÇÁÇπËØçÂàÜÂ∏ÉËÆ∞ÂΩï (‰øùÊåÅ‰∏çÂèò)\n",
    "        all_node_words = []\n",
    "        for record in filtered_records_for_detail:\n",
    "            all_node_words.extend(record['node_word_distributions'])\n",
    "        \n",
    "        node_word_df = pd.DataFrame(all_node_words)\n",
    "        node_word_df.to_csv(f'{base_filename}_node_word_distributions.csv', index=False, encoding='utf-8')\n",
    "        print(f\"‚úÖ node word distribution is saved: {base_filename}_node_word_distributions.csv\")\n",
    "        \n",
    "        # 7. ‰øùÂ≠òË∑≥Ë∑ÉËÆ∞ÂΩï\n",
    "        all_jump_records = []\n",
    "        for record in filtered_records_for_detail:\n",
    "            if 'jump_records_list' in record and record['jump_records_list']: # Check if the key exists and is not empty\n",
    "                all_jump_records.extend(record['jump_records_list']) # extend since it's a list of records from Gibbs\n",
    "        \n",
    "        jump_records_df = pd.DataFrame() # Initialize as empty DataFrame\n",
    "        if all_jump_records: # Only create DataFrame if there are records\n",
    "            jump_records_df = pd.DataFrame(all_jump_records)\n",
    "            # Potentially convert complex objects in jump_records_df to strings if they cause issues with CSV,\n",
    "            # For example, if 'all_probs_summary' is a dict:,\n",
    "            if 'all_probs_summary' in jump_records_df.columns:\n",
    "                    jump_records_df['all_probs_summary_str'] = jump_records_df['all_probs_summary'].astype(str)\n",
    "            jump_records_df.to_csv(f'{base_filename}_jump_records.csv', index=False, encoding='utf-8')\n",
    "            print(f\"‚úÖ jump record info is saved: {base_filename}_jump_records.csv\")\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è no jumpy record info...\")\n",
    "\n",
    "        # 8. ‰øùÂ≠òËØ¶ÁªÜËÆ∞ÂΩï (Â¶ÇÊûúÈúÄË¶Å)\n",
    "        all_detail_records = []\n",
    "        for record in filtered_records_for_detail:\n",
    "            if 'detail_records_list' in record and record['detail_records_list']:\n",
    "                all_detail_records.extend(record['detail_records_list'])\n",
    "        \n",
    "        detail_records_df = pd.DataFrame() # Initialize\n",
    "        if all_detail_records:\n",
    "            detail_records_df = pd.DataFrame(all_detail_records)\n",
    "            # Similar to jump_records, handle complex objects if any before saving to CSV\n",
    "            # For example, if 'delete_path_list' contains Node objects or complex dicts:\n",
    "            # detail_records_df['delete_path_list_str'] = detail_records_df['delete_path_list'].astype(str)\n",
    "            # Be cautious with saving very large structures like full path_list or doc_node_allocation per event.\n",
    "            detail_records_df.to_csv(f'{base_filename}_detail_records.csv', index=False, encoding='utf-8')\n",
    "            print(f\"‚úÖ other detailed info is saved: {base_filename}_detail_records.csv\")\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è no other detailed record info...\")\n",
    "        \n",
    "        return {\n",
    "            'doc_path_df': doc_path_df,\n",
    "            'path_structure_df': path_structure_df,\n",
    "            'word_allocation_df': word_allocation_df,\n",
    "            'node_word_df': node_word_df,\n",
    "            'summary_df': summary_df,\n",
    "            'path_doc_mapping_df': path_doc_mapping_df,\n",
    "            'jump_records_df': jump_records_df,       # Add to returned dict\n",
    "            'detail_records_df': detail_records_df    # Add to returned dict\n",
    "        }\n",
    "\n",
    "    def get_iteration_summary(self):\n",
    "        \"\"\"Ëé∑ÂèñËø≠‰ª£ËøáÁ®ãÊëòË¶Å\"\"\"\n",
    "        if not self.iteration_records:\n",
    "            return None\n",
    "        \n",
    "        summaries = [record['iteration_summary'] for record in self.iteration_records]\n",
    "        return pd.DataFrame(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf397ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "def save_gibbs_checkpoint(filename, recorder, root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation, iteration):\n",
    "    \"\"\"‰øùÂ≠òGibbsÈááÊ†∑ÁöÑÊñ≠ÁÇπÔºàÊâÄÊúâÂÖ≥ÈîÆÂèòÈáèÔºâ\"\"\"\n",
    "    checkpoint = {\n",
    "        'recorder': recorder,\n",
    "        'root_node': root_node,\n",
    "        'path_list': path_list,\n",
    "        'doc_path': doc_path,\n",
    "        'doc_word_allocation': doc_word_allocation,\n",
    "        'doc_node_allocation': doc_node_allocation,\n",
    "        'iteration': iteration\n",
    "    }\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(checkpoint, f)\n",
    "    print(f\"‚úÖ checkpoint is saved to: {filename}ÔºàËø≠‰ª£ {iteration}Ôºâ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daaf8f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gibbs_checkpoint(filename):\n",
    "    \"\"\"‰ªéÊñ≠ÁÇπÊñá‰ª∂ÊÅ¢Â§çGibbsÈááÊ†∑ÁöÑÊâÄÊúâÂÖ≥ÈîÆÂèòÈáè\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        checkpoint = pickle.load(f)\n",
    "    print(f\"‚úÖ from checkpoint file {filename} recover to iteration {checkpoint['iteration']}\"),\n",
    "    return (checkpoint['recorder'], checkpoint['root_node'], checkpoint['path_list'],\n",
    "            checkpoint['doc_path'], checkpoint['doc_word_allocation'],\n",
    "            checkpoint['doc_node_allocation'], checkpoint['iteration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "076ea874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually, this function is designed to be used after all iterations are done, to get stable documents across the last `window` iterations\n",
    "# but we also can use it in a sliding window manner during iterations, if we input the windowed portion of doc_path_lst\n",
    "def get_stable_docs_sliding_window(doc_path_lst_portion, back_window=5): # Ëø≠‰ª£ÂÆåÊàêÂêéËÆ°ÁÆó\n",
    "    \"\"\"\n",
    "    ËøîÂõûÊØè‰∏ÄËΩÆÔºà‰ªéwindowËΩÆÂºÄÂßãÔºâËøûÁª≠windowËΩÆË∑ØÂæÑÈÉΩÊ≤°ÂèòÁöÑÊñáÊ°£IDÈõÜÂêà\n",
    "    ËøîÂõûÊ†ºÂºèÔºö{Ëø≠‰ª£Âè∑: set(Á®≥ÂÆöÊñáÊ°£ID)}\n",
    "    \"\"\"\n",
    "    iter_portion = sorted(doc_path_lst_portion.keys())\n",
    "\n",
    "    doc_ids = sorted(doc_path_lst_portion[iter_portion[-1]].keys())\n",
    "    doc_idx_map = {doc_id: idx for idx, doc_id in enumerate(doc_ids)} # Ëøô‰∏™ÊòØÊãÖÂøÉdoc_ids‰∏çËøûÁª≠\n",
    "    n_docs = len(doc_ids)\n",
    "    n_iters = len(iter_portion) # 15\n",
    "    # ÊûÑÈÄ†ÂÆåÊï¥Ë∑ØÂæÑÁü©Èòµ shape=(n_iters, n_docs)\n",
    "    path_matrix = np.full((n_iters, n_docs), -1, dtype=np.int32)\n",
    "    for i, iter_num in enumerate(iter_portion):\n",
    "        for doc_id, leaf_id in doc_path_lst_portion[iter_num].items():\n",
    "            path_matrix[i, doc_idx_map[doc_id]] = leaf_id\n",
    "    # ÊªëÂä®Á™óÂè£ÁªüËÆ°\n",
    "    stable_dict = {}\n",
    "    # sliding window from 4 and review back from [0,5] to find stable docs\n",
    "    for i in range(back_window-1, n_iters): # range from 4, and review back, is okay\n",
    "        window_matrix = path_matrix[i-back_window+1:i+1, :]  # [0,5] rows # shape=(window, n_docs)\n",
    "        first_row = window_matrix[0]\n",
    "        stable_mask = np.all(window_matrix == first_row, axis=0) & (first_row != -1)\n",
    "        stable_doc_ids = {doc_ids[j] for j in np.where(stable_mask)[0]}\n",
    "        stable_dict[iter_portion[i]] = stable_doc_ids # ‰ªéÁ¨¨Âõõ‰∏™ÂºÄÂßãÁÆóÁ®≥ÂÆöÊñáÊ°£\n",
    "    return stable_dict\n",
    "## now, it can be used for check_inner_convergence\n",
    "\n",
    "def get_jaccard_list_from_stable_dict(stable_dict, doc_node_allocation_lst_portion, back_window=5, depth=3):\n",
    "    \"\"\"\n",
    "    ËøîÂõûÊØè‰∏ÄËΩÆ‰∏éÂâç‰∏ÄËΩÆÁ®≥ÂÆöÊñáÊ°£ÈõÜÂêàÁöÑJaccardÁõ∏‰ººÂ∫¶ÂàóË°®\n",
    "    Âπ∂ÁªüËÆ°Á™óÂè£ÂÜÖÊØèÂ±ÇÊØèËØçÊúÄÂ∞èËØçÈ¢ëÂÄºÂá∫Áé∞Ê¨°Êï∞ÊâÄÂç†ÊØî‰æãÔºàÊúÄÂ∞èÂÄºÁ®≥ÂÆöÊÄßÔºâ\n",
    "    \"\"\"\n",
    "    iters = sorted(stable_dict.keys()) # ‰ªé4ÂºÄÂßã\n",
    "    jaccard_list = []\n",
    "    stable_word_ratio_list = []\n",
    "    for i in range(1, len(iters)):\n",
    "        set1 = stable_dict[iters[i-1]]\n",
    "        set2 = stable_dict[iters[i]]\n",
    "        intersection = len(set1 & set2)\n",
    "        union = len(set1 | set2)\n",
    "        jaccard_index = intersection / union if union != 0 else 0\n",
    "        jaccard_list.append(jaccard_index)\n",
    "\n",
    "    for i in range(len(iters)):\n",
    "        if doc_node_allocation_lst_portion is not None and iters[0] >= back_window-1: # Á°Æ‰øùÊúâË∂≥Â§üÁöÑËø≠‰ª£Ê¨°Êï∞\n",
    "            stable_docs = stable_dict[iters[i]]\n",
    "            min_freq_sum = 0\n",
    "            total_freq_sum = 0\n",
    "            for doc_id in stable_docs:\n",
    "                window_allocs = [doc_node_allocation_lst_portion[j].get(doc_id, {}) for j in range(iters[i]-back_window+1, iters[i]+1)] # Á™óÂè£ÂÜÖÁöÑÂàÜÈÖç\n",
    "                # print(f\"window_allocs: {window_allocs}\")\n",
    "                for layer in range(depth): \n",
    "                    # ÂêàÂπ∂Á™óÂè£ÂÜÖÊâÄÊúâËØç\n",
    "                    words = set()\n",
    "                    for alloc in window_allocs:\n",
    "                        words.update(alloc.get(layer, {}).keys())\n",
    "                    for word in words:\n",
    "                        freq_list = [alloc.get(layer, {}).get(word, 0) for alloc in window_allocs]\n",
    "                        min_freq = min(freq_list)\n",
    "                        min_freq_sum += min_freq\n",
    "                        total_freq_sum += sum(freq_list)\n",
    "            ratio = min_freq_sum / total_freq_sum if total_freq_sum > 0 else 0\n",
    "            stable_word_ratio_list.append(ratio)\n",
    "            # print(f\"iter={iters[i]}, min_freq_sum={min_freq_sum}, total_freq_sum={total_freq_sum}, ratio={ratio}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è unable to calculate word ratio stability, missing doc_node_allocation_lst or not enough iterations\")\n",
    "            exit()\n",
    "    return jaccard_list, stable_word_ratio_list\n",
    "\n",
    "def calculate_gelman_rubin(chain_values, use_differences=False):\n",
    "    \"\"\"\n",
    "    ËÆ°ÁÆó Gelman-Rubin ÁªüËÆ°Èáè (R-hat)\n",
    "    \n",
    "    Args:\n",
    "        chain_values: Â≠óÂÖ∏ÔºåÈîÆ‰∏∫ÈìæIDÔºåÂÄº‰∏∫ËØ•ÈìæÁöÑjaccard list\n",
    "        warmup: È¢ÑÁÉ≠ÊúüÊØî‰æãÔºå‰∏¢ÂºÉÊØèÊù°ÈìæÂâç warmup% ÁöÑÊ†∑Êú¨\n",
    "        \n",
    "    Returns:\n",
    "        float: Gelman-Rubin ÁªüËÆ°Èáè\n",
    "    \"\"\"\n",
    "     # Â§ÑÁêÜÂéüÂßãÊï∞ÊçÆ\n",
    "    if use_differences:\n",
    "        # ËÆ°ÁÆóÂ∑ÆÂÄºÔºàÂΩìÂâçËΩÆÊ¨°ÂáèÂéªÂâç‰∏ÄËΩÆÊ¨°Ôºâ\n",
    "        diff_chains = {}\n",
    "        for chain_id, values in chain_values.items():\n",
    "            diff_values = [abs(values[i] - values[i-1]) for i in range(1, len(values))]\n",
    "            diff_chains[chain_id] = diff_values\n",
    "        chain_values = diff_chains\n",
    "\n",
    "    chains = [chain for chain in chain_values.values()]\n",
    "    \n",
    "    n_chains = len(chains)\n",
    "    n_samples = len(chains[0])\n",
    "    \n",
    "    # ËÆ°ÁÆóÈìæÂÜÖÂùáÂÄº\n",
    "    chain_means = [np.mean(chain) for chain in chains]\n",
    "    \n",
    "    # ËÆ°ÁÆóÂÖ®Â±ÄÂùáÂÄº\n",
    "    global_mean = np.mean(chain_means)\n",
    "    \n",
    "    # ËÆ°ÁÆóÈìæÈó¥ÊñπÂ∑Æ B\n",
    "    between_chain_var = n_samples * np.sum([(mean - global_mean)**2 for mean in chain_means]) / (n_chains - 1)\n",
    "    \n",
    "    # ËÆ°ÁÆóÈìæÂÜÖÊñπÂ∑Æ W\n",
    "    within_chain_var = np.mean([np.var(chain, ddof=1) for chain in chains])\n",
    "    \n",
    "    # ËÆ°ÁÆóÊñπÂ∑Æ‰º∞ËÆ°\n",
    "    var_estimate = ((n_samples - 1) / n_samples) * within_chain_var + (1 / n_samples) * between_chain_var\n",
    "    \n",
    "    # ËÆ°ÁÆó R-hat\n",
    "    r_hat = np.sqrt(var_estimate / within_chain_var)\n",
    "    \n",
    "    return r_hat\n",
    "\n",
    "def check_convergence_with_gelman_rubin(chain_len_docs, mean_jaccard, mean_ratio, iteration, r_hat_threshold=1.1):\n",
    "    \"\"\"\n",
    "    Ë∑ØÂæÑÈÄâÊã©ÂíåÁ®≥ÂÆöËØçÂàÜÈÖçratioÈÉΩË¶ÅR-hatÊî∂ÊïõÊâçËÆ§‰∏∫Êï¥‰ΩìÊî∂Êïõ\n",
    "    \"\"\"\n",
    "    print(f\"check_dual_convergence_with_gelman_rubin: chain_len_docs:{chain_len_docs}, mean_jaccard:{mean_jaccard}, mean_ratio:{mean_ratio}\")\n",
    "    r_hat_len_docs = calculate_gelman_rubin(chain_len_docs, use_differences=False)\n",
    "    is_converged = (r_hat_len_docs < r_hat_threshold)\n",
    "    \n",
    "    convergence_info = {\n",
    "        \"iteration\": iteration,\n",
    "        \"r_hat_len_docs\": r_hat_len_docs,\n",
    "        \"mean_jaccard\": mean_jaccard,\n",
    "        \"mean_ratio\": mean_ratio,\n",
    "        \"threshold\": r_hat_threshold,\n",
    "        \"converged\": is_converged,\n",
    "    }\n",
    "    print(f\"ü•∞ Convergence status: {'‚úÖ Converged!' if is_converged else '‚ùå Not converged...'}\")\n",
    "    print(f\"ü•∞  R-hat info: {r_hat_len_docs:.4f}Ôºåmean_jaccard:{mean_jaccard}, mean_ratio:{mean_ratio}\")\n",
    "    return is_converged, convergence_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0692653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedState:\n",
    "    \"\"\"Â§öËøõÁ®ãÈó¥ÂÖ±‰∫´Áä∂ÊÄÅÁÆ°ÁêÜÁ±ªÔºåÊîØÊåÅJaccardÂíåratioÂéÜÂè≤ÁöÑÂ≠òÂÇ®‰∏éËØªÂèñ\"\"\"\n",
    "    def __init__(self, base_dir):\n",
    "        self.base_dir = base_dir\n",
    "        self.state_file = os.path.join(base_dir, \"shared_state.json\")\n",
    "        self.ensure_dir()\n",
    "        self.init_state()\n",
    "\n",
    "    def ensure_dir(self):\n",
    "        os.makedirs(self.base_dir, exist_ok=True)\n",
    "\n",
    "    def init_state(self):\n",
    "        state = {\n",
    "            \"chains\": {},\n",
    "            \"last_update\": time.time(),\n",
    "            \"rhat_history\": [],\n",
    "            \"convergence\": False\n",
    "        }\n",
    "        self.save_state(state)\n",
    "\n",
    "    def save_state(self, state):\n",
    "        import json\n",
    "        def convert_numpy_types(obj):\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            elif isinstance(obj, np.bool_):\n",
    "                return bool(obj)\n",
    "            elif isinstance(obj, dict):\n",
    "                return {k: convert_numpy_types(v) for k, v in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [convert_numpy_types(i) for i in obj]\n",
    "            return obj\n",
    "        converted_state = convert_numpy_types(state)\n",
    "        with open(self.state_file, \"w\") as f:\n",
    "            json.dump(converted_state, f)\n",
    "\n",
    "    def load_state(self):\n",
    "        import json\n",
    "        try:\n",
    "            with open(self.state_file, \"r\") as f:\n",
    "                return json.load(f)\n",
    "        except (FileNotFoundError, json.JSONDecodeError):\n",
    "            self.init_state()\n",
    "            return self.load_state()\n",
    "\n",
    "    def update_chain_data(self, chain_id, len_stable_docs, jaccard_list, ratio_list, iteration, record_time): # update a single chain's data\n",
    "        \"\"\"\n",
    "        Êõ¥Êñ∞ÊüêÊù°ÈìæÁöÑÊï∞ÊçÆÔºàJaccardÂíåratioÂéÜÂè≤Ôºâ\n",
    "        \"\"\"\n",
    "        state = self.load_state()\n",
    "        if str(chain_id) not in state[\"chains\"]:\n",
    "            state[\"chains\"][str(chain_id)] = {\n",
    "                \"len_stable_docs\":{},\n",
    "                \"jaccard\": {},\n",
    "                \"ratio\": {},\n",
    "                \"record_time\": -1,\n",
    "                \"converged\": False\n",
    "            }\n",
    "        chain_data = state[\"chains\"][str(chain_id)]\n",
    "        \n",
    "        # ÊÑüËßâÂ¶ÇÊûúÊòØÁ®≥ÂÆöË∑ØÂæÑÊï∞ÁöÑËØùÔºåÂÄí‰πü‰∏çÁî®Áî®till nowÁöÑmeanÂíåstdÔºåÂõ†‰∏∫Êï∞ÈáèÊØîËæÉÂ•ΩË°°ÈáèÊòØ‰∏çÊòØÔºü\n",
    "        chain_data[\"len_stable_docs\"][str(record_time)] = len_stable_docs\n",
    "        chain_data[\"jaccard\"][str(record_time)] = jaccard_list\n",
    "        chain_data[\"ratio\"][str(record_time)] = ratio_list\n",
    "        chain_data[\"record_time\"] = record_time\n",
    "        state[\"last_update\"] = time.time()\n",
    "        self.save_state(state)\n",
    "\n",
    "    def update_rhat(self, rhat_info, iteration, converged=False):\n",
    "        state = self.load_state()\n",
    "        state[\"rhat_history\"].append({\n",
    "            \"rhat_len_docs\": rhat_info.get(\"r_hat_len_docs\"),\n",
    "            \"mean_jaccard\": rhat_info.get(\"mean_jaccard\"),\n",
    "            \"mean_ratio\": rhat_info.get(\"mean_ratio\"),\n",
    "            \"rhat_threshold\": rhat_info.get(\"r_hat_threshold\"),\n",
    "            \"converged\": converged,\n",
    "            \"iteration\": iteration,\n",
    "            \"timestamp\": time.time()\n",
    "        })\n",
    "        state[\"convergence\"] = converged\n",
    "        self.save_state(state)\n",
    "\n",
    "    def update_convergence_status(self, converged):\n",
    "        state = self.load_state()\n",
    "        state[\"convergence\"] = converged\n",
    "        self.save_state(state)\n",
    "\n",
    "    def get_chain_data_histories(self, record_time, max_retry=5, wait_sec=2):\n",
    "        for _ in range(max_retry):\n",
    "            state = self.load_state()\n",
    "            len_stable_docs = {}\n",
    "            jaccard_lists = {}\n",
    "            ratio_lists = {}\n",
    "\n",
    "            for chain_id, data in state[\"chains\"].items():\n",
    "                # Âè™ÂèñÂΩìÂâçrecord_timeÂØπÂ∫îÁöÑÊï∞ÊçÆ\n",
    "                len_stable_docs[str(chain_id)] = data[\"len_stable_docs\"][str(record_time)]\n",
    "                jaccard_lists[str(chain_id)] = data[\"jaccard\"][str(record_time)]\n",
    "                ratio_lists[str(chain_id)] = data[\"ratio\"][str(record_time)]\n",
    "\n",
    "        return len_stable_docs, jaccard_lists, ratio_lists\n",
    "    \n",
    "    def get_latest_completed_record(self, n_chains): # find the min last_iteration across all chains\n",
    "        state = self.load_state() # state is a dict from JSON file\n",
    "        \"\"\"\n",
    "        state = {\n",
    "        \"chains\": {},\n",
    "        \"last_update\": time.time(),\n",
    "        \"rhat_history\": [],\n",
    "        \"convergence\": False\n",
    "        }\n",
    "        \"\"\"\n",
    "        if len(state[\"chains\"]) < n_chains: # must have all chains\n",
    "            return -1\n",
    "        chain_record = []\n",
    "        for data in state[\"chains\"].values():\n",
    "            chain_record.append(data.get(\"record_time\", -1))\n",
    "        return min(chain_record)\n",
    "\n",
    "    def get_convergence_status(self):\n",
    "        state = self.load_state()\n",
    "        return state[\"convergence\"]\n",
    "\n",
    "    def all_chains_finished(self, n_chains):\n",
    "        state = self.load_state()\n",
    "        if len(state[\"chains\"]) < n_chains:\n",
    "            return False\n",
    "        return all(data.get(\"converged\", False) for data in state[\"chains\"].values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64aee7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rhat_monitor_process(shared_state, n_chains, max_iterations, burn_in, check_interval, back_window, r_hat_threshold=1.1):\n",
    "    \"\"\"\n",
    "    ÁõëÊéßJaccardÂíåratioÁöÑR-hatÔºåÂèäÊó∂Âà§Êñ≠Â§öÈìæÊî∂Êïõ\n",
    "    shared_state: SharedStateÂÆû‰æã\n",
    "    n_chains: ÈìæÊï∞Èáè\n",
    "    max_iterations: ÊúÄÂ§ßËø≠‰ª£Ê¨°Êï∞\n",
    "    r_hat_threshold: R-hatÊî∂ÊïõÈòàÂÄº\n",
    "    ËØ•ËøõÁ®ãÊåÅÁª≠ËøêË°åÔºåÁõ¥Âà∞Ê£ÄÊµãÂà∞Êî∂ÊïõÊàñÊâÄÊúâÈìæÂÆåÊàê\n",
    "    \"\"\"\n",
    "    print(\"üîç Initialize R-hat independent monitor thread (stable docs & Jaccard & Ratio)\")\n",
    "    convergence_detected = False\n",
    "    record_time = 0\n",
    "\n",
    "    while True:\n",
    "        # ÂÖ∂ÂÆûËøôÈáåÂ∞±‰∏çÈúÄË¶Å‰∫ÜÔºåÂõ†‰∏∫_checkpointÂ∑≤ÁªèÂÅöÂá∫Âà§Êñ≠‰∫ÜÔºåËÄå‰∏îÂæàÂç±Èô©ÁöÑÊòØÔºåÂ¶ÇÊûúmonitorÊôöÁÇπÊ£ÄÊü•Â∞±‰ºöË¢´Êõø‰ª£ÔºåÊâÄ‰ª•Ë¶ÅÊääupdateÂáΩÊï∞Êîπ‰∏∫append\n",
    "        current_record_time = shared_state.get_latest_completed_record(n_chains=n_chains)\n",
    "        # last replaced lsts, iterationÊòØÂêëÂâçÂõûÊ∫ØÁöÑ\n",
    "        \n",
    "        if not current_record_time >= record_time:\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "        \n",
    "        chain_len_docs, chain_jaccard_lists, chain_ratio_lists = shared_state.get_chain_data_histories(record_time = record_time)\n",
    "        # print(f\"chain_len_docs:{chain_len_docs}, chain_ratio_lists: {chain_ratio_lists}, chain_jaccard_lists: {chain_jaccard_lists}\")\n",
    "        \n",
    "        mean_jaccard = [np.mean(jaccard_lst) for jaccard_lst in chain_jaccard_lists.values()]\n",
    "        mean_ratio = [np.mean(ratio_lst) for ratio_lst in chain_ratio_lists.values()]\n",
    "        # print(mean_jaccard, mean_ratio)                   \n",
    "        \n",
    "        current_last_iteration = min(burn_in+back_window+current_record_time*check_interval, max_iterations)\n",
    "        is_converged, convergence_info = check_convergence_with_gelman_rubin(chain_len_docs=chain_len_docs, mean_jaccard=mean_jaccard, mean_ratio=mean_ratio, iteration=current_last_iteration, r_hat_threshold=r_hat_threshold)\n",
    "\n",
    "        \n",
    "        # print(f\"chain_len_docs:{chain_len_docs}, chain_ratio_lists: {chain_ratio_lists}, chain_jaccard_lists: {chain_jaccard_lists}\")\n",
    " \n",
    "        \"\"\"\n",
    "        convergence_info = {\n",
    "        \"iteration\": iteration,\n",
    "        \"r_hat_len_docs\": r_hat_len_docs,\n",
    "        \"mean_jaccard\": mean_jaccard,\n",
    "        \"mean_ratio\": mean_ratio,\n",
    "        \"threshold\": r_hat_threshold,\n",
    "        \"converged\": is_converged,\n",
    "    }\n",
    "        \"\"\"\n",
    "\n",
    "        # ËÆ∞ÂΩïR-hatÂÄº\n",
    "        shared_state.update_rhat(convergence_info, current_last_iteration, converged=is_converged)\n",
    "\n",
    "        if is_converged:\n",
    "            print(f\"‚úÖ r-hat of multi-chains is detected to convergence {burn_in+back_window+record_time*check_interval} in iteration:{current_last_iteration}, R-hat={convergence_info['r_hat_len_docs']:.4f}, JaccardÂπ≥ÂùáÂÄº={convergence_info['mean_jaccard']}, RatioÂπ≥ÂùáÂÄº={convergence_info['mean_ratio']}\")\n",
    "            convergence_detected = True\n",
    "            shared_state.update_convergence_status(True)\n",
    "\n",
    "        record_time += 1\n",
    "\n",
    "        # Ê£ÄÊü•ÊòØÂê¶ÊâÄÊúâÈìæÈÉΩÂ∑≤ÂÆåÊàê\n",
    "        if shared_state.all_chains_finished(n_chains) or current_last_iteration >= max_iterations:\n",
    "            print(\"‚úä All chains finished or max iterations reached, exiting r-hat monitor process.\")\n",
    "            break\n",
    "\n",
    "        if convergence_detected:\n",
    "            print(\"‚úÖ R-hat convergence is detected, waiting for all chains to finish.\")\n",
    "\n",
    "        time.sleep(2)\n",
    "    print(\"‚úÖ R-hat monitor process finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "726e2a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gibbs_with_checkpoint(\n",
    "    corpus, depth=3, gamma=None, eta=None, alpha=None,\n",
    "    max_iterations=200, save_every_n_iter=10, checkpoint_dir='gibbs_checkpoints',\n",
    "    check_interval=10, resume_from_checkpoint=None, base_dir=None,\n",
    "    shared_state=None, chain_id=None, back_window=5, burn_in = 50):\n",
    "    \"\"\"\n",
    "    ÊîØÊåÅÊñ≠ÁÇπ‰øùÂ≠ò‰∏éÊÅ¢Â§çÁöÑGibbsÈááÊ†∑‰∏ªÂæ™ÁéØ\n",
    "    - resume_from_checkpoint: Êñ≠ÁÇπÊñá‰ª∂Ë∑ØÂæÑÔºàÂ¶ÇÈúÄÊÅ¢Â§çÂàôÂ°´ÂÜôÔºåÂê¶Âàô‰ªéÂ§¥ÂºÄÂßã)\n",
    "    \n",
    "     result = run_gibbs_with_checkpoint(\n",
    "        corpus=corpus,\n",
    "        depth=depth,\n",
    "        gamma=gamma,\n",
    "        eta=eta,\n",
    "        alpha=alpha,\n",
    "        max_iterations=max_iterations,\n",
    "        checkpoint_dir=os.path.join(chain_dir, \"checkpoints\"),\n",
    "        base_dir=chain_dir,\n",
    "        chain_id=chain_id,\n",
    "        shared_state=shared_state,\n",
    "        back_window=back_window,\n",
    "        burn_in=burn_in,  # ËÆæÁΩÆburn-inÊúü‰∏∫50ËΩÆ\n",
    "        check_interval=check_interval  # ËÆæÁΩÆÊªëÂä®Á™óÂè£Â§ßÂ∞è\n",
    "    \"\"\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    if resume_from_checkpoint is not None and os.path.exists(resume_from_checkpoint):\n",
    "        # ‰ªéÊñ≠ÁÇπÊÅ¢Â§ç\n",
    "        (recorder, root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation, start_iter) = load_gibbs_checkpoint(resume_from_checkpoint)\n",
    "        print(f\"Resume from checkpoint and continue sampling (from iteration {start_iter+1})\")\n",
    "    else:\n",
    "        # ÂÖ®Êñ∞ÂºÄÂßã,\n",
    "        print(\"üìä Initialize nCRP Process...\")\n",
    "        [root_node, path_list, doc_path, doc_word_allocation] = nCRP(\n",
    "            corpus=corpus, depth=depth, gamma=gamma,\n",
    "        )\n",
    "        # path_list: {leaf_id: [Node1, Node2, ...]}\n",
    "        # doc_path: {doc_id: leaf_id}\n",
    "        # doc_word_allocation: {doc_id: [layer_for_word1, layer_for_word2,...]}\n",
    "        \n",
    "        recorder = Recorder(corpus, depth, eta, alpha) # for Recorder self._init_ setting\n",
    "        doc_node_allocation = aggregate_words(corpus, doc_word_allocation)\n",
    "        # doc_node_allocation: {doc_id: {layer: {word: count}}}\n",
    "        \n",
    "        global_node_word_dist = node_word_distribution(doc_node_allocation, doc_path, path_list, exclude_docs=None)\n",
    "        # global_node_word_dist[node_id]: {layer: {word: count}}\n",
    "        \n",
    "        # ÂÆö‰πâËØ¶ÁªÜËÆ∞ÂΩïÂºÄÂßãÁöÑËø≠‰ª£Ôºà‰∏éGibbsÈááÊ†∑‰∏≠ÁöÑiter_start‰∏ÄËá¥Ôºâ\n",
    "        iter_start_for_log = burn_in + back_window\n",
    "\n",
    "        initial_summary = recorder.record_iteration(\n",
    "            0, root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation,global_node_word_dist,\n",
    "            iter_start_for_detailed_log=iter_start_for_log # ‰º†ÈÄíÁªôÂàùÂßãËÆ∞ÂΩï\n",
    "        ) # record the initial state\n",
    "        \"\"\"\n",
    "        recorder_iteration return: some basic infomation\n",
    "        iteration_summary = {\n",
    "                    'iteration': iteration_num,\n",
    "                    'total_paths': len(path_list),\n",
    "                    'total_documents': len(doc_path),\n",
    "                    'log_likelihood': log_likelihood,\n",
    "                    'changed_docs_count': changed_docs_count, \n",
    "                    'newly_created_paths': len(newly_created_paths or []),\n",
    "                    'avg_path_size',\n",
    "                    'max_path_size',\n",
    "                    'min_path_size'}\n",
    "        \"\"\"\n",
    "        print(f\"üìù Chain {chain_id} initial state after nCRP is recorded: {initial_summary['total_paths']} path, \", \n",
    "                f\"Log-likelihood: {initial_summary['log_likelihood']:.2f}\")\n",
    "        start_iter = 1\n",
    "\n",
    "    previous_log_likelihood = recorder.iteration_records[-1]['iteration_summary']['log_likelihood']\n",
    "    loglikelihood_list = [previous_log_likelihood]\n",
    "    change_docs_list = []\n",
    "\n",
    "    doc_path_lst = {0:doc_path.copy()}# Áî®‰∫éÂ≠òÂÇ®ÊØèÊù°ÈìæÁöÑjaccardÂéÜÂè≤ËÆ∞ÂΩï\n",
    "    doc_node_allocation_lst = {0:doc_node_allocation.copy()} # Áî®‰∫éÂ≠òÂÇ®ÊØèÊù°ÈìæÁöÑdoc_node_allocationÂéÜÂè≤ËÆ∞ÂΩï\n",
    "\n",
    "    record_time = 0\n",
    "    start_window = burn_in + back_window \n",
    "    \n",
    "    for iteration in range(start_iter, max_iterations + 1): # start_iter=1\n",
    "        old_paths = set(path_list.keys())\n",
    "        doc_path_lst[iteration] = doc_path.copy()\n",
    "        \n",
    "        # ÂÆö‰πâËØ¶ÁªÜËÆ∞ÂΩïÂºÄÂßãÁöÑËø≠‰ª£Ôºà‰∏éGibbsÈááÊ†∑‰∏≠ÁöÑiter_start‰∏ÄËá¥Ôºâ\n",
    "        # ‰πüÁî®‰∫éÊéßÂà∂RecorderÁöÑËØ¶ÁªÜÊó•ÂøóËÆ∞ÂΩï           \n",
    "        current_iter_start_for_detailed_log = burn_in + back_window\n",
    "\n",
    "        jumpy_record, detailed_record = Gibbs_sampling(\n",
    "            corpus, root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation, global_node_word_dist,\n",
    "            gamma, eta, alpha, depth, iteration, \n",
    "            iter_start=current_iter_start_for_detailed_log, # GibbsÈááÊ†∑ÂÜÖÈÉ®‰ΩøÁî®\n",
    "            iter_end=max_iterations\n",
    "        )\n",
    "        \"\"\"\n",
    "        detail_record.items() = {\n",
    "            'iteration': iteration,\n",
    "            'doc_id': doc_id,\n",
    "            'deleted_path_list': {leaf_id: [node.node_id for node in path_nodes] for leaf_id, path_nodes in path_list.items()}, # Ê≥®ÊÑèÊ∑±Êã∑Ë¥ùÁöÑÊÄßËÉΩÂΩ±Âìç\n",
    "            'doc_path': {k:v for k, v in doc_path.items()},\n",
    "            'doc_word_allocation': list(doc_word_allocation[doc_id]),\n",
    "            'doc_node_allocation': {layer: {word: count for word, count in word_counts.items()} \n",
    "                                    for layer, word_counts in doc_node_allocation[doc_id].items()}\n",
    "        }\n",
    "\n",
    "        jump_record.items() = {\n",
    "            'iteration': iteration,\n",
    "            'doc_id': doc_id,\n",
    "            'old_leaf_id': current_path[-1].node_id,\n",
    "            'old_path': [n.node_id for n in current_path],\n",
    "            'new_leaf_id': added_path[-1].node_id,\n",
    "            'new_path': [n.node_id for n in added_path],\n",
    "            'origal_probs':all_probs,\n",
    "            'normalized_probs': normalized_probs,\n",
    "            'chosen_path_prob': chosen_path_prob,\n",
    "            'rank': f'{rank} out of {len(values)}',\n",
    "            'create_path': True\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "        doc_path_lst[iteration] = doc_path.copy() # ËÆ∞ÂΩïÂΩìÂâçËø≠‰ª£ÁöÑdoc_pathÔºåÁî®‰∫éÂêéÁª≠ËÆ°ÁÆójaccard\n",
    "        doc_node_allocation_lst[iteration] = doc_node_allocation.copy() # ËÆ∞ÂΩïÂΩìÂâçËø≠‰ª£ÁöÑdoc_node_allocation\n",
    "\n",
    "        new_paths = set(path_list.keys()) - old_paths\n",
    "        iteration_summary = recorder.record_iteration(\n",
    "            iteration, root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation, global_node_word_dist,\n",
    "            jumpy_record, detailed_record, new_paths,\n",
    "            iter_start_for_detailed_log=current_iter_start_for_detailed_log # ‰º†ÈÄíÁªôRecorder\n",
    "        )\n",
    "        print(f\"üîÑ Chain {chain_id} in iteration {iteration}/{max_iterations},\\n\",\n",
    "                f\"üìä path: {iteration_summary['total_paths']}, \",\n",
    "                f\"new path: {iteration_summary['newly_created_paths']}, \",\n",
    "                f\"docs changed path: {iteration_summary['changed_docs_count']}, \",\n",
    "                f\"Log-likelihood: {iteration_summary['log_likelihood']:.2f}\")\n",
    "        \n",
    "        # Êñ≠ÁÇπ‰øùÂ≠ò\n",
    "        # if (iteration % save_every_n_iter == 0) or (iteration == max_iterations):\n",
    "        #     checkpoint_path = os.path.join(checkpoint_dir, f'gibbs_checkpoint_iter{iteration}.pkl')\n",
    "        #     save_gibbs_checkpoint(\n",
    "        #         checkpoint_path, recorder, root_node, path_list, doc_path,\n",
    "        #         doc_word_allocation, doc_node_allocation, iteration,\n",
    "        #     )\n",
    "\n",
    "        # Ê£ÄÊü•Êî∂Êïõ\n",
    "        change_docs_list.append(iteration_summary['changed_docs_count'])\n",
    "        loglikelihood_list.append(iteration_summary['log_likelihood'])\n",
    "        \n",
    "        # iteration starts from 1, burn_in + back_window + check_interval \n",
    "        # print((iteration - start_window) / check_interval)\n",
    "        if (iteration - start_window) / check_interval == 1 or  (iteration == max_iterations): # iteration starts from 1\n",
    "            # ÊØèÊ¨°Ëø≠‰ª£ÂºÄÂßãÊó∂Ê£ÄÊü•ÊòØÂê¶Â∑≤Êî∂ÊïõÔºàÂø´ÈÄüÊ£ÄÊü•ÔºåÊó†ÈúÄÈáçÊñ∞ËÆ°ÁÆóÔºâ\n",
    "            convergence_status = shared_state.get_convergence_status() if shared_state else False\n",
    "            if convergence_status:\n",
    "                final_iteration = iteration\n",
    "                print(f\"üéâ Chain {chain_id} is detected to convergence in iteration {iteration}, sampling ends prematurely.\")\n",
    "                break\n",
    "                \n",
    "            \"\"\"    \n",
    "            check_interval=5,\n",
    "            back_window=3,\n",
    "            burn_in=2,\n",
    "            \"\"\"\n",
    "            window_keys = list(range(iteration-check_interval-back_window+1, iteration+1))\n",
    "            # print(f\"window_keys:{window_keys}\")\n",
    "            \n",
    "            doc_path_window = {k: doc_path_lst[k] for k in window_keys} # 6-20\n",
    "            \n",
    "            stable_docs = get_stable_docs_sliding_window(doc_path_window, back_window=back_window) # interval+1ËΩÆÁöÑstable docs\n",
    "            chain_len_docs = [len(doc_list) for iterd, doc_list in stable_docs.items()]\n",
    "            \n",
    "            doc_node_allocation_window = {k: doc_node_allocation_lst[k] for k in window_keys}\n",
    "            \n",
    "            jaccard_lst, ratio_lst = get_jaccard_list_from_stable_dict(stable_docs, doc_node_allocation_window, back_window=back_window, depth=depth)\n",
    "            \n",
    "            shared_state.update_chain_data(chain_id, chain_len_docs, jaccard_lst, ratio_lst, iteration, record_time)\n",
    "            start_window = iteration\n",
    "            record_time += 1\n",
    "            \n",
    "            # print(f\"stable_docs:{stable_docs},jaccard_lst{jaccard_lst}, ratio_lst{ratio_lst}\")\n",
    "            \n",
    "            \"\"\" def update_chain_data(self, chain_id, len_stable_docs, jaccard_list, ratio_list, iteration, record_time): # update a single chain's data\"\"\"\n",
    "    \n",
    "    print(f\"üéØ Chain {chain_id} finish Gibbs Sampling.\")\n",
    "    print(\"üíæ Save iterations info...\")\n",
    "    saved_files = recorder.save_to_files(base_filename=os.path.join(base_dir, \"iteration\") if base_dir else \"iteration\")\n",
    "    return recorder, root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation, saved_files, change_docs_list, loglikelihood_list, doc_path_lst, doc_node_allocation_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c02a9399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_renyi_entropy(prob_dist, q=2.0):\n",
    "    total = sum(prob_dist.values())\n",
    "    if total <= 0:\n",
    "        return 0.0\n",
    "    moment_q = 0.0\n",
    "    for v in prob_dist.values():\n",
    "        p = v / total\n",
    "        if p > 0:\n",
    "            moment_q += p ** q\n",
    "    if moment_q <= 0:\n",
    "        return float('inf')\n",
    "    return (1.0 / (1.0 - q)) * math.log(moment_q)  # Ëá™ÁÑ∂ÂØπÊï∞\n",
    "\n",
    "def jensen_shannon_divergence(dist1, dist2):\n",
    "    # Áªü‰∏ÄËØçË°®Âπ∂ÂΩí‰∏ÄÂåñ\n",
    "    keys = set(dist1.keys()) | set(dist2.keys())\n",
    "    s1 = float(sum(dist1.get(k, 0.0) for k in keys))\n",
    "    s2 = float(sum(dist2.get(k, 0.0) for k in keys))\n",
    "    if s1 == 0 or s2 == 0:\n",
    "        return 1.0\n",
    "    p = {k: dist1.get(k, 0.0) / s1 for k in keys}\n",
    "    q = {k: dist2.get(k, 0.0) / s2 for k in keys}\n",
    "    m = {k: 0.5 * (p[k] + q[k]) for k in keys}\n",
    "\n",
    "    def _kl(a, b):\n",
    "        val = 0.0\n",
    "        for k in keys:\n",
    "            ak = a[k]\n",
    "            bk = b[k]\n",
    "            if ak > 0 and bk > 0:\n",
    "                val += ak * math.log(ak / bk)\n",
    "        return val\n",
    "\n",
    "    return 0.5 * _kl(p, m) + 0.5 * _kl(q, m)\n",
    "\n",
    "def evaluate_tree_structure_with_nodes(root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation):\n",
    "    \"\"\"\n",
    "    ËøîÂõûÔºö\n",
    "      - node_records: ÊØèËäÇÁÇπ‰∏ÄË°åÔºàlayer/node_id/parent_id/entropy/doc_count/pathÔºâ\n",
    "      - layer_entropy_wavg: {layer: ÊñáÊ°£Êï∞Âä†ÊùÉÁöÑR√©nyiÁÜµ}\n",
    "      - layer_distinctiveness_wavg: {layer: ÊñáÊ°£Êï∞Âä†ÊùÉJSD(‰∏ªÈ¢òÂºÇË¥®ÊÄß)}\n",
    "      - nodes_per_layer: {layer: ËäÇÁÇπÊï∞}\n",
    "    ËØ¥ÊòéÔºö\n",
    "      - ËäÇÁÇπÊñáÊ°£Êï∞ÔºöÁªüËÆ°ËêΩÂú®ËØ•ËäÇÁÇπÔºàËØ•Â±ÇÔºâ‰∏äÁöÑÂéªÈáçÊñáÊ°£Êï∞\n",
    "      - Âä†ÊùÉÂπ≥ÂùáÔºöÁî®ÊàêÂØπÊùÉÈáç m_i*m_jÔºàÂºÇË¥®ÊÄßÔºâ‰∏éËäÇÁÇπÊùÉÈáç m_iÔºàÁÜµÔºâ\n",
    "    \"\"\"\n",
    "    # ËØçÈ¢ë‰∏éÊñáÊ°£ÈõÜÂêà\n",
    "    layer_word_dist = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))  # layer -> node_id -> word -> cnt\n",
    "    node_doc_sets = defaultdict(set)  # node_id -> {doc_ids}\n",
    "\n",
    "    # Ëã• root_node ÊòØ dict[node_id]->obj ‰∏îÂê´ parent_idÔºåÁî®ÂÆÉÔºõÂê¶Âàô parent_id ÁΩÆ None\n",
    "    get_parent_id = (lambda nid: getattr(root_node.get(nid), 'parent_id', None)) if isinstance(root_node, dict) else (lambda nid: None)\n",
    "\n",
    "    # Á¥ØÁßØÊØèËäÇÁÇπÁöÑËØçÈ¢ë‰∏éÊñáÊ°£ÈõÜÂêà\n",
    "    for doc_id, leaf_id in doc_path.items():\n",
    "        path_nodes = path_list[leaf_id]  # [Node1, Node2, ...]\n",
    "        per_doc_alloc = doc_node_allocation.get(doc_id, {})\n",
    "        for node in path_nodes:\n",
    "            lyr = node.layer\n",
    "            if lyr in per_doc_alloc:\n",
    "                # ËÆ∞ÂΩïËØçÈ¢ë\n",
    "                for w, c in per_doc_alloc[lyr].items():\n",
    "                    layer_word_dist[lyr][node.node_id][w] += c\n",
    "                # ËÆ∞ÂΩïÊñáÊ°£\n",
    "                node_doc_sets[node.node_id].add(doc_id)\n",
    "\n",
    "#     # Ê±áÊÄªÈÄêËäÇÁÇπ\n",
    "#     node_records = []\n",
    "#     for layer, nodes_dist in layer_word_dist.items():\n",
    "#         for nid, wdist in nodes_dist.items():\n",
    "#             entropy = calculate_renyi_entropy(wdist, q=2.0)\n",
    "#             doc_count = len(node_doc_sets[nid])\n",
    "#             node_records.append({\n",
    "#                 \"layer\": layer,\n",
    "#                 \"node_id\": nid,\n",
    "#                 \"parent_id\": get_parent_id(nid),\n",
    "#                 \"entropy\": entropy,\n",
    "#                 \"doc_count\": doc_count,\n",
    "#                 # ÂèØÈÄâÔºö‰øùÂ≠òË∑ØÂæÑÂ≠óÁ¨¶‰∏≤ÔºàÂ¶ÇÈúÄË¶ÅÔºå‰Ω†ÂèØ‰ª•Âú®Â§ñÈÉ®È¢ÑÂÖàÊûÑÈÄ† node_id->path_str ÁöÑÊò†Â∞Ñ‰º†ÂÖ•Ôºâ\n",
    "#                 \"path\": None\n",
    "#             })\n",
    "            \n",
    "    # Âú® evaluate_tree_structure ÂáΩÊï∞‰∏≠Ê∑ªÂä†Ë∞ÉËØï‰ø°ÊÅØ\n",
    "    node_records = []\n",
    "    for layer, nodes_dist in layer_word_dist.items():\n",
    "        for nid, wdist in nodes_dist.items():\n",
    "            entropy = calculate_renyi_entropy(wdist, q=2.0)\n",
    "            doc_count = len(node_doc_sets[nid]) if nid in node_doc_sets else 0\n",
    "\n",
    "            # Ê∑ªÂä†Ë∞ÉËØï‰ø°ÊÅØ\n",
    "            if entropy == 0:\n",
    "                print(f\"‚ö†Ô∏è Node {nid} (Layer {layer}) has entropy=0:\")\n",
    "                print(f\"   Word distribution: {dict(wdist)}\")\n",
    "                print(f\"   Number of unique words: {len(wdist)}\")\n",
    "                print(f\"   Total word count: {sum(wdist.values())}\")\n",
    "                print(f\"   Document count: {doc_count}\")\n",
    "                print()\n",
    "\n",
    "            node_records.append({\n",
    "                \"layer\": layer,\n",
    "                \"node_id\": nid,\n",
    "                \"parent_id\": get_parent_id(nid),\n",
    "                \"entropy\": entropy,\n",
    "                \"doc_count\": doc_count,\n",
    "                \"unique_words\": len(wdist),\n",
    "                \"total_words\": sum(wdist.values()),\n",
    "                \"path\": None\n",
    "            })\n",
    "        \n",
    "    # Â±ÇÁ∫ßÊñáÊ°£Âä†ÊùÉÁÜµ\n",
    "    layer_entropy_wavg = {}\n",
    "    nodes_per_layer = {}\n",
    "    for layer, nodes_dist in layer_word_dist.items():\n",
    "        nodes = list(nodes_dist.keys())\n",
    "        nodes_per_layer[layer] = len(nodes)\n",
    "        total_docs = sum(len(node_doc_sets[nid]) for nid in nodes)\n",
    "        if total_docs == 0:\n",
    "            layer_entropy_wavg[layer] = 0.0\n",
    "            continue\n",
    "        wsum = 0.0\n",
    "        for nid in nodes:\n",
    "            H = calculate_renyi_entropy(nodes_dist[nid], q=2.0)\n",
    "            w = len(node_doc_sets[nid])\n",
    "            wsum += H * w\n",
    "        layer_entropy_wavg[layer] = wsum / total_docs\n",
    "\n",
    "    # Â±ÇÁ∫ßÊñáÊ°£Âä†ÊùÉ‰∏ªÈ¢òÂºÇË¥®ÊÄßÔºàÂä†ÊùÉJSDÔºâ\n",
    "    layer_distinctiveness_wavg = {}\n",
    "    for layer, nodes_dist in layer_word_dist.items():\n",
    "        nids = list(nodes_dist.keys())\n",
    "        if len(nids) < 2:\n",
    "            layer_distinctiveness_wavg[layer] = 0.0\n",
    "            continue\n",
    "        wsum_jsd = 0.0\n",
    "        wsum = 0.0\n",
    "        for i in range(len(nids)):\n",
    "            for j in range(i+1, len(nids)):\n",
    "                ni, nj = nids[i], nids[j]\n",
    "                mi, mj = len(node_doc_sets[ni]), len(node_doc_sets[nj])\n",
    "                if mi == 0 or mj == 0:\n",
    "                    continue\n",
    "                jsd = jensen_shannon_divergence(nodes_dist[ni], nodes_dist[nj])\n",
    "                w = mi * mj\n",
    "                wsum_jsd += jsd * w\n",
    "                wsum += w\n",
    "        layer_distinctiveness_wavg[layer] = (wsum_jsd / wsum) if wsum > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"node_records\": node_records,\n",
    "        \"layer_entropy_wavg\": layer_entropy_wavg,\n",
    "        \"layer_distinctiveness_wavg\": layer_distinctiveness_wavg,\n",
    "        \"nodes_per_layer\": nodes_per_layer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1366be79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_single_chain(args):\n",
    "    \"\"\"\n",
    "    ËøêË°åÂçïÊù° hLDA ÈìæÁöÑÂáΩÊï∞ÔºàÁî®‰∫é joblib Âπ∂Ë°åÔºâ\n",
    "    \n",
    "    Args:\n",
    "        args: ÂÖÉÁªÑ (chain_id, corpus, depth, gamma, eta, alpha, max_iterations, general_dir, seed)\n",
    "        \n",
    "    Returns:\n",
    "        dict: ÂåÖÂê´ÈìæÁªìÊûúÁöÑÂ≠óÂÖ∏\n",
    "    \"\"\"\n",
    "    (chain_id, corpus, depth, gamma, eta, alpha, max_iterations, general_dir, seed, shared_state, back_window, check_interval, burn_in) = args\n",
    "\n",
    "    print(f\"‚õìÔ∏è Chain {chain_id} startsÔºàPID: {os.getpid()}Ôºâ\")\n",
    "    \n",
    "    # ËÆæÁΩÆÈöèÊú∫ÁßçÂ≠ê\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # ‰∏∫ÈìæÂàõÂª∫ÁõÆÂΩï\n",
    "    chain_dir = os.path.join(general_dir, f\"depth_{depth}_gamma_{gamma}_run_{chain_id}\")\n",
    "    os.makedirs(chain_dir, exist_ok=True)\n",
    "    \n",
    "    # ËøêË°åGibbsÈááÊ†∑\n",
    "    \"\"\"\n",
    "    def run_gibbs_with_checkpoint(\n",
    "    corpus, depth=3, gamma=None, eta=None, alpha=None,\n",
    "    max_iterations=200, save_every_n_iter=10, checkpoint_dir='gibbs_checkpoints',\n",
    "    check_interval=10, resume_from_checkpoint=None, base_dir=None,\n",
    "    shared_state=None, chain_id=None, back_window=5, burn_in = 50):\n",
    "    \"\"\"\n",
    "    result = run_gibbs_with_checkpoint(\n",
    "        corpus=corpus,\n",
    "        depth=depth,\n",
    "        gamma=gamma,\n",
    "        eta=eta,\n",
    "        alpha=alpha,\n",
    "        max_iterations=max_iterations,\n",
    "        checkpoint_dir=os.path.join(chain_dir, \"checkpoints\"),\n",
    "        base_dir=chain_dir,\n",
    "        chain_id=chain_id,\n",
    "        shared_state=shared_state,\n",
    "        back_window=back_window,\n",
    "        burn_in=burn_in,  # ËÆæÁΩÆburn-inÊúü‰∏∫50ËΩÆ\n",
    "        check_interval=check_interval  # ËÆæÁΩÆÊªëÂä®Á™óÂè£Â§ßÂ∞è\n",
    "    )\n",
    "    # 1. converged: return recorder, root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation, saved_files\n",
    "    # 2. not converged: return recorder, root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation, saved_files, change_docs_list, loglikelihood_list, converge_or_not, doc_path_lst, doc_node_allocation_lst\n",
    "    \n",
    "    # ÊèêÂèñÁªìÊûú\n",
    "    recorder, root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation, saved_files, change_docs_list, loglikelihood_list, doc_path_lst, doc_node_allocation_lst = result\n",
    "    \n",
    "    # ‰øùÂ≠òËΩªÈáèÁ∫ßÁªìÊûúÂà∞Á£ÅÁõò‰ª•‰æø‰∏ªËøõÁ®ãËØªÂèñ\n",
    "    result_file = os.path.join(chain_dir, \"final_checkpoint.pkl\")\n",
    "    with open(result_file, 'wb') as f:\n",
    "        chain_result = {\n",
    "            'chain_id': chain_id,\n",
    "            'loglikelihood_history': loglikelihood_list,\n",
    "            'changed_docs_history': change_docs_list\n",
    "            # ‰∏ç‰øùÂ≠òÂ§™Â§ßÁöÑÂØπË±°\n",
    "        }\n",
    "        pickle.dump(chain_result, f)\n",
    "    \n",
    "    print(f\"‚úÖ Chain {chain_id} Finished !\")\n",
    "    \n",
    "    # 1. ÊåâÂ±Ç‰øùÂ≠ò\n",
    "    \n",
    "    res = evaluate_tree_structure_with_nodes(root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation)\n",
    "    \n",
    "    layers = sorted(res[\"layer_entropy_wavg\"].keys())\n",
    "    layer_rows = []\n",
    "    for L in layers:\n",
    "        layer_rows.append({\n",
    "            \"depth\": depth,\n",
    "            \"gamma\": gamma,\n",
    "            \"eta\": eta,\n",
    "            \"alpha\": alpha,\n",
    "            \"layer\": L,\n",
    "            \"entropy_wavg\": res[\"layer_entropy_wavg\"].get(L, 0.0),\n",
    "            \"distinctiveness_wavg_jsd\": res[\"layer_distinctiveness_wavg\"].get(L, 0.0),\n",
    "            \"nodes_in_layer\": res[\"nodes_per_layer\"].get(L, 0)\n",
    "        })\n",
    "    df_layers = pd.DataFrame(layer_rows)\n",
    "    layers_csv = os.path.join(chain_dir, \"result_layers.csv\")\n",
    "    os.makedirs(chain_dir, exist_ok=True)\n",
    "    if not os.path.exists(layers_csv):\n",
    "        df_layers.to_csv(layers_csv, index=False, mode='w', header=True)\n",
    "    else:\n",
    "        df_layers.to_csv(layers_csv, index=False, mode='a', header=False)\n",
    "\n",
    "    # 2. ÊåâËäÇÁÇπ‰øùÂ≠ò\n",
    "    df_nodes = pd.DataFrame(res[\"node_records\"])\n",
    "    if not df_nodes.empty:\n",
    "        df_nodes.insert(0, \"depth\", depth)\n",
    "        df_nodes.insert(1, \"gamma\", gamma)\n",
    "        df_nodes.insert(2, \"eta\", eta)\n",
    "        df_nodes.insert(3, \"alpha\", alpha)\n",
    "        nodes_csv = os.path.join(chain_dir, \"result_nodes.csv\")\n",
    "        if not os.path.exists(nodes_csv):\n",
    "            df_nodes.to_csv(nodes_csv, index=False, mode='w', header=True)\n",
    "        else:\n",
    "            df_nodes.to_csv(nodes_csv, index=False, mode='a', header=False)\n",
    "\n",
    "    # 3. ÂçïË°åÊëòË¶ÅÔºàÂéüÊù•ÁöÑ result_metrics.csv Ôºâ\n",
    "    summary = {\n",
    "        \"depth\": depth,\n",
    "        \"gamma\": gamma,\n",
    "        \"eta\": eta,\n",
    "        \"alpha\": alpha,\n",
    "        \"avg_entropy_wavg_over_layers\": float(np.mean([res[\"layer_entropy_wavg\"][L] for L in layers])) if layers else 0.0,\n",
    "        \"avg_distinctiveness_wavg_over_layers\": float(np.mean([res[\"layer_distinctiveness_wavg\"][L] for L in layers])) if layers else 0.0,\n",
    "        \"total_layers\": len(layers),\n",
    "        \"total_nodes\": int(sum(res[\"nodes_per_layer\"].values())) if layers else 0\n",
    "    }\n",
    "    df_metrics = pd.DataFrame([summary])\n",
    "    metrics_csv = os.path.join(chain_dir, \"result_metrics.csv\")\n",
    "    if not os.path.exists(metrics_csv):\n",
    "        df_metrics.to_csv(metrics_csv, index=False, mode='w', header=True)\n",
    "    else:\n",
    "        df_metrics.to_csv(metrics_csv, index=False, mode='a', header=False)\n",
    "\n",
    "#     # ËÆ°ÁÆóÊåáÊ†áÂπ∂‰øùÂ≠ò\n",
    "#     metrics = evaluate_tree_structure(root_node, path_list, doc_path, doc_word_allocation, doc_node_allocation)\n",
    "\n",
    "#     metrics.update({\n",
    "#         'gamma': gamma,\n",
    "#         'eta': eta,\n",
    "#         'depth': depth,\n",
    "#         'alpha': alpha,\n",
    "#     })     \n",
    "    \n",
    "#     metrics_df = pd.DataFrame([metrics])  # Â∞ÜÂ≠óÂÖ∏ËΩ¨Êç¢‰∏∫DataFrame\n",
    "#     metrics_df.to_csv(os.path.join(chain_dir, \"result_metrics.csv\"), index=False)\n",
    "    \n",
    "    # ËøîÂõûËΩªÈáèÁ∫ßÁªìÊûú\n",
    "    return {\n",
    "        'chain_id': chain_id,\n",
    "        'loglikelihood_history': loglikelihood_list,\n",
    "        'result_file': result_file,\n",
    "        'changed_docs_history': change_docs_list,\n",
    "        'doc_path_lst': doc_path_lst,\n",
    "        'doc_node_allocation_lst': doc_node_allocation_lst\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c7397f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "import json\n",
    "\n",
    "def run_multi_chain_hlda(\n",
    "    corpus, depth=3, gamma=0.01, eta=0.01, alpha=0.1,\n",
    "    n_chains=3, max_iterations=20, r_hat_threshold=1.1,\n",
    "    general_dir=\"multi_chain_hlda_results\",\n",
    "    back_window=5, check_interval=10, burn_in=50\n",
    "):\n",
    "    \"\"\"\n",
    "    Âπ∂Ë°åËøêË°åÂ§öÊù° hLDA ÈìæÔºåÊî∂ÊïõÊ£ÄÊü•Áî± monitor Á∫øÁ®ãÂÆûÊó∂ÂÆåÊàê„ÄÇ\n",
    "    ËøêË°åÁªìÊùüÂêéËá™Âä®‰øùÂ≠òÊâÄÊúâÊî∂ÊïõÊ£ÄÊü•ÂéÜÂè≤Êï∞ÊçÆ‰∏∫ CSV„ÄÇ\n",
    "    \n",
    "    result = run_multi_chain_hlda(\n",
    "    corpus=corpus,\n",
    "    depth=depth,\n",
    "    gamma=gamma,\n",
    "    eta=eta,\n",
    "    alpha=alpha,\n",
    "    n_chains=n_chains,\n",
    "    max_iterations=30,\n",
    "    check_interval=5,\n",
    "    back_window=3,\n",
    "    burn_in=2,\n",
    "    r_hat_threshold=1.1,\n",
    "    general_dir=\"0809_multi_len_docs\")\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(general_dir, exist_ok=True)\n",
    "\n",
    "    # ÂêØÂä®ÂÖ±‰∫´Áä∂ÊÄÅÂíåÁõëÊéßÁ∫øÁ®ã\n",
    "    shared_state = SharedState(general_dir)\n",
    "    monitor = Thread(\n",
    "        target=rhat_monitor_process,\n",
    "        args=(shared_state, n_chains, max_iterations, burn_in, check_interval, back_window, r_hat_threshold)\n",
    "    )\n",
    "    monitor.daemon = False\n",
    "    monitor.start()\n",
    "    print(\"üöÄ Start R-hat monitor process\")\n",
    "\n",
    "    # ÊûÑÈÄ†ÊØèÊù°ÈìæÁöÑÂèÇÊï∞\n",
    "    args_list = []\n",
    "    for i in range(1, n_chains + 1):\n",
    "        seed = i * 1000 + int(time.time()) % 1000\n",
    "        args_list.append((\n",
    "            i, corpus, depth, gamma, eta, alpha,\n",
    "            max_iterations, general_dir, seed,\n",
    "            shared_state, back_window, check_interval, burn_in\n",
    "        ))\n",
    "        \n",
    "    \"\"\"\n",
    "    (chain_id, corpus, depth, gamma, eta, alpha, max_iterations, general_dir, seed, shared_state, back_window, check_interval, burn_in) = args\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"üöÄ Start {n_chains} hLDA chainÔºåeach chain could have {max_iterations} max iterations...\")\n",
    "\n",
    "    # Âπ∂Ë°åËøêË°åÊâÄÊúâÈìæ\n",
    "    chain_results = Parallel(n_jobs=n_chains, backend='multiprocessing', verbose=10)(\n",
    "        delayed(_run_single_chain)(args) for args in args_list\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ All chains finish sampling and waiting for monitor process finish...\")\n",
    "\n",
    "    # Á≠âÂæÖÁõëÊéßÁ∫øÁ®ãÁªìÊùüÔºàÂç≥ÊâÄÊúâÈìæÈÉΩÊî∂ÊïõÊàñËææÂà∞ÊúÄÂ§ßËø≠‰ª£Ôºâ\n",
    "    monitor.join(timeout=60)\n",
    "    print(\"‚úÖ R-hat monitor process finished !\")\n",
    "\n",
    "    # Êî∂ÈõÜÊØèÊù°ÈìæÁöÑÁªìÊûú\n",
    "    full_results = {}\n",
    "    for result in chain_results:\n",
    "        with open(result['result_file'], 'rb') as f:\n",
    "            full_results[result['chain_id']] = pickle.load(f)\n",
    "\n",
    "    # ‰øùÂ≠òÊî∂ÊïõÊ£ÄÊü•ÂéÜÂè≤Êï∞ÊçÆ\n",
    "    rhat_file = os.path.join(general_dir, \"shared_state.json\")\n",
    "    if os.path.exists(rhat_file):\n",
    "        with open(rhat_file, \"r\") as f:\n",
    "            shared_state_data = json.load(f)\n",
    "        \n",
    "        # 1. ‰øùÂ≠òÊÄª‰ΩìR-hatÊî∂ÊïõÂéÜÂè≤Âà∞general_dir\n",
    "        rhat_history = shared_state_data.get(\"rhat_history\", [])\n",
    "        if rhat_history:\n",
    "            convergence_df = pd.DataFrame(rhat_history)\n",
    "            convergence_csv = os.path.join(general_dir, \"convergence_info.csv\")\n",
    "            convergence_df.to_csv(convergence_csv, index=False, encoding='utf-8')\n",
    "            print(f\"‚úÖ Overall convergence history is saved to CSV: {convergence_csv}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Does not find any convergence history in shared_state.json.\")\n",
    "        \n",
    "        # 2. ‰∏∫ÊØè‰∏™ÈìæÂçïÁã¨‰øùÂ≠òÂÖ∂Êî∂Êïõ‰ø°ÊÅØÂà∞ÂêÑËá™ÁöÑchain_dir\n",
    "        for result in chain_results:\n",
    "            chain_id = result['chain_id']\n",
    "            chain_dir = os.path.join(general_dir, f\"depth_{depth}_gamma_{gamma}_run_{chain_id}\")\n",
    "            \n",
    "            # ÊèêÂèñËØ•ÈìæÁâπÂÆöÁöÑÊï∞ÊçÆ\n",
    "            chain_data = shared_state_data.get(\"chains\", {}).get(str(chain_id), {})\n",
    "            if chain_data:\n",
    "                # ËΩ¨Êç¢‰∏∫DataFrameÊ†ºÂºèÊñπ‰æøÂàÜÊûê\n",
    "                chain_records = []\n",
    "                for record_time, len_docs in chain_data.get('len_stable_docs', {}).items():\n",
    "                    jaccard_list = chain_data.get('jaccard', {}).get(record_time, [])\n",
    "                    ratio_list = chain_data.get('ratio', {}).get(record_time, [])\n",
    "                    \n",
    "                    chain_records.append({\n",
    "                        'record_time': int(record_time),\n",
    "                        'len_stable_docs': len_docs,\n",
    "                        'jaccard_mean': np.mean(jaccard_list) if jaccard_list else 0,\n",
    "                        'jaccard_std': np.std(jaccard_list) if jaccard_list else 0,\n",
    "                        'ratio_mean': np.mean(ratio_list) if ratio_list else 0,\n",
    "                        'ratio_std': np.std(ratio_list) if ratio_list else 0,\n",
    "                        'jaccard_list': jaccard_list,\n",
    "                        'ratio_list': ratio_list\n",
    "                    })\n",
    "                \n",
    "                if chain_records:\n",
    "                    chain_df = pd.DataFrame(chain_records)\n",
    "                    chain_convergence_csv = os.path.join(chain_dir, \"chain_convergence_info.csv\")\n",
    "                    chain_df.to_csv(chain_convergence_csv, index=False, encoding='utf-8')\n",
    "                    print(f\"‚úÖ Chain {chain_id} convergence info saved to: {chain_convergence_csv}\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è No convergence data found for chain {chain_id}\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è No chain data found for chain {chain_id}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Does not find shared_state.json, no convergence history saved.\")\n",
    "\n",
    "    return full_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9943414-7ee6-4bd9-9ff6-95acfa9f7ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Initialize R-hat independent monitor thread (stable docs & Jaccard & Ratio)üöÄ Start R-hat monitor process\n",
      "üöÄ Start 3 hLDA chainÔºåeach chain could have 500 max iterations...\n",
      "\n",
      "‚õìÔ∏è Chain 1 startsÔºàPID: 257931Ôºâ\n",
      "\n",
      "üìä Initialize nCRP Process...‚õìÔ∏è Chain 2 startsÔºàPID: 257932Ôºâ\n",
      "üìä Initialize nCRP Process...\n",
      "‚õìÔ∏è Chain 3 startsÔºàPID: 257933Ôºâ\n",
      "üìä Initialize nCRP Process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend MultiprocessingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Chain 1 initial state after nCRP is recorded: 1 path,  Log-likelihood: -546031.91\n",
      "üìù Chain 2 initial state after nCRP is recorded: 2 path,  Log-likelihood: -545473.15\n",
      "üìù Chain 3 initial state after nCRP is recorded: 1 path,  Log-likelihood: -546029.27\n",
      "üîÑ Chain 3 in iteration 1/500,\n",
      " üìä path: 44,  new path: 43,  docs changed path: 161,  Log-likelihood: -520386.64\n",
      "üîÑ Chain 1 in iteration 1/500,\n",
      " üìä path: 48,  new path: 47,  docs changed path: 154,  Log-likelihood: -519536.36\n",
      "üîÑ Chain 2 in iteration 1/500,\n",
      " üìä path: 60,  new path: 58,  docs changed path: 199,  Log-likelihood: -516023.82\n",
      "üîÑ Chain 3 in iteration 2/500,\n",
      " üìä path: 73,  new path: 34,  docs changed path: 185,  Log-likelihood: -511092.12\n",
      "üîÑ Chain 1 in iteration 2/500,\n",
      " üìä path: 82,  new path: 43,  docs changed path: 172,  Log-likelihood: -509520.09\n",
      "üîÑ Chain 2 in iteration 2/500,\n",
      " üìä path: 89,  new path: 43,  docs changed path: 196,  Log-likelihood: -506397.50\n",
      "üîÑ Chain 3 in iteration 3/500,\n",
      " üìä path: 90,  new path: 33,  docs changed path: 189,  Log-likelihood: -503585.09\n",
      "üîÑ Chain 1 in iteration 3/500,\n",
      " üìä path: 89,  new path: 34,  docs changed path: 168,  Log-likelihood: -504806.10\n",
      "üîÑ Chain 2 in iteration 3/500,\n",
      " üìä path: 95,  new path: 39,  docs changed path: 178,  Log-likelihood: -501050.08\n",
      "üîÑ Chain 3 in iteration 4/500,\n",
      " üìä path: 99,  new path: 35,  docs changed path: 186,  Log-likelihood: -498794.37\n",
      "üîÑ Chain 1 in iteration 4/500,\n",
      " üìä path: 91,  new path: 32,  docs changed path: 160,  Log-likelihood: -501340.82\n",
      "üîÑ Chain 2 in iteration 4/500,\n",
      " üìä path: 106,  new path: 43,  docs changed path: 178,  Log-likelihood: -496135.52\n",
      "üîÑ Chain 3 in iteration 5/500,\n",
      " üìä path: 98,  new path: 28,  docs changed path: 171,  Log-likelihood: -495283.68\n",
      "üîÑ Chain 1 in iteration 5/500,\n",
      " üìä path: 105,  new path: 43,  docs changed path: 178,  Log-likelihood: -496702.84\n",
      "üîÑ Chain 2 in iteration 5/500,\n",
      " üìä path: 111,  new path: 28,  docs changed path: 186,  Log-likelihood: -493520.08\n",
      "üîÑ Chain 3 in iteration 6/500,\n",
      " üìä path: 97,  new path: 30,  docs changed path: 177,  Log-likelihood: -492224.71\n",
      "üîÑ Chain 1 in iteration 6/500,\n",
      " üìä path: 100,  new path: 37,  docs changed path: 165,  Log-likelihood: -494300.99\n",
      "üîÑ Chain 2 in iteration 6/500,\n",
      " üìä path: 108,  new path: 29,  docs changed path: 205,  Log-likelihood: -490995.73\n",
      "üîÑ Chain 3 in iteration 7/500,\n",
      " üìä path: 106,  new path: 34,  docs changed path: 178,  Log-likelihood: -490034.55\n",
      "üîÑ Chain 1 in iteration 7/500,\n",
      " üìä path: 101,  new path: 31,  docs changed path: 164,  Log-likelihood: -492313.64\n",
      "üîÑ Chain 2 in iteration 7/500,\n",
      " üìä path: 116,  new path: 34,  docs changed path: 226,  Log-likelihood: -488462.60\n",
      "üîÑ Chain 3 in iteration 8/500,\n",
      " üìä path: 114,  new path: 28,  docs changed path: 204,  Log-likelihood: -488766.58\n",
      "üîÑ Chain 1 in iteration 8/500,\n",
      " üìä path: 104,  new path: 32,  docs changed path: 172,  Log-likelihood: -489812.50\n",
      "üîÑ Chain 2 in iteration 8/500,\n",
      " üìä path: 105,  new path: 17,  docs changed path: 230,  Log-likelihood: -486495.54\n",
      "üîÑ Chain 3 in iteration 9/500,\n",
      " üìä path: 109,  new path: 29,  docs changed path: 215,  Log-likelihood: -487426.00\n",
      "üîÑ Chain 1 in iteration 9/500,\n",
      " üìä path: 113,  new path: 36,  docs changed path: 198,  Log-likelihood: -487337.62\n",
      "üîÑ Chain 2 in iteration 9/500,\n",
      " üìä path: 113,  new path: 34,  docs changed path: 217,  Log-likelihood: -483405.21\n",
      "üîÑ Chain 3 in iteration 10/500,\n",
      " üìä path: 109,  new path: 22,  docs changed path: 212,  Log-likelihood: -486617.80\n",
      "üîÑ Chain 1 in iteration 10/500,\n",
      " üìä path: 106,  new path: 24,  docs changed path: 199,  Log-likelihood: -486237.35\n",
      "üîÑ Chain 2 in iteration 10/500,\n",
      " üìä path: 118,  new path: 30,  docs changed path: 226,  Log-likelihood: -482835.21\n",
      "üîÑ Chain 3 in iteration 11/500,\n",
      " üîÑ Chain 1 in iteration 11/500,\n",
      "üìä path: 114,   üìä path: 98, new path: 31,   new path: 26,  docs changed path: 226, docs changed path: 179,   Log-likelihood: -483534.50Log-likelihood: -486443.24\n",
      "\n",
      "üîÑ Chain 2 in iteration 11/500,\n",
      " üìä path: 112,  new path: 27,  docs changed path: 224,  Log-likelihood: -482025.72\n",
      "üîÑ Chain 1 in iteration 12/500,\n",
      " üìä path: 102,  new path: 23,  docs changed path: 199,  Log-likelihood: -483292.19\n",
      "üîÑ Chain 3 in iteration 12/500,\n",
      " üìä path: 115,  new path: 26,  docs changed path: 227,  Log-likelihood: -484390.87\n",
      "üîÑ Chain 2 in iteration 12/500,\n",
      " üìä path: 114,  new path: 19,  docs changed path: 245,  Log-likelihood: -480518.28\n",
      "üîÑ Chain 1 in iteration 13/500,\n",
      " üìä path: 103,  new path: 19,  docs changed path: 206,  Log-likelihood: -483865.79\n",
      "üîÑ Chain 3 in iteration 13/500,\n",
      " üìä path: 120,  new path: 32,  docs changed path: 241,  Log-likelihood: -482352.49\n",
      "üîÑ Chain 2 in iteration 13/500,\n",
      " üìä path: 125,  new path: 33,  docs changed path: 250,  Log-likelihood: -480172.78\n",
      "üîÑ Chain 1 in iteration 14/500,\n",
      " üìä path: 109,  new path: 26,  docs changed path: 210,  Log-likelihood: -483170.21\n",
      "üîÑ Chain 3 in iteration 14/500,\n",
      " üìä path: 111,  new path: 20,  docs changed path: 243,  Log-likelihood: -481510.09\n",
      "üîÑ Chain 2 in iteration 14/500,\n",
      " üìä path: 119,  new path: 22,  docs changed path: 234,  Log-likelihood: -480217.20\n",
      "üîÑ Chain 1 in iteration 15/500,\n",
      " üìä path: 113,  new path: 28,  docs changed path: 216,  Log-likelihood: -482818.58\n",
      "üîÑ Chain 3 in iteration 15/500,\n",
      " üìä path: 117,  new path: 27,  docs changed path: 248,  Log-likelihood: -480777.12\n",
      "üîÑ Chain 2 in iteration 15/500,\n",
      " üìä path: 112,  new path: 20,  docs changed path: 236,  Log-likelihood: -479535.20\n",
      "üîÑ Chain 1 in iteration 16/500,\n",
      " üìä path: 121,  new path: 29,  docs changed path: 219,  Log-likelihood: -480688.99\n",
      "üîÑ Chain 3 in iteration 16/500,\n",
      " üìä path: 124,  new path: 23,  docs changed path: 232,  Log-likelihood: -479767.47\n",
      "üîÑ Chain 2 in iteration 16/500,\n",
      " üìä path: 111,  new path: 22,  docs changed path: 252,  Log-likelihood: -479100.17\n",
      "üîÑ Chain 1 in iteration 17/500,\n",
      " üìä path: 113,  new path: 30,  docs changed path: 214,  Log-likelihood: -480311.21\n",
      "üîÑ Chain 3 in iteration 17/500,\n",
      " üìä path: 122,  new path: 19,  docs changed path: 238,  Log-likelihood: -479236.01\n",
      "üîÑ Chain 2 in iteration 17/500,\n",
      " üìä path: 115,  new path: 23,  docs changed path: 236,  Log-likelihood: -478282.37\n",
      "üîÑ Chain 1 in iteration 18/500,\n",
      " üìä path: 119,  new path: 34,  docs changed path: 213,  Log-likelihood: -480818.13\n",
      "üîÑ Chain 3 in iteration 18/500,\n",
      " üìä path: 125,  new path: 27,  docs changed path: 255,  Log-likelihood: -478582.53\n",
      "üîÑ Chain 2 in iteration 18/500,\n",
      " üìä path: 110,  new path: 12,  docs changed path: 236,  Log-likelihood: -479431.75\n",
      "üîÑ Chain 1 in iteration 19/500,\n",
      " üìä path: 116,  new path: 26,  docs changed path: 219,  Log-likelihood: -479740.72\n",
      "üîÑ Chain 3 in iteration 19/500,\n",
      " üìä path: 126,  new path: 24,  docs changed path: 259,  Log-likelihood: -477361.94\n",
      "üîÑ Chain 2 in iteration 19/500,\n",
      " üìä path: 122,  new path: 24,  docs changed path: 248,  Log-likelihood: -477276.37\n",
      "üîÑ Chain 1 in iteration 20/500,\n",
      " üìä path: 119,  new path: 29,  docs changed path: 223,  Log-likelihood: -478231.41\n",
      "üîÑ Chain 3 in iteration 20/500,\n",
      " üìä path: 124,  new path: 18,  docs changed path: 254,  Log-likelihood: -477348.95\n",
      "üîÑ Chain 2 in iteration 20/500,\n",
      " üìä path: 120,  new path: 22,  docs changed path: 240,  Log-likelihood: -477197.25\n",
      "üîÑ Chain 1 in iteration 21/500,\n",
      " üìä path: 113,  new path: 20,  docs changed path: 257,  Log-likelihood: -479449.59\n",
      "üîÑ Chain 3 in iteration 21/500,\n",
      " üìä path: 128,  new path: 23,  docs changed path: 240,  Log-likelihood: -476796.14\n",
      "üîÑ Chain 2 in iteration 21/500,\n",
      " üìä path: 115,  new path: 21,  docs changed path: 239,  Log-likelihood: -475495.35\n",
      "üîÑ Chain 1 in iteration 22/500,\n",
      " üìä path: 113,  new path: 26,  docs changed path: 243,  Log-likelihood: -477702.60\n",
      "üîÑ Chain 3 in iteration 22/500,\n",
      " üìä path: 128,  new path: 22,  docs changed path: 266,  Log-likelihood: -475555.57\n",
      "üîÑ Chain 2 in iteration 22/500,\n",
      " üìä path: 108,  new path: 17,  docs changed path: 232,  Log-likelihood: -477302.91\n",
      "üîÑ Chain 1 in iteration 23/500,\n",
      " üìä path: 114,  new path: 24,  docs changed path: 240,  Log-likelihood: -477245.84\n",
      "üîÑ Chain 2 in iteration 23/500,\n",
      " üìä path: 121,  new path: 29,  docs changed path: 243,  Log-likelihood: -476353.88\n",
      "üîÑ Chain 3 in iteration 23/500,\n",
      " üìä path: 126,  new path: 22,  docs changed path: 263,  Log-likelihood: -476215.30\n",
      "üîÑ Chain 1 in iteration 24/500,\n",
      " üìä path: 116,  new path: 30,  docs changed path: 235,  Log-likelihood: -477549.46\n",
      "üîÑ Chain 2 in iteration 24/500,\n",
      " üìä path: 129,  new path: 29,  docs changed path: 238,  Log-likelihood: -474348.41\n",
      "üîÑ Chain 3 in iteration 24/500,\n",
      " üìä path: 125,  new path: 25,  docs changed path: 255,  Log-likelihood: -474828.24\n",
      "üîÑ Chain 1 in iteration 25/500,\n",
      " üìä path: 112,  new path: 21,  docs changed path: 230,  Log-likelihood: -477550.49\n",
      "üîÑ Chain 2 in iteration 25/500,\n",
      " üìä path: 118,  new path: 14,  docs changed path: 221,  Log-likelihood: -474175.60\n",
      "üîÑ Chain 3 in iteration 25/500,\n",
      " üìä path: 123,  new path: 20,  docs changed path: 253,  Log-likelihood: -473412.93\n",
      "üîÑ Chain 1 in iteration 26/500,\n",
      " üìä path: 123,  new path: 31,  docs changed path: 244,  Log-likelihood: -477042.05\n",
      "üîÑ Chain 2 in iteration 26/500,\n",
      " üìä path: 118,  new path: 24,  docs changed path: 235,  Log-likelihood: -474577.82\n",
      "üîÑ Chain 3 in iteration 26/500,\n",
      " üìä path: 118,  new path: 17,  docs changed path: 271,  Log-likelihood: -474786.15\n",
      "üîÑ Chain 1 in iteration 27/500,\n",
      " üìä path: 121,  new path: 21,  docs changed path: 247,  Log-likelihood: -475790.91\n",
      "üîÑ Chain 2 in iteration 27/500,\n",
      " üìä path: 119,  new path: 15,  docs changed path: 256,  Log-likelihood: -474149.24\n",
      "üîÑ Chain 3 in iteration 27/500,\n",
      " üìä path: 118,  new path: 18,  docs changed path: 263,  Log-likelihood: -472998.48\n",
      "üîÑ Chain 1 in iteration 28/500,\n",
      " üìä path: 116,  new path: 22,  docs changed path: 242,  Log-likelihood: -476600.62\n",
      "üîÑ Chain 2 in iteration 28/500,\n",
      " üìä path: 124,  new path: 23,  docs changed path: 257,  Log-likelihood: -473840.64\n",
      "üîÑ Chain 3 in iteration 28/500,\n",
      " üìä path: 123,  new path: 26,  docs changed path: 258,  Log-likelihood: -472449.38\n",
      "üîÑ Chain 1 in iteration 29/500,\n",
      " üìä path: 114,  new path: 19,  docs changed path: 232,  Log-likelihood: -476421.54\n",
      "üîÑ Chain 2 in iteration 29/500,\n",
      " üìä path: 126,  new path: 28,  docs changed path: 245,  Log-likelihood: -473318.38\n",
      "üîÑ Chain 3 in iteration 29/500,\n",
      " üìä path: 126,  new path: 22,  docs changed path: 274,  Log-likelihood: -472034.99\n",
      "üîÑ Chain 1 in iteration 30/500,\n",
      " üìä path: 115,  new path: 21,  docs changed path: 256,  Log-likelihood: -477342.30\n",
      "üîÑ Chain 2 in iteration 30/500,\n",
      " üìä path: 120,  new path: 16,  docs changed path: 258,  Log-likelihood: -472807.78\n",
      "üîÑ Chain 3 in iteration 30/500,\n",
      " üìä path: 128,  new path: 23,  docs changed path: 286,  Log-likelihood: -470643.57\n",
      "üîÑ Chain 1 in iteration 31/500,\n",
      " üìä path: 117,  new path: 24,  docs changed path: 242,  Log-likelihood: -474954.57\n",
      "üîÑ Chain 2 in iteration 31/500,\n",
      " üìä path: 125,  new path: 24,  docs changed path: 241,  Log-likelihood: -472404.97\n",
      "üîÑ Chain 3 in iteration 31/500,\n",
      " üìä path: 123,  new path: 20,  docs changed path: 302,  Log-likelihood: -471525.08\n",
      "üîÑ Chain 1 in iteration 32/500,\n",
      " üìä path: 121,  new path: 27,  docs changed path: 237,  Log-likelihood: -475522.84\n",
      "üîÑ Chain 2 in iteration 32/500,\n",
      " üìä path: 121,  new path: 22,  docs changed path: 241,  Log-likelihood: -471894.88\n",
      "üîÑ Chain 3 in iteration 32/500,\n",
      " üìä path: 123,  new path: 20,  docs changed path: 297,  Log-likelihood: -470700.26\n",
      "üîÑ Chain 1 in iteration 33/500,\n",
      " üìä path: 114,  new path: 23,  docs changed path: 240,  Log-likelihood: -474830.34\n",
      "üîÑ Chain 2 in iteration 33/500,\n",
      " üìä path: 120,  new path: 21,  docs changed path: 250,  Log-likelihood: -472711.22\n",
      "üîÑ Chain 3 in iteration 33/500,\n",
      " üìä path: 124,  new path: 18,  docs changed path: 287,  Log-likelihood: -471587.49\n",
      "üîÑ Chain 1 in iteration 34/500,\n",
      " üìä path: 112,  new path: 23,  docs changed path: 240,  Log-likelihood: -473847.41\n",
      "üîÑ Chain 2 in iteration 34/500,\n",
      " üìä path: 125,  new path: 24,  docs changed path: 256,  Log-likelihood: -470546.26\n",
      "üîÑ Chain 3 in iteration 34/500,\n",
      " üìä path: 127,  new path: 20,  docs changed path: 300,  Log-likelihood: -469721.80\n",
      "üîÑ Chain 1 in iteration 35/500,\n",
      " üìä path: 118,  new path: 22,  docs changed path: 232,  Log-likelihood: -474045.46\n",
      "üîÑ Chain 2 in iteration 35/500,\n",
      " üìä path: 126,  new path: 20,  docs changed path: 238,  Log-likelihood: -471258.37\n",
      "üîÑ Chain 3 in iteration 35/500,\n",
      " üìä path: 128,  new path: 25,  docs changed path: 302,  Log-likelihood: -469121.27\n",
      "üîÑ Chain 1 in iteration 36/500,\n",
      " üìä path: 121,  new path: 23,  docs changed path: 245,  Log-likelihood: -474108.53\n",
      "üîÑ Chain 2 in iteration 36/500,\n",
      " üìä path: 130,  new path: 23,  docs changed path: 249,  Log-likelihood: -469776.62\n",
      "üîÑ Chain 1 in iteration 37/500,\n",
      " üìä path: 122,  new path: 23,  docs changed path: 252,  Log-likelihood: -473038.02\n",
      "üîÑ Chain 3 in iteration 36/500,\n",
      " üìä path: 128,  new path: 20,  docs changed path: 310,  Log-likelihood: -468532.88\n",
      "üîÑ Chain 2 in iteration 37/500,\n",
      " üìä path: 129,  new path: 21,  docs changed path: 239,  Log-likelihood: -470529.51\n",
      "üîÑ Chain 1 in iteration 38/500,\n",
      " üìä path: 130,  new path: 31,  docs changed path: 248,  Log-likelihood: -472552.33\n",
      "üîÑ Chain 3 in iteration 37/500,\n",
      " üìä path: 128,  new path: 19,  docs changed path: 310,  Log-likelihood: -469031.55\n",
      "üîÑ Chain 2 in iteration 38/500,\n",
      " üìä path: 127,  new path: 21,  docs changed path: 256,  Log-likelihood: -470286.22\n",
      "üîÑ Chain 1 in iteration 39/500,\n",
      " üìä path: 125,  new path: 22,  docs changed path: 244,  Log-likelihood: -472641.68\n",
      "üîÑ Chain 3 in iteration 38/500,\n",
      " üìä path: 133,  new path: 24,  docs changed path: 304,  Log-likelihood: -467241.28\n",
      "üîÑ Chain 1 in iteration 40/500,\n",
      " üìä path: 125,  new path: 19,  docs changed path: 248,  Log-likelihood: -471884.72\n",
      "üîÑ Chain 2 in iteration 39/500,\n",
      " üìä path: 136,  new path: 27,  docs changed path: 226,  Log-likelihood: -470966.74\n",
      "üîÑ Chain 3 in iteration 39/500,\n",
      " üìä path: 137,  new path: 23,  docs changed path: 307,  Log-likelihood: -467300.49\n",
      "üîÑ Chain 1 in iteration 41/500,\n",
      " üìä path: 132,  new path: 29,  docs changed path: 258,  Log-likelihood: -472057.51\n",
      "üîÑ Chain 2 in iteration 40/500,\n",
      " üìä path: 134,  new path: 22,  docs changed path: 228,  Log-likelihood: -468925.78\n",
      "üîÑ Chain 3 in iteration 40/500,\n",
      " üìä path: 140,  new path: 19,  docs changed path: 297,  Log-likelihood: -465073.74\n",
      "üîÑ Chain 1 in iteration 42/500,\n",
      " üìä path: 134,  new path: 28,  docs changed path: 260,  Log-likelihood: -471055.64\n",
      "üîÑ Chain 2 in iteration 41/500,\n",
      " üìä path: 132,  new path: 22,  docs changed path: 229,  Log-likelihood: -469251.29\n",
      "üîÑ Chain 3 in iteration 41/500,\n",
      " üìä path: 139,  new path: 24,  docs changed path: 300,  Log-likelihood: -465878.65\n",
      "üîÑ Chain 1 in iteration 43/500,\n",
      " üìä path: 125,  new path: 17,  docs changed path: 251,  Log-likelihood: -471823.96\n",
      "üîÑ Chain 2 in iteration 42/500,\n",
      " üìä path: 136,  new path: 27,  docs changed path: 233,  Log-likelihood: -469903.32\n",
      "üîÑ Chain 3 in iteration 42/500,\n",
      " üìä path: 135,  new path: 22,  docs changed path: 304,  Log-likelihood: -464985.34\n",
      "üîÑ Chain 1 in iteration 44/500,\n",
      " üìä path: 129,  new path: 23,  docs changed path: 264,  Log-likelihood: -471909.73\n",
      "üîÑ Chain 2 in iteration 43/500,\n",
      " üìä path: 129,  new path: 16,  docs changed path: 239,  Log-likelihood: -469532.08\n",
      "üîÑ Chain 3 in iteration 43/500,\n",
      " üìä path: 134,  new path: 15,  docs changed path: 307,  Log-likelihood: -465461.82\n",
      "üîÑ Chain 1 in iteration 45/500,\n",
      " üìä path: 131,  new path: 25,  docs changed path: 265,  Log-likelihood: -470334.84\n",
      "üîÑ Chain 2 in iteration 44/500,\n",
      " üìä path: 137,  new path: 25,  docs changed path: 237,  Log-likelihood: -467560.09\n",
      "üîÑ Chain 3 in iteration 44/500,\n",
      " üìä path: 126,  new path: 11,  docs changed path: 316,  Log-likelihood: -466050.08\n",
      "üîÑ Chain 1 in iteration 46/500,\n",
      " üìä path: 140,  new path: 31,  docs changed path: 265,  Log-likelihood: -469141.48\n",
      "üîÑ Chain 2 in iteration 45/500,\n",
      " üìä path: 140,  new path: 30,  docs changed path: 238,  Log-likelihood: -467758.47\n",
      "üîÑ Chain 3 in iteration 45/500,\n",
      " üìä path: 137,  new path: 25,  docs changed path: 311,  Log-likelihood: -464667.80\n",
      "üîÑ Chain 1 in iteration 47/500,\n",
      " üìä path: 139,  new path: 29,  docs changed path: 263,  Log-likelihood: -468760.01\n",
      "üîÑ Chain 2 in iteration 46/500,\n",
      " üìä path: 137,  new path: 23,  docs changed path: 253,  Log-likelihood: -467664.78\n",
      "üîÑ Chain 3 in iteration 46/500,\n",
      " üìä path: 137,  new path: 18,  docs changed path: 323,  Log-likelihood: -463602.92\n",
      "üîÑ Chain 1 in iteration 48/500,\n",
      " üìä path: 143,  new path: 29,  docs changed path: 255,  Log-likelihood: -469492.13\n",
      "üîÑ Chain 2 in iteration 47/500,\n",
      " üìä path: 133,  new path: 22,  docs changed path: 235,  Log-likelihood: -467372.53\n",
      "üîÑ Chain 3 in iteration 47/500,\n",
      " üìä path: 131,  new path: 15,  docs changed path: 304,  Log-likelihood: -464779.28\n",
      "üîÑ Chain 1 in iteration 49/500,\n",
      " üìä path: 138,  new path: 22,  docs changed path: 256,  Log-likelihood: -468440.74\n",
      "üîÑ Chain 2 in iteration 48/500,\n",
      " üìä path: 132,  new path: 24,  docs changed path: 246,  Log-likelihood: -466723.19\n",
      "üîÑ Chain 3 in iteration 48/500,\n",
      " üìä path: 133,  new path: 27,  docs changed path: 301,  Log-likelihood: -463163.93\n",
      "üîÑ Chain 1 in iteration 50/500,\n",
      " üìä path: 139,  new path: 23,  docs changed path: 261,  Log-likelihood: -469032.18\n",
      "üîÑ Chain 2 in iteration 49/500,\n",
      " üìä path: 129,  new path: 21,  docs changed path: 233,  Log-likelihood: -466681.66\n",
      "üîÑ Chain 3 in iteration 49/500,\n",
      " üìä path: 143,  new path: 26,  docs changed path: 301,  Log-likelihood: -461819.03\n",
      "üîÑ Chain 1 in iteration 51/500,\n",
      " üìä path: 135,  new path: 24,  docs changed path: 275,  Log-likelihood: -468419.35\n",
      "üîÑ Chain 2 in iteration 50/500,\n",
      " üìä path: 133,  new path: 20,  docs changed path: 235,  Log-likelihood: -466320.26\n",
      "üîÑ Chain 3 in iteration 50/500,\n",
      " üìä path: 144,  new path: 23,  docs changed path: 309,  Log-likelihood: -461646.25\n",
      "üîÑ Chain 1 in iteration 52/500,\n",
      " üìä path: 132,  new path: 23,  docs changed path: 273,  Log-likelihood: -467759.09\n",
      "üîÑ Chain 2 in iteration 51/500,\n",
      " üìä path: 130,  new path: 18,  docs changed path: 215,  Log-likelihood: -466128.37\n",
      "üîÑ Chain 3 in iteration 51/500,\n",
      " üìä path: 146,  new path: 22,  docs changed path: 317,  Log-likelihood: -461901.77\n",
      "üîÑ Chain 1 in iteration 53/500,\n",
      " üìä path: 133,  new path: 20,  docs changed path: 268,  Log-likelihood: -467328.78\n",
      "üîÑ Chain 2 in iteration 52/500,\n",
      " üìä path: 136,  new path: 26,  docs changed path: 224,  Log-likelihood: -467554.16\n",
      "üîÑ Chain 3 in iteration 52/500,\n",
      " üìä path: 146,  new path: 18,  docs changed path: 294,  Log-likelihood: -461243.96\n",
      "üîÑ Chain 1 in iteration 54/500,\n",
      " üìä path: 138,  new path: 26,  docs changed path: 263,  Log-likelihood: -468297.41\n",
      "üîÑ Chain 2 in iteration 53/500,\n",
      " üìä path: 130,  new path: 22,  docs changed path: 242,  Log-likelihood: -467080.33\n",
      "üîÑ Chain 3 in iteration 53/500,\n",
      " üìä path: 147,  new path: 23,  docs changed path: 298,  Log-likelihood: -461662.51\n",
      "üîÑ Chain 2 in iteration 54/500,\n",
      " üìä path: 134,  new path: 25,  docs changed path: 250,  Log-likelihood: -466410.89\n",
      "üîÑ Chain 1 in iteration 55/500,\n",
      " üìä path: 137,  new path: 20,  docs changed path: 276,  Log-likelihood: -466680.89\n",
      "üîÑ Chain 3 in iteration 54/500,\n",
      " üìä path: 145,  new path: 19,  docs changed path: 316,  Log-likelihood: -461661.34\n",
      "üîÑ Chain 2 in iteration 55/500,\n",
      " üìä path: 127,  new path: 13,  docs changed path: 232,  Log-likelihood: -467472.86\n",
      "üîÑ Chain 1 in iteration 56/500,\n",
      " üìä path: 134,  new path: 22,  docs changed path: 273,  Log-likelihood: -467379.69\n",
      "üîÑ Chain 3 in iteration 55/500,\n",
      " üìä path: 136,  new path: 18,  docs changed path: 311,  Log-likelihood: -461629.56\n",
      "üîÑ Chain 2 in iteration 56/500,\n",
      " üìä path: 128,  new path: 22,  docs changed path: 231,  Log-likelihood: -466385.77\n",
      "üîÑ Chain 1 in iteration 57/500,\n",
      " üìä path: 135,  new path: 22,  docs changed path: 262,  Log-likelihood: -467549.39\n",
      "üîÑ Chain 3 in iteration 56/500,\n",
      " üìä path: 145,  new path: 22,  docs changed path: 300,  Log-likelihood: -460627.99\n",
      "üîÑ Chain 2 in iteration 57/500,\n",
      " üìä path: 137,  new path: 26,  docs changed path: 237,  Log-likelihood: -466856.84\n",
      "üîÑ Chain 1 in iteration 58/500,\n",
      " üìä path: 132,  new path: 22,  docs changed path: 254,  Log-likelihood: -466654.42\n",
      "üîÑ Chain 3 in iteration 57/500,\n",
      " üìä path: 148,  new path: 25,  docs changed path: 297,  Log-likelihood: -459930.96\n",
      "üîÑ Chain 2 in iteration 58/500,\n",
      " üìä path: 133,  new path: 15,  docs changed path: 240,  Log-likelihood: -466282.12\n",
      "üîÑ Chain 1 in iteration 59/500,\n",
      " üìä path: 136,  new path: 28,  docs changed path: 260,  Log-likelihood: -466088.85\n",
      "üîÑ Chain 3 in iteration 58/500,\n",
      " üìä path: 138,  new path: 12,  docs changed path: 293,  Log-likelihood: -459099.20\n",
      "üîÑ Chain 2 in iteration 59/500,\n",
      " üìä path: 140,  new path: 24,  docs changed path: 237,  Log-likelihood: -465771.38\n",
      "üîÑ Chain 1 in iteration 60/500,\n",
      " üìä path: 139,  new path: 24,  docs changed path: 254,  Log-likelihood: -466947.72\n",
      "üîÑ Chain 3 in iteration 59/500,\n",
      " üìä path: 138,  new path: 19,  docs changed path: 303,  Log-likelihood: -460434.04\n",
      "üîÑ Chain 1 in iteration 61/500,\n",
      " üìä path: 138,  new path: 23,  docs changed path: 244,  Log-likelihood: -465563.13\n",
      "üîÑ Chain 2 in iteration 60/500,\n",
      " üìä path: 135,  new path: 13,  docs changed path: 229,  Log-likelihood: -465907.40\n",
      "üîÑ Chain 3 in iteration 60/500,\n",
      " üìä path: 142,  new path: 17,  docs changed path: 302,  Log-likelihood: -458770.84\n",
      "üîÑ Chain 1 in iteration 62/500,\n",
      " üìä path: 140,  new path: 29,  docs changed path: 252,  Log-likelihood: -464978.37\n",
      "üîÑ Chain 2 in iteration 61/500,\n",
      " üìä path: 142,  new path: 22,  docs changed path: 222,  Log-likelihood: -465249.92\n",
      "üîÑ Chain 3 in iteration 61/500,\n",
      " üìä path: 142,  new path: 19,  docs changed path: 286,  Log-likelihood: -458848.41\n",
      "üîÑ Chain 1 in iteration 63/500,\n",
      " üìä path: 148,  new path: 36,  docs changed path: 246,  Log-likelihood: -465023.98\n",
      "üîÑ Chain 2 in iteration 62/500,\n",
      " üìä path: 137,  new path: 14,  docs changed path: 214,  Log-likelihood: -465111.62\n",
      "üîÑ Chain 3 in iteration 62/500,\n",
      " üìä path: 139,  new path: 21,  docs changed path: 270,  Log-likelihood: -458983.59\n",
      "üîÑ Chain 1 in iteration 64/500,\n",
      " üìä path: 142,  new path: 25,  docs changed path: 272,  Log-likelihood: -466616.02\n",
      "üîÑ Chain 2 in iteration 63/500,\n",
      " üìä path: 134,  new path: 20,  docs changed path: 220,  Log-likelihood: -465174.36\n",
      "üîÑ Chain 3 in iteration 63/500,\n",
      " üìä path: 134,  new path: 13,  docs changed path: 277,  Log-likelihood: -459694.19\n",
      "üîÑ Chain 1 in iteration 65/500,\n",
      " üìä path: 135,  new path: 18,  docs changed path: 270,  Log-likelihood: -464346.58\n",
      "üîÑ Chain 2 in iteration 64/500,\n",
      " üìä path: 139,  new path: 24,  docs changed path: 230,  Log-likelihood: -464569.32\n",
      "üîÑ Chain 3 in iteration 64/500,\n",
      " üìä path: 142,  new path: 19,  docs changed path: 285,  Log-likelihood: -458679.16\n",
      "üîÑ Chain 1 in iteration 66/500,\n",
      " üìä path: 132,  new path: 17,  docs changed path: 265,  Log-likelihood: -465785.85\n",
      "üîÑ Chain 2 in iteration 65/500,\n",
      " üìä path: 142,  new path: 23,  docs changed path: 233,  Log-likelihood: -463160.11\n",
      "üîÑ Chain 3 in iteration 65/500,\n",
      " üìä path: 146,  new path: 27,  docs changed path: 282,  Log-likelihood: -457769.34\n",
      "üîÑ Chain 1 in iteration 67/500,\n",
      " üìä path: 135,  new path: 22,  docs changed path: 257,  Log-likelihood: -464029.27\n",
      "üîÑ Chain 2 in iteration 66/500,\n",
      " üìä path: 137,  new path: 23,  docs changed path: 232,  Log-likelihood: -465638.59\n",
      "üîÑ Chain 3 in iteration 66/500,\n",
      " üìä path: 141,  new path: 19,  docs changed path: 294,  Log-likelihood: -458768.10\n",
      "üîÑ Chain 1 in iteration 68/500,\n",
      " üìä path: 138,  new path: 23,  docs changed path: 258,  Log-likelihood: -464047.22\n",
      "üîÑ Chain 2 in iteration 67/500,\n",
      " üìä path: 129,  new path: 21,  docs changed path: 222,  Log-likelihood: -464037.43\n",
      "üîÑ Chain 3 in iteration 67/500,\n",
      " üìä path: 149,  new path: 24,  docs changed path: 291,  Log-likelihood: -457397.86\n",
      "üîÑ Chain 1 in iteration 69/500,\n",
      " üìä path: 137,  new path: 22,  docs changed path: 243,  Log-likelihood: -463212.48\n",
      "üîÑ Chain 2 in iteration 68/500,\n",
      " üìä path: 135,  new path: 23,  docs changed path: 215,  Log-likelihood: -464065.71\n",
      "üîÑ Chain 3 in iteration 68/500,\n",
      " üìä path: 144,  new path: 21,  docs changed path: 291,  Log-likelihood: -458048.62\n",
      "üîÑ Chain 1 in iteration 70/500,\n",
      " üìä path: 130,  new path: 17,  docs changed path: 251,  Log-likelihood: -463751.09\n",
      "üîÑ Chain 2 in iteration 69/500,\n",
      " üìä path: 133,  new path: 17,  docs changed path: 209,  Log-likelihood: -463883.28\n",
      "üîÑ Chain 3 in iteration 69/500,\n",
      " üìä path: 144,  new path: 20,  docs changed path: 294,  Log-likelihood: -456573.93\n",
      "üîÑ Chain 1 in iteration 71/500,\n",
      " üìä path: 140,  new path: 30,  docs changed path: 254,  Log-likelihood: -463775.45\n",
      "üîÑ Chain 2 in iteration 70/500,\n",
      " üìä path: 135,  new path: 18,  docs changed path: 225,  Log-likelihood: -464270.73\n",
      "üîÑ Chain 3 in iteration 70/500,\n",
      " üìä path: 136,  new path: 16,  docs changed path: 302,  Log-likelihood: -456991.06\n",
      "üîÑ Chain 1 in iteration 72/500,\n",
      " üìä path: 133,  new path: 17,  docs changed path: 246,  Log-likelihood: -463554.01\n",
      "üîÑ Chain 2 in iteration 71/500,\n",
      " üìä path: 134,  new path: 20,  docs changed path: 233,  Log-likelihood: -463510.47\n",
      "üîÑ Chain 3 in iteration 71/500,\n",
      " üìä path: 140,  new path: 19,  docs changed path: 278,  Log-likelihood: -456884.43\n",
      "üîÑ Chain 1 in iteration 73/500,\n",
      " üìä path: 141,  new path: 25,  docs changed path: 261,  Log-likelihood: -463280.21\n",
      "üîÑ Chain 2 in iteration 72/500,\n",
      " üìä path: 135,  new path: 16,  docs changed path: 210,  Log-likelihood: -463582.87\n",
      "üîÑ Chain 3 in iteration 72/500,\n",
      " üìä path: 143,  new path: 19,  docs changed path: 285,  Log-likelihood: -456364.05\n",
      "üîÑ Chain 1 in iteration 74/500,\n",
      " üìä path: 136,  new path: 20,  docs changed path: 267,  Log-likelihood: -463124.65\n",
      "üîÑ Chain 2 in iteration 73/500,\n",
      " üìä path: 137,  new path: 20,  docs changed path: 222,  Log-likelihood: -462947.22\n",
      "üîÑ Chain 3 in iteration 73/500,\n",
      " üìä path: 145,  new path: 26,  docs changed path: 273,  Log-likelihood: -456768.60\n",
      "üîÑ Chain 1 in iteration 75/500,\n",
      " üìä path: 129,  new path: 18,  docs changed path: 245,  Log-likelihood: -463623.09\n",
      "üîÑ Chain 2 in iteration 74/500,\n",
      " üìä path: 137,  new path: 19,  docs changed path: 221,  Log-likelihood: -462622.17\n",
      "üîÑ Chain 3 in iteration 74/500,\n",
      " üìä path: 155,  new path: 31,  docs changed path: 270,  Log-likelihood: -455448.95\n",
      "üîÑ Chain 1 in iteration 76/500,\n",
      " üìä path: 137,  new path: 28,  docs changed path: 233,  Log-likelihood: -462501.94\n",
      "üîÑ Chain 2 in iteration 75/500,\n",
      " üìä path: 143,  new path: 25,  docs changed path: 222,  Log-likelihood: -461337.87\n",
      "üîÑ Chain 3 in iteration 75/500,\n",
      " üìä path: 153,  new path: 23,  docs changed path: 283,  Log-likelihood: -456103.79\n",
      "üîÑ Chain 1 in iteration 77/500,\n",
      " üìä path: 140,  new path: 31,  docs changed path: 253,  Log-likelihood: -462127.18\n",
      "check_dual_convergence_with_gelman_rubin: chain_len_docs:{'1': [584, 591, 586, 590, 599, 602, 611, 604, 608, 590, 589, 583, 580, 588, 599, 598, 592, 606, 600, 604, 606], '2': [629, 625, 624, 623, 622, 629, 631, 635, 636, 641, 646, 646, 650, 646, 650, 651, 644, 639, 623, 633, 635], '3': [539, 539, 531, 542, 546, 545, 550, 546, 553, 559, 577, 570, 565, 566, 548, 546, 557, 565, 573, 578, 571]}, mean_jaccard:[0.9410791683902163, 0.9491574344547793, 0.9343460887351833], mean_ratio:[0.20000000000000004, 0.20000000000000004, 0.20000000000000004]\n",
      "ü•∞ Convergence status: ‚ùå Not converged...\n",
      "ü•∞  R-hat info: 3.7463Ôºåmean_jaccard:[0.9410791683902163, 0.9491574344547793, 0.9343460887351833], mean_ratio:[0.20000000000000004, 0.20000000000000004, 0.20000000000000004]\n",
      "üîÑ Chain 2 in iteration 76/500,\n",
      " üìä path: 143,  new path: 19,  docs changed path: 243,  Log-likelihood: -461576.65\n",
      "üîÑ Chain 1 in iteration 78/500,\n",
      " üìä path: 146,  new path: 32,  docs changed path: 255,  Log-likelihood: -462148.14\n",
      "üîÑ Chain 3 in iteration 76/500,\n",
      " üìä path: 140,  new path: 14,  docs changed path: 273,  Log-likelihood: -455861.40\n",
      "üîÑ Chain 2 in iteration 77/500,\n",
      " üìä path: 143,  new path: 20,  docs changed path: 238,  Log-likelihood: -462841.55\n",
      "üîÑ Chain 3 in iteration 77/500,\n",
      " üìä path: 132,  new path: 9,  docs changed path: 280,  Log-likelihood: -456558.72\n",
      "üîÑ Chain 1 in iteration 79/500,\n",
      " üìä path: 149,  new path: 34,  docs changed path: 248,  Log-likelihood: -462202.89\n",
      "üîÑ Chain 2 in iteration 78/500,\n",
      " üìä path: 145,  new path: 23,  docs changed path: 236,  Log-likelihood: -461918.00\n",
      "üîÑ Chain 3 in iteration 78/500,\n",
      " üìä path: 141,  new path: 23,  docs changed path: 267,  Log-likelihood: -455050.84\n",
      "üîÑ Chain 1 in iteration 80/500,\n",
      " üìä path: 140,  new path: 18,  docs changed path: 265,  Log-likelihood: -462156.01\n",
      "üîÑ Chain 2 in iteration 79/500,\n",
      " üìä path: 145,  new path: 22,  docs changed path: 224,  Log-likelihood: -462601.46\n",
      "üîÑ Chain 3 in iteration 79/500,\n",
      " üìä path: 139,   new path: 17,  docs changed path: 266, Log-likelihood: -455195.28\n",
      "üîÑ Chain 1 in iteration 81/500,\n",
      " üìä path: 141,  new path: 24,  docs changed path: 253,  Log-likelihood: -461565.13\n",
      "üîÑ Chain 2 in iteration 80/500,\n",
      " üìä path: 149,  new path: 24,  docs changed path: 249,  Log-likelihood: -462429.49\n",
      "üîÑ Chain 3 in iteration 80/500,\n",
      " üìä path: 134,  new path: 19,  docs changed path: 271,  Log-likelihood: -454963.91\n",
      "üîÑ Chain 1 in iteration 82/500,\n",
      " üìä path: 136,  new path: 19,  docs changed path: 253,  Log-likelihood: -462090.98\n",
      "üîÑ Chain 2 in iteration 81/500,\n",
      " üìä path: 143,  new path: 19,  docs changed path: 230,  Log-likelihood: -463317.93\n",
      "üîÑ Chain 3 in iteration 81/500,\n",
      " üìä path: 137,  new path: 18,  docs changed path: 261,  Log-likelihood: -454755.32\n",
      "üîÑ Chain 1 in iteration 83/500,\n",
      " üìä path: 137,  new path: 22,  docs changed path: 230,  Log-likelihood: -462035.24\n",
      "üîÑ Chain 2 in iteration 82/500,\n",
      " üìä path: 143,  new path: 23,  docs changed path: 240,  Log-likelihood: -461873.59\n",
      "üîÑ Chain 3 in iteration 82/500,\n",
      " üìä path: 147,  new path: 22,  docs changed path: 263,  Log-likelihood: -454161.11\n",
      "üîÑ Chain 1 in iteration 84/500,\n",
      " üìä path: 139,  new path: 25,  docs changed path: 243,  Log-likelihood: -460193.71\n",
      "üîÑ Chain 2 in iteration 83/500,\n",
      " üìä path: 141,  new path: 21,  docs changed path: 233,  Log-likelihood: -462479.91\n",
      "üîÑ Chain 1 in iteration 85/500,\n",
      " üìä path: 141,  new path: 27,  docs changed path: 248,  Log-likelihood: -461634.37\n",
      "üîÑ Chain 3 in iteration 83/500,\n",
      " üìä path: 145,  new path: 22,  docs changed path: 259,  Log-likelihood: -454801.92\n",
      "üîÑ Chain 2 in iteration 84/500,\n",
      " üìä path: 137,  new path: 18,  docs changed path: 227,  Log-likelihood: -461490.55\n",
      "üîÑ Chain 1 in iteration 86/500,\n",
      " üìä path: 137,  new path: 17,  docs changed path: 240,  Log-likelihood: -461030.47\n",
      "üîÑ Chain 3 in iteration 84/500,\n",
      " üìä path: 143,  new path: 21,  docs changed path: 270,  Log-likelihood: -454801.56\n",
      "üîÑ Chain 2 in iteration 85/500,\n",
      " üìä path: 143,  new path: 23,  docs changed path: 230,  Log-likelihood: -461108.17\n",
      "üîÑ Chain 1 in iteration 87/500,\n",
      " üìä path: 136,  new path: 17,  docs changed path: 232,  Log-likelihood: -460499.31\n",
      "üîÑ Chain 3 in iteration 85/500,\n",
      " üìä path: 139,  new path: 19,  docs changed path: 249,  Log-likelihood: -454458.72\n",
      "üîÑ Chain 2 in iteration 86/500,\n",
      " üìä path: 138,  new path: 14,  docs changed path: 229,  Log-likelihood: -461821.09\n",
      "üîÑ Chain 1 in iteration 88/500,\n",
      " üìä path: 130,  new path: 14,  docs changed path: 237,  Log-likelihood: -460863.28\n",
      "üîÑ Chain 3 in iteration 86/500,\n",
      " üìä path: 149,  new path: 32,  docs changed path: 240,  Log-likelihood: -453725.96\n",
      "üîÑ Chain 2 in iteration 87/500,\n",
      " üìä path: 139,  new path: 21,  docs changed path: 220,  Log-likelihood: -462290.76\n",
      "üîÑ Chain 1 in iteration 89/500,\n",
      " üìä path: 134,  new path: 22,  docs changed path: 237,  Log-likelihood: -460538.41\n",
      "üîÑ Chain 3 in iteration 87/500,\n",
      " üìä path: 137,  new path: 15,  docs changed path: 247,  Log-likelihood: -454448.27\n",
      "üîÑ Chain 2 in iteration 88/500,\n",
      " üìä path: 145,  new path: 21,  docs changed path: 234,  Log-likelihood: -461737.36\n",
      "üîÑ Chain 1 in iteration 90/500,\n",
      " üìä path: 143,  new path: 25,  docs changed path: 232,  Log-likelihood: -460338.08\n",
      "üîÑ Chain 3 in iteration 88/500,\n",
      " üìä path: 146,  new path: 24,  docs changed path: 240,  Log-likelihood: -453846.53\n",
      "üîÑ Chain 2 in iteration 89/500,\n",
      " üìä path: 144,  new path: 23,  docs changed path: 227,  Log-likelihood: -461207.91\n",
      "üîÑ Chain 1 in iteration 91/500,\n",
      " üìä path: 142,  new path: 19,  docs changed path: 233,  Log-likelihood: -459980.57\n",
      "üîÑ Chain 3 in iteration 89/500,\n",
      " üìä path: 135,  new path: 15,  docs changed path: 258,  Log-likelihood: -454108.95\n",
      "üîÑ Chain 2 in iteration 90/500,\n",
      " üìä path: 152,  new path: 31,  docs changed path: 243,  Log-likelihood: -461112.65\n",
      "üîÑ Chain 1 in iteration 92/500,\n",
      " üìä path: 146,  new path: 23,  docs changed path: 236,  Log-likelihood: -458620.08\n",
      "üîÑ Chain 3 in iteration 90/500,\n",
      " üìä path: 143,  new path: 23,  docs changed path: 240,  Log-likelihood: -453202.49\n",
      "üîÑ Chain 2 in iteration 91/500,\n",
      " üìä path: 151,  new path: 20,  docs changed path: 234,  Log-likelihood: -459995.05\n",
      "üîÑ Chain 1 in iteration 93/500,\n",
      " üìä path: 145,  new path: 23,  docs changed path: 223,  Log-likelihood: -459946.83\n",
      "üîÑ Chain 3 in iteration 91/500,\n",
      " üìä path: 147,  new path: 14,  docs changed path: 255,  Log-likelihood: -452693.41\n",
      "üîÑ Chain 2 in iteration 92/500,\n",
      " üìä path: 150,  new path: 16,  docs changed path: 215,  Log-likelihood: -460277.81\n",
      "üîÑ Chain 1 in iteration 94/500,\n",
      " üìä path: 148,  new path: 23,  docs changed path: 256,  Log-likelihood: -456675.60\n",
      "üîÑ Chain 3 in iteration 92/500,\n",
      " üìä path: 150,  new path: 18,  docs changed path: 254,  Log-likelihood: -452191.18\n",
      "üîÑ Chain 2 in iteration 93/500,\n",
      " üìä path: 148,  new path: 26,  docs changed path: 224,  Log-likelihood: -461148.80\n",
      "üîÑ Chain 1 in iteration 95/500,\n",
      " üìä path: 146,  new path: 16,  docs changed path: 258,  Log-likelihood: -458498.83\n",
      "üîÑ Chain 3 in iteration 93/500,\n",
      " üìä path: 140,  new path: 16,  docs changed path: 243,  Log-likelihood: -453989.21\n",
      "üîÑ Chain 2 in iteration 94/500,\n",
      " üìä path: 146,  new path: 19,  docs changed path: 232,  Log-likelihood: -460267.84\n",
      "üîÑ Chain 3 in iteration 94/500,\n",
      " üìä path: 142,  new path: 19,  docs changed path: 243,  Log-likelihood: -452930.45\n",
      "üîÑ Chain 1 in iteration 96/500,\n",
      " üìä path: 138,  new path: 19,  docs changed path: 245,  Log-likelihood: -458163.81\n",
      "üîÑ Chain 2 in iteration 95/500,\n",
      " üìä path: 146,  new path: 25,  docs changed path: 247,  Log-likelihood: -460140.77\n",
      "üîÑ Chain 3 in iteration 95/500,\n",
      " üìä path: 145,  new path: 23,  docs changed path: 247,  Log-likelihood: -453066.85\n",
      "üîÑ Chain 1 in iteration 97/500,\n",
      " üìä path: 139,  new path: 18,  docs changed path: 223,  Log-likelihood: -458177.38\n",
      "check_dual_convergence_with_gelman_rubin: chain_len_docs:{'1': [606, 595, 606, 609, 613, 601, 593, 585, 585, 600, 607, 616, 610, 616, 625, 624, 627, 616, 631, 620, 617], '2': [635, 624, 629, 628, 628, 634, 637, 631, 625, 623, 626, 636, 642, 643, 643, 649, 647, 644, 639, 639, 628], '3': [571, 577, 575, 578, 580, 591, 610, 610, 602, 597, 597, 595, 601, 606, 603, 610, 611, 610, 622, 619, 609]}, mean_jaccard:[0.939284509112629, 0.947162548442755, 0.9415520246184739], mean_ratio:[0.20000000000000004, 0.20000000000000004, 0.20000000000000004]\n",
      "ü•∞ Convergence status: ‚ùå Not converged...\n",
      "ü•∞  R-hat info: 1.7857Ôºåmean_jaccard:[0.939284509112629, 0.947162548442755, 0.9415520246184739], mean_ratio:[0.20000000000000004, 0.20000000000000004, 0.20000000000000004]\n",
      "üîÑ Chain 2 in iteration 96/500,\n",
      " üìä path: 148,  new path: 22,  docs changed path: 234,  Log-likelihood: -460144.59\n",
      "üîÑ Chain 1 in iteration 98/500,\n",
      " üìä path: 144,  new path: 27,  docs changed path: 233,  Log-likelihood: -457214.38\n",
      "üîÑ Chain 3 in iteration 96/500,\n",
      " üìä path: 156,  new path: 29,  docs changed path: 248,  Log-likelihood: -452646.50\n",
      "üîÑ Chain 2 in iteration 97/500,\n",
      " üìä path: 144,  new path: 22,  docs changed path: 240,  Log-likelihood: -460138.84\n",
      "üîÑ Chain 1 in iteration 99/500,\n",
      " üìä path: 142,  new path: 19,  docs changed path: 223,  Log-likelihood: -458152.12\n",
      "üîÑ Chain 3 in iteration 97/500,\n",
      " üìä path: 146,  new path: 16,  docs changed path: 245,  Log-likelihood: -452679.73\n",
      "üîÑ Chain 2 in iteration 98/500,\n",
      " üìä path: 149,  new path: 25,  docs changed path: 242,  Log-likelihood: -459752.32\n",
      "üîÑ Chain 1 in iteration 100/500,\n",
      " üìä path: 149,  new path: 29,  docs changed path: 231,  Log-likelihood: -458000.39\n",
      "üîÑ Chain 3 in iteration 98/500,\n",
      " üìä path: 153,  new path: 25,  docs changed path: 239,  Log-likelihood: -451598.38\n",
      "üîÑ Chain 2 in iteration 99/500,\n",
      " üìä path: 150,  new path: 26,  docs changed path: 238,  Log-likelihood: -458640.86\n",
      "üîÑ Chain 1 in iteration 101/500,\n",
      " üìä path: 144,  new path: 20,  docs changed path: 244,  Log-likelihood: -457371.89\n",
      "üîÑ Chain 3 in iteration 99/500,\n",
      " üìä path: 145,  new path: 16,  docs changed path: 247,  Log-likelihood: -452174.60\n",
      "üîÑ Chain 2 in iteration 100/500,\n",
      " üìä path: 140,  new path: 16,  docs changed path: 219,  Log-likelihood: -460562.13\n",
      "üîÑ Chain 1 in iteration 102/500,\n",
      " üìä path: 153,  new path: 25,  docs changed path: 250,  Log-likelihood: -456762.31\n",
      "üîÑ Chain 3 in iteration 100/500,\n",
      " üìä path: 145,  new path: 23,  docs changed path: 240,  Log-likelihood: -452481.20\n",
      "üîÑ Chain 2 in iteration 101/500,\n",
      " üìä path: 152,  new path: 32,  docs changed path: 219,  Log-likelihood: -459697.05\n",
      "üîÑ Chain 1 in iteration 103/500,\n",
      " üìä path: 151,  new path: 21,  docs changed path: 242,  Log-likelihood: -455192.52\n",
      "üîÑ Chain 3 in iteration 101/500,\n",
      " üìä path: 141,  new path: 13,  docs changed path: 249,  Log-likelihood: -452176.25\n",
      "üîÑ Chain 2 in iteration 102/500,\n",
      " üìä path: 149,  new path: 19,  docs changed path: 228,  Log-likelihood: -458522.93\n",
      "üîÑ Chain 1 in iteration 104/500,\n",
      " üìä path: 145,  new path: 19,  docs changed path: 242,  Log-likelihood: -455963.58\n",
      "üîÑ Chain 3 in iteration 102/500,\n",
      " üìä path: 140,  new path: 19,  docs changed path: 250,  Log-likelihood: -452535.41\n",
      "üîÑ Chain 2 in iteration 103/500,\n",
      " üìä path: 148,  new path: 30,  docs changed path: 234,  Log-likelihood: -459479.83\n",
      "üîÑ Chain 3 in iteration 103/500,\n",
      " üìä path: 141,  new path: 18,  docs changed path: 240,  Log-likelihood: -450644.56\n",
      "üîÑ Chain 1 in iteration 105/500,\n",
      " üìä path: 146,  new path: 17,  docs changed path: 252,  Log-likelihood: -456400.54\n",
      "üîÑ Chain 2 in iteration 104/500,\n",
      " üìä path: 146,  new path: 21,  docs changed path: 236,  Log-likelihood: -459254.60\n",
      "üîÑ Chain 3 in iteration 104/500,\n",
      " üìä path: 139,  new path: 17,  docs changed path: 239,  Log-likelihood: -451613.10\n",
      "üîÑ Chain 1 in iteration 106/500,\n",
      " üìä path: 149,  new path: 20,  docs changed path: 243,  Log-likelihood: -456314.69\n",
      "üîÑ Chain 2 in iteration 105/500,\n",
      " üìä path: 146,  new path: 21,  docs changed path: 234,  Log-likelihood: -460289.81\n",
      "üîÑ Chain 3 in iteration 105/500,\n",
      " üìä path: 148,  new path: 23,  docs changed path: 234,  Log-likelihood: -451431.48\n",
      "üîÑ Chain 1 in iteration 107/500,\n",
      " üìä path: 157,  new path: 29,  docs changed path: 240,  Log-likelihood: -455753.63\n",
      "üîÑ Chain 2 in iteration 106/500,\n",
      " üìä path: 147,  new path: 25,  docs changed path: 249,  Log-likelihood: -458465.90\n",
      "üîÑ Chain 3 in iteration 106/500,\n",
      " üìä path: 147,  new path: 24,  docs changed path: 240,  Log-likelihood: -451184.28\n",
      "üîÑ Chain 1 in iteration 108/500,\n",
      " üìä path: 148,  new path: 18,  docs changed path: 259,  Log-likelihood: -455940.50\n",
      "üîÑ Chain 2 in iteration 107/500,\n",
      " üìä path: 146,  new path: 19,  docs changed path: 221,  Log-likelihood: -459026.72\n",
      "üîÑ Chain 1 in iteration 109/500,\n",
      " üìä path: 147,  new path: 18,  docs changed path: 252,  Log-likelihood: -455791.51\n",
      "üîÑ Chain 3 in iteration 107/500,\n",
      " üìä path: 151,  new path: 20,  docs changed path: 249,  Log-likelihood: -450451.42\n",
      "üîÑ Chain 2 in iteration 108/500,\n",
      " üìä path: 150,  new path: 21,  docs changed path: 233,  Log-likelihood: -459988.53\n",
      "üîÑ Chain 1 in iteration 110/500,\n",
      " üìä path: 149,  new path: 24,  Log-likelihood: -455598.17docs changed path: 243,  \n",
      "üîÑ Chain 3 in iteration 108/500,\n",
      " üìä path: 144,  new path: 16,  docs changed path: 246,  Log-likelihood: -450571.83\n",
      "üîÑ Chain 2 in iteration 109/500,\n",
      " üìä path: 147,  new path: 21,  docs changed path: 228,  Log-likelihood: -458257.89\n",
      "üîÑ Chain 3 in iteration 109/500,\n",
      " üìä path: 147,  new path: 25,  docs changed path: 244,  Log-likelihood: -450602.30\n",
      "üîÑ Chain 1 in iteration 111/500,\n",
      " üìä path: 144,  new path: 18,  docs changed path: 239,  Log-likelihood: -457000.45\n",
      "üîÑ Chain 2 in iteration 110/500,\n",
      " üìä path: 148,  new path: 21,  docs changed path: 230,  Log-likelihood: -459378.61\n",
      "üîÑ Chain 1 in iteration 112/500,\n",
      " üìä path: 144,  new path: 16,  docs changed path: 248,  Log-likelihood: -456165.17\n",
      "üîÑ Chain 3 in iteration 110/500,\n",
      " üìä path: 145,  new path: 24,  docs changed path: 239,  Log-likelihood: -450901.10\n",
      "üîÑ Chain 2 in iteration 111/500,\n",
      " üìä path: 150,  new path: 20,  docs changed path: 228,  Log-likelihood: -458763.49\n",
      "üîÑ Chain 1 in iteration 113/500,\n",
      " üìä path: 140,  new path: 17,  docs changed path: 241,  Log-likelihood: -455760.96\n",
      "üîÑ Chain 3 in iteration 111/500,\n",
      " üìä path: 145,  new path: 21,  docs changed path: 248,  Log-likelihood: -451122.68\n",
      "üîÑ Chain 2 in iteration 112/500,\n",
      " üìä path: 154,  new path: 29,  docs changed path: 224,  Log-likelihood: -457344.90\n",
      "üîÑ Chain 1 in iteration 114/500,\n",
      " üìä path: 141,  new path: 21,  docs changed path: 236,  Log-likelihood: -454844.86\n",
      "üîÑ Chain 3 in iteration 112/500,\n",
      " üìä path: 150,  new path: 20,  docs changed path: 238,  Log-likelihood: -450129.98\n",
      "üîÑ Chain 2 in iteration 113/500,\n",
      " üìä path: 148,  new path: 22,  docs changed path: 222,  Log-likelihood: -458677.15\n",
      "üîÑ Chain 1 in iteration 115/500,\n",
      " üìä path: 148,  new path: 24,  docs changed path: 232,  Log-likelihood: -455040.00\n",
      "üîÑ Chain 3 in iteration 113/500,\n",
      " üìä path: 152,  new path: 20,  docs changed path: 228,  Log-likelihood: -449843.03\n",
      "üîÑ Chain 2 in iteration 114/500,\n",
      " üìä path: 152,  new path: 26,  docs changed path: 242,  Log-likelihood: -459670.96\n",
      "üîÑ Chain 1 in iteration 116/500,\n",
      " üìä path: 149,  new path: 21,  docs changed path: 228,  Log-likelihood: -454753.85\n",
      "üîÑ Chain 3 in iteration 114/500,\n",
      " üìä path: 143,  new path: 19,  docs changed path: 242,  Log-likelihood: -450691.69\n",
      "üîÑ Chain 2 in iteration 115/500,\n",
      " üìä path: 153,  new path: 27,  docs changed path: 241,  Log-likelihood: -458174.71\n",
      "üîÑ Chain 1 in iteration 117/500,\n",
      " üìä path: 143,  new path: 17,  docs changed path: 238,  Log-likelihood: -455205.60\n",
      "üîÑ Chain 3 in iteration 115/500,\n",
      " üìä path: 147,  new path: 23,  docs changed path: 250,  Log-likelihood: -448809.93\n",
      "check_dual_convergence_with_gelman_rubin: chain_len_docs:{'1': [617, 611, 611, 611, 627, 637, 625, 620, 618, 610, 611, 615, 608, 608, 614, 609, 614, 626, 622, 620, 619], '2': [628, 626, 626, 621, 630, 630, 627, 632, 631, 632, 626, 629, 629, 628, 633, 640, 635, 633, 629, 631, 630], '3': [609, 612, 617, 623, 617, 627, 634, 618, 619, 614, 615, 620, 628, 625, 615, 616, 618, 630, 635, 626, 629]}, mean_jaccard:[0.9452993221858167, 0.946450094183654, 0.9423885674156205], mean_ratio:[0.20000000000000004, 0.20000000000000004, 0.20000000000000004]\n",
      "ü•∞ Convergence status: ‚ùå Not converged...\n",
      "ü•∞  R-hat info: 1.4141Ôºåmean_jaccard:[0.9452993221858167, 0.946450094183654, 0.9423885674156205], mean_ratio:[0.20000000000000004, 0.20000000000000004, 0.20000000000000004]\n",
      "üîÑ Chain 2 in iteration 116/500,\n",
      " üìä path: 169,  new path: 39,  docs changed path: 236,  Log-likelihood: -456752.61\n",
      "üîÑ Chain 1 in iteration 118/500,\n",
      " üìä path: 144,  new path: 24,  docs changed path: 232,  Log-likelihood: -454615.28\n",
      "üîÑ Chain 3 in iteration 116/500,\n",
      " üìä path: 151,  new path: 21,  docs changed path: 239,  Log-likelihood: -449113.90\n",
      "üîÑ Chain 2 in iteration 117/500,\n",
      " üìä path: 159,  new path: 22,  docs changed path: 244,  Log-likelihood: -458457.82\n",
      "üîÑ Chain 1 in iteration 119/500,\n",
      " üìä path: 151,  new path: 32,  docs changed path: 227,  Log-likelihood: -454830.60\n",
      "üîÑ Chain 3 in iteration 117/500,\n",
      " üìä path: 150,  new path: 23,  docs changed path: 252,  Log-likelihood: -448366.45\n",
      "üîÑ Chain 2 in iteration 118/500,\n",
      " üìä path: 155,  new path: 22,  docs changed path: 240,  Log-likelihood: -458109.28\n",
      "üîÑ Chain 1 in iteration 120/500,\n",
      " üìä path: 140,  new path: 18,  docs changed path: 233,  Log-likelihood: -454754.43\n",
      "üîÑ Chain 3 in iteration 118/500,\n",
      " üìä path: 144,  new path: 20,  docs changed path: 236,  Log-likelihood: -449024.79\n",
      "üîÑ Chain 2 in iteration 119/500,\n",
      " üìä path: 150,  new path: 22,  docs changed path: 222,  Log-likelihood: -456741.53\n",
      "üîÑ Chain 1 in iteration 121/500,\n",
      " üìä path: 146,  new path: 19,  docs changed path: 241,  Log-likelihood: -455055.86\n",
      "üîÑ Chain 3 in iteration 119/500,\n",
      " üìä path: 157,  new path: 30,  docs changed path: 236,  Log-likelihood: -448737.09\n",
      "üîÑ Chain 2 in iteration 120/500,\n",
      " üìä path: 152,  new path: 21,  docs changed path: 218,  Log-likelihood: -456248.94\n",
      "üîÑ Chain 1 in iteration 122/500,\n",
      " üìä path: 146,  new path: 21,  docs changed path: 224,  Log-likelihood: -454255.51\n",
      "üîÑ Chain 3 in iteration 120/500,\n",
      " üìä path: 151,  new path: 20,  docs changed path: 247,  Log-likelihood: -448581.96\n",
      "üîÑ Chain 2 in iteration 121/500,\n",
      " üìä path: 148,  new path: 18,  docs changed path: 232,  Log-likelihood: -457075.29\n",
      "üîÑ Chain 1 in iteration 123/500,\n",
      " üìä path: 151,  new path: 22,  docs changed path: 232,  Log-likelihood: -455056.73\n",
      "üîÑ Chain 3 in iteration 121/500,\n",
      " üìä path: 161,  new path: 27,  docs changed path: 243,  Log-likelihood: -448038.44\n",
      "üîÑ Chain 2 in iteration 122/500,\n",
      " üìä path: 148,  new path: 15,  docs changed path: 232,  Log-likelihood: -457137.60\n",
      "üîÑ Chain 1 in iteration 124/500,\n",
      " üìä path: 149,  new path: 24,  docs changed path: 243,  Log-likelihood: -454289.99\n",
      "üîÑ Chain 3 in iteration 122/500,\n",
      " üìä path: 162,  new path: 22,  docs changed path: 268,  Log-likelihood: -448098.08\n",
      "üîÑ Chain 2 in iteration 123/500,\n",
      " üìä path: 149,  new path: 19,  docs changed path: 230,  Log-likelihood: -458000.58\n",
      "üîÑ Chain 1 in iteration 125/500,\n",
      " üìä path: 147,  new path: 18,  docs changed path: 231,  Log-likelihood: -454274.81\n",
      "üîÑ Chain 3 in iteration 123/500,\n",
      " üìä path: 153,  new path: 18,  docs changed path: 252,  Log-likelihood: -448933.50\n",
      "üîÑ Chain 2 in iteration 124/500,\n",
      " üìä path: 145,  new path: 25,  docs changed path: 234,  Log-likelihood: -457746.29\n",
      "üîÑ Chain 1 in iteration 126/500,\n",
      " üìä path: 146,  new path: 22,  docs changed path: 235,  Log-likelihood: -454317.98\n",
      "üîÑ Chain 3 in iteration 124/500,\n",
      " üìä path: 141,  new path: 13,  docs changed path: 228,  Log-likelihood: -449052.07\n",
      "üîÑ Chain 2 in iteration 125/500,\n",
      " üìä path: 153,  new path: 31,  docs changed path: 216,  Log-likelihood: -456746.25\n",
      "üîÑ Chain 1 in iteration 127/500,\n",
      " üìä path: 149,  new path: 24,  docs changed path: 237,  Log-likelihood: -454237.60\n",
      "üîÑ Chain 3 in iteration 125/500,\n",
      " üìä path: 153,  new path: 26,  docs changed path: 239,  Log-likelihood: -449117.64\n",
      "üîÑ Chain 2 in iteration 126/500,\n",
      " üìä path: 157,  new path: 25,  docs changed path: 218,  Log-likelihood: -456495.08\n",
      "üîÑ Chain 1 in iteration 128/500,\n",
      " üìä path: 144,  new path: 16,  docs changed path: 235,  Log-likelihood: -455317.31\n",
      "üîÑ Chain 3 in iteration 126/500,\n",
      " üìä path: 148,  new path: 15,  docs changed path: 243,  Log-likelihood: -450376.55\n",
      "üîÑ Chain 2 in iteration 127/500,\n",
      " üìä path: 153,  new path: 24,  docs changed path: 212,  Log-likelihood: -456878.22\n",
      "üîÑ Chain 1 in iteration 129/500,\n",
      " üìä path: 143,  new path: 25,  docs changed path: 240,  Log-likelihood: -454313.32\n",
      "üîÑ Chain 3 in iteration 127/500,\n",
      " üìä path: 150,  new path: 18,  docs changed path: 241,  Log-likelihood: -448875.62\n",
      "üîÑ Chain 2 in iteration 128/500,\n",
      " üìä path: 153,  new path: 22,  docs changed path: 215,  Log-likelihood: -456775.38\n",
      "üîÑ Chain 1 in iteration 130/500,\n",
      " üìä path: 151,  new path: 30,  docs changed path: 251,  Log-likelihood: -453620.57\n",
      "üîÑ Chain 3 in iteration 128/500,\n",
      " üìä path: 150,  new path: 21,  docs changed path: 240,  Log-likelihood: -448407.66\n",
      "üîÑ Chain 2 in iteration 129/500,\n",
      " üìä path: 146,  new path: 18,  docs changed path: 218,  Log-likelihood: -456611.37\n",
      "üîÑ Chain 1 in iteration 131/500,\n",
      " üìä path: 147,  new path: 24,  docs changed path: 235,  Log-likelihood: -453690.79\n",
      "üîÑ Chain 3 in iteration 129/500,\n",
      " üìä path: 147,  new path: 16,  docs changed path: 229,  Log-likelihood: -449192.58\n",
      "üîÑ Chain 2 in iteration 130/500,\n",
      " üìä path: 147,  new path: 22,  docs changed path: 219,  Log-likelihood: -457220.24\n",
      "üîÑ Chain 1 in iteration 132/500,\n",
      " üìä path: 148,  new path: 23,  docs changed path: 242,  Log-likelihood: -454363.39\n",
      "üîÑ Chain 3 in iteration 130/500,\n",
      " üìä path: 145,  new path: 11,  docs changed path: 227,  Log-likelihood: -449268.55\n",
      "üîÑ Chain 2 in iteration 131/500,\n",
      " üìä path: 151,  new path: 22,  docs changed path: 218,  Log-likelihood: -457419.35\n",
      "üîÑ Chain 1 in iteration 133/500,\n",
      " üìä path: 147,  new path: 21,  docs changed path: 245,  Log-likelihood: -454169.83\n",
      "üîÑ Chain 3 in iteration 131/500,\n",
      " üìä path: 147,  new path: 18,  docs changed path: 222,  Log-likelihood: -449420.89\n",
      "üîÑ Chain 2 in iteration 132/500,\n",
      " üìä path: 144,  new path: 17,  docs changed path: 220,  Log-likelihood: -455958.76\n",
      "üîÑ Chain 1 in iteration 134/500,\n",
      " üìä path: 149,  new path: 25,  docs changed path: 243,  Log-likelihood: -453886.75\n",
      "üîÑ Chain 3 in iteration 132/500,\n",
      " üìä path: 145,  new path: 22,  docs changed path: 219,  Log-likelihood: -448524.58\n",
      "üîÑ Chain 2 in iteration 133/500,\n",
      " üìä path: 142,  new path: 19,  docs changed path: 218,  Log-likelihood: -455749.30\n",
      "üîÑ Chain 1 in iteration 135/500,\n",
      " üìä path: 149,  new path: 22,  docs changed path: 242,  Log-likelihood: -453751.19\n",
      "üîÑ Chain 3 in iteration 133/500,\n",
      " üìä path: 151,  new path: 20,  docs changed path: 224,  Log-likelihood: -449354.81\n",
      "üîÑ Chain 2 in iteration 134/500,\n",
      " üìä path: 149,  new path: 19,  docs changed path: 216,  Log-likelihood: -455008.82\n",
      "üîÑ Chain 1 in iteration 136/500,\n",
      " üìä path: 158,  new path: 26,  docs changed path: 232,  Log-likelihood: -453450.21\n",
      "üîÑ Chain 3 in iteration 134/500,\n",
      " üìä path: 148,  new path: 18,  docs changed path: 227,  Log-likelihood: -448570.18\n",
      "üîÑ Chain 2 in iteration 135/500,\n",
      " üìä path: 152,  new path: 24,  docs changed path: 224,  Log-likelihood: -455791.53\n",
      "üîÑ Chain 1 in iteration 137/500,\n",
      " üìä path: 157,  new path: 24,  docs changed path: 220,  Log-likelihood: -452481.17\n",
      "üîÑ Chain 3 in iteration 135/500,\n",
      " üìä path: 152,  new path: 27,  docs changed path: 240,  Log-likelihood: -448040.63\n",
      "üîÑ Chain 2 in iteration 136/500,\n",
      " üìä path: 154,  new path: 23,  docs changed path: 237,  Log-likelihood: -453319.24\n",
      "check_dual_convergence_with_gelman_rubin: chain_len_docs:{'1': [619, 623, 623, 628, 631, 630, 624, 638, 640, 636, 634, 637, 631, 625, 615, 616, 622, 628, 634, 633, 620], '2': [630, 630, 625, 626, 624, 626, 625, 632, 631, 627, 626, 632, 641, 650, 647, 653, 645, 643, 655, 659, 653], '3': [629, 630, 629, 625, 618, 619, 622, 619, 613, 619, 619, 624, 628, 628, 634, 647, 645, 646, 634, 641, 647]}, mean_jaccard:[0.9472483075521987, 0.9457588336677988, 0.946987280066786], mean_ratio:[0.20000000000000004, 0.20000000000000004, 0.20000000000000004]\n",
      "ü•∞ Convergence status: ‚úÖ Converged!\n",
      "ü•∞  R-hat info: 1.0920Ôºåmean_jaccard:[0.9472483075521987, 0.9457588336677988, 0.946987280066786], mean_ratio:[0.20000000000000004, 0.20000000000000004, 0.20000000000000004]\n",
      "‚úÖ r-hat of multi-chains is detected to convergence 115 in iteration:115, R-hat=1.0920, JaccardÂπ≥ÂùáÂÄº=[0.9472483075521987, 0.9457588336677988, 0.946987280066786], RatioÂπ≥ÂùáÂÄº=[0.20000000000000004, 0.20000000000000004, 0.20000000000000004]\n",
      "‚úÖ R-hat convergence is detected, waiting for all chains to finish.\n",
      "üîÑ Chain 1 in iteration 138/500,\n",
      " üìä path: 156,  new path: 18,  docs changed path: 241,  Log-likelihood: -453774.15\n",
      "üîÑ Chain 3 in iteration 136/500,\n",
      " üìä path: 150,  new path: 16,  docs changed path: 247,  Log-likelihood: -448423.94\n",
      "üîÑ Chain 2 in iteration 137/500,\n",
      " üìä path: 149,  new path: 23,  docs changed path: 223,  Log-likelihood: -455673.05\n",
      "üîÑ Chain 1 in iteration 139/500,\n",
      " üìä path: 156,  new path: 24,  docs changed path: 259,  Log-likelihood: -452620.73\n",
      "üîÑ Chain 3 in iteration 137/500,\n",
      " üìä path: 148,  new path: 16,  docs changed path: 245,  Log-likelihood: -447842.49\n",
      "üîÑ Chain 2 in iteration 138/500,\n",
      " üìä path: 157,  new path: 26,  docs changed path: 228,  Log-likelihood: -454375.80\n",
      "üîÑ Chain 1 in iteration 140/500,\n",
      " üìä path: 154,  new path: 26,  docs changed path: 248,  Log-likelihood: -453864.90\n",
      "üîÑ Chain 3 in iteration 138/500,\n",
      " üìä path: 154,  new path: 22,  docs changed path: 222,  Log-likelihood: -448327.47\n",
      "üîÑ Chain 2 in iteration 139/500,\n",
      " üìä path: 154,  new path: 20,  docs changed path: 220,  Log-likelihood: -455423.68\n",
      "üîÑ Chain 1 in iteration 141/500,\n",
      " üìä path: 161,  new path: 28,  docs changed path: 253,  Log-likelihood: -452680.43\n",
      "üîÑ Chain 3 in iteration 139/500,\n",
      " üìä path: 147,  new path: 17,  docs changed path: 236,  Log-likelihood: -447823.17\n",
      "üîÑ Chain 2 in iteration 140/500,\n",
      " üìä path: 154,  new path: 20,  docs changed path: 234,  Log-likelihood: -455511.42\n",
      "üîÑ Chain 1 in iteration 142/500,\n",
      " üìä path: 157,  new path: 22,  docs changed path: 235,  Log-likelihood: -453239.76\n",
      "üîÑ Chain 2 in iteration 141/500,\n",
      " üìä path: 153,  new path: 23,  docs changed path: 233,  Log-likelihood: -454469.78\n",
      "üîÑ Chain 3 in iteration 140/500,\n",
      " üìä path: 155,  new path: 20,  docs changed path: 245,  Log-likelihood: -447715.95\n",
      "üîÑ Chain 1 in iteration 143/500,\n",
      " üìä path: 155,  new path: 20,  docs changed path: 241,  Log-likelihood: -451877.80\n",
      "üîÑ Chain 2 in iteration 142/500,\n",
      " üìä path: 144,  new path: 18,  docs changed path: 231,  Log-likelihood: -455874.90\n",
      "üîÑ Chain 3 in iteration 141/500,\n",
      " üìä path: 150,  new path: 17,  docs changed path: 243,  Log-likelihood: -449228.71\n",
      "üîÑ Chain 1 in iteration 144/500,\n",
      " üìä path: 155,  new path: 19,  docs changed path: 254,  Log-likelihood: -452447.92\n",
      "üîÑ Chain 2 in iteration 143/500,\n",
      " üìä path: 157,  new path: 25,  docs changed path: 219,  Log-likelihood: -454823.57\n",
      "üîÑ Chain 3 in iteration 142/500,\n",
      " üìä path: 151,  new path: 15,  docs changed path: 238,  Log-likelihood: -448054.92\n",
      "üîÑ Chain 1 in iteration 145/500,\n",
      " üìä path: 153,  new path: 22,  docs changed path: 238,  Log-likelihood: -451430.10\n",
      "üîÑ Chain 2 in iteration 144/500,\n",
      " üìä path: 151,  new path: 23,  docs changed path: 231,  Log-likelihood: -454481.71\n",
      "üîÑ Chain 3 in iteration 143/500,\n",
      " üìä path: 151,  new path: 19,  docs changed path: 241,  Log-likelihood: -448087.82\n",
      "üîÑ Chain 1 in iteration 146/500,\n",
      " üìä path: 154,  new path: 23,  docs changed path: 243,  Log-likelihood: -451988.87\n",
      "üîÑ Chain 2 in iteration 145/500,\n",
      " üìä path: 150,  new path: 22,  docs changed path: 231,  Log-likelihood: -455355.31\n",
      "üîÑ Chain 3 in iteration 144/500,\n",
      " üìä path: 152,  new path: 19,  docs changed path: 239,  Log-likelihood: -447591.80\n",
      "üîÑ Chain 1 in iteration 147/500,\n",
      " üìä path: 153,  new path: 27,  docs changed path: 233,  Log-likelihood: -451493.51\n",
      "üîÑ Chain 2 in iteration 146/500,\n",
      " üìä path: 160,  new path: 29,  docs changed path: 222,  Log-likelihood: -454135.36\n",
      "üîÑ Chain 3 in iteration 145/500,\n",
      " üìä path: 154,  new path: 20,  docs changed path: 232,  Log-likelihood: -447565.34\n",
      "üîÑ Chain 1 in iteration 148/500,\n",
      " üìä path: 157,  new path: 25,  docs changed path: 236,  Log-likelihood: -452080.77\n",
      "üîÑ Chain 2 in iteration 147/500,\n",
      " üìä path: 146,  new path: 12,  docs changed path: 228,  Log-likelihood: -455357.70\n",
      "üîÑ Chain 3 in iteration 146/500,\n",
      " üìä path: 163,  new path: 27,  docs changed path: 243,  Log-likelihood: -447790.78\n",
      "üîÑ Chain 2 in iteration 148/500,\n",
      " üìä path: 143,  new path: 20,  docs changed path: 230,  Log-likelihood: -455480.74\n",
      "üîÑ Chain 1 in iteration 149/500,\n",
      " üìä path: 148,  new path: 20,  docs changed path: 246,  Log-likelihood: -453706.48\n",
      "üîÑ Chain 3 in iteration 147/500,\n",
      " üìä path: 157,  new path: 21,  docs changed path: 240,  Log-likelihood: -448238.39\n",
      "üîÑ Chain 2 in iteration 149/500,\n",
      " üìä path: 150,  new path: 24,  docs changed path: 232,  Log-likelihood: -454119.71\n",
      "üîÑ Chain 1 in iteration 150/500,\n",
      " üìä path: 148,  new path: 26,  docs changed path: 229,  Log-likelihood: -452615.97\n",
      "üîÑ Chain 3 in iteration 148/500,\n",
      " üìä path: 150,  new path: 17,  docs changed path: 249,  Log-likelihood: -448994.22\n",
      "üîÑ Chain 2 in iteration 150/500,\n",
      " üìä path: 141,  new path: 15,  docs changed path: 239,  Log-likelihood: -455330.16\n",
      "üîÑ Chain 1 in iteration 151/500,\n",
      " üìä path: 146,  new path: 17,  docs changed path: 223,  Log-likelihood: -453421.37\n",
      "üîÑ Chain 3 in iteration 149/500,\n",
      " üìä path: 154,  new path: 22,  docs changed path: 248,  Log-likelihood: -448238.58\n",
      "üîÑ Chain 2 in iteration 151/500,\n",
      " üìä path: 145,  new path: 22,  docs changed path: 235,  Log-likelihood: -455549.39\n",
      "üîÑ Chain 1 in iteration 152/500,\n",
      " üìä path: 150,  new path: 21,  docs changed path: 251,  Log-likelihood: -452411.41\n",
      "üîÑ Chain 3 in iteration 150/500,\n",
      " üìä path: 148,  new path: 18,  docs changed path: 229,  Log-likelihood: -449001.10\n",
      "üîÑ Chain 2 in iteration 152/500,\n",
      " üìä path: 143,  new path: 18,  docs changed path: 241,  Log-likelihood: -456001.18\n",
      "üîÑ Chain 1 in iteration 153/500,\n",
      " üìä path: 156,  new path: 27,  docs changed path: 260,  Log-likelihood: -452118.44\n",
      "üîÑ Chain 3 in iteration 151/500,\n",
      " üìä path: 146,  new path: 23,  docs changed path: 212,  Log-likelihood: -447765.63\n",
      "üîÑ Chain 2 in iteration 153/500,\n",
      " üìä path: 157,  new path: 29,  docs changed path: 235,  Log-likelihood: -454442.91\n",
      "üîÑ Chain 1 in iteration 154/500,\n",
      " üìä path: 158,  new path: 23,  docs changed path: 242,  Log-likelihood: -451874.89\n",
      "üîÑ Chain 3 in iteration 152/500,\n",
      " üìä path: 142,  new path: 11,  docs changed path: 222,  Log-likelihood: -448181.52\n",
      "üîÑ Chain 2 in iteration 154/500,\n",
      " üìä path: 149,  new path: 22,  docs changed path: 232,  Log-likelihood: -455209.16\n",
      "üîÑ Chain 1 in iteration 155/500,\n",
      " üìä path: 148,  new path: 17,  docs changed path: 226,  Log-likelihood: -452183.59\n",
      "üéâ Chain 1 is detected to convergence in iteration 155, sampling ends prematurely.\n",
      "üéØ Chain 1 finish Gibbs Sampling.\n",
      "üíæ Save iterations info...\n",
      "‚úÖ doc-path allocation is saved : machine2_step1_d3_g001/depth_3_gamma_0.01_run_1/iteration_document_paths.csv\n",
      "‚úÖ structure of tree path is saved: machine2_step1_d3_g001/depth_3_gamma_0.01_run_1/iteration_path_structures.csv\n",
      "‚úÖ info recoreded in iteration is saved: machine2_step1_d3_g001/depth_3_gamma_0.01_run_1/iteration_iteration_summaries.csv\n",
      "‚úÖ doc-path mapping is saved: machine2_step1_d3_g001/depth_3_gamma_0.01_run_1/iteration_path_document_mapping.csv\n",
      "üîÑ Chain 3 in iteration 153/500,\n",
      " üìä path: 145,  new path: 18,  docs changed path: 236,  Log-likelihood: -448058.05\n",
      "‚úÖ doc's words allocation to node is saved: machine2_step1_d3_g001/depth_3_gamma_0.01_run_1/iteration_word_allocations.csv\n",
      "‚úÖ node word distribution is saved: machine2_step1_d3_g001/depth_3_gamma_0.01_run_1/iteration_node_word_distributions.csv\n",
      "‚úÖ jump record info is saved: machine2_step1_d3_g001/depth_3_gamma_0.01_run_1/iteration_jump_records.csv\n",
      "‚úÖ other detailed info is saved: machine2_step1_d3_g001/depth_3_gamma_0.01_run_1/iteration_detail_records.csv\n",
      "‚úÖ Chain 1 Finished !\n",
      "‚ö†Ô∏è Node 3626 (Layer 1) has entropy=0:\n",
      "   Word distribution: {'concern': 0, 'reducedorder': 0, 'computational': 0, 'quality': 0, 'solution': 0, 'basis': 0, 'fix': 0, 'efficient': 0, 'step': 0, 'compute': 0, 'vector': 0, 'full': 0, 'allow': 0, 'application': 0, 'algorithm': 0, 'point': 0, 'computation': 0, 'numerical': 0, 'study': 0, 'reduction': 0, 'control': 0, 'correction': 0, 'navierstokes': 0, 'require': 0, 'curve': 0, 'viscoelastic': 0, 'structure': 0, 'key': 0, 'method': 0, 'analyse': 0, 'define': 0, 'strategy': 0, 'tangent': 0, 'matrix': 0, 'determine': 0, 'response': 0, 'definition': 0, 'reduce': 0, 'quantity': 0, 'apply': 0, 'second': 0, 'previously': 0, 'first': 0, 'update': 0, 'two': 0, 'procedure': 0, 'series': 0, 'use': 0, 'discuss': 0, 'consider': 0, 'new': 0, 'within': 0, 'equation': 0, 'propose': 0, 'set': 0, 'calculation': 0, 'many': 0, 'projection': 0, 'class': 0, 'recent': 0, 'difficult': 0, 'strong': 0, 'especially': 0, 'develop': 0, 'necessary': 0, 'operate': 0, 'wavelet': 0, 'transformation': 0, 'scale': 0, 'design': 0, 'govern': 0, 'fe': 0, 'enhance': 0, 'result': 0, 'model': 0, 'show': 0, 'different': 0, 'problem': 0, 'large': 0, 'mechanical': 0, 'field': 0, 'dynamic': 0, 'widely': 0, 'framework': 0, 'simulation': 0, 'coupled': 0, 'multiphysics': 0, 'frequency': 0, 'solver': 0, 'accuracy': 0, 'comparison': 0, 'single': 0, 'ratio': 0, 'demonstrate': 0, 'complex': 0, 'relation': 0, 'paper': 0, 'simulate': 0, 'highly': 0, 'couple': 0, 'cycle': 0, 'effective': 0, 'development': 0, 'compare': 0, 'finite': 0, 'transient': 0, 'induce': 0, 'efficiency': 0, 'example': 0, 'element': 0, 'patch': 0, 'temporal': 0, 'regime': 0, 'range': 0, 'improve': 0, 'accurate': 0, 'thermomechanical': 0, 'predict': 0, 'thermal': 0, 'behavior': 0, 'high': 0, 'order': 0, 'surrogate': 0, 'system': 0, 'work': 0, 'error': 0, 'dependency': 0, 'boundary': 0, 'parameter': 0, 'condition': 0, 'investigate': 0, 'capture': 0, 'interest': 0, 'machine': 0, 'associate': 0, 'however': 0, 'create': 0, 'parametric': 0, 'weakly': 0, 'subspace': 0, 'theoretical': 0, 'address': 0, 'original': 0, 'tool': 0, 'therefore': 0, 'value': 0, 'additionally': 0, 'concept': 0, 'describe': 0, 'link': 0, 'evaluation': 0, 'similar': 0, 'present': 0, 'one': 0, 'modal': 0, 'differential': 0, 'bound': 0, 'acoustic': 0, 'topology': 0, 'optimization': 0, 'call': 0, 'static': 0, 'well': 0, 'sensitivity': 0, 'calculate': 0, 'base': 0, 'multiple': 0, 'domain': 0, 'several': 0, 'usually': 0, 'significant': 0, 'pressure': 0, 'objective': 0, 'effectively': 0, 'adopt': 0, 'approach': 0, 'scheme': 0, 'minimize': 0, 'reliability': 0, 'mathematical': 1, 'partial': 0, 'amount': 0, 'successfully': 0, 'among': 0, 'size': 0, 'verify': 0, 'research': 0, 'increasingly': 0, 'strongly': 0, 'vary': 0, 'although': 0, 'macroscale': 0, 'simple': 0, 'beam': 0, 'plate': 0, 'depend': 0, 'spatially': 0, 'geometrical': 0, 'accelerate': 0, 'manufacture': 0, 'structural': 0, 'space': 0, 'also': 0, 'challenge': 0, 'represent': 0, 'interface': 0, 'next': 0, 'gain': 0, 'time': 0, 'absorption': 0, 'consist': 0, 'current': 0, 'inside': 0, 'construction': 0, 'mesh': 0, 'enable': 0, 'geometry': 0, 'lattice': 0, 'provide': 0, 'part': 0, 'improvement': 0, 'additive': 0, 'hand': 0, 'pose': 0, 'way': 0, 'stiffness': 0, 'energy': 0, 'andor': 0, 'make': 0, 'concurrent': 0, 'solid': 0, 'flexible': 0, 'largescale': 0, 'total': 0, 'bridge': 0, 'mass': 0, 'manufacturing': 0, 'elastic': 0, 'exploit': 0, 'employ': 0, 'still': 0, 'build': 0}\n",
      "   Number of unique words: 242\n",
      "   Total word count: 1\n",
      "   Document count: 5\n",
      "\n",
      "‚ö†Ô∏è Node 3803 (Layer 1) has entropy=0:\n",
      "   Word distribution: {'buckle': 0, 'inverse': 0, 'propose': 0, 'yet': 0, 'use': 0, 'property': 0, 'due': 0, 'greatly': 0, 'even': 0, 'system': 0, 'imperfection': 0, 'modify': 0, 'show': 0, 'nonlinear': 0, 'response': 0, 'procedure': 0, 'necessary': 0, 'test': 0, 'possible': 0, 'constitutive': 0, 'involve': 0, 'defect': 0, 'approach': 0, 'characterization': 0, 'material': 0, 'influence': 0, 'comparison': 0, 'progressive': 0, 'govern': 0, 'know': 0, 'extract': 0, 'information': 0, 'geometric': 0, 'eigenvalue': 0, 'problem': 0, 'error': 0, 'specimen': 0, 'also': 0, 'equivalent': 0, 'dedicate': 0, 'formulation': 0, 'construct': 0, 'examples': 0, 'localize': 0, 'single': 0, 'multiple': 0, 'compressive': 0, 'compression': 0, 'tackle': 0, 'issue': 0, 'potential': 0, 'present': 0, 'implement': 0, 'base': 0, 'linear': 0, 'methodology': 0, 'identify': 0, 'part': 0, 'illustrate': 0, 'localization': 0, 'relation': 0, 'knowledge': 0, 'mainly': 0, 'publish': 0, 'original': 0, 'article': 0}\n",
      "   Number of unique words: 66\n",
      "   Total word count: 0\n",
      "   Document count: 2\n",
      "\n",
      "‚ö†Ô∏è Node 3821 (Layer 2) has entropy=0:\n",
      "   Word distribution: {'twopart': 0, 'new': 0, 'composite': 0, 'structure': 0, 'base': 0, 'represent': 0, 'shell': 0, 'continuum': 0, 'mechanic': 0, 'damage': 0, 'formulation': 0, 'introduce': 0, 'delamination': 0, 'well': 0, 'part': 0, 'series': 0, 'focus': 0, 'application': 0, 'simulation': 0, 'result': 0, 'impact': 0, 'propose': 0, 'approach': 0, 'significant': 0, 'method': 0, 'stem': 0, 'individual': 0, 'laminate': 0, 'enable': 0, 'shear': 0, 'deformation': 0, 'unlike': 0, 'solid': 0, 'traditional': 0, 'u': 0, 'use': 0, 'degree': 0, 'efficiency': 0, 'surface': 0, 'compliance': 0, 'validation': 0, 'exist': 0, 'thickness': 0, 'combination': 0, 'midsurface': 0, 'inplane': 0, 'rotational': 0, 'benefit': 0, 'relative': 0, 'analysis': 0, 'permit': 0, 'assessment': 0, 'advantage': 0, 'representation': 0, 'relatively': 0, 'coarse': 0, 'paper': 0, 'adopt': 0, 'field': 0, 'discretizations': 0, 'freedom': 0, 'additional': 0, 'isogeometric': 0, 'transverse': 0, 'cohesive': 0, 'accurate': 0, 'ii': 0, 'limit': 0, 'smooth': 0, 'without': 0, 'give': 0, 'progressive': 0, 'mode': 0, 'standard': 0, 'model': 0, 'kirchhofflove': 0, 'allow': 0, 'lock': 0, 'higherorder': 0, 'interface': 0, 'framework': 0, 'thin': 0, 'test': 0, 'displacement': 0, 'furthermore': 0, 'present': 0, 'work': 0, 'novel': 0, 'optimization': 0, 'novelty': 0, 'grade': 0, 'material': 0, 'two': 0, 'numerical': 0, 'discuss': 0, 'sensitivity': 0, 'study': 0, 'observe': 0, 'exploit': 0, 'flexibility': 0, 'come': 0, 'additive': 0, 'manufacturing': 0, 'topology': 0, 'introduction': 0, 'phasefield': 0, 'algorithm': 0, 'variable': 0, 'onto': 0, 'add': 1, 'main': 0, 'continuous': 0, 'effect': 0, 'design': 0, 'perform': 0, 'high': 0, 'example': 0, 'property': 0, 'different': 0, 'parameter': 0, 'classical': 0}\n",
      "   Number of unique words: 121\n",
      "   Total word count: 1\n",
      "   Document count: 2\n",
      "\n",
      "‚ö†Ô∏è Node 3721 (Layer 2) has entropy=0:\n",
      "   Word distribution: {'last': 0, 'decade': 0, 'layer': 0, 'desire': 0, 'comprise': 0, 'ii': 0, 'weight': 0, 'n': 0, 'input': 0, 'data': 0, 'match': 0, 'overall': 0, 'primary': 0, 'issue': 0, 'calibration': 0, 'optimization': 0, 'difference': 0, 'select': 0, 'algorithms': 0, 'goal': 0, 'short': 0, 'illustrate': 0, 'artificial': 0, 'neural': 0, 'adaptive': 0, 'training': 0, 'cast': 0, 'observe': 0, 'extremely': 0, 'base': 0, 'set': 0, 'problem': 0, 'form': 0, 'one': 0, 'whereby': 0, 'function': 0, 'attention': 0, 'type': 0, 'minimize': 0, 'family': 0}\n",
      "   Number of unique words: 40\n",
      "   Total word count: 0\n",
      "   Document count: 1\n",
      "\n",
      "üîÑ Chain 2 in iteration 155/500,\n",
      " üìä path: 154,  new path: 23,  docs changed path: 223,  Log-likelihood: -454528.60\n",
      "üéâ Chain 2 is detected to convergence in iteration 155, sampling ends prematurely.\n",
      "üéØ Chain 2 finish Gibbs Sampling.\n",
      "üíæ Save iterations info...\n",
      "‚úÖ doc-path allocation is saved : machine2_step1_d3_g001/depth_3_gamma_0.01_run_2/iteration_document_paths.csv\n",
      "‚úÖ structure of tree path is saved: machine2_step1_d3_g001/depth_3_gamma_0.01_run_2/iteration_path_structures.csv\n",
      "‚úÖ info recoreded in iteration is saved: machine2_step1_d3_g001/depth_3_gamma_0.01_run_2/iteration_iteration_summaries.csv\n",
      "‚úÖ doc-path mapping is saved: machine2_step1_d3_g001/depth_3_gamma_0.01_run_2/iteration_path_document_mapping.csv\n",
      "‚úÖ doc's words allocation to node is saved: machine2_step1_d3_g001/depth_3_gamma_0.01_run_2/iteration_word_allocations.csv\n",
      "‚úÖ node word distribution is saved: machine2_step1_d3_g001/depth_3_gamma_0.01_run_2/iteration_node_word_distributions.csv\n",
      "‚úÖ jump record info is saved: machine2_step1_d3_g001/depth_3_gamma_0.01_run_2/iteration_jump_records.csv\n",
      "üîÑ Chain 3 in iteration 154/500,\n",
      " üìä path: 148,  new path: 21,  docs changed path: 229,  Log-likelihood: -447541.46\n",
      "‚úÖ other detailed info is saved: machine2_step1_d3_g001/depth_3_gamma_0.01_run_2/iteration_detail_records.csv\n",
      "‚úÖ Chain 2 Finished !\n",
      "‚ö†Ô∏è Node 2506 (Layer 1) has entropy=0:\n",
      "   Word distribution: {'fracture': 0, 'process': 0, 'multiple': 0, 'simultaneously': 0, 'paper': 0, 'numerical': 0, 'couple': 0, 'formation': 0, 'flow': 0, 'fluid': 0, 'xfem': 0, 'propose': 0, 'fully': 0, 'nonlinear': 0, 'compare': 0, 'equation': 0, 'iterative': 1, 'method': 0, 'base': 0, 'stress': 0, 'capture': 0, 'verify': 0, 'propagation': 0, 'influence': 0, 'loss': 0, 'investigate': 0, 'usually': 0, 'drive': 0, 'develop': 0, 'partition': 0, 'extend': 0, 'model': 0, 'newton': 0, 'iteration': 0, 'solve': 0, 'efficient': 0, 'factor': 0, 'result': 0, 'theoretical': 0, 'accuracy': 0, 'simulate': 0, 'newly': 0, 'pressure': 0, 'deformation': 0, 'need': 0, 'impose': 0, 'velocity': 0, 'algorithm': 0, 'interaction': 0, 'finite': 0, 'adopt': 0, 'widely': 0, 'boundary': 0, 'arbitrary': 0, 'condition': 0, 'different': 0, 'effect': 0, 'solid': 0, 'solution': 0, 'regime': 0, 'operation': 0, 'propagate': 0, 'element': 0, 'growth': 0, 'rock': 0, 'imagebased': 0, 'use': 0, 'fluidstructure': 0, 'fsi': 0, 'come': 0, 'zss': 0, 'discretization': 0, 'wall': 0, 'form': 0, 'guess': 0, 'compute': 0, 'apply': 0, 'target': 0, 'shape': 0, 'arterial': 0, 'isogeometric': 0, 'basis': 0, 'achieve': 0, 'representation': 0, 'complex': 0, 'specify': 0, 'integration': 0, 'straightforward': 0, 'control': 0, 'initial': 0, 'estimation': 0, 'test': 0, 'computation': 0, 'show': 0, 'branch': 0, 'aorta': 0, 'state': 0, 'give': 0, 'shell': 0, 'enable': 0, 'retain': 0, 'function': 0, 'may': 0, 'much': 0, 'image': 0, 'human': 0, 'geometry': 0, 'match': 0, 'feature': 0, 'able': 0, 'first': 0, 'include': 0, 'require': 0, 'calculate': 0, 'deal': 0, 'similar': 0, 'level': 0, 'load': 0, 'addition': 0, 'within': 0, 'patientspecific': 0, 'start': 0, 'component': 0, 'point': 0, 'ductile': 0, 'material': 0, 'step': 0, 'formulation': 0, 'damage': 0, 'novel': 0, 'aspect': 0, 'enhancement': 0, 'plasticity': 0, 'void': 0, 'mechanism': 0, 'nucleation': 0, 'coalescence': 0, 'soften': 0, 'heat': 0, 'localize': 0, 'strength': 0, 'degradation': 0, 'strong': 0, 'weak': 0, 'time': 0, 'together': 0, 'discrete': 0, 'two': 0, 'problem': 0, 'strain': 0, 'rate': 0, 'significant': 0, 'reproduce': 0, 'performance': 0, 'convergence': 0, 'engineering': 0, 'predict': 0, 'diffusion': 0, 'plastic': 0, 'scheme': 0, 'approximation': 0, 'failure': 0, 'detail': 0, 'illustrate': 0, 'metallic': 0, 'metal': 0, 'algebraic': 0, 'numerically': 0, 'mean': 0, 'evolution': 0, 'report': 0, 'design': 0, 'work': 0, 'lead': 0, 'dynamic': 0, 'employ': 0, 'assumption': 0, 'mixed': 0, 'galerkin': 0}\n",
      "   Number of unique words: 179\n",
      "   Total word count: 1\n",
      "   Document count: 3\n",
      "\n",
      "‚ö†Ô∏è Node 3622 (Layer 2) has entropy=0:\n",
      "   Word distribution: {'evolve': 0, 'initially': 0, 'build': 0, 'influence': 0, 'field': 0, 'paper': 0, 'sufficiently': 0, 'branch': 0, 'simulate': 0, 'behavior': 0, 'medium': 0, 'microscale': 0, 'electric': 0, 'allow': 0, 'rapidly': 0, 'application': 0, 'occur': 0, 'trajectory': 0, 'variety': 0, 'order': 0, 'describe': 0, 'explore': 0, 'relevant': 0, 'scenario': 0, 'dependency': 0, 'parameter': 0, 'strong': 0, 'multiple': 0, 'generate': 0, 'physical': 0, 'flexible': 0, 'interest': 0}\n",
      "   Number of unique words: 32\n",
      "   Total word count: 0\n",
      "   Document count: 1\n",
      "\n",
      "‚ö†Ô∏è Node 3541 (Layer 2) has entropy=0:\n",
      "   Word distribution: {'via': 0, 'droplet': 0, 'differential': 0, 'equation': 0, 'physic': 0, 'high': 0, 'fidelity': 0, 'appropriate': 1, 'numerical': 0, 'scheme': 0, 'recent': 0, 'evaluation': 0, 'show': 0, 'well': 0, 'give': 0, 'characteristic': 0, 'partial': 0, 'process': 0, 'several': 0, 'take': 0, 'transmission': 0, 'describe': 0, 'build': 0, 'environment': 0, 'ordinary': 0, 'solve': 0, 'optimal': 0, 'mechanical': 0, 'present': 0, 'example': 0}\n",
      "   Number of unique words: 30\n",
      "   Total word count: 1\n",
      "   Document count: 1\n",
      "\n",
      "‚ö†Ô∏è Node 3651 (Layer 2) has entropy=0:\n",
      "   Word distribution: {'model': 0, 'two': 0, 'dynamic': 0, 'analysis': 0, 'apply': 0, 'method': 0, 'stochastic': 0, 'couple': 0, 'complex': 0, 'consider': 0, 'random': 0, 'approach': 0, 'system': 0, 'algorithm': 0, 'mechanism': 0, 'sensitivity': 0, 'eg': 0, 'contribution': 0, 'first': 0, 'perform': 0, 'analytical': 0, 'efficiency': 0, 'uncertainty': 0, 'carry': 0, 'test': 0, 'rotate': 0, 'flat': 0, 'show': 0, 'hybrid': 0, 'robust': 0, 'computationally': 0, 'compare': 0, 'solver': 0, 'reasonable': 0, 'either': 0, 'probability': 0, 'density': 0, 'partial': 0, 'instability': 0, 'hence': 0, 'introduce': 0, 'analyze': 0, 'objective': 0, 'assess': 0, 'propagate': 0, 'output': 0, 'fem': 0, 'widely': 0, 'within': 0, 'input': 0, 'variable': 0, 'frequency': 0, 'many': 0, 'accuracy': 0, 'fast': 0, 'previous': 0, 'consist': 0, 'compute': 0, 'efficient': 0, 'deal': 0, 'reduced': 0, 'predict': 0, 'fourier': 0, 'comparison': 0, 'prediction': 0, 'since': 0, 'noise': 0, 'design': 0, 'eigenvalue': 0, 'element': 0, 'approximate': 0, 'periodic': 0}\n",
      "   Number of unique words: 72\n",
      "   Total word count: 0\n",
      "   Document count: 1\n",
      "\n",
      "üîÑ Chain 3 in iteration 155/500,\n",
      " üìä path: 156,  new path: 27,  docs changed path: 217,  Log-likelihood: -447271.07\n",
      "üéâ Chain 3 is detected to convergence in iteration 155, sampling ends prematurely.\n",
      "üéØ Chain 3 finish Gibbs Sampling.\n",
      "üíæ Save iterations info...\n",
      "‚úÖ doc-path allocation is saved : machine2_step1_d3_g001/depth_3_gamma_0.01_run_3/iteration_document_paths.csv\n",
      "‚úÖ structure of tree path is saved: machine2_step1_d3_g001/depth_3_gamma_0.01_run_3/iteration_path_structures.csv\n",
      "‚úÖ info recoreded in iteration is saved: machine2_step1_d3_g001/depth_3_gamma_0.01_run_3/iteration_iteration_summaries.csv\n",
      "‚úÖ doc-path mapping is saved: machine2_step1_d3_g001/depth_3_gamma_0.01_run_3/iteration_path_document_mapping.csv\n",
      "‚úÖ doc's words allocation to node is saved: machine2_step1_d3_g001/depth_3_gamma_0.01_run_3/iteration_word_allocations.csv\n",
      "‚úÖ node word distribution is saved: machine2_step1_d3_g001/depth_3_gamma_0.01_run_3/iteration_node_word_distributions.csv\n",
      "‚úÖ jump record info is saved: machine2_step1_d3_g001/depth_3_gamma_0.01_run_3/iteration_jump_records.csv\n",
      "‚úÖ other detailed info is saved: machine2_step1_d3_g001/depth_3_gamma_0.01_run_3/iteration_detail_records.csv\n",
      "‚úÖ Chain 3 Finished !\n",
      "‚ö†Ô∏è Node 3373 (Layer 2) has entropy=0:\n",
      "   Word distribution: {'surface': 0, 'crystalline': 0, 'like': 0, 'property': 0, 'method': 0, 'model': 0, 'stress': 0, 'shell': 0, 'geometric': 0, 'outofplane': 0, 'result': 0, 'curve': 0, 'element': 0, 'several': 0, 'example': 0, 'present': 0, 'solid': 0, 'classical': 0, 'finite': 0, 'implementation': 0, 'equation': 0, 'capability': 0, 'base': 0, 'demonstrate': 0, 'equivalent': 0, 'enable': 0, 'deform': 0, 'thus': 0, 'nonlinearity': 0, 'effect': 0, 'tension': 0, 'capture': 0, 'elasticity': 0, 'show': 0, 'topology': 0, 'necessary': 0, 'rough': 0, 'manufacturing': 0, 'detail': 0, 'feature': 0, 'reconstruction': 0, 'micromechanical': 0, 'indicate': 0, 'powder': 0, 'size': 0, 'therefore': 0, 'contain': 0, 'characterize': 0, 'concentration': 0, 'porosity': 0, 'carry': 0, 'load': 0, 'difference': 0, 'direct': 0, 'relevant': 0, 'damage': 0, 'understand': 0, 'via': 0, 'laser': 0, 'resolution': 0, 'information': 0, 'crack': 0, 'initiation': 0, 'particularly': 0, 'additive': 0, 'measurement': 0, 'create': 0, 'provide': 0, 'accumulation': 0, 'high': 0, 'complex': 0, 'highly': 0, 'influence': 0, 'distribution': 0, 'make': 0, 'conventional': 0, 'use': 0}\n",
      "   Number of unique words: 77\n",
      "   Total word count: 0\n",
      "   Document count: 2\n",
      "\n",
      "‚ö†Ô∏è Node 3401 (Layer 2) has entropy=0:\n",
      "   Word distribution: {'often': 0, 'complex': 0, 'nonlinear': 0, 'behavior': 0, 'within': 0, 'computational': 0, 'formulation': 0, 'describe': 0, 'transient': 0, 'eg': 0, 'viscoelasticity': 0, 'base': 0, 'theoretical': 0, 'directly': 0, 'connect': 0, 'bond': 0, 'macroscopic': 0, 'element': 0, 'method': 0, 'domain': 0, 'validate': 0, 'dynamic': 0, 'flow': 0, 'polymer': 0, 'material': 0, 'molecular': 0, 'level': 0, 'soft': 0, 'deformation': 0, 'description': 0, 'kinematics': 0, 'framework': 0, 'finite': 0, 'use': 0, 'field': 0, 'govern': 0, 'model': 0, 'response': 0, 'context': 0, 'involve': 0, 'mechanical': 0, 'due': 0, 'network': 0, 'paper': 0, 'present': 0, 'large': 0, 'equation': 0, 'extend': 0, 'variable': 0, 'elasticity': 0, 'discretize': 0, 'chain': 0, 'concrete': 0, 'continuum': 0, 'derive': 0, 'twoscale': 0, 'effective': 0, 'problem': 0, 'define': 0, 'condition': 0, 'test': 0, 'relation': 0, 'independent': 0, 'result': 0, 'crack': 0, 'reinforcement': 0, 'comprise': 0, 'slip': 0, 'representative': 0, 'largescale': 0, 'rve': 0, 'incorporate': 0, 'investigate': 0, 'mean': 0, 'beam': 0, 'compare': 0, 'produce': 0, 'estimation': 0, 'bar': 0, 'volume': 0, 'size': 0, 'analysis': 0, 'basis': 0, 'solid': 0, 'obtain': 0, 'strain': 0, 'dirichlet': 0, 'deep': 0, 'reinforce': 0, 'represent': 0}\n",
      "   Number of unique words: 90\n",
      "   Total word count: 0\n",
      "   Document count: 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed: 58.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All chains finish sampling and waiting for monitor process finish...\n",
      "‚úÖ R-hat monitor process finished !\n",
      "‚úÖ Overall convergence history is saved to CSV: machine2_step1_d3_g001/convergence_info.csv\n",
      "‚úÖ Chain 1 convergence info saved to: machine2_step1_d3_g001/depth_3_gamma_0.01_run_1/chain_convergence_info.csv\n",
      "‚úÖ Chain 2 convergence info saved to: machine2_step1_d3_g001/depth_3_gamma_0.01_run_2/chain_convergence_info.csv\n",
      "‚úÖ Chain 3 convergence info saved to: machine2_step1_d3_g001/depth_3_gamma_0.01_run_3/chain_convergence_info.csv\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "# ËÆæÁΩÆÂèÇÊï∞Ôºà‰ΩøÁî®Â∞èÂÜôÂèòÈáèÂêçÔºâ\n",
    "depth = 3\n",
    "gamma = 0.01\n",
    "eta = 0.1\n",
    "alpha = 0.1\n",
    "n_chains = 3  # 3Êù°ÈìæÂπ∂Ë°åËøêË°å\n",
    "\n",
    "# ‰ΩøÁî® joblib ËøêË°åÂ§öÈìæ\n",
    "result = run_multi_chain_hlda(\n",
    "    corpus=corpus,\n",
    "    depth=depth,\n",
    "    gamma=gamma,\n",
    "    eta=eta,\n",
    "    alpha=alpha,\n",
    "    n_chains=n_chains,\n",
    "    max_iterations=500,\n",
    "    check_interval=20,\n",
    "    back_window=5,\n",
    "    burn_in=50,\n",
    "    r_hat_threshold=1.1,\n",
    "    general_dir=\"machine2_step1_d3_g001\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
