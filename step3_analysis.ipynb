{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08758cbd",
   "metadata": {},
   "source": [
    "## Summary\n",
    "### we do the same for steps as step2, detailed comment see step2_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18ef8259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from scipy.special import gammaln\n",
    "\n",
    "def calculate_renyi_entropy_vectorized(node_data, all_words, eta_prior=1.0, renyi_alpha=2.0):\n",
    "    \"\"\"\n",
    "    Vectorized version of Renyi entropy calculation\n",
    "    \n",
    "    Parameters:\n",
    "    node_data: DataFrame, node data containing word and count columns\n",
    "    all_words: list, complete vocabulary\n",
    "    eta_prior: float, Dirichlet prior smoothing parameter (obtained from eta value)\n",
    "    renyi_alpha: float, order parameter for Renyi entropy\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (entropy, nonzero_word_count) Renyi entropy value and non-zero word count\n",
    "    \"\"\"\n",
    "    if len(all_words) == 0:\n",
    "        return 0.0, 0\n",
    "    \n",
    "    # Create word-to-index mapping\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "    \n",
    "    # Initialize count vector\n",
    "    counts = np.zeros(len(all_words))\n",
    "    \n",
    "    # Fill actual counts\n",
    "    for _, row in node_data.iterrows():\n",
    "        word = row['word']\n",
    "        if pd.notna(word) and word in word_to_idx:\n",
    "            counts[word_to_idx[word]] = row['count']\n",
    "    \n",
    "    # Count non-zero words (before smoothing)\n",
    "    nonzero_word_count = np.sum(counts > 0)\n",
    "    \n",
    "    # Add eta smoothing\n",
    "    smoothed_counts = counts + eta_prior\n",
    "    \n",
    "    # Calculate probability distribution\n",
    "    probabilities = smoothed_counts / np.sum(smoothed_counts)\n",
    "    \n",
    "    # Calculate Renyi entropy (using natural logarithm)\n",
    "    if renyi_alpha == 1.0:\n",
    "        # Shannon entropy (since alpha smoothing makes all probabilities > 0, no need to add small constant)\n",
    "        entropy = -np.sum(probabilities * np.log(probabilities))\n",
    "    else:\n",
    "        # General Renyi entropy\n",
    "        entropy = (1 / (1 - renyi_alpha)) * np.log(np.sum(probabilities ** renyi_alpha))\n",
    "    \n",
    "    return entropy, int(nonzero_word_count)\n",
    "\n",
    "def process_all_iteration_files_by_alpha(base_path=\".\", renyi_alpha=2.0):\n",
    "    \"\"\"\n",
    "    Process each iteration_node_word_distributions.csv file separately and save results\n",
    "    Corrected version: Use fixed eta=0.05 as Dirichlet smoothing parameter (adapted for step3)\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_node_word_distributions.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    # Remove duplicates, ensure each file is processed only once\n",
    "    files = list(set(files))\n",
    "    files.sort()  # Sort for ordered processing\n",
    "    \n",
    "    print(f\"Found {len(files)} files to process\")\n",
    "    \n",
    "    for idx, file_path in enumerate(files, 1):\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        \n",
    "        # In step3, eta value is fixed at 0.05 (for Dirichlet smoothing)\n",
    "        eta_prior = 0.05  # Fixed: Use 0.05 as smoothing parameter\n",
    "        \n",
    "        # Extract alpha value from folder name (used only for recording folder information)\n",
    "        alpha = 0.1  # Default value\n",
    "        if 'alpha_' in folder_name:\n",
    "            try:\n",
    "                alpha_part = folder_name.split('alpha_')[1].split('_')[0]\n",
    "                alpha = float(alpha_part)\n",
    "            except (IndexError, ValueError) as e:\n",
    "                # Pattern matching through folder name\n",
    "                if 'a001' in folder_name:\n",
    "                    alpha = 0.01\n",
    "                elif 'a005' in folder_name:\n",
    "                    alpha = 0.05\n",
    "                elif 'a02' in folder_name:\n",
    "                    alpha = 0.2\n",
    "                elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                    alpha = 0.5\n",
    "                elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                    alpha = 1.0\n",
    "                elif 'a01' in folder_name:\n",
    "                    alpha = 0.1\n",
    "        else:\n",
    "            # Pattern matching through folder name\n",
    "            if 'a001' in folder_name:\n",
    "                alpha = 0.01\n",
    "            elif 'a005' in folder_name:\n",
    "                alpha = 0.05\n",
    "            elif 'a02' in folder_name:\n",
    "                alpha = 0.2\n",
    "            elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                alpha = 0.5\n",
    "            elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                alpha = 1.0\n",
    "            elif 'a01' in folder_name:\n",
    "                alpha = 0.1\n",
    "        \n",
    "        print(f\"\\n[{idx}/{len(files)}] Processing file: {file_path}\")\n",
    "        print(f\"Folder: {folder_name}\")\n",
    "        print(f\"Extracted alpha value: {alpha} (for recording only)\")\n",
    "        print(f\"Using eta smoothing value: {eta_prior}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Clean column names, remove single quotes, double quotes and spaces\n",
    "            df.columns = [col.strip(\"'\\\" \") for col in df.columns]\n",
    "            \n",
    "            if 'node_id' not in df.columns:\n",
    "                print(f\"Warning: {file_path} missing node_id column, skipping this file\")\n",
    "                continue\n",
    "                \n",
    "            max_iteration = df['iteration'].max()\n",
    "            last_iteration_data = df[df['iteration'] == max_iteration]\n",
    "            all_words = list(last_iteration_data['word'].dropna().unique())\n",
    "            \n",
    "            print(f\"Last iteration: {max_iteration}, vocabulary size: {len(all_words)}, node count: {last_iteration_data['node_id'].nunique()}\")\n",
    "            \n",
    "            results = []\n",
    "            for node_id in last_iteration_data['node_id'].unique():\n",
    "                node_data = last_iteration_data[last_iteration_data['node_id'] == node_id]\n",
    "                \n",
    "                # Use fixed eta_prior=0.05 for Dirichlet smoothing\n",
    "                entropy, nonzero_words = calculate_renyi_entropy_vectorized(\n",
    "                    node_data, all_words, eta_prior, renyi_alpha  # Use eta_prior\n",
    "                )\n",
    "                \n",
    "                # Calculate sparsity (proportion of non-zero words)\n",
    "                sparsity_ratio = nonzero_words / len(all_words) if len(all_words) > 0 else 0\n",
    "                \n",
    "                results.append({\n",
    "                    'node_id': node_id,\n",
    "                    'renyi_entropy_corrected': entropy,\n",
    "                    'nonzero_word_count': nonzero_words,\n",
    "                    'total_vocabulary_size': len(all_words),\n",
    "                    'sparsity_ratio': sparsity_ratio,\n",
    "                    'eta_used': eta_prior,  # Corrected: Record actually used eta value\n",
    "                    'alpha_folder': alpha,  # Corrected: Record folder's alpha value\n",
    "                    'renyi_alpha': renyi_alpha,\n",
    "                    'iteration': max_iteration\n",
    "                })\n",
    "            \n",
    "            # Save new corrected_renyi_entropy.csv file\n",
    "            results_df = pd.DataFrame(results)\n",
    "            output_path = os.path.join(folder_path, 'corrected_renyi_entropy.csv')\n",
    "            results_df.to_csv(output_path, index=False)\n",
    "            print(f\"✓ Saved corrected Renyi entropy results to: {output_path}\")\n",
    "            \n",
    "            # Output some statistics\n",
    "            print(f\"Node vocabulary sparsity statistics:\")\n",
    "            print(f\"  - Average non-zero word count: {results_df['nonzero_word_count'].mean():.1f}\")\n",
    "            print(f\"  - Non-zero word count range: {results_df['nonzero_word_count'].min()}-{results_df['nonzero_word_count'].max()}\")\n",
    "            print(f\"  - Average sparsity: {results_df['sparsity_ratio'].mean():.3f}\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"❌ Error processing file {file_path}: {str(e)}\")\n",
    "            print(\"Detailed error information:\")\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b21f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Step3: Starting analysis of Alpha parameter impact on the model\n",
      "================================================================================\n",
      "Starting corrected Renyi entropy calculation...\n",
      "Found 17 files to process\n",
      "\n",
      "[1/17] Processing file: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_1/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_1\n",
      "Extracted alpha value: 0.01 (for recording only)\n",
      "Using eta smoothing value: 0.05\n",
      "Last iteration: 285, vocabulary size: 1490, node count: 312\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_1/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero word count: 60.7\n",
      "  - Non-zero word count range: 0-829\n",
      "  - Average sparsity: 0.041\n",
      "==================================================\n",
      "\n",
      "[2/17] Processing file: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_2/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_2\n",
      "Extracted alpha value: 0.01 (for recording only)\n",
      "Using eta smoothing value: 0.05\n",
      "Last iteration: 285, vocabulary size: 1490, node count: 313\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_2/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero word count: 63.2\n",
      "  - Non-zero word count range: 0-764\n",
      "  - Average sparsity: 0.042\n",
      "==================================================\n",
      "\n",
      "[3/17] Processing file: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_3/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_3\n",
      "Extracted alpha value: 0.01 (for recording only)\n",
      "Using eta smoothing value: 0.05\n",
      "Last iteration: 285, vocabulary size: 1490, node count: 296\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_3/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero word count: 63.9\n",
      "  - Non-zero word count range: 0-796\n",
      "  - Average sparsity: 0.043\n",
      "==================================================\n",
      "\n",
      "[4/17] Processing file: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_1/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_1\n",
      "Extracted alpha value: 0.05 (for recording only)\n",
      "Using eta smoothing value: 0.05\n",
      "Last iteration: 175, vocabulary size: 1490, node count: 316\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_1/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero word count: 60.8\n",
      "  - Non-zero word count range: 0-787\n",
      "  - Average sparsity: 0.041\n",
      "==================================================\n",
      "\n",
      "[5/17] Processing file: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_2/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_2\n",
      "Extracted alpha value: 0.05 (for recording only)\n",
      "Using eta smoothing value: 0.05\n",
      "Last iteration: 175, vocabulary size: 1490, node count: 305\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_2/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero word count: 61.3\n",
      "  - Non-zero word count range: 0-789\n",
      "  - Average sparsity: 0.041\n",
      "==================================================\n",
      "\n",
      "[6/17] Processing file: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_3/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_3\n",
      "Extracted alpha value: 0.05 (for recording only)\n",
      "Using eta smoothing value: 0.05\n",
      "Last iteration: 175, vocabulary size: 1490, node count: 275\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_3/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero word count: 59.9\n",
      "  - Non-zero word count range: 0-941\n",
      "  - Average sparsity: 0.040\n",
      "==================================================\n",
      "\n",
      "[7/17] Processing file: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1\n",
      "Extracted alpha value: 0.1 (for recording only)\n",
      "Using eta smoothing value: 0.05\n",
      "Last iteration: 90, vocabulary size: 1490, node count: 315\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero word count: 63.0\n",
      "  - Non-zero word count range: 0-747\n",
      "  - Average sparsity: 0.042\n",
      "==================================================\n",
      "\n",
      "[8/17] Processing file: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_2/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_2\n",
      "Extracted alpha value: 0.1 (for recording only)\n",
      "Using eta smoothing value: 0.05\n",
      "Last iteration: 90, vocabulary size: 1490, node count: 299\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_2/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero word count: 64.3\n",
      "  - Non-zero word count range: 0-792\n",
      "  - Average sparsity: 0.043\n",
      "==================================================\n",
      "\n",
      "[9/17] Processing file: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_3/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_3\n",
      "Extracted alpha value: 0.1 (for recording only)\n",
      "Using eta smoothing value: 0.05\n",
      "Last iteration: 90, vocabulary size: 1490, node count: 322\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_3/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero word count: 61.6\n",
      "  - Non-zero word count range: 0-737\n",
      "  - Average sparsity: 0.041\n",
      "==================================================\n",
      "\n",
      "[10/17] Processing file: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_1/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_1\n",
      "Extracted alpha value: 0.2 (for recording only)\n",
      "Using eta smoothing value: 0.05\n",
      "Last iteration: 170, vocabulary size: 1490, node count: 325\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_1/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero word count: 61.0\n",
      "  - Non-zero word count range: 0-753\n",
      "  - Average sparsity: 0.041\n",
      "==================================================\n",
      "\n",
      "[11/17] Processing file: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_2/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_2\n",
      "Extracted alpha value: 0.2 (for recording only)\n",
      "Using eta smoothing value: 0.05\n",
      "Last iteration: 170, vocabulary size: 1490, node count: 333\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_2/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero word count: 60.7\n",
      "  - Non-zero word count range: 0-727\n",
      "  - Average sparsity: 0.041\n",
      "==================================================\n",
      "\n",
      "[12/17] Processing file: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_3/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_3\n",
      "Extracted alpha value: 0.2 (for recording only)\n",
      "Using eta smoothing value: 0.05\n",
      "Last iteration: 170, vocabulary size: 1490, node count: 325\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_3/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero word count: 62.7\n",
      "  - Non-zero word count range: 3-735\n",
      "  - Average sparsity: 0.042\n",
      "==================================================\n",
      "\n",
      "[13/17] Processing file: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_1/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_1\n",
      "Extracted alpha value: 0.5 (for recording only)\n",
      "Using eta smoothing value: 0.05\n",
      "Last iteration: 275, vocabulary size: 1490, node count: 282\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_1/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero word count: 65.7\n",
      "  - Non-zero word count range: 0-731\n",
      "  - Average sparsity: 0.044\n",
      "==================================================\n",
      "\n",
      "[14/17] Processing file: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_2/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_2\n",
      "Extracted alpha value: 0.5 (for recording only)\n",
      "Using eta smoothing value: 0.05\n",
      "Last iteration: 275, vocabulary size: 1490, node count: 292\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_2/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero word count: 65.1\n",
      "  - Non-zero word count range: 0-706\n",
      "  - Average sparsity: 0.044\n",
      "==================================================\n",
      "\n",
      "[15/17] Processing file: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_3/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_3\n",
      "Extracted alpha value: 0.5 (for recording only)\n",
      "Using eta smoothing value: 0.05\n",
      "Last iteration: 275, vocabulary size: 1490, node count: 292\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_3/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero word count: 61.7\n",
      "  - Non-zero word count range: 0-767\n",
      "  - Average sparsity: 0.041\n",
      "==================================================\n",
      "\n",
      "[16/17] Processing file: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_2/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.05_alpha_1_run_2\n",
      "Extracted alpha value: 1.0 (for recording only)\n",
      "Using eta smoothing value: 0.05\n",
      "Last iteration: 245, vocabulary size: 1490, node count: 321\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_2/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero word count: 62.2\n",
      "  - Non-zero word count range: 1-683\n",
      "  - Average sparsity: 0.042\n",
      "==================================================\n",
      "\n",
      "[17/17] Processing file: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_3/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.05_alpha_1_run_3\n",
      "Extracted alpha value: 1.0 (for recording only)\n",
      "Using eta smoothing value: 0.05\n",
      "Last iteration: 265, vocabulary size: 1490, node count: 307\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_3/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero word count: 61.5\n",
      "  - Non-zero word count range: 0-728\n",
      "  - Average sparsity: 0.041\n",
      "==================================================\n",
      "==================================================\n",
      "✅ Step3 Renyi entropy calculation completed!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Set parameters - adapted for step3\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step3\"  # step3 path\n",
    "renyi_alpha = 2.0  # Renyi entropy order parameter\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Step3: Starting analysis of Alpha parameter impact on the model\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Calculate corrected Renyi entropy (automatically adjust prior by alpha value)\n",
    "print(\"Starting corrected Renyi entropy calculation...\")\n",
    "process_all_iteration_files_by_alpha(base_path, renyi_alpha)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"✅ Step3 Renyi entropy calculation completed!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "761d9bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_layer_statistics_by_alpha(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    Aggregate JS distance and weighted entropy statistics by alpha value, generate summary tables at the same level as run folders\n",
    "    Corrected version: adapted for alpha parameter in step3 instead of eta parameter\n",
    "    \"\"\"\n",
    "    # Find all layer_average_js_distances.csv files\n",
    "    pattern = os.path.join(base_path, \"**\", \"layer_average_js_distances.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    # Store all data and grouping information\n",
    "    all_data = []\n",
    "    alpha_groups = {}  # Store parent directory for each alpha combination\n",
    "    \n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        parent_folder = os.path.dirname(folder_path)  # Parent directory of run folders\n",
    "        \n",
    "        # Extract alpha value from folder name (adapted for step3)\n",
    "        alpha = None\n",
    "        if 'alpha_' in folder_name:\n",
    "            try:\n",
    "                alpha_part = folder_name.split('alpha_')[1].split('_')[0]\n",
    "                alpha = float(alpha_part)\n",
    "            except (IndexError, ValueError):\n",
    "                # Pattern matching through folder name\n",
    "                if 'a001' in folder_name:\n",
    "                    alpha = 0.01\n",
    "                elif 'a005' in folder_name:\n",
    "                    alpha = 0.05\n",
    "                elif 'a02' in folder_name:\n",
    "                    alpha = 0.2\n",
    "                elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                    alpha = 0.5\n",
    "                elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                    alpha = 1.0\n",
    "                elif 'a01' in folder_name:\n",
    "                    alpha = 0.1\n",
    "        else:\n",
    "            # Pattern matching through folder name\n",
    "            if 'a001' in folder_name:\n",
    "                alpha = 0.01\n",
    "            elif 'a005' in folder_name:\n",
    "                alpha = 0.05\n",
    "            elif 'a02' in folder_name:\n",
    "                alpha = 0.2\n",
    "            elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                alpha = 0.5\n",
    "            elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                alpha = 1.0\n",
    "            elif 'a01' in folder_name:\n",
    "                alpha = 0.1\n",
    "        \n",
    "        if alpha is None:\n",
    "            print(f\"Warning: unable to extract alpha value from folder name {folder_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Extract run number\n",
    "        run_match = folder_name.split('_run_')\n",
    "        if len(run_match) > 1:\n",
    "            run_id = run_match[1]\n",
    "        else:\n",
    "            print(f\"Warning: unable to extract run number from folder name {folder_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Record parent directory for alpha combination\n",
    "        if alpha not in alpha_groups:\n",
    "            alpha_groups[alpha] = parent_folder\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                all_data.append({\n",
    "                    'alpha': alpha,  # Corrected: use alpha instead of eta\n",
    "                    'run_id': run_id,\n",
    "                    'layer': row['layer'],\n",
    "                    'node_count': row['node_count'],\n",
    "                    'avg_js_distance': row['avg_js_distance'],\n",
    "                    'weighted_avg_renyi_entropy': row['weighted_avg_renyi_entropy'],\n",
    "                    'total_documents': row['total_documents'],\n",
    "                    'eta_used': row.get('eta_used', 0.05),  # Record actually used eta value (fixed 0.05)\n",
    "                    'alpha_folder': row.get('alpha_folder', alpha),  # Record folder's alpha value\n",
    "                    'parent_folder': parent_folder\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_path}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    summary_df = pd.DataFrame(all_data)\n",
    "    \n",
    "    if summary_df.empty:\n",
    "        print(\"No valid data found\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"Layer summary statistics by ALPHA value (Step3)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Generate summary files grouped by alpha\n",
    "    for alpha, group_data in summary_df.groupby('alpha'):\n",
    "        parent_folder = group_data['parent_folder'].iloc[0]\n",
    "        \n",
    "        print(f\"\\nProcessing Alpha={alpha}\")\n",
    "        print(f\"Output directory: {parent_folder}\")\n",
    "        \n",
    "        # Calculate summary statistics for each layer\n",
    "        layer_summary = group_data.groupby('layer').agg({\n",
    "            'avg_js_distance': ['mean', 'std', 'count'],\n",
    "            'weighted_avg_renyi_entropy': ['mean', 'std', 'count'],\n",
    "            'node_count': ['mean', 'std'],\n",
    "            'total_documents': 'mean',\n",
    "            'eta_used': 'first',  # Record used eta value\n",
    "            'run_id': lambda x: ', '.join(sorted(x.unique()))\n",
    "        }).round(4)\n",
    "        \n",
    "        # Flatten column names\n",
    "        layer_summary.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in layer_summary.columns]\n",
    "        layer_summary = layer_summary.reset_index()\n",
    "        \n",
    "        # Rename columns for clarity\n",
    "        column_mapping = {\n",
    "            'avg_js_distance_mean': 'avg_js_distance_mean',\n",
    "            'avg_js_distance_std': 'avg_js_distance_std', \n",
    "            'avg_js_distance_count': 'run_count',\n",
    "            'weighted_avg_renyi_entropy_mean': 'weighted_avg_renyi_entropy_mean',\n",
    "            'weighted_avg_renyi_entropy_std': 'weighted_avg_renyi_entropy_std',\n",
    "            'weighted_avg_renyi_entropy_count': 'entropy_run_count',\n",
    "            'node_count_mean': 'avg_node_count',\n",
    "            'node_count_std': 'node_count_std',\n",
    "            'total_documents_mean': 'avg_total_documents',\n",
    "            'eta_used_first': 'eta_used',\n",
    "            'run_id_<lambda>': 'included_runs'\n",
    "        }\n",
    "        \n",
    "        for old_name, new_name in column_mapping.items():\n",
    "            if old_name in layer_summary.columns:\n",
    "                layer_summary = layer_summary.rename(columns={old_name: new_name})\n",
    "        \n",
    "        # Add alpha information\n",
    "        layer_summary.insert(0, 'alpha', alpha)\n",
    "        \n",
    "        # Save summary results at the same level as run folders\n",
    "        output_filename = f'alpha_{alpha}_layer_summary.csv'\n",
    "        output_path = os.path.join(parent_folder, output_filename)\n",
    "        layer_summary.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"  Summary file saved: {output_path}\")\n",
    "        print(f\"  Included runs: {layer_summary['included_runs'].iloc[0] if 'included_runs' in layer_summary.columns else 'N/A'}\")\n",
    "        print(f\"  Number of layers: {len(layer_summary)}\")\n",
    "        print(f\"  Used Eta value: {layer_summary.get('eta_used', pd.Series([0.05])).iloc[0]}\")\n",
    "        \n",
    "        # Display brief statistics\n",
    "        for _, row in layer_summary.iterrows():\n",
    "            layer_num = int(row['layer'])\n",
    "            js_mean = row['avg_js_distance_mean']\n",
    "            js_std = row['avg_js_distance_std'] if 'avg_js_distance_std' in row else 0\n",
    "            entropy_mean = row['weighted_avg_renyi_entropy_mean']\n",
    "            entropy_std = row['weighted_avg_renyi_entropy_std'] if 'weighted_avg_renyi_entropy_std' in row else 0\n",
    "            node_count = row['avg_node_count']\n",
    "            run_count = int(row['run_count']) if 'run_count' in row else 0\n",
    "            \n",
    "            print(f\"    Layer {layer_num}: JS={js_mean:.4f}(±{js_std:.4f}), entropy={entropy_mean:.4f}(±{entropy_std:.4f}), nodes={node_count:.1f}, runs={run_count}\")\n",
    "    \n",
    "    # Generate overall comparison file (saved under base_path)\n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"Generating overall comparison file\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    overall_summary = summary_df.groupby(['alpha', 'layer']).agg({\n",
    "        'avg_js_distance': ['mean', 'std'],\n",
    "        'weighted_avg_renyi_entropy': ['mean', 'std'],\n",
    "        'node_count': ['mean', 'std'],\n",
    "        'run_id': 'count'\n",
    "    }).round(4)\n",
    "    \n",
    "    # Flatten column names\n",
    "    overall_summary.columns = ['_'.join(col).strip() for col in overall_summary.columns]\n",
    "    overall_summary = overall_summary.reset_index()\n",
    "    \n",
    "    overall_output_path = os.path.join(base_path, 'alpha_layer_comparison.csv')\n",
    "    overall_summary.to_csv(overall_output_path, index=False)\n",
    "    print(f\"Overall comparison file saved to: {overall_output_path}\")\n",
    "    \n",
    "    # Display cross-alpha comparison\n",
    "    for layer in sorted(summary_df['layer'].unique()):\n",
    "        print(f\"\\nLayer {int(layer)} Cross-Alpha Comparison:\")\n",
    "        print(\"Alpha Value     JS Distance(±std)      Weighted Entropy(±std)      Node Count(±std)   Run Count\")\n",
    "        print(\"-\" * 75)\n",
    "        \n",
    "        layer_data = overall_summary[overall_summary['layer'] == layer]\n",
    "        for _, row in layer_data.iterrows():\n",
    "            alpha = row['alpha']\n",
    "            js_mean = row['avg_js_distance_mean']\n",
    "            js_std = row['avg_js_distance_std']\n",
    "            entropy_mean = row['weighted_avg_renyi_entropy_mean']\n",
    "            entropy_std = row['weighted_avg_renyi_entropy_std']\n",
    "            node_mean = row['node_count_mean']\n",
    "            node_std = row['node_count_std']\n",
    "            run_count = int(row['run_id_count'])\n",
    "            \n",
    "            print(f\"{alpha:7.3f}    {js_mean:6.4f}(±{js_std:5.4f})   {entropy_mean:6.4f}(±{entropy_std:5.4f})   {node_mean:6.1f}(±{node_std:4.1f})   {run_count:4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83fa2dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_node_document_counts(path_structures_df):\n",
    "    \"\"\"\n",
    "    Aggregate from leaf nodes upward to calculate document counts and hierarchical relationships for each node\n",
    "    \n",
    "    Parameters:\n",
    "    path_structures_df: DataFrame, data from iteration_path_structures.csv (filtered to last iteration)\n",
    "    \n",
    "    Returns:\n",
    "    dict: {node_id: {'document_count': int, 'layer': int, 'parent_id': int, 'child_ids': list}} mapping\n",
    "    \"\"\"\n",
    "    # Get all layer columns - fix regular expression\n",
    "    layer_columns = [col for col in path_structures_df.columns if col.startswith('layer_') and col.endswith('_node_id')]\n",
    "    layer_columns.sort()  # Ensure ordered arrangement\n",
    "    max_layer_idx = len(layer_columns) - 1\n",
    "    \n",
    "    print(f\"[DEBUG] Found layer columns: {layer_columns}\")\n",
    "    print(f\"[DEBUG] Maximum layer index: {max_layer_idx}\")\n",
    "    \n",
    "    # Initialize node information dictionary\n",
    "    node_info = {}\n",
    "    \n",
    "    # First establish all nodes' layer and parent-child relationships\n",
    "    for _, row in path_structures_df.iterrows():\n",
    "        path_nodes = []\n",
    "        for layer_idx in range(max_layer_idx + 1):\n",
    "            layer_col = f'layer_{layer_idx}_node_id'\n",
    "            if layer_col in path_structures_df.columns and pd.notna(row[layer_col]):\n",
    "                path_nodes.append(row[layer_col])\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # Establish layer and parent-child relationships for each node in the path\n",
    "        for i, node in enumerate(path_nodes):\n",
    "            if node not in node_info:\n",
    "                node_info[node] = {\n",
    "                    'document_count': 0,\n",
    "                    'layer': i,\n",
    "                    'parent_id': None,\n",
    "                    'child_ids': [],\n",
    "                    'child_count': 0\n",
    "                }\n",
    "            else:\n",
    "                # Update layer information (ensure consistency)\n",
    "                node_info[node]['layer'] = i\n",
    "            \n",
    "            # Set parent node relationship\n",
    "            if i > 0:  # Not root node\n",
    "                parent_node = path_nodes[i-1]\n",
    "                node_info[node]['parent_id'] = parent_node\n",
    "                \n",
    "                # Add current node to parent node's child list\n",
    "                if parent_node not in node_info:\n",
    "                    node_info[parent_node] = {\n",
    "                        'document_count': 0,\n",
    "                        'layer': i-1,\n",
    "                        'parent_id': None,\n",
    "                        'child_ids': [],\n",
    "                        'child_count': 0\n",
    "                    }\n",
    "                \n",
    "                if node not in node_info[parent_node]['child_ids']:\n",
    "                    node_info[parent_node]['child_ids'].append(node)\n",
    "    \n",
    "    # Then process leaf node document counts - after hierarchical relationships are established\n",
    "    for _, row in path_structures_df.iterrows():\n",
    "        leaf_node = row['leaf_node_id']\n",
    "        if pd.notna(leaf_node) and leaf_node in node_info:\n",
    "            node_info[leaf_node]['document_count'] += row['document_count']\n",
    "    \n",
    "    # Aggregate document counts upward from second-to-last layer\n",
    "    for layer_idx in range(max_layer_idx - 1, -1, -1):  # From second-to-last layer to layer 0\n",
    "        layer_col = f'layer_{layer_idx}_node_id'\n",
    "        \n",
    "        if layer_col not in path_structures_df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Get all unique nodes in this layer\n",
    "        layer_nodes = path_structures_df[layer_col].dropna().unique()\n",
    "        \n",
    "        for node in layer_nodes:\n",
    "            if node in node_info and node_info[node]['document_count'] == 0:\n",
    "                # Calculate document count: sum all child nodes' document counts\n",
    "                child_doc_count = 0\n",
    "                for child_id in node_info[node]['child_ids']:\n",
    "                    if child_id in node_info:\n",
    "                        child_doc_count += node_info[child_id]['document_count']\n",
    "                \n",
    "                # If no child node document count, calculate directly from path structure\n",
    "                if child_doc_count == 0:\n",
    "                    total_docs = path_structures_df[path_structures_df[layer_col] == node]['document_count'].sum()\n",
    "                    node_info[node]['document_count'] = total_docs\n",
    "                else:\n",
    "                    node_info[node]['document_count'] = child_doc_count\n",
    "\n",
    "    # Calculate child count for each node\n",
    "    for node_id, info in node_info.items():\n",
    "        info['child_count'] = len(info['child_ids'])\n",
    "    \n",
    "    return node_info\n",
    "\n",
    "def add_document_counts_to_entropy_files(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    Add document counts and layer information to corrected_renyi_entropy.csv files (adapted for step3)\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_path_structures.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        \n",
    "        print(f\"\\nProcessing path structure file: {folder_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Read path_structures file\n",
    "            df = pd.read_csv(file_path)\n",
    "            df.columns = [col.strip(\"'\\\" \") for col in df.columns]\n",
    "            \n",
    "            # Get last iteration data\n",
    "            max_iteration = df['iteration'].max()\n",
    "            last_iteration_data = df[df['iteration'] == max_iteration]\n",
    "            \n",
    "            print(f\"Last iteration: {max_iteration}, path count: {len(last_iteration_data)}\")\n",
    "            \n",
    "            # Calculate document counts and hierarchical relationships for each node\n",
    "            node_info = calculate_node_document_counts(last_iteration_data)\n",
    "            \n",
    "            print(f\"Calculated information for {len(node_info)} nodes\")\n",
    "            \n",
    "            # Read corresponding corrected_renyi_entropy.csv\n",
    "            entropy_file = os.path.join(folder_path, 'corrected_renyi_entropy.csv')\n",
    "            if os.path.exists(entropy_file):\n",
    "                entropy_df = pd.read_csv(entropy_file)\n",
    "                \n",
    "                # Add new columns - fix child_ids format and child_count calculation\n",
    "                entropy_df['document_count'] = entropy_df['node_id'].map(lambda x: node_info.get(x, {}).get('document_count', 0))\n",
    "                entropy_df['layer'] = entropy_df['node_id'].map(lambda x: node_info.get(x, {}).get('layer', -1))\n",
    "                entropy_df['parent_id'] = entropy_df['node_id'].map(lambda x: node_info.get(x, {}).get('parent_id', None))\n",
    "                \n",
    "                # Fix child_ids format: use square brackets instead of commas\n",
    "                entropy_df['child_ids'] = entropy_df['node_id'].map(\n",
    "                    lambda x: '[' + ','.join(map(str, node_info.get(x, {}).get('child_ids', []))) + ']' \n",
    "                    if node_info.get(x, {}).get('child_ids') else ''\n",
    "                )\n",
    "                \n",
    "                # Fix child_count: directly use list length\n",
    "                entropy_df['child_count'] = entropy_df['node_id'].map(lambda x: len(node_info.get(x, {}).get('child_ids', [])))\n",
    "\n",
    "                # Save updated file\n",
    "                entropy_df.to_csv(entropy_file, index=False)\n",
    "                print(f\"Updated {entropy_file}, added document_count, layer, parent_id, child_ids, child_count columns\")\n",
    "                \n",
    "                # Display some statistics\n",
    "                print(f\"Node layer statistics:\")\n",
    "                print(f\"  - Layer distribution: {entropy_df['layer'].value_counts().sort_index().to_dict()}\")\n",
    "                print(f\"  - Document count range: {entropy_df['document_count'].min()}-{entropy_df['document_count'].max()}\")\n",
    "                print(f\"  - Root node count: {entropy_df[entropy_df['parent_id'].isna()].shape[0]}\")\n",
    "                print(f\"  - Leaf node count: {entropy_df[entropy_df['child_ids'] == ''].shape[0]}\")\n",
    "                print(f\"  - Child count distribution: {entropy_df['child_count'].value_counts().sort_index().to_dict()}\")\n",
    "            else:\n",
    "                print(f\"Warning: Corresponding entropy file not found {entropy_file}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "            print(\"Detailed error information:\")\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4039076d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Step3: Starting to add document counts and layer information to entropy files...\n",
      "==================================================\n",
      "\n",
      "Processing path structure file: depth_3_gamma_0.05_eta_0.05_alpha_1_run_3\n",
      "Last iteration: 265, path count: 242\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 307 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_3/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 64, 2: 242}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 242\n",
      "  - Child count distribution: {0: 242, 1: 14, 2: 15, 3: 11, 4: 7, 5: 7, 6: 5, 7: 3, 9: 1, 42: 1, 64: 1}\n",
      "\n",
      "Processing path structure file: depth_3_gamma_0.05_eta_0.05_alpha_1_run_2\n",
      "Last iteration: 245, path count: 263\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 321 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_2/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 57, 2: 263}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 263\n",
      "  - Child count distribution: {0: 263, 1: 5, 2: 24, 3: 10, 4: 7, 5: 5, 6: 1, 7: 3, 11: 1, 57: 1, 89: 1}\n",
      "\n",
      "Processing path structure file: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_3\n",
      "Last iteration: 170, path count: 246\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 325 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_3/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 78, 2: 246}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 246\n",
      "  - Child count distribution: {0: 246, 1: 21, 2: 21, 3: 17, 4: 9, 5: 4, 6: 2, 7: 1, 8: 1, 10: 1, 39: 1, 78: 1}\n",
      "\n",
      "Processing path structure file: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_1\n",
      "Last iteration: 170, path count: 257\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 325 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_1/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 67, 2: 257}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 257\n",
      "  - Child count distribution: {0: 257, 1: 14, 2: 15, 3: 19, 4: 5, 5: 8, 6: 1, 7: 2, 8: 1, 9: 1, 59: 1, 67: 1}\n",
      "\n",
      "Processing path structure file: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_2\n",
      "Last iteration: 170, path count: 261\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 333 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_2/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 71, 2: 261}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 261\n",
      "  - Child count distribution: {0: 261, 1: 12, 2: 23, 3: 14, 4: 14, 5: 4, 6: 3, 67: 1, 71: 1}\n",
      "\n",
      "Processing path structure file: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_3\n",
      "Last iteration: 275, path count: 236\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 292 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_3/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 55, 2: 236}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 236\n",
      "  - Child count distribution: {0: 236, 1: 14, 2: 13, 3: 12, 4: 8, 5: 5, 6: 1, 11: 1, 55: 1, 86: 1}\n",
      "\n",
      "Processing path structure file: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_1\n",
      "Last iteration: 275, path count: 236\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 282 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_1/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 45, 2: 236}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 236\n",
      "  - Child count distribution: {0: 236, 1: 13, 2: 12, 3: 4, 4: 7, 5: 2, 6: 2, 7: 2, 8: 1, 9: 1, 45: 1, 106: 1}\n",
      "\n",
      "Processing path structure file: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_2\n",
      "Last iteration: 275, path count: 255\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 292 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_2/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 36, 2: 255}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 255\n",
      "  - Child count distribution: {0: 255, 1: 4, 2: 6, 3: 9, 4: 7, 5: 1, 6: 3, 7: 2, 9: 2, 10: 1, 36: 1, 119: 1}\n",
      "\n",
      "Processing path structure file: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_3\n",
      "Last iteration: 175, path count: 212\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 275 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_3/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 62, 2: 212}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 212\n",
      "  - Child count distribution: {0: 212, 1: 8, 2: 14, 3: 22, 4: 10, 5: 4, 6: 2, 7: 1, 31: 1, 62: 1}\n",
      "\n",
      "Processing path structure file: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_1\n",
      "Last iteration: 175, path count: 255\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 316 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_1/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 60, 2: 255}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 255\n",
      "  - Child count distribution: {0: 255, 1: 9, 2: 11, 3: 15, 4: 12, 5: 6, 6: 4, 8: 2, 60: 1, 61: 1}\n",
      "\n",
      "Processing path structure file: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_2\n",
      "Last iteration: 175, path count: 244\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 305 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_2/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 60, 2: 244}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 244\n",
      "  - Child count distribution: {0: 244, 1: 8, 2: 18, 3: 15, 4: 6, 5: 7, 6: 5, 60: 1, 66: 1}\n",
      "\n",
      "Processing path structure file: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_2\n",
      "Last iteration: 90, path count: 243\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 299 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_2/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 55, 2: 243}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 243\n",
      "  - Child count distribution: {0: 243, 1: 5, 2: 16, 3: 11, 4: 7, 5: 6, 6: 5, 7: 2, 8: 1, 10: 1, 53: 1, 55: 1}\n",
      "\n",
      "Processing path structure file: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_3\n",
      "Last iteration: 90, path count: 262\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 322 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_3/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 59, 2: 262}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 262\n",
      "  - Child count distribution: {0: 262, 1: 7, 2: 15, 3: 12, 4: 11, 5: 5, 6: 3, 7: 4, 8: 1, 59: 1, 66: 1}\n",
      "\n",
      "Processing path structure file: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1\n",
      "Last iteration: 90, path count: 252\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 315 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 62, 2: 252}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 252\n",
      "  - Child count distribution: {0: 252, 1: 6, 2: 21, 3: 14, 4: 7, 5: 6, 6: 3, 7: 1, 8: 2, 9: 1, 54: 1, 62: 1}\n",
      "\n",
      "Processing path structure file: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_3\n",
      "Last iteration: 285, path count: 229\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 296 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_3/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 66, 2: 229}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 229\n",
      "  - Child count distribution: {0: 229, 1: 11, 2: 22, 3: 19, 4: 7, 5: 5, 6: 1, 58: 1, 66: 1}\n",
      "\n",
      "Processing path structure file: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_2\n",
      "Last iteration: 285, path count: 253\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 313 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_2/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 59, 2: 253}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 253\n",
      "  - Child count distribution: {0: 253, 1: 6, 2: 16, 3: 22, 4: 8, 5: 4, 6: 2, 59: 1, 85: 1}\n",
      "\n",
      "Processing path structure file: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_1\n",
      "Last iteration: 285, path count: 246\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 312 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_1/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 65, 2: 246}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 246\n",
      "  - Child count distribution: {0: 246, 1: 7, 2: 17, 3: 17, 4: 13, 5: 6, 6: 4, 48: 1, 65: 1}\n",
      "==================================================\n",
      "Step3: Document counts and layer information addition completed!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Main function: Add document counts and layer information to entropy files (adapted for step3)\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd \n",
    "\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step3\"  # Change to step3 path\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Step3: Starting to add document counts and layer information to entropy files...\")\n",
    "print(\"=\" * 50)\n",
    "add_document_counts_to_entropy_files(base_path)\n",
    "print(\"=\" * 50)\n",
    "print(\"Step3: Document counts and layer information addition completed!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bd33143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jensen_shannon_distance(p, q):\n",
    "    \"\"\"\n",
    "    Calculate Jensen-Shannon distance between two probability distributions\n",
    "    \n",
    "    Parameters:\n",
    "    p, q: array-like, probability distributions (should be normalized)\n",
    "    \n",
    "    Returns:\n",
    "    float: Jensen-Shannon distance\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays\n",
    "    p = np.array(p)\n",
    "    q = np.array(q)\n",
    "    \n",
    "    # Calculate midpoint distribution\n",
    "    m = 0.5 * (p + q)\n",
    "    \n",
    "    # Calculate KL divergence, add small constant to avoid log(0)\n",
    "    eps = 1e-10\n",
    "    kl_pm = np.sum(p * np.log((p + eps) / (m + eps)))\n",
    "    kl_qm = np.sum(q * np.log((q + eps) / (m + eps)))\n",
    "    \n",
    "    # Jensen-Shannon divergence\n",
    "    js_divergence = 0.5 * kl_pm + 0.5 * kl_qm\n",
    "    \n",
    "    # Jensen-Shannon distance (square root of divergence)\n",
    "    js_distance = np.sqrt(js_divergence)\n",
    "    \n",
    "    return js_distance\n",
    "\n",
    "def calculate_jensen_shannon_distances_with_weighted_entropy_by_alpha(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    Revised version: Calculate Jensen-Shannon distances between nodes in each layer and document-weighted average Renyi entropy\n",
    "    Use fixed eta=0.05 as Dirichlet smoothing parameter, alpha value is only used for folder identification\n",
    "    \n",
    "    Note: Renyi entropy calculation uses natural logarithm (loge), unit is nats\n",
    "    \"\"\"\n",
    "    # Find all iteration_node_word_distributions.csv files\n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_node_word_distributions.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    print(f\"Found {len(files)} word distribution files to process\")\n",
    "    \n",
    "    # In step3, eta value is fixed at 0.05 (for Dirichlet smoothing)\n",
    "    eta = 0.05  # Revised: fixed use of 0.05 as smoothing parameter\n",
    "    \n",
    "    # Group files by alpha value for display\n",
    "    files_by_alpha = {}\n",
    "    for file_path in files:\n",
    "        folder_name = os.path.basename(os.path.dirname(file_path))\n",
    "        alpha = 0.1  # Default value\n",
    "        if 'alpha_' in folder_name:\n",
    "            try:\n",
    "                alpha_part = folder_name.split('alpha_')[1].split('_')[0]\n",
    "                alpha = float(alpha_part)\n",
    "            except:\n",
    "                # Pattern matching through folder name\n",
    "                if 'a001' in folder_name:\n",
    "                    alpha = 0.01\n",
    "                elif 'a005' in folder_name:\n",
    "                    alpha = 0.05\n",
    "                elif 'a02' in folder_name:\n",
    "                    alpha = 0.2\n",
    "                elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                    alpha = 0.5\n",
    "                elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                    alpha = 1.0\n",
    "                elif 'a01' in folder_name:\n",
    "                    alpha = 0.1\n",
    "        else:\n",
    "            # Pattern matching through folder name\n",
    "            if 'a001' in folder_name:\n",
    "                alpha = 0.01\n",
    "            elif 'a005' in folder_name:\n",
    "                alpha = 0.05\n",
    "            elif 'a02' in folder_name:\n",
    "                alpha = 0.2\n",
    "            elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                alpha = 0.5\n",
    "            elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                alpha = 1.0\n",
    "            elif 'a01' in folder_name:\n",
    "                alpha = 0.1\n",
    "        \n",
    "        if alpha not in files_by_alpha:\n",
    "            files_by_alpha[alpha] = []\n",
    "        files_by_alpha[alpha].append(file_path)\n",
    "    \n",
    "    print(\"File distribution:\")\n",
    "    for alpha in sorted(files_by_alpha.keys()):\n",
    "        print(f\"  Alpha {alpha}: {len(files_by_alpha[alpha])} files\")\n",
    "    print(f\"Using fixed Eta value: {eta}\")\n",
    "    print()\n",
    "    \n",
    "    # Process each file\n",
    "    for idx, file_path in enumerate(files, 1):\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        \n",
    "        # Extract alpha value and run information from folder name (for recording only)\n",
    "        alpha = 0.1  # Default value\n",
    "        run_id = \"unknown\"\n",
    "        \n",
    "        if 'alpha_' in folder_name:\n",
    "            try:\n",
    "                alpha_part = folder_name.split('alpha_')[1].split('_')[0]\n",
    "                alpha = float(alpha_part)\n",
    "            except:\n",
    "                # Pattern matching through folder name\n",
    "                if 'a001' in folder_name:\n",
    "                    alpha = 0.01\n",
    "                elif 'a005' in folder_name:\n",
    "                    alpha = 0.05\n",
    "                elif 'a02' in folder_name:\n",
    "                    alpha = 0.2\n",
    "                elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                    alpha = 0.5\n",
    "                elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                    alpha = 1.0\n",
    "                elif 'a01' in folder_name:\n",
    "                    alpha = 0.1\n",
    "        else:\n",
    "            # Pattern matching through folder name\n",
    "            if 'a001' in folder_name:\n",
    "                alpha = 0.01\n",
    "            elif 'a005' in folder_name:\n",
    "                alpha = 0.05\n",
    "            elif 'a02' in folder_name:\n",
    "                alpha = 0.2\n",
    "            elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                alpha = 0.5\n",
    "            elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                alpha = 1.0\n",
    "            elif 'a01' in folder_name:\n",
    "                alpha = 0.1\n",
    "        \n",
    "        if '_run_' in folder_name:\n",
    "            try:\n",
    "                run_id = folder_name.split('_run_')[1]\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(f\"[{idx}/{len(files)}] Processing Alpha={alpha}, Run={run_id}\")\n",
    "        print(f\"Using fixed Eta={eta} for Dirichlet smoothing\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        try:\n",
    "            # Read word distribution data\n",
    "            word_df = pd.read_csv(file_path)\n",
    "            word_df.columns = [col.strip(\"'\\\" \") for col in word_df.columns]\n",
    "            \n",
    "            # Get last iteration data\n",
    "            max_iteration = word_df['iteration'].max()\n",
    "            last_iteration_data = word_df[word_df['iteration'] == max_iteration]\n",
    "            \n",
    "            # Get complete vocabulary\n",
    "            all_words = sorted(list(last_iteration_data['word'].dropna().unique()))\n",
    "            \n",
    "            # Read entropy file to get layer information\n",
    "            entropy_file = os.path.join(folder_path, 'corrected_renyi_entropy.csv')\n",
    "            if not os.path.exists(entropy_file):\n",
    "                print(f\"⚠️  Entropy file not found, skipping this file\")\n",
    "                continue\n",
    "                \n",
    "            entropy_df = pd.read_csv(entropy_file)\n",
    "            \n",
    "            # Basic information\n",
    "            print(f\"📊 Basic Information:\")\n",
    "            print(f\"   Vocabulary size: {len(all_words)}\")\n",
    "            print(f\"   Last iteration: {max_iteration}\")\n",
    "            \n",
    "            # Group nodes by layer\n",
    "            layers = entropy_df.groupby('layer')['node_id'].apply(list).to_dict()\n",
    "            print(f\"   Layer distribution: {[(layer, len(nodes)) for layer, nodes in layers.items()]}\")\n",
    "            \n",
    "            # Build probability distributions for each node\n",
    "            print(f\"🔄 Building probability distributions...\")\n",
    "            node_distributions = {}\n",
    "            \n",
    "            for node_id in entropy_df['node_id'].unique():\n",
    "                # Get word distribution for this node\n",
    "                node_words = last_iteration_data[last_iteration_data['node_id'] == node_id]\n",
    "                \n",
    "                # Initialize count vector\n",
    "                counts = np.zeros(len(all_words))\n",
    "                word_to_idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "                \n",
    "                # Fill actual counts\n",
    "                for _, row in node_words.iterrows():\n",
    "                    word = row['word']\n",
    "                    if pd.notna(word) and word in word_to_idx:\n",
    "                        counts[word_to_idx[word]] = row['count']\n",
    "                \n",
    "                # Revised: Use fixed eta value for Dirichlet smoothing\n",
    "                smoothed_counts = counts + eta  # Use eta=0.05 instead of alpha\n",
    "                \n",
    "                # Calculate probability distribution\n",
    "                probabilities = smoothed_counts / np.sum(smoothed_counts)\n",
    "                node_distributions[node_id] = probabilities\n",
    "            \n",
    "            print(f\"   ✓ Completed {len(node_distributions)} node probability distributions\")\n",
    "            \n",
    "            # Calculate JS distances and weighted average entropy within each layer\n",
    "            all_js_distances = []\n",
    "            layer_avg_distances = []\n",
    "            \n",
    "            print(f\"📐 Calculating JS distances...\")\n",
    "            for layer, layer_nodes in layers.items():\n",
    "                layer_js_distances = []\n",
    "                n = len(layer_nodes)\n",
    "                \n",
    "                # Calculate JS distances for all node pairs within this layer\n",
    "                for i, node1 in enumerate(layer_nodes):\n",
    "                    for j, node2 in enumerate(layer_nodes):\n",
    "                        if i < j:  # Only calculate upper triangular matrix, avoid duplicates and self-comparison\n",
    "                            if node1 in node_distributions and node2 in node_distributions:\n",
    "                                p = node_distributions[node1]\n",
    "                                q = node_distributions[node2]\n",
    "                                \n",
    "                                # Calculate Jensen-Shannon distance\n",
    "                                js_distance = jensen_shannon_distance(p, q)\n",
    "                                \n",
    "                                layer_js_distances.append({\n",
    "                                    'layer': layer,\n",
    "                                    'node1_id': node1,\n",
    "                                    'node2_id': node2,\n",
    "                                    'js_distance': js_distance,\n",
    "                                    'node1_doc_count': entropy_df[entropy_df['node_id'] == node1]['document_count'].iloc[0] if len(entropy_df[entropy_df['node_id'] == node1]) > 0 else 0,\n",
    "                                    'node2_doc_count': entropy_df[entropy_df['node_id'] == node2]['document_count'].iloc[0] if len(entropy_df[entropy_df['node_id'] == node2]) > 0 else 0\n",
    "                                })\n",
    "                \n",
    "                all_js_distances.extend(layer_js_distances)\n",
    "                \n",
    "                # Calculate average JS distance for this layer\n",
    "                avg_js_distance = 0.0\n",
    "                if layer_js_distances and n > 1:\n",
    "                    total_js_distance = sum(d['js_distance'] for d in layer_js_distances)\n",
    "                    max_pairs = n * (n - 1) // 2\n",
    "                    avg_js_distance = total_js_distance / max_pairs\n",
    "                \n",
    "                # Calculate document-weighted average Renyi entropy for this layer\n",
    "                layer_entropy_data = entropy_df[entropy_df['layer'] == layer]\n",
    "                total_docs = layer_entropy_data['document_count'].sum()\n",
    "                \n",
    "                if total_docs > 0:\n",
    "                    weighted_entropy = (layer_entropy_data['document_count'] * layer_entropy_data['renyi_entropy_corrected']).sum() / total_docs\n",
    "                else:\n",
    "                    weighted_entropy = 0.0\n",
    "                \n",
    "                layer_avg_distances.append({\n",
    "                    'layer': layer,\n",
    "                    'node_count': n,\n",
    "                    'total_pairs': len(layer_js_distances),\n",
    "                    'max_pairs': n * (n - 1) // 2 if n > 1 else 0,\n",
    "                    'sum_js_distance': sum(d['js_distance'] for d in layer_js_distances),\n",
    "                    'avg_js_distance': avg_js_distance,\n",
    "                    'total_documents': total_docs,\n",
    "                    'weighted_avg_renyi_entropy': weighted_entropy,\n",
    "                    'eta_used': eta,  # Revised: record actually used eta value\n",
    "                    'alpha_folder': alpha  # Revised: record folder's alpha value\n",
    "                })\n",
    "                \n",
    "                # Concise layer statistics output\n",
    "                print(f\"   Layer {layer}: {n}nodes, JS={avg_js_distance:.4f}, entropy={weighted_entropy:.4f}\")\n",
    "            \n",
    "            # Save result files\n",
    "            if all_js_distances:\n",
    "                js_df = pd.DataFrame(all_js_distances)\n",
    "                output_path = os.path.join(folder_path, 'jensen_shannon_distances.csv')\n",
    "                js_df.to_csv(output_path, index=False)\n",
    "            \n",
    "            if layer_avg_distances:\n",
    "                avg_df = pd.DataFrame(layer_avg_distances)\n",
    "                avg_output_path = os.path.join(folder_path, 'layer_average_js_distances.csv')\n",
    "                avg_df.to_csv(avg_output_path, index=False)\n",
    "            \n",
    "            print(f\"💾 Results saved\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Processing failed: {str(e)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"✅ All files processed!\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3cea591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Step3: Starting calculation of Jensen-Shannon distances and weighted average Renyi entropy (automatically adjusted by alpha value)...\n",
      "==================================================\n",
      "Found 17 word distribution files to process\n",
      "File distribution:\n",
      "  Alpha 0.01: 3 files\n",
      "  Alpha 0.05: 3 files\n",
      "  Alpha 0.1: 3 files\n",
      "  Alpha 0.2: 3 files\n",
      "  Alpha 0.5: 3 files\n",
      "  Alpha 1.0: 2 files\n",
      "Using fixed Eta value: 0.05\n",
      "\n",
      "================================================================================\n",
      "[1/17] Processing Alpha=1.0, Run=3\n",
      "Using fixed Eta=0.05 for Dirichlet smoothing\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 265\n",
      "   Layer distribution: [(0, 1), (1, 64), (2, 242)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 307 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1nodes, JS=0.0000, entropy=4.9219\n",
      "   Layer 1: 64nodes, JS=0.6081, entropy=4.6239\n",
      "   Layer 2: 242nodes, JS=0.5566, entropy=4.5810\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[2/17] Processing Alpha=1.0, Run=2\n",
      "Using fixed Eta=0.05 for Dirichlet smoothing\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 245\n",
      "   Layer distribution: [(0, 1), (1, 57), (2, 263)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 321 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1nodes, JS=0.0000, entropy=4.9374\n",
      "   Layer 1: 57nodes, JS=0.5959, entropy=4.4893\n",
      "   Layer 2: 263nodes, JS=0.5677, entropy=4.4976\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[3/17] Processing Alpha=0.2, Run=3\n",
      "Using fixed Eta=0.05 for Dirichlet smoothing\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 170\n",
      "   Layer distribution: [(0, 1), (1, 78), (2, 246)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 325 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1nodes, JS=0.0000, entropy=4.9341\n",
      "   Layer 1: 78nodes, JS=0.5843, entropy=4.4484\n",
      "   Layer 2: 246nodes, JS=0.5702, entropy=4.5200\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[4/17] Processing Alpha=0.2, Run=1\n",
      "Using fixed Eta=0.05 for Dirichlet smoothing\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 170\n",
      "   Layer distribution: [(0, 1), (1, 67), (2, 257)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 325 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1nodes, JS=0.0000, entropy=4.9279\n",
      "   Layer 1: 67nodes, JS=0.5784, entropy=4.4602\n",
      "   Layer 2: 257nodes, JS=0.5663, entropy=4.5185\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[5/17] Processing Alpha=0.2, Run=2\n",
      "Using fixed Eta=0.05 for Dirichlet smoothing\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 170\n",
      "   Layer distribution: [(0, 1), (1, 71), (2, 261)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 333 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1nodes, JS=0.0000, entropy=4.9620\n",
      "   Layer 1: 71nodes, JS=0.5820, entropy=4.5827\n",
      "   Layer 2: 261nodes, JS=0.5629, entropy=4.5345\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[6/17] Processing Alpha=0.5, Run=3\n",
      "Using fixed Eta=0.05 for Dirichlet smoothing\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 275\n",
      "   Layer distribution: [(0, 1), (1, 55), (2, 236)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 292 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1nodes, JS=0.0000, entropy=4.9473\n",
      "   Layer 1: 55nodes, JS=0.5663, entropy=4.6287\n",
      "   Layer 2: 236nodes, JS=0.5642, entropy=4.5321\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[7/17] Processing Alpha=0.5, Run=1\n",
      "Using fixed Eta=0.05 for Dirichlet smoothing\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 275\n",
      "   Layer distribution: [(0, 1), (1, 45), (2, 236)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 282 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1nodes, JS=0.0000, entropy=4.9814\n",
      "   Layer 1: 45nodes, JS=0.5566, entropy=4.5808\n",
      "   Layer 2: 236nodes, JS=0.5794, entropy=4.4689\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[8/17] Processing Alpha=0.5, Run=2\n",
      "Using fixed Eta=0.05 for Dirichlet smoothing\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 275\n",
      "   Layer distribution: [(0, 1), (1, 36), (2, 255)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 292 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1nodes, JS=0.0000, entropy=4.9606\n",
      "   Layer 1: 36nodes, JS=0.6094, entropy=4.5044\n",
      "   Layer 2: 255nodes, JS=0.5702, entropy=4.5200\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[9/17] Processing Alpha=0.05, Run=3\n",
      "Using fixed Eta=0.05 for Dirichlet smoothing\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 175\n",
      "   Layer distribution: [(0, 1), (1, 62), (2, 212)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 275 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1nodes, JS=0.0000, entropy=5.0622\n",
      "   Layer 1: 62nodes, JS=0.5404, entropy=4.9330\n",
      "   Layer 2: 212nodes, JS=0.5595, entropy=4.8250\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[10/17] Processing Alpha=0.05, Run=1\n",
      "Using fixed Eta=0.05 for Dirichlet smoothing\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 175\n",
      "   Layer distribution: [(0, 1), (1, 60), (2, 255)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 316 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1nodes, JS=0.0000, entropy=4.9605\n",
      "   Layer 1: 60nodes, JS=0.5274, entropy=4.7746\n",
      "   Layer 2: 255nodes, JS=0.5677, entropy=4.5770\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[11/17] Processing Alpha=0.05, Run=2\n",
      "Using fixed Eta=0.05 for Dirichlet smoothing\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 175\n",
      "   Layer distribution: [(0, 1), (1, 60), (2, 244)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 305 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1nodes, JS=0.0000, entropy=5.0117\n",
      "   Layer 1: 60nodes, JS=0.5456, entropy=4.7415\n",
      "   Layer 2: 244nodes, JS=0.5679, entropy=4.5716\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[12/17] Processing Alpha=0.1, Run=2\n",
      "Using fixed Eta=0.05 for Dirichlet smoothing\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 90\n",
      "   Layer distribution: [(0, 1), (1, 55), (2, 243)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 299 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1nodes, JS=0.0000, entropy=4.9828\n",
      "   Layer 1: 55nodes, JS=0.5651, entropy=4.6981\n",
      "   Layer 2: 243nodes, JS=0.5757, entropy=4.4911\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[13/17] Processing Alpha=0.1, Run=3\n",
      "Using fixed Eta=0.05 for Dirichlet smoothing\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 90\n",
      "   Layer distribution: [(0, 1), (1, 59), (2, 262)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 322 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1nodes, JS=0.0000, entropy=4.9232\n",
      "   Layer 1: 59nodes, JS=0.5652, entropy=4.5501\n",
      "   Layer 2: 262nodes, JS=0.5722, entropy=4.4548\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[14/17] Processing Alpha=0.1, Run=1\n",
      "Using fixed Eta=0.05 for Dirichlet smoothing\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 90\n",
      "   Layer distribution: [(0, 1), (1, 62), (2, 252)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 315 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1nodes, JS=0.0000, entropy=4.9626\n",
      "   Layer 1: 62nodes, JS=0.5686, entropy=4.5881\n",
      "   Layer 2: 252nodes, JS=0.5737, entropy=4.5019\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[15/17] Processing Alpha=0.01, Run=3\n",
      "Using fixed Eta=0.05 for Dirichlet smoothing\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 285\n",
      "   Layer distribution: [(0, 1), (1, 66), (2, 229)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 296 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1nodes, JS=0.0000, entropy=5.0147\n",
      "   Layer 1: 66nodes, JS=0.5509, entropy=4.8154\n",
      "   Layer 2: 229nodes, JS=0.5689, entropy=4.6449\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[16/17] Processing Alpha=0.01, Run=2\n",
      "Using fixed Eta=0.05 for Dirichlet smoothing\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 285\n",
      "   Layer distribution: [(0, 1), (1, 59), (2, 253)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 313 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1nodes, JS=0.0000, entropy=4.9980\n",
      "   Layer 1: 59nodes, JS=0.5281, entropy=4.8442\n",
      "   Layer 2: 253nodes, JS=0.5753, entropy=4.5284\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[17/17] Processing Alpha=0.01, Run=1\n",
      "Using fixed Eta=0.05 for Dirichlet smoothing\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 285\n",
      "   Layer distribution: [(0, 1), (1, 65), (2, 246)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 312 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1nodes, JS=0.0000, entropy=5.0255\n",
      "   Layer 1: 65nodes, JS=0.5294, entropy=4.9124\n",
      "   Layer 2: 246nodes, JS=0.5656, entropy=4.6512\n",
      "💾 Results saved\n",
      "\n",
      "================================================================================\n",
      "✅ All files processed!\n",
      "================================================================================\n",
      "==================================================\n",
      "Starting aggregation of layer statistics by alpha value...\n",
      "==================================================\n",
      "======================================================================\n",
      "Layer summary statistics by ALPHA value (Step3)\n",
      "======================================================================\n",
      "\n",
      "Processing Alpha=0.01\n",
      "Output directory: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001\n",
      "  Summary file saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/alpha_0.01_layer_summary.csv\n",
      "  Included runs: 1, 2, 3\n",
      "  Number of layers: 3\n",
      "  Used Eta value: 0.05\n",
      "    Layer 0: JS=0.0000(±0.0000), entropy=5.0127(±0.0138), nodes=1.0, runs=3\n",
      "    Layer 1: JS=0.5361(±0.0128), entropy=4.8573(±0.0498), nodes=63.3, runs=3\n",
      "    Layer 2: JS=0.5699(±0.0050), entropy=4.6082(±0.0692), nodes=242.7, runs=3\n",
      "\n",
      "Processing Alpha=0.05\n",
      "Output directory: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005\n",
      "  Summary file saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/alpha_0.05_layer_summary.csv\n",
      "  Included runs: 1, 2, 3\n",
      "  Number of layers: 3\n",
      "  Used Eta value: 0.05\n",
      "    Layer 0: JS=0.0000(±0.0000), entropy=5.0114(±0.0508), nodes=1.0, runs=3\n",
      "    Layer 1: JS=0.5378(±0.0094), entropy=4.8164(±0.1024), nodes=60.7, runs=3\n",
      "    Layer 2: JS=0.5650(±0.0048), entropy=4.6579(±0.1448), nodes=237.0, runs=3\n",
      "\n",
      "Processing Alpha=0.1\n",
      "Output directory: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01\n",
      "  Summary file saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/alpha_0.1_layer_summary.csv\n",
      "  Included runs: 1, 2, 3\n",
      "  Number of layers: 3\n",
      "  Used Eta value: 0.05\n",
      "    Layer 0: JS=0.0000(±0.0000), entropy=4.9562(±0.0303), nodes=1.0, runs=3\n",
      "    Layer 1: JS=0.5663(±0.0020), entropy=4.6121(±0.0769), nodes=58.7, runs=3\n",
      "    Layer 2: JS=0.5739(±0.0017), entropy=4.4826(±0.0247), nodes=252.3, runs=3\n",
      "\n",
      "Processing Alpha=0.2\n",
      "Output directory: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02\n",
      "  Summary file saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/alpha_0.2_layer_summary.csv\n",
      "  Included runs: 1, 2, 3\n",
      "  Number of layers: 3\n",
      "  Used Eta value: 0.05\n",
      "    Layer 0: JS=0.0000(±0.0000), entropy=4.9413(±0.0181), nodes=1.0, runs=3\n",
      "    Layer 1: JS=0.5816(±0.0030), entropy=4.4971(±0.0743), nodes=72.0, runs=3\n",
      "    Layer 2: JS=0.5665(±0.0037), entropy=4.5243(±0.0089), nodes=254.7, runs=3\n",
      "\n",
      "Processing Alpha=0.5\n",
      "Output directory: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05\n",
      "  Summary file saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/alpha_0.5_layer_summary.csv\n",
      "  Included runs: 1, 2, 3\n",
      "  Number of layers: 3\n",
      "  Used Eta value: 0.05\n",
      "    Layer 0: JS=0.0000(±0.0000), entropy=4.9631(±0.0171), nodes=1.0, runs=3\n",
      "    Layer 1: JS=0.5774(±0.0281), entropy=4.5713(±0.0627), nodes=45.3, runs=3\n",
      "    Layer 2: JS=0.5713(±0.0077), entropy=4.5070(±0.0335), nodes=242.3, runs=3\n",
      "\n",
      "Processing Alpha=1.0\n",
      "Output directory: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1\n",
      "  Summary file saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/alpha_1.0_layer_summary.csv\n",
      "  Included runs: 2, 3\n",
      "  Number of layers: 3\n",
      "  Used Eta value: 0.05\n",
      "    Layer 0: JS=0.0000(±0.0000), entropy=4.9297(±0.0110), nodes=1.0, runs=2\n",
      "    Layer 1: JS=0.6020(±0.0086), entropy=4.5566(±0.0952), nodes=60.5, runs=2\n",
      "    Layer 2: JS=0.5621(±0.0079), entropy=4.5393(±0.0590), nodes=252.5, runs=2\n",
      "\n",
      "======================================================================\n",
      "Generating overall comparison file\n",
      "======================================================================\n",
      "Overall comparison file saved to: /Volumes/My Passport/收敛结果/step3/alpha_layer_comparison.csv\n",
      "\n",
      "Layer 0 Cross-Alpha Comparison:\n",
      "Alpha Value     JS Distance(±std)      Weighted Entropy(±std)      Node Count(±std)   Run Count\n",
      "---------------------------------------------------------------------------\n",
      "  0.010    0.0000(±0.0000)   5.0127(±0.0138)      1.0(± 0.0)      3\n",
      "  0.050    0.0000(±0.0000)   5.0114(±0.0508)      1.0(± 0.0)      3\n",
      "  0.100    0.0000(±0.0000)   4.9562(±0.0303)      1.0(± 0.0)      3\n",
      "  0.200    0.0000(±0.0000)   4.9413(±0.0181)      1.0(± 0.0)      3\n",
      "  0.500    0.0000(±0.0000)   4.9631(±0.0171)      1.0(± 0.0)      3\n",
      "  1.000    0.0000(±0.0000)   4.9297(±0.0110)      1.0(± 0.0)      2\n",
      "\n",
      "Layer 1 Cross-Alpha Comparison:\n",
      "Alpha Value     JS Distance(±std)      Weighted Entropy(±std)      Node Count(±std)   Run Count\n",
      "---------------------------------------------------------------------------\n",
      "  0.010    0.5361(±0.0128)   4.8573(±0.0498)     63.3(± 3.8)      3\n",
      "  0.050    0.5378(±0.0094)   4.8164(±0.1024)     60.7(± 1.2)      3\n",
      "  0.100    0.5663(±0.0020)   4.6121(±0.0769)     58.7(± 3.5)      3\n",
      "  0.200    0.5816(±0.0030)   4.4971(±0.0743)     72.0(± 5.6)      3\n",
      "  0.500    0.5774(±0.0281)   4.5713(±0.0627)     45.3(± 9.5)      3\n",
      "  1.000    0.6020(±0.0086)   4.5566(±0.0952)     60.5(± 4.9)      2\n",
      "\n",
      "Layer 2 Cross-Alpha Comparison:\n",
      "Alpha Value     JS Distance(±std)      Weighted Entropy(±std)      Node Count(±std)   Run Count\n",
      "---------------------------------------------------------------------------\n",
      "  0.010    0.5699(±0.0050)   4.6082(±0.0692)    242.7(±12.3)      3\n",
      "  0.050    0.5650(±0.0048)   4.6579(±0.1448)    237.0(±22.3)      3\n",
      "  0.100    0.5739(±0.0017)   4.4826(±0.0247)    252.3(± 9.5)      3\n",
      "  0.200    0.5665(±0.0037)   4.5243(±0.0089)    254.7(± 7.8)      3\n",
      "  0.500    0.5713(±0.0077)   4.5070(±0.0335)    242.3(±11.0)      3\n",
      "  1.000    0.5621(±0.0079)   4.5393(±0.0590)    252.5(±14.8)      2\n",
      "==================================================\n",
      "Step3: Jensen-Shannon distances and weighted average Renyi entropy calculation completed!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd \n",
    "\n",
    "# Main function: Calculate Jensen-Shannon distances and weighted average Renyi entropy\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step3\"  # Root directory\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Step3: Starting calculation of Jensen-Shannon distances and weighted average Renyi entropy (automatically adjusted by alpha value)...\")\n",
    "print(\"=\" * 50)\n",
    "calculate_jensen_shannon_distances_with_weighted_entropy_by_alpha(base_path)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Starting aggregation of layer statistics by alpha value...\")\n",
    "print(\"=\" * 50)\n",
    "# Add the missing aggregation function call\n",
    "aggregate_layer_statistics_by_alpha(base_path)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Step3: Jensen-Shannon distances and weighted average Renyi entropy calculation completed!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "856d59c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# =========================\n",
    "# 1) JSD (ln basis, returns distance)\n",
    "# =========================\n",
    "def jensen_shannon_distance(p, q, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Jensen–Shannon distance (natural logarithm ln basis, returns distance = sqrt(JSD_divergence)).\n",
    "    p, q: 1D probability vectors (can be non-normalized, function will normalize internally)\n",
    "    \"\"\"\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    q = np.asarray(q, dtype=float)\n",
    "\n",
    "    # Normalize + numerical smoothing\n",
    "    p = np.clip(p, eps, None); p = p / p.sum()\n",
    "    q = np.clip(q, eps, None); q = q / q.sum()\n",
    "    m = 0.5 * (p + q)\n",
    "\n",
    "    def _kl(a, b):\n",
    "        # ln basis\n",
    "        return np.sum(a * (np.log(a) - np.log(b)))\n",
    "\n",
    "    js_div = 0.5 * _kl(p, m) + 0.5 * _kl(q, m)  # JSD (divergence, ln)\n",
    "    js_dist = math.sqrt(js_div)                 # distance\n",
    "    return js_dist\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 2) Tools: weighted/unweighted statistical functions\n",
    "# ==============================\n",
    "def weighted_mean(values, weights, eps=1e-12):\n",
    "    v = np.asarray(values, dtype=float)\n",
    "    w = np.asarray(weights, dtype=float)\n",
    "    s = w.sum()\n",
    "    return float((v * w).sum() / max(s, eps))\n",
    "\n",
    "def dict_to_prob_vector(count_df_for_node, vocab, count_col='count', eta=0.05):\n",
    "    \"\"\"\n",
    "    Convert word counts (or weights) for a specific node to probability vector; supports Dirichlet smoothing eta.\n",
    "    count_df_for_node: DataFrame (subset) containing only (word, count) rows for this node\n",
    "    vocab: list[str] unified vocabulary ordering\n",
    "    count_col: count field name (default 'count')\n",
    "    eta: Dirichlet smoothing (commonly 0.05)\n",
    "    \"\"\"\n",
    "    word_to_idx = {w:i for i, w in enumerate(vocab)}\n",
    "    vec = np.zeros(len(vocab), dtype=float)\n",
    "    for _, row in count_df_for_node.iterrows():\n",
    "        w = row['word']\n",
    "        if pd.isna(w): \n",
    "            continue\n",
    "        i = word_to_idx.get(w)\n",
    "        if i is not None:\n",
    "            vec[i] = float(row[count_col])\n",
    "    vec = vec + eta\n",
    "    s = vec.sum()\n",
    "    if s <= 0:\n",
    "        return np.full(len(vocab), 1.0/len(vocab))\n",
    "    return vec / s\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# 3) Directory-wise calculation: inter-layer parent-child JSD (weighted & unweighted output)\n",
    "# ===========================================================\n",
    "def calculate_inter_layer_jensen_shannon_distances_weighted(\n",
    "    base_path=\".\",\n",
    "    count_filename=\"iteration_node_word_distributions.csv\",\n",
    "    entropy_filename=\"corrected_renyi_entropy.csv\",\n",
    "    count_col='count',\n",
    "    eta=0.05,\n",
    "    default_alpha=0.1,\n",
    "    weight_field_candidates=('child_token_count', 'child_doc_count'),  # weight field priority\n",
    "    output_suffix=\"_weighted\"  # output filename suffix\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate parent-child JSD between adjacent layers (inter-layer JSD), output both weighted and unweighted statistics.\n",
    "    - Weights prioritize the first existing column from weight_field_candidates; if none exist, use equal weights.\n",
    "    - JSD basis: ln (natural logarithm), returns \"distance\" (sqrt JSD divergence).\n",
    "    - Probability construction: Add eta Dirichlet smoothing to node word counts then normalize.\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(base_path, \"**\", count_filename)\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    print(\"=\"*80)\n",
    "    print(\"Starting inter-layer Jensen–Shannon distance calculation (weighted version, ln basis, distance)...\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Found {len(files)} word distribution files to process\\n\")\n",
    "\n",
    "    def parse_alpha_from_folder(folder_name, default_val=default_alpha):\n",
    "        alpha = default_val\n",
    "        if 'alpha_' in folder_name:\n",
    "            try:\n",
    "                alpha_part = folder_name.split('alpha_')[1].split('_')[0]\n",
    "                alpha = float(alpha_part)\n",
    "                return alpha\n",
    "            except Exception:\n",
    "                pass\n",
    "        # Fallback: pattern matching by naming convention\n",
    "        if 'a001' in folder_name: alpha = 0.01\n",
    "        elif 'a005' in folder_name: alpha = 0.05\n",
    "        elif 'a02'  in folder_name: alpha = 0.2\n",
    "        elif 'a05'  in folder_name and 'a005' not in folder_name: alpha = 0.5\n",
    "        elif 'a1_'  in folder_name or folder_name.split('_')[-1] == 'a1': alpha = 1.0\n",
    "        elif 'a01'  in folder_name: alpha = 0.1\n",
    "        return alpha\n",
    "\n",
    "    for idx, file_path in enumerate(files, 1):\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "\n",
    "        alpha = parse_alpha_from_folder(folder_name, default_val=default_alpha)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"[{idx}/{len(files)}] Calculating inter-layer JSD (weighted version): Alpha={alpha}\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        try:\n",
    "            # 1) Read node-word counts\n",
    "            word_df = pd.read_csv(file_path)\n",
    "            word_df.columns = [str(c).strip(\"'\\\" \") for c in word_df.columns]\n",
    "\n",
    "            max_iter = word_df['iteration'].max()\n",
    "            last_iter_df = word_df[word_df['iteration'] == max_iter].copy()\n",
    "\n",
    "            # Unified vocabulary\n",
    "            vocab = sorted(list(last_iter_df['word'].dropna().unique()))\n",
    "            print(f\"📊 Basic Information:\\n   Vocabulary size: {len(vocab)}\\n   Last iteration: {max_iter}\")\n",
    "\n",
    "            # 2) Read layer and parent-child information\n",
    "            entropy_path = os.path.join(folder_path, entropy_filename)\n",
    "            if not os.path.exists(entropy_path):\n",
    "                print(\"⚠️ Entropy file not found, skipping\")\n",
    "                continue\n",
    "            ent = pd.read_csv(entropy_path)\n",
    "            # Expected fields: node_id, layer, child_ids (format \"[1, 5, 9]\"), document_count, (optional)token_count\n",
    "            ent.columns = [str(c).strip() for c in ent.columns]\n",
    "            print(f\"   Node count: {ent.shape[0]}\")\n",
    "\n",
    "            # 3) Construct probability distribution φ for each node (based on last iteration)\n",
    "            node_ids = ent['node_id'].unique().tolist()\n",
    "            node_phi = {}\n",
    "            for nid in node_ids:\n",
    "                sub = last_iter_df[last_iter_df['node_id'] == nid]\n",
    "                phi = dict_to_prob_vector(sub, vocab, count_col=count_col, eta=eta)\n",
    "                node_phi[nid] = phi\n",
    "            print(f\"   ✓ Completed {len(node_phi)} node probability distributions (eta={eta})\")\n",
    "\n",
    "            # 4) Calculate JSD for adjacent layer parent-child edges and construct weights\n",
    "            layers = sorted([l for l in ent['layer'].unique() if l >= 0])\n",
    "            print(\"📐 Calculating inter-layer JSD...\")\n",
    "            print(f\"   Available layers: {layers}\")\n",
    "\n",
    "            records = []\n",
    "            for i in range(len(layers) - 1):\n",
    "                parent_layer = layers[i]\n",
    "                child_layer  = layers[i + 1]\n",
    "                print(f\"   Calculating Layer {parent_layer} -> Layer {child_layer}\")\n",
    "\n",
    "                parent_nodes = ent[ent['layer'] == parent_layer]\n",
    "                child_nodes  = ent[ent['layer'] == child_layer]\n",
    "                # For quick lookup\n",
    "                child_meta   = child_nodes.set_index('node_id')\n",
    "\n",
    "                layer_jsd_vals   = []\n",
    "                layer_weights    = []\n",
    "\n",
    "                for _, prow in parent_nodes.iterrows():\n",
    "                    pid = prow['node_id']\n",
    "                    if pid not in node_phi: \n",
    "                        continue\n",
    "\n",
    "                    # Parse child_ids\n",
    "                    child_ids_raw = prow.get('child_ids', '')\n",
    "                    if pd.isna(child_ids_raw) or str(child_ids_raw).strip() == '':\n",
    "                        continue\n",
    "                    try:\n",
    "                        cid_list = str(child_ids_raw).strip('[]')\n",
    "                        child_ids = [int(x.strip()) for x in cid_list.split(',') if str(x).strip()!='']\n",
    "                    except Exception:\n",
    "                        child_ids = []\n",
    "\n",
    "                    for cid in child_ids:\n",
    "                        if cid not in node_phi: \n",
    "                            continue\n",
    "                        # Confirm child node layer is correct\n",
    "                        if cid in child_meta.index and int(child_meta.loc[cid, 'layer']) != child_layer:\n",
    "                            continue\n",
    "\n",
    "                        # Get φ and calculate JSD distance (ln basis)\n",
    "                        jsd_dist = jensen_shannon_distance(node_phi[pid], node_phi[cid])\n",
    "\n",
    "                        # Select weight\n",
    "                        w = 1.0\n",
    "                        # Prioritize first existing field from candidates\n",
    "                        if cid in child_meta.index:\n",
    "                            for wf in weight_field_candidates:\n",
    "                                if wf in child_meta.columns:\n",
    "                                    val = child_meta.loc[cid, wf]\n",
    "                                    if pd.notna(val):\n",
    "                                        w = float(val)\n",
    "                                        break\n",
    "                            # If none exist, try parent/child document_count as approximation\n",
    "                            if w == 1.0:\n",
    "                                # child_doc_count priority, then parent_doc_count\n",
    "                                if 'document_count' in child_meta.columns:\n",
    "                                    val = child_meta.loc[cid, 'document_count']\n",
    "                                    if pd.notna(val): \n",
    "                                        w = float(val)\n",
    "                                if (w == 1.0) and ('document_count' in parent_nodes.columns):\n",
    "                                    pv = prow.get('document_count', np.nan)\n",
    "                                    if pd.notna(pv):\n",
    "                                        w = float(pv)\n",
    "\n",
    "                        # Record\n",
    "                        rec = {\n",
    "                            'parent_layer': int(parent_layer),\n",
    "                            'child_layer':  int(child_layer),\n",
    "                            'parent_node_id': int(pid),\n",
    "                            'child_node_id':  int(cid),\n",
    "                            'js_distance':    float(jsd_dist),\n",
    "                            'weight_edge':    float(w),\n",
    "                            'alpha':          float(alpha),\n",
    "                            'eta_used':       float(eta),\n",
    "                        }\n",
    "                        # For convenience, also include doc counts (if exist)\n",
    "                        if 'document_count' in parent_nodes.columns:\n",
    "                            rec['parent_doc_count'] = float(prow.get('document_count', np.nan))\n",
    "                        if cid in child_meta.index and 'document_count' in child_meta.columns:\n",
    "                            rec['child_doc_count'] = float(child_meta.loc[cid, 'document_count'])\n",
    "                        if cid in child_meta.index and weight_field_candidates[0] in child_meta.columns:\n",
    "                            rec[weight_field_candidates[0]] = float(child_meta.loc[cid, weight_field_candidates[0]])\n",
    "                        layer_jsd_vals.append(jsd_dist)\n",
    "                        layer_weights.append(w)\n",
    "                        records.append(rec)\n",
    "\n",
    "                # Layer-wise statistics\n",
    "                if layer_jsd_vals:\n",
    "                    simple_avg = float(np.mean(layer_jsd_vals))\n",
    "                    weighted_avg = weighted_mean(layer_jsd_vals, layer_weights)\n",
    "                    std_val = float(np.std(layer_jsd_vals))\n",
    "                    print(f\"     📊 Layer {parent_layer}->{child_layer}: \"\n",
    "                          f\"{len(layer_jsd_vals)} pairs, simple mean={simple_avg:.4f}, weighted mean={weighted_avg:.4f}, σ={std_val:.4f}\")\n",
    "\n",
    "            # 5) Output details and layer summary\n",
    "            if records:\n",
    "                inter_df = pd.DataFrame(records)\n",
    "                out_detail = os.path.join(folder_path, f'inter_layer_jensen_shannon_distances{output_suffix}.csv')\n",
    "                inter_df.to_csv(out_detail, index=False)\n",
    "\n",
    "                # Layer summary\n",
    "                sums = []\n",
    "                for pl in sorted(inter_df['parent_layer'].unique()):\n",
    "                    cl = pl + 1\n",
    "                    sub = inter_df[(inter_df['parent_layer']==pl) & (inter_df['child_layer']==cl)]\n",
    "                    if sub.empty: \n",
    "                        continue\n",
    "                    sa = sub['js_distance'].mean()\n",
    "                    wa = weighted_mean(sub['js_distance'].values, sub['weight_edge'].values)\n",
    "                    med = sub['js_distance'].median()\n",
    "                    std = sub['js_distance'].std()\n",
    "                    mn = sub['js_distance'].min()\n",
    "                    mx = sub['js_distance'].max()\n",
    "                    sums.append({\n",
    "                        'parent_layer': pl,\n",
    "                        'child_layer':  cl,\n",
    "                        'pair_count':   int(len(sub)),\n",
    "                        'simple_avg_js_distance':   float(sa),\n",
    "                        'weighted_avg_js_distance': float(wa),\n",
    "                        'std_js_distance':           float(std),\n",
    "                        'median_js_distance':        float(med),\n",
    "                        'min_js_distance':           float(mn),\n",
    "                        'max_js_distance':           float(mx),\n",
    "                        'alpha': float(alpha),\n",
    "                        'eta_used': float(eta),\n",
    "                    })\n",
    "                sum_df = pd.DataFrame(sums)\n",
    "                out_summary = os.path.join(folder_path, f'inter_layer_js_summary{output_suffix}.csv')\n",
    "                sum_df.to_csv(out_summary, index=False)\n",
    "\n",
    "                print(\"💾 Inter-layer JSD results saved:\")\n",
    "                print(f\"   Detailed results: {out_detail}\")\n",
    "                print(f\"   Summary results: {out_summary}\")\n",
    "                print(\"📊 Inter-layer JSD layer summary (weighted + unweighted):\")\n",
    "                for _, row in sum_df.iterrows():\n",
    "                    pl, cl = int(row['parent_layer']), int(row['child_layer'])\n",
    "                    pc     = int(row['pair_count'])\n",
    "                    sa     = row['simple_avg_js_distance']\n",
    "                    wa     = row['weighted_avg_js_distance']\n",
    "                    sd     = row['std_js_distance']\n",
    "                    md     = row['median_js_distance']\n",
    "                    print(f\"   L{pl}->{cl}: {pc} pairs, simple mean={sa:.4f}, weighted mean={wa:.4f}, median={md:.4f}, σ={sd:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"❌ Processing failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    print(\"\\n✅ Inter-layer JSD calculation completed! (weighted version, ln basis)\")\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# 4) Aggregate by alpha (read *weighted.csv)\n",
    "# ===========================================================\n",
    "def aggregate_inter_layer_jsd_by_alpha_weighted(\n",
    "    base_path=\".\",\n",
    "    summary_filename_pattern=\"inter_layer_js_summary_weighted.csv\",\n",
    "    output_overall='alpha_inter_layer_jsd_comparison_weighted.csv'\n",
    "):\n",
    "    \"\"\"\n",
    "    Aggregate all inter_layer_js_summary_weighted.csv files from all directories, group by alpha & layer pairs,\n",
    "    output mean and standard deviation of (simple mean/weighted mean).\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(base_path, \"**\", summary_filename_pattern)\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    print(\"=\"*80)\n",
    "    print(\"Starting aggregation of inter-layer JSD by alpha value (weighted version)...\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"🔍 Found {len(files)} inter-layer JSD summary files (weighted)\")\n",
    "\n",
    "    all_rows = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            df = pd.read_csv(f)\n",
    "            # Infer alpha & run_id (adjust according to your directory structure)\n",
    "            folder = os.path.dirname(f)\n",
    "            folder_name = os.path.basename(folder)\n",
    "            parent_folder = os.path.dirname(folder)\n",
    "\n",
    "            # Parse alpha\n",
    "            alpha = None\n",
    "            if 'alpha_' in folder_name:\n",
    "                try:\n",
    "                    alpha = float(folder_name.split('alpha_')[1].split('_')[0])\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if alpha is None:\n",
    "                # Fallback\n",
    "                if 'a001' in folder_name: alpha = 0.01\n",
    "                elif 'a005' in folder_name: alpha = 0.05\n",
    "                elif 'a02'  in folder_name: alpha = 0.2\n",
    "                elif 'a05'  in folder_name and 'a005' not in folder_name: alpha = 0.5\n",
    "                elif 'a1_'  in folder_name or folder_name.split('_')[-1]=='a1': alpha = 1.0\n",
    "                elif 'a01'  in folder_name: alpha = 0.1\n",
    "\n",
    "            # Parse run_id (adjust according to your naming)\n",
    "            run_id = None\n",
    "            if '_run_' in folder_name:\n",
    "                run_id = folder_name.split('_run_')[-1]\n",
    "\n",
    "            for _, r in df.iterrows():\n",
    "                all_rows.append({\n",
    "                    'alpha': alpha,\n",
    "                    'parent_layer': r['parent_layer'],\n",
    "                    'child_layer':  r['child_layer'],\n",
    "                    'pair_count':   r['pair_count'],\n",
    "                    'simple_avg_js_distance':   r['simple_avg_js_distance'],\n",
    "                    'weighted_avg_js_distance': r['weighted_avg_js_distance'],\n",
    "                    'std_js_distance':          r['std_js_distance'],\n",
    "                    'median_js_distance':       r['median_js_distance'],\n",
    "                    'min_js_distance':          r['min_js_distance'],\n",
    "                    'max_js_distance':          r['max_js_distance'],\n",
    "                    'eta_used':                 r.get('eta_used', np.nan),\n",
    "                    'run_id': run_id,\n",
    "                    'parent_folder': parent_folder\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {f}: {e}\")\n",
    "\n",
    "    if not all_rows:\n",
    "        print(\"No valid data found, ending.\")\n",
    "        return\n",
    "\n",
    "    big = pd.DataFrame(all_rows)\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"Inter-layer JSD summary by ALPHA (weighted + unweighted)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Aggregate by alpha & layer pairs\n",
    "    grouped = big.groupby(['alpha', 'parent_layer', 'child_layer'])\n",
    "    summary = grouped.agg({\n",
    "        'simple_avg_js_distance':   ['mean', 'std', 'count'],\n",
    "        'weighted_avg_js_distance': ['mean', 'std', 'count'],\n",
    "        'median_js_distance':       ['mean', 'std'],\n",
    "        'pair_count':               'mean',\n",
    "    }).round(6)\n",
    "\n",
    "    # Flatten column names\n",
    "    summary.columns = ['_'.join(col) if isinstance(col, tuple) else col for col in summary.columns]\n",
    "    summary = summary.reset_index()\n",
    "\n",
    "    # Save\n",
    "    out_overall = os.path.join(base_path, output_overall)\n",
    "    summary.to_csv(out_overall, index=False)\n",
    "    print(f\"\\nOverall comparison file saved to: {out_overall}\")\n",
    "\n",
    "    # Brief display\n",
    "    for _, row in summary.iterrows():\n",
    "        alpha = row['alpha']\n",
    "        pl, cl = int(row['parent_layer']), int(row['child_layer'])\n",
    "        sa_m, sa_s, sa_n = row['simple_avg_js_distance_mean'], row['simple_avg_js_distance_std'], int(row['simple_avg_js_distance_count'])\n",
    "        wa_m, wa_s, wa_n = row['weighted_avg_js_distance_mean'], row['weighted_avg_js_distance_std'], int(row['weighted_avg_js_distance_count'])\n",
    "        print(f\"  α={alpha} | L{pl}->{cl}: \"\n",
    "              f\"simple mean={sa_m:.4f}±{(sa_s or 0):.4f} (n={sa_n}), \"\n",
    "              f\"weighted mean={wa_m:.4f}±{(wa_s or 0):.4f} (n={wa_n})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd944e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Starting inter-layer Jensen–Shannon distance calculation (weighted version, ln basis, distance)...\n",
      "================================================================================\n",
      "Found 17 word distribution files to process\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[1/17] Calculating inter-layer JSD (weighted version): Alpha=1.0\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 265\n",
      "   Node count: 307\n",
      "   ✓ Completed 307 node probability distributions (eta=0.05)\n",
      "📐 Calculating inter-layer JSD...\n",
      "   Available layers: [0, 1, 2]\n",
      "   Calculating Layer 0 -> Layer 1\n",
      "     📊 Layer 0->1: 64 pairs, simple mean=0.6598, weighted mean=0.6508, σ=0.0161\n",
      "   Calculating Layer 1 -> Layer 2\n",
      "     📊 Layer 1->2: 242 pairs, simple mean=0.6084, weighted mean=0.6265, σ=0.0610\n",
      "💾 Inter-layer JSD results saved:\n",
      "   Detailed results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_3/inter_layer_jensen_shannon_distances_weighted.csv\n",
      "   Summary results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_3/inter_layer_js_summary_weighted.csv\n",
      "📊 Inter-layer JSD layer summary (weighted + unweighted):\n",
      "   L0->1: 64 pairs, simple mean=0.6598, weighted mean=0.6508, median=0.6647, σ=0.0162\n",
      "   L1->2: 242 pairs, simple mean=0.6084, weighted mean=0.6265, median=0.6168, σ=0.0612\n",
      "\n",
      "================================================================================\n",
      "[2/17] Calculating inter-layer JSD (weighted version): Alpha=1.0\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 245\n",
      "   Node count: 321\n",
      "   ✓ Completed 321 node probability distributions (eta=0.05)\n",
      "📐 Calculating inter-layer JSD...\n",
      "   Available layers: [0, 1, 2]\n",
      "   Calculating Layer 0 -> Layer 1\n",
      "     📊 Layer 0->1: 57 pairs, simple mean=0.6583, weighted mean=0.6002, σ=0.0241\n",
      "   Calculating Layer 1 -> Layer 2\n",
      "     📊 Layer 1->2: 263 pairs, simple mean=0.6208, weighted mean=0.6616, σ=0.0699\n",
      "💾 Inter-layer JSD results saved:\n",
      "   Detailed results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_2/inter_layer_jensen_shannon_distances_weighted.csv\n",
      "   Summary results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_2/inter_layer_js_summary_weighted.csv\n",
      "📊 Inter-layer JSD layer summary (weighted + unweighted):\n",
      "   L0->1: 57 pairs, simple mean=0.6583, weighted mean=0.6002, median=0.6611, σ=0.0243\n",
      "   L1->2: 263 pairs, simple mean=0.6208, weighted mean=0.6616, median=0.6185, σ=0.0700\n",
      "\n",
      "================================================================================\n",
      "[3/17] Calculating inter-layer JSD (weighted version): Alpha=0.2\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 170\n",
      "   Node count: 325\n",
      "   ✓ Completed 325 node probability distributions (eta=0.05)\n",
      "📐 Calculating inter-layer JSD...\n",
      "   Available layers: [0, 1, 2]\n",
      "   Calculating Layer 0 -> Layer 1\n",
      "     📊 Layer 0->1: 78 pairs, simple mean=0.6557, weighted mean=0.6401, σ=0.0196\n",
      "   Calculating Layer 1 -> Layer 2\n",
      "     📊 Layer 1->2: 246 pairs, simple mean=0.6028, weighted mean=0.6209, σ=0.0730\n",
      "💾 Inter-layer JSD results saved:\n",
      "   Detailed results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_3/inter_layer_jensen_shannon_distances_weighted.csv\n",
      "   Summary results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_3/inter_layer_js_summary_weighted.csv\n",
      "📊 Inter-layer JSD layer summary (weighted + unweighted):\n",
      "   L0->1: 78 pairs, simple mean=0.6557, weighted mean=0.6401, median=0.6569, σ=0.0197\n",
      "   L1->2: 246 pairs, simple mean=0.6028, weighted mean=0.6209, median=0.6053, σ=0.0731\n",
      "\n",
      "================================================================================\n",
      "[4/17] Calculating inter-layer JSD (weighted version): Alpha=0.2\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 170\n",
      "   Node count: 325\n",
      "   ✓ Completed 325 node probability distributions (eta=0.05)\n",
      "📐 Calculating inter-layer JSD...\n",
      "   Available layers: [0, 1, 2]\n",
      "   Calculating Layer 0 -> Layer 1\n",
      "     📊 Layer 0->1: 67 pairs, simple mean=0.6530, weighted mean=0.6445, σ=0.0194\n",
      "   Calculating Layer 1 -> Layer 2\n",
      "     📊 Layer 1->2: 257 pairs, simple mean=0.6025, weighted mean=0.6503, σ=0.0736\n",
      "💾 Inter-layer JSD results saved:\n",
      "   Detailed results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_1/inter_layer_jensen_shannon_distances_weighted.csv\n",
      "   Summary results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_1/inter_layer_js_summary_weighted.csv\n",
      "📊 Inter-layer JSD layer summary (weighted + unweighted):\n",
      "   L0->1: 67 pairs, simple mean=0.6530, weighted mean=0.6445, median=0.6527, σ=0.0195\n",
      "   L1->2: 257 pairs, simple mean=0.6025, weighted mean=0.6503, median=0.6044, σ=0.0738\n",
      "\n",
      "================================================================================\n",
      "[5/17] Calculating inter-layer JSD (weighted version): Alpha=0.2\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 170\n",
      "   Node count: 333\n",
      "   ✓ Completed 333 node probability distributions (eta=0.05)\n",
      "📐 Calculating inter-layer JSD...\n",
      "   Available layers: [0, 1, 2]\n",
      "   Calculating Layer 0 -> Layer 1\n",
      "     📊 Layer 0->1: 71 pairs, simple mean=0.6522, weighted mean=0.5985, σ=0.0247\n",
      "   Calculating Layer 1 -> Layer 2\n",
      "     📊 Layer 1->2: 261 pairs, simple mean=0.6002, weighted mean=0.6136, σ=0.0726\n",
      "💾 Inter-layer JSD results saved:\n",
      "   Detailed results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_2/inter_layer_jensen_shannon_distances_weighted.csv\n",
      "   Summary results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_2/inter_layer_js_summary_weighted.csv\n",
      "📊 Inter-layer JSD layer summary (weighted + unweighted):\n",
      "   L0->1: 71 pairs, simple mean=0.6522, weighted mean=0.5985, median=0.6515, σ=0.0249\n",
      "   L1->2: 261 pairs, simple mean=0.6002, weighted mean=0.6136, median=0.5956, σ=0.0728\n",
      "\n",
      "================================================================================\n",
      "[6/17] Calculating inter-layer JSD (weighted version): Alpha=0.5\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 275\n",
      "   Node count: 292\n",
      "   ✓ Completed 292 node probability distributions (eta=0.05)\n",
      "📐 Calculating inter-layer JSD...\n",
      "   Available layers: [0, 1, 2]\n",
      "   Calculating Layer 0 -> Layer 1\n",
      "     📊 Layer 0->1: 55 pairs, simple mean=0.6435, weighted mean=0.6006, σ=0.0228\n",
      "   Calculating Layer 1 -> Layer 2\n",
      "     📊 Layer 1->2: 236 pairs, simple mean=0.6128, weighted mean=0.6487, σ=0.0767\n",
      "💾 Inter-layer JSD results saved:\n",
      "   Detailed results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_3/inter_layer_jensen_shannon_distances_weighted.csv\n",
      "   Summary results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_3/inter_layer_js_summary_weighted.csv\n",
      "📊 Inter-layer JSD layer summary (weighted + unweighted):\n",
      "   L0->1: 55 pairs, simple mean=0.6435, weighted mean=0.6006, median=0.6461, σ=0.0230\n",
      "   L1->2: 236 pairs, simple mean=0.6128, weighted mean=0.6487, median=0.6189, σ=0.0769\n",
      "\n",
      "================================================================================\n",
      "[7/17] Calculating inter-layer JSD (weighted version): Alpha=0.5\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 275\n",
      "   Node count: 282\n",
      "   ✓ Completed 282 node probability distributions (eta=0.05)\n",
      "📐 Calculating inter-layer JSD...\n",
      "   Available layers: [0, 1, 2]\n",
      "   Calculating Layer 0 -> Layer 1\n",
      "     📊 Layer 0->1: 45 pairs, simple mean=0.6395, weighted mean=0.5870, σ=0.0365\n",
      "   Calculating Layer 1 -> Layer 2\n",
      "     📊 Layer 1->2: 236 pairs, simple mean=0.6267, weighted mean=0.6581, σ=0.0853\n",
      "💾 Inter-layer JSD results saved:\n",
      "   Detailed results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_1/inter_layer_jensen_shannon_distances_weighted.csv\n",
      "   Summary results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_1/inter_layer_js_summary_weighted.csv\n",
      "📊 Inter-layer JSD layer summary (weighted + unweighted):\n",
      "   L0->1: 45 pairs, simple mean=0.6395, weighted mean=0.5870, median=0.6428, σ=0.0369\n",
      "   L1->2: 236 pairs, simple mean=0.6267, weighted mean=0.6581, median=0.6612, σ=0.0855\n",
      "\n",
      "================================================================================\n",
      "[8/17] Calculating inter-layer JSD (weighted version): Alpha=0.5\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 275\n",
      "   Node count: 292\n",
      "   ✓ Completed 292 node probability distributions (eta=0.05)\n",
      "📐 Calculating inter-layer JSD...\n",
      "   Available layers: [0, 1, 2]\n",
      "   Calculating Layer 0 -> Layer 1\n",
      "     📊 Layer 0->1: 36 pairs, simple mean=0.6535, weighted mean=0.6017, σ=0.0304\n",
      "   Calculating Layer 1 -> Layer 2\n",
      "     📊 Layer 1->2: 255 pairs, simple mean=0.6359, weighted mean=0.6648, σ=0.0724\n",
      "💾 Inter-layer JSD results saved:\n",
      "   Detailed results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_2/inter_layer_jensen_shannon_distances_weighted.csv\n",
      "   Summary results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_2/inter_layer_js_summary_weighted.csv\n",
      "📊 Inter-layer JSD layer summary (weighted + unweighted):\n",
      "   L0->1: 36 pairs, simple mean=0.6535, weighted mean=0.6017, median=0.6571, σ=0.0309\n",
      "   L1->2: 255 pairs, simple mean=0.6359, weighted mean=0.6648, median=0.6568, σ=0.0725\n",
      "\n",
      "================================================================================\n",
      "[9/17] Calculating inter-layer JSD (weighted version): Alpha=0.05\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 175\n",
      "   Node count: 275\n",
      "   ✓ Completed 275 node probability distributions (eta=0.05)\n",
      "📐 Calculating inter-layer JSD...\n",
      "   Available layers: [0, 1, 2]\n",
      "   Calculating Layer 0 -> Layer 1\n",
      "     📊 Layer 0->1: 62 pairs, simple mean=0.6280, weighted mean=0.6119, σ=0.0227\n",
      "   Calculating Layer 1 -> Layer 2\n",
      "     📊 Layer 1->2: 212 pairs, simple mean=0.5650, weighted mean=0.6066, σ=0.0693\n",
      "💾 Inter-layer JSD results saved:\n",
      "   Detailed results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_3/inter_layer_jensen_shannon_distances_weighted.csv\n",
      "   Summary results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_3/inter_layer_js_summary_weighted.csv\n",
      "📊 Inter-layer JSD layer summary (weighted + unweighted):\n",
      "   L0->1: 62 pairs, simple mean=0.6280, weighted mean=0.6119, median=0.6272, σ=0.0229\n",
      "   L1->2: 212 pairs, simple mean=0.5650, weighted mean=0.6066, median=0.5675, σ=0.0694\n",
      "\n",
      "================================================================================\n",
      "[10/17] Calculating inter-layer JSD (weighted version): Alpha=0.05\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 175\n",
      "   Node count: 316\n",
      "   ✓ Completed 316 node probability distributions (eta=0.05)\n",
      "📐 Calculating inter-layer JSD...\n",
      "   Available layers: [0, 1, 2]\n",
      "   Calculating Layer 0 -> Layer 1\n",
      "     📊 Layer 0->1: 60 pairs, simple mean=0.6340, weighted mean=0.5850, σ=0.0285\n",
      "   Calculating Layer 1 -> Layer 2\n",
      "     📊 Layer 1->2: 255 pairs, simple mean=0.5773, weighted mean=0.6132, σ=0.0752\n",
      "💾 Inter-layer JSD results saved:\n",
      "   Detailed results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_1/inter_layer_jensen_shannon_distances_weighted.csv\n",
      "   Summary results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_1/inter_layer_js_summary_weighted.csv\n",
      "📊 Inter-layer JSD layer summary (weighted + unweighted):\n",
      "   L0->1: 60 pairs, simple mean=0.6340, weighted mean=0.5850, median=0.6390, σ=0.0287\n",
      "   L1->2: 255 pairs, simple mean=0.5773, weighted mean=0.6132, median=0.5795, σ=0.0753\n",
      "\n",
      "================================================================================\n",
      "[11/17] Calculating inter-layer JSD (weighted version): Alpha=0.05\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 175\n",
      "   Node count: 305\n",
      "   ✓ Completed 305 node probability distributions (eta=0.05)\n",
      "📐 Calculating inter-layer JSD...\n",
      "   Available layers: [0, 1, 2]\n",
      "   Calculating Layer 0 -> Layer 1\n",
      "     📊 Layer 0->1: 60 pairs, simple mean=0.6289, weighted mean=0.5843, σ=0.0328\n",
      "   Calculating Layer 1 -> Layer 2\n",
      "     📊 Layer 1->2: 244 pairs, simple mean=0.5850, weighted mean=0.6160, σ=0.0711\n",
      "💾 Inter-layer JSD results saved:\n",
      "   Detailed results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_2/inter_layer_jensen_shannon_distances_weighted.csv\n",
      "   Summary results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_2/inter_layer_js_summary_weighted.csv\n",
      "📊 Inter-layer JSD layer summary (weighted + unweighted):\n",
      "   L0->1: 60 pairs, simple mean=0.6289, weighted mean=0.5843, median=0.6299, σ=0.0330\n",
      "   L1->2: 244 pairs, simple mean=0.5850, weighted mean=0.6160, median=0.5877, σ=0.0713\n",
      "\n",
      "================================================================================\n",
      "[12/17] Calculating inter-layer JSD (weighted version): Alpha=0.1\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 90\n",
      "   Node count: 299\n",
      "   ✓ Completed 299 node probability distributions (eta=0.05)\n",
      "📐 Calculating inter-layer JSD...\n",
      "   Available layers: [0, 1, 2]\n",
      "   Calculating Layer 0 -> Layer 1\n",
      "     📊 Layer 0->1: 55 pairs, simple mean=0.6431, weighted mean=0.6434, σ=0.0206\n",
      "   Calculating Layer 1 -> Layer 2\n",
      "     📊 Layer 1->2: 243 pairs, simple mean=0.6005, weighted mean=0.6377, σ=0.0699\n",
      "💾 Inter-layer JSD results saved:\n",
      "   Detailed results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_2/inter_layer_jensen_shannon_distances_weighted.csv\n",
      "   Summary results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_2/inter_layer_js_summary_weighted.csv\n",
      "📊 Inter-layer JSD layer summary (weighted + unweighted):\n",
      "   L0->1: 55 pairs, simple mean=0.6431, weighted mean=0.6434, median=0.6441, σ=0.0208\n",
      "   L1->2: 243 pairs, simple mean=0.6005, weighted mean=0.6377, median=0.6020, σ=0.0701\n",
      "\n",
      "================================================================================\n",
      "[13/17] Calculating inter-layer JSD (weighted version): Alpha=0.1\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 90\n",
      "   Node count: 322\n",
      "   ✓ Completed 322 node probability distributions (eta=0.05)\n",
      "📐 Calculating inter-layer JSD...\n",
      "   Available layers: [0, 1, 2]\n",
      "   Calculating Layer 0 -> Layer 1\n",
      "     📊 Layer 0->1: 59 pairs, simple mean=0.6492, weighted mean=0.6130, σ=0.0221\n",
      "   Calculating Layer 1 -> Layer 2\n",
      "     📊 Layer 1->2: 262 pairs, simple mean=0.5992, weighted mean=0.6300, σ=0.0785\n",
      "💾 Inter-layer JSD results saved:\n",
      "   Detailed results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_3/inter_layer_jensen_shannon_distances_weighted.csv\n",
      "   Summary results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_3/inter_layer_js_summary_weighted.csv\n",
      "📊 Inter-layer JSD layer summary (weighted + unweighted):\n",
      "   L0->1: 59 pairs, simple mean=0.6492, weighted mean=0.6130, median=0.6510, σ=0.0223\n",
      "   L1->2: 262 pairs, simple mean=0.5992, weighted mean=0.6300, median=0.6033, σ=0.0786\n",
      "\n",
      "================================================================================\n",
      "[14/17] Calculating inter-layer JSD (weighted version): Alpha=0.1\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 90\n",
      "   Node count: 315\n",
      "   ✓ Completed 315 node probability distributions (eta=0.05)\n",
      "📐 Calculating inter-layer JSD...\n",
      "   Available layers: [0, 1, 2]\n",
      "   Calculating Layer 0 -> Layer 1\n",
      "     📊 Layer 0->1: 62 pairs, simple mean=0.6460, weighted mean=0.6179, σ=0.0225\n",
      "   Calculating Layer 1 -> Layer 2\n",
      "     📊 Layer 1->2: 252 pairs, simple mean=0.5990, weighted mean=0.6200, σ=0.0765\n",
      "💾 Inter-layer JSD results saved:\n",
      "   Detailed results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1/inter_layer_jensen_shannon_distances_weighted.csv\n",
      "   Summary results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1/inter_layer_js_summary_weighted.csv\n",
      "📊 Inter-layer JSD layer summary (weighted + unweighted):\n",
      "   L0->1: 62 pairs, simple mean=0.6460, weighted mean=0.6179, median=0.6452, σ=0.0226\n",
      "   L1->2: 252 pairs, simple mean=0.5990, weighted mean=0.6200, median=0.6046, σ=0.0766\n",
      "\n",
      "================================================================================\n",
      "[15/17] Calculating inter-layer JSD (weighted version): Alpha=0.01\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 285\n",
      "   Node count: 296\n",
      "   ✓ Completed 296 node probability distributions (eta=0.05)\n",
      "📐 Calculating inter-layer JSD...\n",
      "   Available layers: [0, 1, 2]\n",
      "   Calculating Layer 0 -> Layer 1\n",
      "     📊 Layer 0->1: 66 pairs, simple mean=0.6296, weighted mean=0.5597, σ=0.0400\n",
      "   Calculating Layer 1 -> Layer 2\n",
      "     📊 Layer 1->2: 229 pairs, simple mean=0.5784, weighted mean=0.6003, σ=0.0663\n",
      "💾 Inter-layer JSD results saved:\n",
      "   Detailed results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_3/inter_layer_jensen_shannon_distances_weighted.csv\n",
      "   Summary results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_3/inter_layer_js_summary_weighted.csv\n",
      "📊 Inter-layer JSD layer summary (weighted + unweighted):\n",
      "   L0->1: 66 pairs, simple mean=0.6296, weighted mean=0.5597, median=0.6339, σ=0.0403\n",
      "   L1->2: 229 pairs, simple mean=0.5784, weighted mean=0.6003, median=0.5793, σ=0.0665\n",
      "\n",
      "================================================================================\n",
      "[16/17] Calculating inter-layer JSD (weighted version): Alpha=0.01\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 285\n",
      "   Node count: 313\n",
      "   ✓ Completed 313 node probability distributions (eta=0.05)\n",
      "📐 Calculating inter-layer JSD...\n",
      "   Available layers: [0, 1, 2]\n",
      "   Calculating Layer 0 -> Layer 1\n",
      "     📊 Layer 0->1: 59 pairs, simple mean=0.6292, weighted mean=0.4959, σ=0.0402\n",
      "   Calculating Layer 1 -> Layer 2\n",
      "     📊 Layer 1->2: 253 pairs, simple mean=0.5814, weighted mean=0.6096, σ=0.0724\n",
      "💾 Inter-layer JSD results saved:\n",
      "   Detailed results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_2/inter_layer_jensen_shannon_distances_weighted.csv\n",
      "   Summary results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_2/inter_layer_js_summary_weighted.csv\n",
      "📊 Inter-layer JSD layer summary (weighted + unweighted):\n",
      "   L0->1: 59 pairs, simple mean=0.6292, weighted mean=0.4959, median=0.6363, σ=0.0405\n",
      "   L1->2: 253 pairs, simple mean=0.5814, weighted mean=0.6096, median=0.5867, σ=0.0726\n",
      "\n",
      "================================================================================\n",
      "[17/17] Calculating inter-layer JSD (weighted version): Alpha=0.01\n",
      "================================================================================\n",
      "📊 Basic Information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 285\n",
      "   Node count: 312\n",
      "   ✓ Completed 312 node probability distributions (eta=0.05)\n",
      "📐 Calculating inter-layer JSD...\n",
      "   Available layers: [0, 1, 2]\n",
      "   Calculating Layer 0 -> Layer 1\n",
      "     📊 Layer 0->1: 65 pairs, simple mean=0.6236, weighted mean=0.5564, σ=0.0316\n",
      "   Calculating Layer 1 -> Layer 2\n",
      "     📊 Layer 1->2: 246 pairs, simple mean=0.5657, weighted mean=0.6026, σ=0.0723\n",
      "💾 Inter-layer JSD results saved:\n",
      "   Detailed results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_1/inter_layer_jensen_shannon_distances_weighted.csv\n",
      "   Summary results: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_1/inter_layer_js_summary_weighted.csv\n",
      "📊 Inter-layer JSD layer summary (weighted + unweighted):\n",
      "   L0->1: 65 pairs, simple mean=0.6236, weighted mean=0.5564, median=0.6258, σ=0.0319\n",
      "   L1->2: 246 pairs, simple mean=0.5657, weighted mean=0.6026, median=0.5711, σ=0.0725\n",
      "\n",
      "✅ Inter-layer JSD calculation completed! (weighted version, ln basis)\n"
     ]
    }
   ],
   "source": [
    "calculate_inter_layer_jensen_shannon_distances_weighted(\n",
    "    base_path=\"/Volumes/My Passport/收敛结果/step3\",      # Top-level path\n",
    "    count_filename=\"iteration_node_word_distributions.csv\",\n",
    "    entropy_filename=\"corrected_renyi_entropy.csv\",\n",
    "    count_col=\"count\",                    # Your count field\n",
    "    eta=0.05,                             # Consistent with experiment\n",
    "    default_alpha=0.1,\n",
    "    weight_field_candidates=(\"child_token_count\",\"child_doc_count\"),\n",
    "    output_suffix=\"_weighted\"             # Output filename suffix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b60a537c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Starting aggregation of inter-layer JSD by alpha value (weighted version)...\n",
      "================================================================================\n",
      "🔍 Found 18 inter-layer JSD summary files (weighted)\n",
      "======================================================================\n",
      "Inter-layer JSD summary by ALPHA (weighted + unweighted)\n",
      "======================================================================\n",
      "\n",
      "Overall comparison file saved to: /Volumes/My Passport/收敛结果/step3/alpha_inter_layer_jsd_comparison_weighted.csv\n",
      "  α=0.01 | L0->1: simple mean=0.6275±0.0033 (n=3), weighted mean=0.5373±0.0359 (n=3)\n",
      "  α=0.01 | L1->2: simple mean=0.5752±0.0083 (n=3), weighted mean=0.6042±0.0049 (n=3)\n",
      "  α=0.05 | L0->1: simple mean=0.6303±0.0032 (n=3), weighted mean=0.5938±0.0158 (n=3)\n",
      "  α=0.05 | L1->2: simple mean=0.5758±0.0101 (n=3), weighted mean=0.6119±0.0048 (n=3)\n",
      "  α=0.1 | L0->1: simple mean=0.6461±0.0031 (n=3), weighted mean=0.6247±0.0163 (n=3)\n",
      "  α=0.1 | L1->2: simple mean=0.5995±0.0008 (n=3), weighted mean=0.6292±0.0089 (n=3)\n",
      "  α=0.2 | L0->1: simple mean=0.6536±0.0019 (n=3), weighted mean=0.6277±0.0254 (n=3)\n",
      "  α=0.2 | L1->2: simple mean=0.6018±0.0015 (n=3), weighted mean=0.6282±0.0194 (n=3)\n",
      "  α=0.5 | L0->1: simple mean=0.6455±0.0072 (n=3), weighted mean=0.5964±0.0082 (n=3)\n",
      "  α=0.5 | L1->2: simple mean=0.6251±0.0116 (n=3), weighted mean=0.6572±0.0081 (n=3)\n",
      "  α=1.0 | L0->1: simple mean=0.6595±0.0010 (n=3), weighted mean=0.6327±0.0282 (n=3)\n",
      "  α=1.0 | L1->2: simple mean=0.6137±0.0064 (n=3), weighted mean=0.6446±0.0176 (n=3)\n"
     ]
    }
   ],
   "source": [
    "aggregate_inter_layer_jsd_by_alpha_weighted(\n",
    "    base_path=\"/Volumes/My Passport/收敛结果/step3\",\n",
    "    summary_filename_pattern=\"inter_layer_js_summary_weighted.csv\",\n",
    "    output_overall=\"alpha_inter_layer_jsd_comparison_weighted.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ac939d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "def calculate_standard_coherence_from_corpus_corrected(corpus, word_distributions_df, top_k=15):\n",
    "    \"\"\"\n",
    "    Corrected version: Calculate comprehensive node-level and global-level coherence metrics (adapted for step3)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"📊 Preparing to calculate standard coherence metrics...\")\n",
    "    print(f\"   Corpus document count: {len(corpus)}\")\n",
    "    print(f\"   Node count: {word_distributions_df['node_id'].nunique()}\")\n",
    "    \n",
    "    # 1. Prepare texts and dictionary\n",
    "    texts = list(corpus.values())\n",
    "    dictionary = Dictionary(texts)\n",
    "    \n",
    "    print(f\"   Total documents: {len(texts)}\")\n",
    "    print(f\"   Dictionary size: {len(dictionary)}\")\n",
    "    \n",
    "    # 2. Prepare topics and node mapping\n",
    "    topics = []\n",
    "    node_topic_mapping = {}\n",
    "    node_to_topic_idx = {}  # New: Direct mapping from node ID to topic index\n",
    "    \n",
    "    topic_idx = 0\n",
    "    for node_id in word_distributions_df['node_id'].unique():\n",
    "        node_data = word_distributions_df[word_distributions_df['node_id'] == node_id]\n",
    "        top_words = node_data.nlargest(top_k, 'count')['word'].tolist()\n",
    "        \n",
    "        valid_words = []\n",
    "        for word in top_words:\n",
    "            if pd.notna(word) and word in dictionary.token2id:\n",
    "                valid_words.append(word)\n",
    "        \n",
    "        if len(valid_words) >= 2:\n",
    "            topics.append(valid_words)\n",
    "            node_topic_mapping[node_id] = valid_words\n",
    "            node_to_topic_idx[node_id] = topic_idx  # Direct mapping\n",
    "            topic_idx += 1\n",
    "    \n",
    "    print(f\"   Valid topic count: {len(topics)}\")\n",
    "    \n",
    "    if len(topics) == 0:\n",
    "        return {}, {}, {}\n",
    "    \n",
    "    # 3. Calculate all coherence metrics (global + per-topic)\n",
    "    coherence_measures = ['c_npmi', 'c_v', 'u_mass']\n",
    "    global_coherence = {}\n",
    "    per_topic_coherence = {}\n",
    "    \n",
    "    for measure in coherence_measures:\n",
    "        try:\n",
    "            print(f\"   Calculating {measure}...\")\n",
    "            \n",
    "            cm = CoherenceModel(\n",
    "                topics=topics,\n",
    "                texts=texts,\n",
    "                dictionary=dictionary,\n",
    "                coherence=measure,\n",
    "                processes=1\n",
    "            )\n",
    "            \n",
    "            # Global average coherence\n",
    "            global_score = cm.get_coherence()\n",
    "            global_coherence[measure] = global_score\n",
    "            \n",
    "            # Per-topic coherence\n",
    "            per_topic_scores = cm.get_coherence_per_topic()\n",
    "            per_topic_coherence[measure] = per_topic_scores\n",
    "            \n",
    "            print(f\"   ✓ {measure}: Global={global_score:.4f}, Range=[{min(per_topic_scores):.4f}, {max(per_topic_scores):.4f}]\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error calculating {measure}: {e}\")\n",
    "            global_coherence[measure] = 0.0\n",
    "            per_topic_coherence[measure] = [0.0] * len(topics)\n",
    "    \n",
    "    return global_coherence, per_topic_coherence, node_to_topic_idx\n",
    "\n",
    "def process_coherence_with_original_corpus_corrected_step3(base_path=\".\", corpus=None, top_k=15):\n",
    "    \"\"\"\n",
    "    Corrected version: Comprehensive calculation of node-level and global-level coherence metrics (adapted for step3 alpha parameters)\n",
    "    \"\"\"\n",
    "    \n",
    "    if corpus is None:\n",
    "        print(\"❌ Must provide original corpus\")\n",
    "        return\n",
    "    \n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_node_word_distributions.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    print(f\"🔍 Found {len(files)} word distribution files to process\")\n",
    "    \n",
    "    for idx, file_path in enumerate(files, 1):\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        \n",
    "        # Extract alpha value from folder name (adapted for step3)\n",
    "        alpha = 0.1  # Default value\n",
    "        if 'alpha_' in folder_name:\n",
    "            try:\n",
    "                alpha_part = folder_name.split('alpha_')[1].split('_')[0]\n",
    "                alpha = float(alpha_part)\n",
    "            except (IndexError, ValueError):\n",
    "                # Pattern matching through folder name\n",
    "                if 'a001' in folder_name:\n",
    "                    alpha = 0.01\n",
    "                elif 'a005' in folder_name:\n",
    "                    alpha = 0.05\n",
    "                elif 'a02' in folder_name:\n",
    "                    alpha = 0.2\n",
    "                elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                    alpha = 0.5\n",
    "                elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                    alpha = 1.0\n",
    "                elif 'a01' in folder_name:\n",
    "                    alpha = 0.1\n",
    "        else:\n",
    "            # Pattern matching through folder name\n",
    "            if 'a001' in folder_name:\n",
    "                alpha = 0.01\n",
    "            elif 'a005' in folder_name:\n",
    "                alpha = 0.05\n",
    "            elif 'a02' in folder_name:\n",
    "                alpha = 0.2\n",
    "            elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                alpha = 0.5\n",
    "            elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                alpha = 1.0\n",
    "            elif 'a01' in folder_name:\n",
    "                alpha = 0.1\n",
    "        \n",
    "        # Fixed parameters (adapted for step3)\n",
    "        eta = 0.05  # eta is fixed at 0.05 in step3\n",
    "        gamma = 0.05  # Fixed value\n",
    "        depth = 3  # Fixed value\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"[{idx}/{len(files)}] Processing file: {folder_name}\")\n",
    "        print(f\"Parameters - Alpha: {alpha}, Eta: {eta}, Gamma: {gamma}, Depth: {depth}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            # Read data\n",
    "            df = pd.read_csv(file_path)\n",
    "            df.columns = [col.strip(\"'\\\" \") for col in df.columns]\n",
    "            \n",
    "            max_iteration = df['iteration'].max()\n",
    "            last_iteration_data = df[df['iteration'] == max_iteration]\n",
    "            \n",
    "            print(f\"📈 Last iteration: {max_iteration}\")\n",
    "            print(f\"📈 Node count: {last_iteration_data['node_id'].nunique()}\")\n",
    "            \n",
    "            # Calculate coherence (corrected version)\n",
    "            global_coherence, per_topic_coherence, node_to_topic_idx = calculate_standard_coherence_from_corpus_corrected(\n",
    "                corpus, last_iteration_data, top_k=top_k\n",
    "            )\n",
    "            \n",
    "            if not global_coherence:\n",
    "                print(\"⚠️ Coherence calculation failed, skipping this file\")\n",
    "                continue\n",
    "            \n",
    "            # Prepare data for saving\n",
    "            results_data = []\n",
    "            \n",
    "            for node_id in last_iteration_data['node_id'].unique():\n",
    "                node_words = last_iteration_data[last_iteration_data['node_id'] == node_id]\n",
    "                top_words = node_words.nlargest(top_k, 'count')['word'].tolist()\n",
    "                top_words = [word for word in top_words if pd.notna(word)]\n",
    "                \n",
    "                # Get coherence metrics for this node (corrected version)\n",
    "                node_coherence_scores = {}\n",
    "                \n",
    "                if node_id in node_to_topic_idx:\n",
    "                    # Get metrics directly through index\n",
    "                    topic_idx = node_to_topic_idx[node_id]\n",
    "                    \n",
    "                    for measure in ['c_npmi', 'c_v', 'u_mass']:\n",
    "                        if measure in per_topic_coherence:\n",
    "                            measure_name = measure.replace('c_', '') if measure.startswith('c_') else measure\n",
    "                            node_coherence_scores[f'node_{measure_name}'] = per_topic_coherence[measure][topic_idx]\n",
    "                        else:\n",
    "                            measure_name = measure.replace('c_', '') if measure.startswith('c_') else measure\n",
    "                            node_coherence_scores[f'node_{measure_name}'] = 0.0\n",
    "                else:\n",
    "                    # If node is not in mapping, set to 0\n",
    "                    for measure in ['npmi', 'v', 'u_mass']:\n",
    "                        node_coherence_scores[f'node_{measure}'] = 0.0\n",
    "                \n",
    "                results_data.append({\n",
    "                    'node_id': node_id,\n",
    "                    'alpha': alpha,  # Corrected: use alpha instead of eta\n",
    "                    'eta': eta,  # Record fixed eta value\n",
    "                    'gamma': gamma, \n",
    "                    'depth': depth,\n",
    "                    'top_k': top_k,\n",
    "                    'top_words': ', '.join(top_words[:10]),\n",
    "                    'word_count': len(top_words),\n",
    "                    \n",
    "                    # Node-level coherence metrics (corrected)\n",
    "                    'node_npmi': node_coherence_scores.get('node_npmi', 0.0),\n",
    "                    'node_c_v': node_coherence_scores.get('node_v', 0.0),\n",
    "                    'node_u_mass': node_coherence_scores.get('node_u_mass', 0.0),\n",
    "                    \n",
    "                    # Global-level coherence metrics\n",
    "                    'global_npmi': global_coherence.get('c_npmi', 0.0),\n",
    "                    'global_c_v': global_coherence.get('c_v', 0.0),\n",
    "                    'global_u_mass': global_coherence.get('u_mass', 0.0),\n",
    "                    \n",
    "                    'iteration': max_iteration\n",
    "                })\n",
    "            \n",
    "            # Save results\n",
    "            results_df = pd.DataFrame(results_data)\n",
    "            output_path = os.path.join(folder_path, 'standard_coherence.csv')\n",
    "            results_df.to_csv(output_path, index=False)\n",
    "            \n",
    "            print(f\"💾 Standard coherence results saved to: {output_path}\")\n",
    "            print(f\"📊 Results summary:\")\n",
    "            print(f\"   - Global NPMI: {global_coherence.get('c_npmi', 0.0):.4f}\")\n",
    "            print(f\"   - Global C_V: {global_coherence.get('c_v', 0.0):.4f}\")\n",
    "            print(f\"   - Global U_Mass: {global_coherence.get('u_mass', 0.0):.4f}\")\n",
    "            \n",
    "            # Display node-level metric ranges\n",
    "            if len(results_df) > 0:\n",
    "                print(f\"   - Node NPMI range: [{results_df['node_npmi'].min():.4f}, {results_df['node_npmi'].max():.4f}]\")\n",
    "                print(f\"   - Node C_V range: [{results_df['node_c_v'].min():.4f}, {results_df['node_c_v'].max():.4f}]\")\n",
    "                print(f\"   - Node U_Mass range: [{results_df['node_u_mass'].min():.4f}, {results_df['node_u_mass'].max():.4f}]\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"❌ Error processing file {file_path}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    print(f\"\\n✅ Standard coherence calculation for all files completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc225850",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 0. Setup part: import necessary libraries and set up environment \"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from threading import Thread\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "import json\n",
    "import cProfile\n",
    "\n",
    "# Download NLTK data one time\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# Chinese character support in matplotlib\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']  \n",
    "plt.rcParams['axes.unicode_minus'] = False  \n",
    "\n",
    "\n",
    "\"\"\" 1.1 Data Preprocessing: load data, clean text, lemmatization, remove low-frequency words\"\"\"\n",
    "\n",
    "# Map POS tags to WordNet format, Penn Treebank annotation: fine-grained (45 tags), WordNet annotation: coarse-grained (4 tags: a, v, n, r)\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'  # adjective\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'  # verb\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n'  # noun\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'  # adverb\n",
    "    else:\n",
    "        return 'n'  # default noun\n",
    "\n",
    "# Text cleaning and lemmatization preprocessing function\n",
    "def clean_and_lemmatize(text):\n",
    "    if pd.isnull(text):\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove non-alphabetic characters using regex\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmatized = [lemmatizer.lemmatize(w, get_wordnet_pos(pos)) for w, pos in pos_tags]\n",
    "    return lemmatized  \n",
    "\n",
    "#-----------------Load data----------------\n",
    "data = pd.read_excel('/Volumes/My Passport/收敛结果/step2/papers_CM.xlsx', usecols=['PaperID', 'Abstract', 'Keywords', 'Year'])\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Clean and lemmatize the abstracts\n",
    "data['Lemmatized_Tokens'] = data['Abstract'].apply(clean_and_lemmatize)\n",
    "\n",
    "# Count word frequencies\n",
    "all_tokens = [word for tokens in data['Lemmatized_Tokens'] for word in tokens]\n",
    "word_counts = Counter(all_tokens)\n",
    "\n",
    "# Set a minimum frequency threshold for valid words\n",
    "min_freq = 10\n",
    "valid_words = set([word for word, freq in word_counts.items() if freq >= min_freq])\n",
    "\n",
    "# Remove rare words based on frequency threshold\n",
    "def remove_rare_words(tokens):\n",
    "    return [word for word in tokens if word in valid_words]\n",
    "\n",
    "data['Filtered_Tokens'] = data['Lemmatized_Tokens'].apply(remove_rare_words)\n",
    "\n",
    "# Join tokens back into cleaned abstracts\n",
    "data['Cleaned_Abstract'] = data['Filtered_Tokens'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "# Create a cleaned DataFrame with relevant columns\n",
    "cleaned_data = data[['PaperID', 'Year', 'Cleaned_Abstract']]\n",
    "cleaned_data = cleaned_data[~(cleaned_data['PaperID'] == 57188)] # this paper has no abstract\n",
    "cleaned_data = cleaned_data.reset_index(drop=True) \n",
    "cleaned_data.insert(0, 'Document_ID', range(len(cleaned_data))) \n",
    "abstract_list = cleaned_data['Cleaned_Abstract'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "corpus = {doc_id: abstract_list for doc_id, abstract_list in enumerate(abstract_list)}\n",
    "# cleaned_data.to_csv('./data/processed/cleaned_data.xlsx', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13c96c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🗑️ Cleaning old incomplete files...\n",
      "================================================================================\n",
      "✓ Deleted old file: depth_3_gamma_0.05_eta_0.05_alpha_1_run_3\n",
      "✓ Deleted old file: depth_3_gamma_0.05_eta_0.05_alpha_1_run_2\n",
      "✓ Deleted old file: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_3\n",
      "✓ Deleted old file: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_1\n",
      "✓ Deleted old file: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_2\n",
      "✓ Deleted old file: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_3\n",
      "✓ Deleted old file: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_1\n",
      "✓ Deleted old file: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_2\n",
      "✓ Deleted old file: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_3\n",
      "✓ Deleted old file: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_1\n",
      "✓ Deleted old file: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_2\n",
      "✓ Deleted old file: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_2\n",
      "✓ Deleted old file: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_3\n",
      "✓ Deleted old file: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1\n",
      "✓ Deleted old file: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_3\n",
      "✓ Deleted old file: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_2\n",
      "✓ Deleted old file: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_1\n",
      "🗑️ Total 17 old coherence files deleted\n",
      "\n",
      "================================================================================\n",
      "🔄 Starting recalculation of complete coherence metrics...\n",
      "================================================================================\n",
      "🔍 Found 17 word distribution files to process\n",
      "\n",
      "================================================================================\n",
      "[1/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_1_run_3\n",
      "Parameters - Alpha: 1.0, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 265\n",
      "📈 Node count: 307\n",
      "📊 Preparing to calculate standard coherence metrics...\n",
      "   Corpus document count: 970\n",
      "   Node count: 307\n",
      "   Total documents: 970\n",
      "   Dictionary size: 1490\n",
      "   Valid topic count: 307\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Global=0.0431, Range=[-0.5374, 0.5347]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Global=0.5465, Range=[0.1659, 0.9386]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Global=-2.8831, Range=[-13.0335, -0.5808]\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_3/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0431\n",
      "   - Global C_V: 0.5465\n",
      "   - Global U_Mass: -2.8831\n",
      "   - Node NPMI range: [-0.5374, 0.5347]\n",
      "   - Node C_V range: [0.1659, 0.9386]\n",
      "   - Node U_Mass range: [-13.0335, -0.5808]\n",
      "\n",
      "================================================================================\n",
      "[2/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_1_run_2\n",
      "Parameters - Alpha: 1.0, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 245\n",
      "📈 Node count: 321\n",
      "📊 Preparing to calculate standard coherence metrics...\n",
      "   Corpus document count: 970\n",
      "   Node count: 321\n",
      "   Total documents: 970\n",
      "   Dictionary size: 1490\n",
      "   Valid topic count: 321\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Global=0.0498, Range=[-0.3688, 0.7414]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Global=0.5613, Range=[0.2380, 0.9900]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Global=-2.6670, Range=[-10.5801, -0.2882]\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_2/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0498\n",
      "   - Global C_V: 0.5613\n",
      "   - Global U_Mass: -2.6670\n",
      "   - Node NPMI range: [-0.3688, 0.7414]\n",
      "   - Node C_V range: [0.2380, 0.9900]\n",
      "   - Node U_Mass range: [-10.5801, -0.2882]\n",
      "\n",
      "================================================================================\n",
      "[3/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_3\n",
      "Parameters - Alpha: 0.2, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 170\n",
      "📈 Node count: 325\n",
      "📊 Preparing to calculate standard coherence metrics...\n",
      "   Corpus document count: 970\n",
      "   Node count: 325\n",
      "   Total documents: 970\n",
      "   Dictionary size: 1490\n",
      "   Valid topic count: 325\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Global=0.0497, Range=[-0.4379, 0.4935]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Global=0.5496, Range=[0.1772, 0.9101]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Global=-2.8802, Range=[-11.7456, -0.5808]\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_3/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0497\n",
      "   - Global C_V: 0.5496\n",
      "   - Global U_Mass: -2.8802\n",
      "   - Node NPMI range: [-0.4379, 0.4935]\n",
      "   - Node C_V range: [0.1772, 0.9101]\n",
      "   - Node U_Mass range: [-11.7456, -0.5808]\n",
      "\n",
      "================================================================================\n",
      "[4/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_1\n",
      "Parameters - Alpha: 0.2, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 170\n",
      "📈 Node count: 325\n",
      "📊 Preparing to calculate standard coherence metrics...\n",
      "   Corpus document count: 970\n",
      "   Node count: 325\n",
      "   Total documents: 970\n",
      "   Dictionary size: 1490\n",
      "   Valid topic count: 325\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Global=0.0648, Range=[-0.4024, 0.7459]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Global=0.5670, Range=[0.2064, 0.9893]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Global=-2.7642, Range=[-12.6462, -0.5066]\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_1/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0648\n",
      "   - Global C_V: 0.5670\n",
      "   - Global U_Mass: -2.7642\n",
      "   - Node NPMI range: [-0.4024, 0.7459]\n",
      "   - Node C_V range: [0.2064, 0.9893]\n",
      "   - Node U_Mass range: [-12.6462, -0.5066]\n",
      "\n",
      "================================================================================\n",
      "[5/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_2\n",
      "Parameters - Alpha: 0.2, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 170\n",
      "📈 Node count: 333\n",
      "📊 Preparing to calculate standard coherence metrics...\n",
      "   Corpus document count: 970\n",
      "   Node count: 333\n",
      "   Total documents: 970\n",
      "   Dictionary size: 1490\n",
      "   Valid topic count: 333\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Global=0.0499, Range=[-0.4532, 0.4266]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Global=0.5483, Range=[0.1618, 0.9069]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Global=-2.8537, Range=[-14.9312, -0.5701]\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_2/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0499\n",
      "   - Global C_V: 0.5483\n",
      "   - Global U_Mass: -2.8537\n",
      "   - Node NPMI range: [-0.4532, 0.4266]\n",
      "   - Node C_V range: [0.1618, 0.9069]\n",
      "   - Node U_Mass range: [-14.9312, -0.5701]\n",
      "\n",
      "================================================================================\n",
      "[6/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_3\n",
      "Parameters - Alpha: 0.5, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 275\n",
      "📈 Node count: 292\n",
      "📊 Preparing to calculate standard coherence metrics...\n",
      "   Corpus document count: 970\n",
      "   Node count: 292\n",
      "   Total documents: 970\n",
      "   Dictionary size: 1490\n",
      "   Valid topic count: 292\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Global=0.0541, Range=[-0.4084, 0.7823]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Global=0.5484, Range=[0.1684, 0.9937]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Global=-2.7990, Range=[-11.7283, -0.4761]\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_3/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0541\n",
      "   - Global C_V: 0.5484\n",
      "   - Global U_Mass: -2.7990\n",
      "   - Node NPMI range: [-0.4084, 0.7823]\n",
      "   - Node C_V range: [0.1684, 0.9937]\n",
      "   - Node U_Mass range: [-11.7283, -0.4761]\n",
      "\n",
      "================================================================================\n",
      "[7/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_1\n",
      "Parameters - Alpha: 0.5, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 275\n",
      "📈 Node count: 282\n",
      "📊 Preparing to calculate standard coherence metrics...\n",
      "   Corpus document count: 970\n",
      "   Node count: 282\n",
      "   Total documents: 970\n",
      "   Dictionary size: 1490\n",
      "   Valid topic count: 282\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Global=0.0490, Range=[-0.3266, 0.4322]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Global=0.5431, Range=[0.1668, 0.9101]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Global=-2.9521, Range=[-13.3835, -0.5894]\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_1/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0490\n",
      "   - Global C_V: 0.5431\n",
      "   - Global U_Mass: -2.9521\n",
      "   - Node NPMI range: [-0.3266, 0.4322]\n",
      "   - Node C_V range: [0.1668, 0.9101]\n",
      "   - Node U_Mass range: [-13.3835, -0.5894]\n",
      "\n",
      "================================================================================\n",
      "[8/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_2\n",
      "Parameters - Alpha: 0.5, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 275\n",
      "📈 Node count: 292\n",
      "📊 Preparing to calculate standard coherence metrics...\n",
      "   Corpus document count: 970\n",
      "   Node count: 292\n",
      "   Total documents: 970\n",
      "   Dictionary size: 1490\n",
      "   Valid topic count: 292\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Global=0.0521, Range=[-0.4715, 0.4921]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Global=0.5462, Range=[0.1654, 0.9531]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Global=-2.9811, Range=[-14.8770, -0.7126]\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_2/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0521\n",
      "   - Global C_V: 0.5462\n",
      "   - Global U_Mass: -2.9811\n",
      "   - Node NPMI range: [-0.4715, 0.4921]\n",
      "   - Node C_V range: [0.1654, 0.9531]\n",
      "   - Node U_Mass range: [-14.8770, -0.7126]\n",
      "\n",
      "================================================================================\n",
      "[9/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_3\n",
      "Parameters - Alpha: 0.05, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 175\n",
      "📈 Node count: 275\n",
      "📊 Preparing to calculate standard coherence metrics...\n",
      "   Corpus document count: 970\n",
      "   Node count: 275\n",
      "   Total documents: 970\n",
      "   Dictionary size: 1490\n",
      "   Valid topic count: 275\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Global=0.0617, Range=[-0.3261, 0.4319]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Global=0.5617, Range=[0.2371, 0.9223]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Global=-2.6878, Range=[-8.7231, -0.4796]\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_3/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0617\n",
      "   - Global C_V: 0.5617\n",
      "   - Global U_Mass: -2.6878\n",
      "   - Node NPMI range: [-0.3261, 0.4319]\n",
      "   - Node C_V range: [0.2371, 0.9223]\n",
      "   - Node U_Mass range: [-8.7231, -0.4796]\n",
      "\n",
      "================================================================================\n",
      "[10/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_1\n",
      "Parameters - Alpha: 0.05, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 175\n",
      "📈 Node count: 316\n",
      "📊 Preparing to calculate standard coherence metrics...\n",
      "   Corpus document count: 970\n",
      "   Node count: 316\n",
      "   Total documents: 970\n",
      "   Dictionary size: 1490\n",
      "   Valid topic count: 316\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Global=0.0551, Range=[-0.3203, 0.7459]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Global=0.5463, Range=[0.2145, 0.9893]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Global=-2.7108, Range=[-10.5361, -0.4713]\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_1/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0551\n",
      "   - Global C_V: 0.5463\n",
      "   - Global U_Mass: -2.7108\n",
      "   - Node NPMI range: [-0.3203, 0.7459]\n",
      "   - Node C_V range: [0.2145, 0.9893]\n",
      "   - Node U_Mass range: [-10.5361, -0.4713]\n",
      "\n",
      "================================================================================\n",
      "[11/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_2\n",
      "Parameters - Alpha: 0.05, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 175\n",
      "📈 Node count: 305\n",
      "📊 Preparing to calculate standard coherence metrics...\n",
      "   Corpus document count: 970\n",
      "   Node count: 305\n",
      "   Total documents: 970\n",
      "   Dictionary size: 1490\n",
      "   Valid topic count: 305\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Global=0.0650, Range=[-0.3018, 0.7414]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Global=0.5584, Range=[0.1491, 0.9900]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Global=-2.7776, Range=[-12.9308, -0.2991]\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_2/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0650\n",
      "   - Global C_V: 0.5584\n",
      "   - Global U_Mass: -2.7776\n",
      "   - Node NPMI range: [-0.3018, 0.7414]\n",
      "   - Node C_V range: [0.1491, 0.9900]\n",
      "   - Node U_Mass range: [-12.9308, -0.2991]\n",
      "\n",
      "================================================================================\n",
      "[12/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_2\n",
      "Parameters - Alpha: 0.1, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 90\n",
      "📈 Node count: 299\n",
      "📊 Preparing to calculate standard coherence metrics...\n",
      "   Corpus document count: 970\n",
      "   Node count: 299\n",
      "   Total documents: 970\n",
      "   Dictionary size: 1490\n",
      "   Valid topic count: 299\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Global=0.0626, Range=[-0.3949, 0.4990]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Global=0.5506, Range=[0.1782, 0.9313]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Global=-2.6775, Range=[-13.4126, -0.3905]\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_2/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0626\n",
      "   - Global C_V: 0.5506\n",
      "   - Global U_Mass: -2.6775\n",
      "   - Node NPMI range: [-0.3949, 0.4990]\n",
      "   - Node C_V range: [0.1782, 0.9313]\n",
      "   - Node U_Mass range: [-13.4126, -0.3905]\n",
      "\n",
      "================================================================================\n",
      "[13/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_3\n",
      "Parameters - Alpha: 0.1, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 90\n",
      "📈 Node count: 322\n",
      "📊 Preparing to calculate standard coherence metrics...\n",
      "   Corpus document count: 970\n",
      "   Node count: 322\n",
      "   Total documents: 970\n",
      "   Dictionary size: 1490\n",
      "   Valid topic count: 322\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Global=0.0640, Range=[-0.4658, 0.7414]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Global=0.5491, Range=[0.1757, 0.9900]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Global=-2.8860, Range=[-13.1559, -0.4397]\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_3/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0640\n",
      "   - Global C_V: 0.5491\n",
      "   - Global U_Mass: -2.8860\n",
      "   - Node NPMI range: [-0.4658, 0.7414]\n",
      "   - Node C_V range: [0.1757, 0.9900]\n",
      "   - Node U_Mass range: [-13.1559, -0.4397]\n",
      "\n",
      "================================================================================\n",
      "[14/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1\n",
      "Parameters - Alpha: 0.1, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 90\n",
      "📈 Node count: 315\n",
      "📊 Preparing to calculate standard coherence metrics...\n",
      "   Corpus document count: 970\n",
      "   Node count: 315\n",
      "   Total documents: 970\n",
      "   Dictionary size: 1490\n",
      "   Valid topic count: 315\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Global=0.0474, Range=[-0.3846, 0.5347]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Global=0.5476, Range=[0.2051, 0.9493]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Global=-2.8742, Range=[-13.0123, -0.5701]\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0474\n",
      "   - Global C_V: 0.5476\n",
      "   - Global U_Mass: -2.8742\n",
      "   - Node NPMI range: [-0.3846, 0.5347]\n",
      "   - Node C_V range: [0.2051, 0.9493]\n",
      "   - Node U_Mass range: [-13.0123, -0.5701]\n",
      "\n",
      "================================================================================\n",
      "[15/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_3\n",
      "Parameters - Alpha: 0.01, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 285\n",
      "📈 Node count: 296\n",
      "📊 Preparing to calculate standard coherence metrics...\n",
      "   Corpus document count: 970\n",
      "   Node count: 296\n",
      "   Total documents: 970\n",
      "   Dictionary size: 1490\n",
      "   Valid topic count: 296\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Global=0.0480, Range=[-0.3819, 0.3893]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Global=0.5504, Range=[0.1895, 0.8958]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Global=-2.7904, Range=[-12.3008, -0.5370]\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_3/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0480\n",
      "   - Global C_V: 0.5504\n",
      "   - Global U_Mass: -2.7904\n",
      "   - Node NPMI range: [-0.3819, 0.3893]\n",
      "   - Node C_V range: [0.1895, 0.8958]\n",
      "   - Node U_Mass range: [-12.3008, -0.5370]\n",
      "\n",
      "================================================================================\n",
      "[16/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_2\n",
      "Parameters - Alpha: 0.01, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 285\n",
      "📈 Node count: 313\n",
      "📊 Preparing to calculate standard coherence metrics...\n",
      "   Corpus document count: 970\n",
      "   Node count: 313\n",
      "   Total documents: 970\n",
      "   Dictionary size: 1490\n",
      "   Valid topic count: 313\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Global=0.0680, Range=[-0.3703, 0.7414]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Global=0.5640, Range=[0.1762, 0.9900]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Global=-2.6294, Range=[-11.8439, -0.3369]\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_2/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0680\n",
      "   - Global C_V: 0.5640\n",
      "   - Global U_Mass: -2.6294\n",
      "   - Node NPMI range: [-0.3703, 0.7414]\n",
      "   - Node C_V range: [0.1762, 0.9900]\n",
      "   - Node U_Mass range: [-11.8439, -0.3369]\n",
      "\n",
      "================================================================================\n",
      "[17/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_1\n",
      "Parameters - Alpha: 0.01, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 285\n",
      "📈 Node count: 312\n",
      "📊 Preparing to calculate standard coherence metrics...\n",
      "   Corpus document count: 970\n",
      "   Node count: 312\n",
      "   Total documents: 970\n",
      "   Dictionary size: 1490\n",
      "   Valid topic count: 312\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Global=0.0571, Range=[-0.4547, 0.3556]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Global=0.5489, Range=[0.2105, 0.9292]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Global=-2.7138, Range=[-10.7770, -0.5928]\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_1/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0571\n",
      "   - Global C_V: 0.5489\n",
      "   - Global U_Mass: -2.7138\n",
      "   - Node NPMI range: [-0.4547, 0.3556]\n",
      "   - Node C_V range: [0.2105, 0.9292]\n",
      "   - Node U_Mass range: [-10.7770, -0.5928]\n",
      "\n",
      "✅ Standard coherence calculation for all files completed!\n",
      "================================================================================\n",
      "✅ Corrected version coherence calculation completed!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Delete old incomplete files\n",
    "def clean_old_coherence_files(base_path=\".\"):\n",
    "    \"\"\"Delete old incomplete standard coherence files\"\"\"\n",
    "    pattern = os.path.join(base_path, \"**\", \"standard_coherence.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    deleted_count = 0\n",
    "    for file_path in files:\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            print(f\"✓ Deleted old file: {os.path.basename(os.path.dirname(file_path))}\")\n",
    "            deleted_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Deletion failed: {file_path} - {e}\")\n",
    "    \n",
    "    print(f\"🗑️ Total {deleted_count} old coherence files deleted\")\n",
    "\n",
    "# Execute corrected version calculation\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step3\"\n",
    "top_k = 5\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"🗑️ Cleaning old incomplete files...\")\n",
    "print(\"=\" * 80)\n",
    "clean_old_coherence_files(base_path)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🔄 Starting recalculation of complete coherence metrics...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use corrected version function\n",
    "process_coherence_with_original_corpus_corrected_step3(base_path, corpus, top_k)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"✅ Corrected version coherence calculation completed!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5329afab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def add_layer_and_document_info_to_coherence(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    Add layer and document_count information from corrected_renyi_entropy.csv to standard_coherence.csv\n",
    "    \n",
    "    Parameters:\n",
    "    base_path: str, root directory of result files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find all folders containing standard_coherence.csv\n",
    "    pattern = os.path.join(base_path, \"**\", \"standard_coherence.csv\")\n",
    "    coherence_files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    print(f\"🔍 Found {len(coherence_files)} standard coherence files to process\")\n",
    "    \n",
    "    for idx, coherence_file_path in enumerate(coherence_files, 1):\n",
    "        folder_path = os.path.dirname(coherence_file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"[{idx}/{len(coherence_files)}] Processing folder: {folder_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Check if corresponding corrected_renyi_entropy.csv exists\n",
    "        entropy_file_path = os.path.join(folder_path, 'corrected_renyi_entropy.csv')\n",
    "        \n",
    "        if not os.path.exists(entropy_file_path):\n",
    "            print(f\"⚠️  Corresponding entropy file not found: {entropy_file_path}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Read both files\n",
    "            print(\"📖 Reading files...\")\n",
    "            coherence_df = pd.read_csv(coherence_file_path)\n",
    "            entropy_df = pd.read_csv(entropy_file_path)\n",
    "            \n",
    "            print(f\"   Coherence file: {len(coherence_df)} rows\")\n",
    "            print(f\"   Entropy file: {len(entropy_df)} rows\")\n",
    "            \n",
    "            # Check if layer and document_count columns already exist\n",
    "            existing_cols = coherence_df.columns.tolist()\n",
    "            has_layer = 'layer' in existing_cols\n",
    "            has_doc_count = 'document_count' in existing_cols\n",
    "            \n",
    "            print(f\"   Current columns: {existing_cols}\")\n",
    "            print(f\"   Has layer column: {has_layer}\")\n",
    "            print(f\"   Has document_count column: {has_doc_count}\")\n",
    "            \n",
    "            # Create node_id to layer and document_count mappings\n",
    "            node_layer_map = entropy_df.set_index('node_id')['layer'].to_dict()\n",
    "            node_doc_count_map = entropy_df.set_index('node_id')['document_count'].to_dict()\n",
    "            \n",
    "            print(f\"   Mappable nodes: {len(node_layer_map)}\")\n",
    "            \n",
    "            # Add or update layer column\n",
    "            if not has_layer:\n",
    "                coherence_df['layer'] = coherence_df['node_id'].map(node_layer_map)\n",
    "                print(\"   ✓ Added layer column\")\n",
    "            else:\n",
    "                coherence_df['layer'] = coherence_df['node_id'].map(node_layer_map)\n",
    "                print(\"   ✓ Updated layer column\")\n",
    "            \n",
    "            # Add or update document_count column\n",
    "            if not has_doc_count:\n",
    "                coherence_df['document_count'] = coherence_df['node_id'].map(node_doc_count_map)\n",
    "                print(\"   ✓ Added document_count column\")\n",
    "            else:\n",
    "                coherence_df['document_count'] = coherence_df['node_id'].map(node_doc_count_map)\n",
    "                print(\"   ✓ Updated document_count column\")\n",
    "            \n",
    "            # Check mapping results\n",
    "            layer_null_count = coherence_df['layer'].isnull().sum()\n",
    "            doc_count_null_count = coherence_df['document_count'].isnull().sum()\n",
    "            \n",
    "            if layer_null_count > 0:\n",
    "                print(f\"   ⚠️  {layer_null_count} nodes missing layer information\")\n",
    "            \n",
    "            if doc_count_null_count > 0:\n",
    "                print(f\"   ⚠️  {doc_count_null_count} nodes missing document_count information\")\n",
    "            \n",
    "            # Display layer distribution statistics\n",
    "            layer_stats = coherence_df['layer'].value_counts().sort_index()\n",
    "            print(f\"   📊 Layer distribution: {layer_stats.to_dict()}\")\n",
    "            \n",
    "            # Display document count statistics\n",
    "            doc_stats = coherence_df['document_count'].describe()\n",
    "            print(f\"   📊 Document count statistics:\")\n",
    "            print(f\"      Min: {doc_stats['min']:.0f}\")\n",
    "            print(f\"      Max: {doc_stats['max']:.0f}\")\n",
    "            print(f\"      Mean: {doc_stats['mean']:.1f}\")\n",
    "            \n",
    "            # Save updated file\n",
    "            coherence_df.to_csv(coherence_file_path, index=False)\n",
    "            print(f\"💾 Updated and saved: {coherence_file_path}\")\n",
    "            \n",
    "            # Display updated column structure\n",
    "            updated_cols = coherence_df.columns.tolist()\n",
    "            print(f\"   Updated columns: {updated_cols}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"❌ Error processing file {coherence_file_path}: {str(e)}\")\n",
    "            print(\"Detailed error information:\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    print(f\"\\n✅ Layer and document_count information update completed for all standard coherence files!\")\n",
    "\n",
    "def verify_coherence_files_update(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    Verify update status of standard_coherence.csv files (adapted for step3)\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(base_path, \"**\", \"standard_coherence.csv\")\n",
    "    coherence_files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    print(\"🔍 Verifying update results (Step3):\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    all_have_layer = True\n",
    "    all_have_doc_count = True\n",
    "    \n",
    "    for file_path in coherence_files:\n",
    "        folder_name = os.path.basename(os.path.dirname(file_path))\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            has_layer = 'layer' in df.columns\n",
    "            has_doc_count = 'document_count' in df.columns\n",
    "            \n",
    "            layer_null = df['layer'].isnull().sum() if has_layer else \"No column\"\n",
    "            doc_null = df['document_count'].isnull().sum() if has_doc_count else \"No column\"\n",
    "            \n",
    "            status = \"✅\" if (has_layer and has_doc_count and layer_null == 0 and doc_null == 0) else \"⚠️\"\n",
    "            \n",
    "            print(f\"{status} {folder_name}\")\n",
    "            print(f\"   Layer column: {'Yes' if has_layer else 'No'} (null values: {layer_null})\")\n",
    "            print(f\"   DocCount column: {'Yes' if has_doc_count else 'No'} (null values: {doc_null})\")\n",
    "            \n",
    "            if not has_layer:\n",
    "                all_have_layer = False\n",
    "            if not has_doc_count:\n",
    "                all_have_doc_count = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ {folder_name}: Read failed - {e}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"📋 Summary:\")\n",
    "    print(f\"   Total files: {len(coherence_files)}\")\n",
    "    print(f\"   All have layer column: {'Yes' if all_have_layer else 'No'}\")\n",
    "    print(f\"   All have document_count column: {'Yes' if all_have_doc_count else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71ff855d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Starting to add layer and document_count information to standard_coherence.csv...\n",
      "================================================================================\n",
      "🔍 Found 17 standard coherence files to process\n",
      "\n",
      "================================================================================\n",
      "[1/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_1_run_3\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 307 rows\n",
      "   Entropy file: 307 rows\n",
      "   Current columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 307\n",
      "   ✓ Added layer column\n",
      "   ✓ Added document_count column\n",
      "   📊 Layer distribution: {0: 1, 1: 64, 2: 242}\n",
      "   📊 Document count statistics:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 9.5\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_3/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[2/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_1_run_2\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 321 rows\n",
      "   Entropy file: 321 rows\n",
      "   Current columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 321\n",
      "   ✓ Added layer column\n",
      "   ✓ Added document_count column\n",
      "   📊 Layer distribution: {0: 1, 1: 57, 2: 263}\n",
      "   📊 Document count statistics:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 9.1\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_2/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[3/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_3\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 325 rows\n",
      "   Entropy file: 325 rows\n",
      "   Current columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 325\n",
      "   ✓ Added layer column\n",
      "   ✓ Added document_count column\n",
      "   📊 Layer distribution: {0: 1, 1: 78, 2: 246}\n",
      "   📊 Document count statistics:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 9.0\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_3/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[4/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_1\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 325 rows\n",
      "   Entropy file: 325 rows\n",
      "   Current columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 325\n",
      "   ✓ Added layer column\n",
      "   ✓ Added document_count column\n",
      "   📊 Layer distribution: {0: 1, 1: 67, 2: 257}\n",
      "   📊 Document count statistics:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 9.0\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_1/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[5/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_2\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 333 rows\n",
      "   Entropy file: 333 rows\n",
      "   Current columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 333\n",
      "   ✓ Added layer column\n",
      "   ✓ Added document_count column\n",
      "   📊 Layer distribution: {0: 1, 1: 71, 2: 261}\n",
      "   📊 Document count statistics:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 8.7\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_2/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[6/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_3\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 292 rows\n",
      "   Entropy file: 292 rows\n",
      "   Current columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 292\n",
      "   ✓ Added layer column\n",
      "   ✓ Added document_count column\n",
      "   📊 Layer distribution: {0: 1, 1: 55, 2: 236}\n",
      "   📊 Document count statistics:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 10.0\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_3/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[7/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_1\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 282 rows\n",
      "   Entropy file: 282 rows\n",
      "   Current columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 282\n",
      "   ✓ Added layer column\n",
      "   ✓ Added document_count column\n",
      "   📊 Layer distribution: {0: 1, 1: 45, 2: 236}\n",
      "   📊 Document count statistics:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 10.3\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_1/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[8/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_2\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 292 rows\n",
      "   Entropy file: 292 rows\n",
      "   Current columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 292\n",
      "   ✓ Added layer column\n",
      "   ✓ Added document_count column\n",
      "   📊 Layer distribution: {0: 1, 1: 36, 2: 255}\n",
      "   📊 Document count statistics:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 10.0\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_2/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[9/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_3\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 275 rows\n",
      "   Entropy file: 275 rows\n",
      "   Current columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 275\n",
      "   ✓ Added layer column\n",
      "   ✓ Added document_count column\n",
      "   📊 Layer distribution: {0: 1, 1: 62, 2: 212}\n",
      "   📊 Document count statistics:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 10.6\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_3/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[10/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_1\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 316 rows\n",
      "   Entropy file: 316 rows\n",
      "   Current columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 316\n",
      "   ✓ Added layer column\n",
      "   ✓ Added document_count column\n",
      "   📊 Layer distribution: {0: 1, 1: 60, 2: 255}\n",
      "   📊 Document count statistics:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 9.2\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_1/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[11/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_2\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 305 rows\n",
      "   Entropy file: 305 rows\n",
      "   Current columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 305\n",
      "   ✓ Added layer column\n",
      "   ✓ Added document_count column\n",
      "   📊 Layer distribution: {0: 1, 1: 60, 2: 244}\n",
      "   📊 Document count statistics:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 9.5\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_2/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[12/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_2\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 299 rows\n",
      "   Entropy file: 299 rows\n",
      "   Current columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 299\n",
      "   ✓ Added layer column\n",
      "   ✓ Added document_count column\n",
      "   📊 Layer distribution: {0: 1, 1: 55, 2: 243}\n",
      "   📊 Document count statistics:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 9.7\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_2/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[13/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_3\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 322 rows\n",
      "   Entropy file: 322 rows\n",
      "   Current columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 322\n",
      "   ✓ Added layer column\n",
      "   ✓ Added document_count column\n",
      "   📊 Layer distribution: {0: 1, 1: 59, 2: 262}\n",
      "   📊 Document count statistics:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 9.0\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_3/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[14/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 315 rows\n",
      "   Entropy file: 315 rows\n",
      "   Current columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 315\n",
      "   ✓ Added layer column\n",
      "   ✓ Added document_count column\n",
      "   📊 Layer distribution: {0: 1, 1: 62, 2: 252}\n",
      "   📊 Document count statistics:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 9.2\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[15/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_3\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 296 rows\n",
      "   Entropy file: 296 rows\n",
      "   Current columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 296\n",
      "   ✓ Added layer column\n",
      "   ✓ Added document_count column\n",
      "   📊 Layer distribution: {0: 1, 1: 66, 2: 229}\n",
      "   📊 Document count statistics:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 9.8\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_3/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[16/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_2\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 313 rows\n",
      "   Entropy file: 313 rows\n",
      "   Current columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 313\n",
      "   ✓ Added layer column\n",
      "   ✓ Added document_count column\n",
      "   📊 Layer distribution: {0: 1, 1: 59, 2: 253}\n",
      "   📊 Document count statistics:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 9.3\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_2/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[17/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_1\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 312 rows\n",
      "   Entropy file: 312 rows\n",
      "   Current columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 312\n",
      "   ✓ Added layer column\n",
      "   ✓ Added document_count column\n",
      "   📊 Layer distribution: {0: 1, 1: 65, 2: 246}\n",
      "   📊 Document count statistics:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 9.3\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_1/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'alpha', 'eta', 'gamma', 'depth', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "✅ Layer and document_count information update completed for all standard coherence files!\n",
      "\n",
      "================================================================================\n",
      "Verifying update results...\n",
      "================================================================================\n",
      "🔍 Verifying update results (Step3):\n",
      "================================================================================\n",
      "✅ depth_3_gamma_0.05_eta_0.05_alpha_1_run_3\n",
      "   Layer column: Yes (null values: 0)\n",
      "   DocCount column: Yes (null values: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.05_alpha_1_run_2\n",
      "   Layer column: Yes (null values: 0)\n",
      "   DocCount column: Yes (null values: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_3\n",
      "   Layer column: Yes (null values: 0)\n",
      "   DocCount column: Yes (null values: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_1\n",
      "   Layer column: Yes (null values: 0)\n",
      "   DocCount column: Yes (null values: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_2\n",
      "   Layer column: Yes (null values: 0)\n",
      "   DocCount column: Yes (null values: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_3\n",
      "   Layer column: Yes (null values: 0)\n",
      "   DocCount column: Yes (null values: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_1\n",
      "   Layer column: Yes (null values: 0)\n",
      "   DocCount column: Yes (null values: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_2\n",
      "   Layer column: Yes (null values: 0)\n",
      "   DocCount column: Yes (null values: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_3\n",
      "   Layer column: Yes (null values: 0)\n",
      "   DocCount column: Yes (null values: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_1\n",
      "   Layer column: Yes (null values: 0)\n",
      "   DocCount column: Yes (null values: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_2\n",
      "   Layer column: Yes (null values: 0)\n",
      "   DocCount column: Yes (null values: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_2\n",
      "   Layer column: Yes (null values: 0)\n",
      "   DocCount column: Yes (null values: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_3\n",
      "   Layer column: Yes (null values: 0)\n",
      "   DocCount column: Yes (null values: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1\n",
      "   Layer column: Yes (null values: 0)\n",
      "   DocCount column: Yes (null values: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_3\n",
      "   Layer column: Yes (null values: 0)\n",
      "   DocCount column: Yes (null values: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_2\n",
      "   Layer column: Yes (null values: 0)\n",
      "   DocCount column: Yes (null values: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_1\n",
      "   Layer column: Yes (null values: 0)\n",
      "   DocCount column: Yes (null values: 0)\n",
      "================================================================================\n",
      "📋 Summary:\n",
      "   Total files: 17\n",
      "   All have layer column: Yes\n",
      "   All have document_count column: Yes\n",
      "\n",
      "================================================================================\n",
      "✅ Layer and document_count information addition completed!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Execute update\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step3\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Starting to add layer and document_count information to standard_coherence.csv...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Add layer and document_count information\n",
    "add_layer_and_document_info_to_coherence(base_path)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Verifying update results...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Verify update results\n",
    "verify_coherence_files_update(base_path)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ Layer and document_count information addition completed!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02540932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coherence_layered_analysis(base_path=\".\", corpus=None, top_k=15):\n",
    "    \"\"\"\n",
    "    Calculate node coherence metrics and perform weighted hierarchical analysis (adapted for step3)\n",
    "    \"\"\"\n",
    "    \n",
    "    if corpus is None:\n",
    "        print(\"❌ Must provide original corpus\")\n",
    "        return\n",
    "    \n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_node_word_distributions.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    print(f\"🔍 Found {len(files)} word distribution files to process (top_k={top_k})\")\n",
    "    \n",
    "    for idx, file_path in enumerate(files, 1):\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        \n",
    "        # Corrected: Adapt step3 parameter extraction (mainly alpha parameter)\n",
    "        eta = 0.05  # eta is fixed at 0.05 in step3\n",
    "        gamma = 0.05\n",
    "        depth = 3\n",
    "        alpha = 0.1  # Main varying parameter in step3\n",
    "        \n",
    "        # Extract alpha value from folder name (adapted for step3)\n",
    "        if 'alpha_' in folder_name:\n",
    "            try:\n",
    "                alpha_part = folder_name.split('alpha_')[1].split('_')[0]\n",
    "                alpha = float(alpha_part)\n",
    "            except (IndexError, ValueError):\n",
    "                # Pattern matching through folder name\n",
    "                if 'a001' in folder_name:\n",
    "                    alpha = 0.01\n",
    "                elif 'a005' in folder_name:\n",
    "                    alpha = 0.05\n",
    "                elif 'a02' in folder_name:\n",
    "                    alpha = 0.2\n",
    "                elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                    alpha = 0.5\n",
    "                elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                    alpha = 1.0\n",
    "                elif 'a01' in folder_name:\n",
    "                    alpha = 0.1\n",
    "        else:\n",
    "            # Pattern matching through folder name\n",
    "            if 'a001' in folder_name:\n",
    "                alpha = 0.01\n",
    "            elif 'a005' in folder_name:\n",
    "                alpha = 0.05\n",
    "            elif 'a02' in folder_name:\n",
    "                alpha = 0.2\n",
    "            elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                alpha = 0.5\n",
    "            elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                alpha = 1.0\n",
    "            elif 'a01' in folder_name:\n",
    "                alpha = 0.1\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"[{idx}/{len(files)}] Processing file: {folder_name} (k={top_k})\")\n",
    "        print(f\"Parameters - Alpha: {alpha}, Eta: {eta}, Gamma: {gamma}, Depth: {depth}\")  # Corrected: highlight alpha parameter\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            # Read data\n",
    "            df = pd.read_csv(file_path)\n",
    "            df.columns = [col.strip(\"'\\\" \") for col in df.columns]\n",
    "            \n",
    "            max_iteration = df['iteration'].max()\n",
    "            last_iteration_data = df[df['iteration'] == max_iteration]\n",
    "            \n",
    "            # Read layer and document count information\n",
    "            entropy_file = os.path.join(folder_path, 'corrected_renyi_entropy.csv')\n",
    "            if not os.path.exists(entropy_file):\n",
    "                print(\"⚠️ Entropy file not found, skipping this file\")\n",
    "                continue\n",
    "                \n",
    "            entropy_df = pd.read_csv(entropy_file)\n",
    "            \n",
    "            print(f\"📈 Last iteration: {max_iteration}\")\n",
    "            print(f\"📈 Node count: {last_iteration_data['node_id'].nunique()}\")\n",
    "            \n",
    "            # Calculate node-level coherence (keep only node-level)\n",
    "            texts = list(corpus.values())\n",
    "            dictionary = Dictionary(texts)\n",
    "            \n",
    "            topics = []\n",
    "            node_to_topic_idx = {}\n",
    "            \n",
    "            topic_idx = 0\n",
    "            for node_id in last_iteration_data['node_id'].unique():\n",
    "                node_data = last_iteration_data[last_iteration_data['node_id'] == node_id]\n",
    "                top_words = node_data.nlargest(top_k, 'count')['word'].tolist()\n",
    "                \n",
    "                valid_words = []\n",
    "                for word in top_words:\n",
    "                    if pd.notna(word) and word in dictionary.token2id:\n",
    "                        valid_words.append(word)\n",
    "                \n",
    "                if len(valid_words) >= 2:\n",
    "                    topics.append(valid_words)\n",
    "                    node_to_topic_idx[node_id] = topic_idx\n",
    "                    topic_idx += 1\n",
    "            \n",
    "            if len(topics) == 0:\n",
    "                print(\"⚠️ No valid topics, skipping this file\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate various coherence metrics\n",
    "            coherence_measures = ['c_npmi', 'c_v', 'u_mass']\n",
    "            per_topic_coherence = {}\n",
    "            \n",
    "            for measure in coherence_measures:\n",
    "                try:\n",
    "                    print(f\"   Calculating {measure}...\")\n",
    "                    \n",
    "                    cm = CoherenceModel(\n",
    "                        topics=topics,\n",
    "                        texts=texts,\n",
    "                        dictionary=dictionary,\n",
    "                        coherence=measure,\n",
    "                        processes=1\n",
    "                    )\n",
    "                    \n",
    "                    per_topic_scores = cm.get_coherence_per_topic()\n",
    "                    per_topic_coherence[measure] = per_topic_scores\n",
    "                    \n",
    "                    print(f\"   ✓ {measure}: Range=[{min(per_topic_scores):.4f}, {max(per_topic_scores):.4f}]\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   ❌ Error calculating {measure}: {e}\")\n",
    "                    per_topic_coherence[measure] = [0.0] * len(topics)\n",
    "            \n",
    "            # Merge node-level coherence with layer information\n",
    "            node_coherence_data = []\n",
    "            \n",
    "            for node_id in last_iteration_data['node_id'].unique():\n",
    "                node_words = last_iteration_data[last_iteration_data['node_id'] == node_id]\n",
    "                top_words = node_words.nlargest(top_k, 'count')['word'].tolist()\n",
    "                top_words = [word for word in top_words if pd.notna(word)]\n",
    "                \n",
    "                # Get layer and document count information\n",
    "                node_entropy_info = entropy_df[entropy_df['node_id'] == node_id]\n",
    "                if len(node_entropy_info) > 0:\n",
    "                    layer = node_entropy_info['layer'].iloc[0]\n",
    "                    document_count = node_entropy_info['document_count'].iloc[0]\n",
    "                else:\n",
    "                    layer = -1\n",
    "                    document_count = 0\n",
    "                \n",
    "                # Get node coherence scores\n",
    "                node_coherence_scores = {}\n",
    "                if node_id in node_to_topic_idx:\n",
    "                    topic_idx = node_to_topic_idx[node_id]\n",
    "                    for measure in ['c_npmi', 'c_v', 'u_mass']:\n",
    "                        if measure in per_topic_coherence:\n",
    "                            measure_name = measure.replace('c_', '') if measure.startswith('c_') else measure\n",
    "                            node_coherence_scores[f'node_{measure_name}'] = per_topic_coherence[measure][topic_idx]\n",
    "                        else:\n",
    "                            measure_name = measure.replace('c_', '') if measure.startswith('c_') else measure\n",
    "                            node_coherence_scores[f'node_{measure_name}'] = 0.0\n",
    "                else:\n",
    "                    for measure in ['npmi', 'v', 'u_mass']:\n",
    "                        node_coherence_scores[f'node_{measure}'] = 0.0\n",
    "                \n",
    "                node_coherence_data.append({\n",
    "                    'node_id': node_id,\n",
    "                    'alpha': alpha,  # Corrected: use alpha as main parameter\n",
    "                    'eta': eta,  # Record fixed eta value\n",
    "                    'gamma': gamma, \n",
    "                    'depth': depth,\n",
    "                    'layer': layer,\n",
    "                    'document_count': document_count,\n",
    "                    'top_k': top_k,\n",
    "                    'top_words': ', '.join(top_words[:10]),\n",
    "                    'word_count': len(top_words),\n",
    "                    \n",
    "                    # Keep only node-level coherence metrics\n",
    "                    'node_npmi': node_coherence_scores.get('node_npmi', 0.0),\n",
    "                    'node_c_v': node_coherence_scores.get('node_v', 0.0),\n",
    "                    'node_u_mass': node_coherence_scores.get('node_u_mass', 0.0),\n",
    "                    \n",
    "                    'iteration': max_iteration\n",
    "                })\n",
    "            \n",
    "            # Save node-level coherence results (with k value)\n",
    "            coherence_df = pd.DataFrame(node_coherence_data)\n",
    "            node_output_path = os.path.join(folder_path, f'node_coherence_k{top_k}.csv')\n",
    "            coherence_df.to_csv(node_output_path, index=False)\n",
    "            \n",
    "            # Calculate layer weighted average coherence\n",
    "            layer_coherence_summary = []\n",
    "            \n",
    "            for layer in coherence_df['layer'].unique():\n",
    "                if layer == -1:  # Skip invalid layers\n",
    "                    continue\n",
    "                    \n",
    "                layer_data = coherence_df[coherence_df['layer'] == layer]\n",
    "                total_docs = layer_data['document_count'].sum()\n",
    "                \n",
    "                if total_docs > 0:\n",
    "                    # Document count weighted average\n",
    "                    weighted_npmi = (layer_data['document_count'] * layer_data['node_npmi']).sum() / total_docs\n",
    "                    weighted_c_v = (layer_data['document_count'] * layer_data['node_c_v']).sum() / total_docs\n",
    "                    weighted_u_mass = (layer_data['document_count'] * layer_data['node_u_mass']).sum() / total_docs\n",
    "                    \n",
    "                    # Simple average (unweighted)\n",
    "                    simple_npmi = layer_data['node_npmi'].mean()\n",
    "                    simple_c_v = layer_data['node_c_v'].mean()\n",
    "                    simple_u_mass = layer_data['node_u_mass'].mean()\n",
    "                    \n",
    "                    layer_coherence_summary.append({\n",
    "                        'layer': layer,\n",
    "                        'node_count': len(layer_data),\n",
    "                        'total_documents': total_docs,\n",
    "                        'avg_documents_per_node': total_docs / len(layer_data),\n",
    "                        \n",
    "                        # Document count weighted average coherence\n",
    "                        'weighted_avg_npmi': weighted_npmi,\n",
    "                        'weighted_avg_c_v': weighted_c_v,\n",
    "                        'weighted_avg_u_mass': weighted_u_mass,\n",
    "                        \n",
    "                        # Simple average coherence\n",
    "                        'simple_avg_npmi': simple_npmi,\n",
    "                        'simple_avg_c_v': simple_c_v,\n",
    "                        'simple_avg_u_mass': simple_u_mass,\n",
    "                        \n",
    "                        # Standard deviation\n",
    "                        'std_npmi': layer_data['node_npmi'].std(),\n",
    "                        'std_c_v': layer_data['node_c_v'].std(),\n",
    "                        'std_u_mass': layer_data['node_u_mass'].std(),\n",
    "                        \n",
    "                        'top_k': top_k,  # Add k value record\n",
    "                        'alpha': alpha,  # Corrected: use alpha\n",
    "                        'eta': eta,\n",
    "                        'gamma': gamma,\n",
    "                        'depth': depth\n",
    "                    })\n",
    "            \n",
    "            # Save layer summary results (with k value)\n",
    "            if layer_coherence_summary:\n",
    "                layer_summary_df = pd.DataFrame(layer_coherence_summary)\n",
    "                layer_output_path = os.path.join(folder_path, f'layer_coherence_summary_k{top_k}.csv')\n",
    "                layer_summary_df.to_csv(layer_output_path, index=False)\n",
    "                \n",
    "                print(f\"💾 Node coherence results saved to: {node_output_path}\")\n",
    "                print(f\"💾 Layer summary results saved to: {layer_output_path}\")\n",
    "                \n",
    "                print(f\"📊 Layer coherence summary (k={top_k}):\")\n",
    "                for _, row in layer_summary_df.iterrows():\n",
    "                    layer_num = int(row['layer'])\n",
    "                    node_count = int(row['node_count'])\n",
    "                    w_npmi = row['weighted_avg_npmi']\n",
    "                    w_cv = row['weighted_avg_c_v']\n",
    "                    w_umass = row['weighted_avg_u_mass']\n",
    "                    print(f\"   Layer {layer_num} ({node_count} nodes): NPMI={w_npmi:.4f}, C_V={w_cv:.4f}, U_Mass={w_umass:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"❌ Error processing file {file_path}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    print(f\"\\n✅ Coherence layered analysis completed for all files! (k={top_k})\")\n",
    "\n",
    "def aggregate_coherence_by_alpha(base_path=\".\", top_k=15):\n",
    "    \"\"\"\n",
    "    Aggregate coherence statistics by alpha value for each layer (adapted for step3)\n",
    "    \"\"\"\n",
    "    # Find all layer_coherence_summary_k{top_k}.csv files\n",
    "    pattern = os.path.join(base_path, \"**\", f\"layer_coherence_summary_k{top_k}.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    print(f\"🔍 Search pattern: layer_coherence_summary_k{top_k}.csv\")\n",
    "    print(f\"🔍 Found {len(files)} layer summary files\")\n",
    "    \n",
    "    all_data = []\n",
    "    alpha_groups = {}\n",
    "    \n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        parent_folder = os.path.dirname(folder_path)\n",
    "        \n",
    "        # Corrected: Extract alpha value (adapted for step3)\n",
    "        alpha = None\n",
    "        if 'alpha_' in folder_name:\n",
    "            try:\n",
    "                alpha_part = folder_name.split('alpha_')[1].split('_')[0]\n",
    "                alpha = float(alpha_part)\n",
    "            except:\n",
    "                # Pattern matching through folder name\n",
    "                if 'a001' in folder_name:\n",
    "                    alpha = 0.01\n",
    "                elif 'a005' in folder_name:\n",
    "                    alpha = 0.05\n",
    "                elif 'a02' in folder_name:\n",
    "                    alpha = 0.2\n",
    "                elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                    alpha = 0.5\n",
    "                elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                    alpha = 1.0\n",
    "                elif 'a01' in folder_name:\n",
    "                    alpha = 0.1\n",
    "        else:\n",
    "            # Pattern matching through folder name\n",
    "            if 'a001' in folder_name:\n",
    "                alpha = 0.01\n",
    "            elif 'a005' in folder_name:\n",
    "                alpha = 0.05\n",
    "            elif 'a02' in folder_name:\n",
    "                alpha = 0.2\n",
    "            elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                alpha = 0.5\n",
    "            elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                alpha = 1.0\n",
    "            elif 'a01' in folder_name:\n",
    "                alpha = 0.1\n",
    "        \n",
    "        if alpha is None:\n",
    "            continue\n",
    "        \n",
    "        # Extract run number\n",
    "        run_match = folder_name.split('_run_')\n",
    "        if len(run_match) > 1:\n",
    "            run_id = run_match[1]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if alpha not in alpha_groups:\n",
    "            alpha_groups[alpha] = parent_folder\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                all_data.append({\n",
    "                    'alpha': alpha,  # Corrected: use alpha\n",
    "                    'run_id': run_id,\n",
    "                    'layer': row['layer'],\n",
    "                    'node_count': row['node_count'],\n",
    "                    'total_documents': row['total_documents'],\n",
    "                    'weighted_avg_npmi': row['weighted_avg_npmi'],\n",
    "                    'weighted_avg_c_v': row['weighted_avg_c_v'],\n",
    "                    'weighted_avg_u_mass': row['weighted_avg_u_mass'],\n",
    "                    'simple_avg_npmi': row['simple_avg_npmi'],\n",
    "                    'simple_avg_c_v': row['simple_avg_c_v'],\n",
    "                    'simple_avg_u_mass': row['simple_avg_u_mass'],\n",
    "                    'top_k': top_k,  # Add k value record\n",
    "                    'parent_folder': parent_folder\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_path}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame and aggregate by alpha groups\n",
    "    summary_df = pd.DataFrame(all_data)\n",
    "    \n",
    "    if summary_df.empty:\n",
    "        print(\"No valid data found\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Coherence layer summary statistics by ALPHA value (k={top_k})\")  # Corrected: display ALPHA\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Generate summary files grouped by alpha\n",
    "    for alpha, group_data in summary_df.groupby('alpha'):\n",
    "        parent_folder = group_data['parent_folder'].iloc[0]\n",
    "        \n",
    "        print(f\"\\nProcessing Alpha={alpha} (k={top_k})\")  # Corrected: display Alpha\n",
    "        \n",
    "        layer_summary = group_data.groupby('layer').agg({\n",
    "            'weighted_avg_npmi': ['mean', 'std', 'count'],\n",
    "            'weighted_avg_c_v': ['mean', 'std', 'count'],\n",
    "            'weighted_avg_u_mass': ['mean', 'std', 'count'],\n",
    "            'simple_avg_npmi': ['mean', 'std'],\n",
    "            'simple_avg_c_v': ['mean', 'std'],\n",
    "            'simple_avg_u_mass': ['mean', 'std'],\n",
    "            'node_count': 'mean',\n",
    "            'total_documents': 'mean',\n",
    "            'run_id': lambda x: ', '.join(sorted(x.unique()))\n",
    "        }).round(4)\n",
    "        \n",
    "        # Flatten column names\n",
    "        layer_summary.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in layer_summary.columns]\n",
    "        layer_summary = layer_summary.reset_index()\n",
    "        layer_summary.insert(0, 'alpha', alpha)  # Corrected: use alpha\n",
    "        layer_summary.insert(1, 'top_k', top_k)  # Add k value column\n",
    "        \n",
    "        # Save summary results (filename includes k value and alpha)\n",
    "        output_filename = f'alpha_{alpha}_coherence_layer_summary_k{top_k}.csv'  # Corrected: use alpha\n",
    "        output_path = os.path.join(parent_folder, output_filename)\n",
    "        layer_summary.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"  Summary file saved: {output_path}\")\n",
    "        print(f\"  Number of layers: {len(layer_summary)}\")\n",
    "        \n",
    "        # Display brief statistics\n",
    "        for _, row in layer_summary.iterrows():\n",
    "            layer_num = int(row['layer'])\n",
    "            w_npmi = row['weighted_avg_npmi_mean']\n",
    "            w_cv = row['weighted_avg_c_v_mean']\n",
    "            w_umass = row['weighted_avg_u_mass_mean']\n",
    "            run_count = int(row['weighted_avg_npmi_count'])\n",
    "            \n",
    "            print(f\"    Layer {layer_num}: W_NPMI={w_npmi:.4f}, W_C_V={w_cv:.4f}, W_U_Mass={w_umass:.4f}, runs={run_count}\")\n",
    "    \n",
    "    # Generate overall comparison file (filename includes k value and alpha)\n",
    "    overall_summary = summary_df.groupby(['alpha', 'layer']).agg({\n",
    "        'weighted_avg_npmi': ['mean', 'std'],\n",
    "        'weighted_avg_c_v': ['mean', 'std'],\n",
    "        'weighted_avg_u_mass': ['mean', 'std'],\n",
    "        'run_id': 'count'\n",
    "    }).round(4)\n",
    "    \n",
    "    overall_summary.columns = ['_'.join(col).strip() for col in overall_summary.columns]\n",
    "    overall_summary = overall_summary.reset_index()\n",
    "    overall_summary.insert(2, 'top_k', top_k)  # Add k value column\n",
    "    \n",
    "    overall_output_path = os.path.join(base_path, f'alpha_coherence_layer_comparison_k{top_k}.csv')  # Corrected: use alpha\n",
    "    overall_summary.to_csv(overall_output_path, index=False)\n",
    "    print(f\"\\nOverall comparison file saved: {overall_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3803f4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Starting node coherence metric calculation and hierarchical analysis (k=5)...\n",
      "================================================================================\n",
      "🔍 Found 17 word distribution files to process (top_k=5)\n",
      "\n",
      "================================================================================\n",
      "[1/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_1_run_3 (k=5)\n",
      "Parameters - Alpha: 1.0, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 265\n",
      "📈 Node count: 307\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.5374, 0.5347]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.1659, 0.9386]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-13.0335, -0.5808]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_3/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_3/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=-0.0013, C_V=0.4948, U_Mass=-0.5808\n",
      "   Layer 2 (242 nodes): NPMI=0.0459, C_V=0.5337, U_Mass=-2.7186\n",
      "   Layer 1 (64 nodes): NPMI=0.0520, C_V=0.5971, U_Mass=-2.1536\n",
      "\n",
      "================================================================================\n",
      "[2/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_1_run_2 (k=5)\n",
      "Parameters - Alpha: 1.0, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 245\n",
      "📈 Node count: 321\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.3688, 0.7414]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.2380, 0.9900]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-10.5801, -0.2882]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_2/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_2/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=0.0051, C_V=0.5174, U_Mass=-0.5701\n",
      "   Layer 1 (57 nodes): NPMI=0.0945, C_V=0.5761, U_Mass=-1.4951\n",
      "   Layer 2 (263 nodes): NPMI=0.0587, C_V=0.5625, U_Mass=-2.7607\n",
      "\n",
      "================================================================================\n",
      "[3/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_3 (k=5)\n",
      "Parameters - Alpha: 0.2, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 170\n",
      "📈 Node count: 325\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.4379, 0.4935]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.1772, 0.9101]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-11.7456, -0.5808]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_3/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_3/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=-0.0013, C_V=0.4948, U_Mass=-0.5808\n",
      "   Layer 1 (78 nodes): NPMI=0.0631, C_V=0.5850, U_Mass=-2.3658\n",
      "   Layer 2 (246 nodes): NPMI=0.0569, C_V=0.5549, U_Mass=-2.9771\n",
      "\n",
      "================================================================================\n",
      "[4/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_1 (k=5)\n",
      "Parameters - Alpha: 0.2, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 170\n",
      "📈 Node count: 325\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.4024, 0.7459]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.2064, 0.9893]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-12.6462, -0.5066]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_1/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_1/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=0.0139, C_V=0.4857, U_Mass=-0.6050\n",
      "   Layer 1 (67 nodes): NPMI=0.0654, C_V=0.6025, U_Mass=-2.2885\n",
      "   Layer 2 (257 nodes): NPMI=0.0813, C_V=0.5837, U_Mass=-2.7334\n",
      "\n",
      "================================================================================\n",
      "[5/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_2 (k=5)\n",
      "Parameters - Alpha: 0.2, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 170\n",
      "📈 Node count: 333\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.4532, 0.4266]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.1618, 0.9069]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-14.9312, -0.5701]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_2/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_2/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=0.0051, C_V=0.5174, U_Mass=-0.5701\n",
      "   Layer 1 (71 nodes): NPMI=0.0863, C_V=0.5722, U_Mass=-1.9301\n",
      "   Layer 2 (261 nodes): NPMI=0.0521, C_V=0.5511, U_Mass=-2.9342\n",
      "\n",
      "================================================================================\n",
      "[6/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_3 (k=5)\n",
      "Parameters - Alpha: 0.5, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 275\n",
      "📈 Node count: 292\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.4084, 0.7823]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.1684, 0.9937]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-11.7283, -0.4761]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_3/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_3/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=0.0139, C_V=0.4857, U_Mass=-0.5917\n",
      "   Layer 1 (55 nodes): NPMI=0.0167, C_V=0.5636, U_Mass=-2.0311\n",
      "   Layer 2 (236 nodes): NPMI=0.0677, C_V=0.5526, U_Mass=-2.6573\n",
      "\n",
      "================================================================================\n",
      "[7/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_1 (k=5)\n",
      "Parameters - Alpha: 0.5, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 275\n",
      "📈 Node count: 282\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.3266, 0.4322]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.1668, 0.9101]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-13.3835, -0.5894]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_1/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_1/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=-0.0100, C_V=0.4994, U_Mass=-0.7126\n",
      "   Layer 1 (45 nodes): NPMI=0.0448, C_V=0.5497, U_Mass=-1.3259\n",
      "   Layer 2 (236 nodes): NPMI=0.0625, C_V=0.5538, U_Mass=-3.0883\n",
      "\n",
      "================================================================================\n",
      "[8/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_2 (k=5)\n",
      "Parameters - Alpha: 0.5, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 275\n",
      "📈 Node count: 292\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.4715, 0.4921]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.1654, 0.9531]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-14.8770, -0.7126]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_2/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_2/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=-0.0100, C_V=0.4994, U_Mass=-0.7126\n",
      "   Layer 1 (36 nodes): NPMI=0.1076, C_V=0.6168, U_Mass=-1.2974\n",
      "   Layer 2 (255 nodes): NPMI=0.0621, C_V=0.5484, U_Mass=-2.9437\n",
      "\n",
      "================================================================================\n",
      "[9/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_3 (k=5)\n",
      "Parameters - Alpha: 0.05, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 175\n",
      "📈 Node count: 275\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.3261, 0.4319]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.2371, 0.9223]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-8.7231, -0.4796]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_3/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_3/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=-0.0013, C_V=0.4948, U_Mass=-0.5686\n",
      "   Layer 1 (62 nodes): NPMI=0.0551, C_V=0.6201, U_Mass=-2.0433\n",
      "   Layer 2 (212 nodes): NPMI=0.0540, C_V=0.5328, U_Mass=-2.2633\n",
      "\n",
      "================================================================================\n",
      "[10/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_1 (k=5)\n",
      "Parameters - Alpha: 0.05, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 175\n",
      "📈 Node count: 316\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.3203, 0.7459]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.2145, 0.9893]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-10.5361, -0.4713]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_1/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_1/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=0.0002, C_V=0.4710, U_Mass=-0.6608\n",
      "   Layer 1 (60 nodes): NPMI=0.0311, C_V=0.5869, U_Mass=-2.2510\n",
      "   Layer 2 (255 nodes): NPMI=0.0788, C_V=0.5674, U_Mass=-2.5941\n",
      "\n",
      "================================================================================\n",
      "[11/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_2 (k=5)\n",
      "Parameters - Alpha: 0.05, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 175\n",
      "📈 Node count: 305\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.3018, 0.7414]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.1491, 0.9900]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-12.9308, -0.2991]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_2/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_2/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=0.0139, C_V=0.4857, U_Mass=-0.5922\n",
      "   Layer 1 (60 nodes): NPMI=0.0487, C_V=0.5499, U_Mass=-1.5640\n",
      "   Layer 2 (244 nodes): NPMI=0.0785, C_V=0.5843, U_Mass=-2.6713\n",
      "\n",
      "================================================================================\n",
      "[12/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_2 (k=5)\n",
      "Parameters - Alpha: 0.1, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 90\n",
      "📈 Node count: 299\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.3949, 0.4990]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.1782, 0.9313]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-13.4126, -0.3905]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_2/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_2/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=0.0139, C_V=0.4857, U_Mass=-0.6050\n",
      "   Layer 1 (55 nodes): NPMI=0.0297, C_V=0.5589, U_Mass=-2.0098\n",
      "   Layer 2 (243 nodes): NPMI=0.0879, C_V=0.5786, U_Mass=-2.5301\n",
      "\n",
      "================================================================================\n",
      "[13/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_3 (k=5)\n",
      "Parameters - Alpha: 0.1, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 90\n",
      "📈 Node count: 322\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.4658, 0.7414]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.1757, 0.9900]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-13.1559, -0.4397]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_3/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_3/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=0.0139, C_V=0.4857, U_Mass=-0.5941\n",
      "   Layer 2 (262 nodes): NPMI=0.0886, C_V=0.5748, U_Mass=-2.7801\n",
      "   Layer 1 (59 nodes): NPMI=0.0712, C_V=0.5415, U_Mass=-2.3718\n",
      "\n",
      "================================================================================\n",
      "[14/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1 (k=5)\n",
      "Parameters - Alpha: 0.1, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 90\n",
      "📈 Node count: 315\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.3846, 0.5347]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.2051, 0.9493]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-13.0123, -0.5701]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=0.0051, C_V=0.5174, U_Mass=-0.5701\n",
      "   Layer 1 (62 nodes): NPMI=0.0709, C_V=0.5708, U_Mass=-2.1004\n",
      "   Layer 2 (252 nodes): NPMI=0.0534, C_V=0.5571, U_Mass=-3.0668\n",
      "\n",
      "================================================================================\n",
      "[15/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_3 (k=5)\n",
      "Parameters - Alpha: 0.01, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 285\n",
      "📈 Node count: 296\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.3819, 0.3893]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.1895, 0.8958]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-12.3008, -0.5370]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_3/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_3/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=-0.0013, C_V=0.4948, U_Mass=-0.5686\n",
      "   Layer 1 (66 nodes): NPMI=0.0486, C_V=0.5432, U_Mass=-1.7730\n",
      "   Layer 2 (229 nodes): NPMI=0.0537, C_V=0.5632, U_Mass=-2.6929\n",
      "\n",
      "================================================================================\n",
      "[16/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_2 (k=5)\n",
      "Parameters - Alpha: 0.01, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 285\n",
      "📈 Node count: 313\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.3703, 0.7414]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.1762, 0.9900]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-11.8439, -0.3369]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_2/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_2/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=0.0139, C_V=0.4857, U_Mass=-0.5928\n",
      "   Layer 1 (59 nodes): NPMI=0.0376, C_V=0.5289, U_Mass=-1.4274\n",
      "   Layer 2 (253 nodes): NPMI=0.0790, C_V=0.5876, U_Mass=-2.5811\n",
      "\n",
      "================================================================================\n",
      "[17/17] Processing file: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_1 (k=5)\n",
      "Parameters - Alpha: 0.01, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 285\n",
      "📈 Node count: 312\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.4547, 0.3556]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.2105, 0.9292]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-10.7770, -0.5928]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_1/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_1/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=0.0139, C_V=0.4857, U_Mass=-0.5928\n",
      "   Layer 1 (65 nodes): NPMI=0.0104, C_V=0.5165, U_Mass=-2.0456\n",
      "   Layer 2 (246 nodes): NPMI=0.0611, C_V=0.5420, U_Mass=-2.5840\n",
      "\n",
      "✅ Coherence layered analysis completed for all files! (k=5)\n",
      "\n",
      "================================================================================\n",
      "Starting aggregation of layer coherence statistics by eta value (k=5)...\n",
      "================================================================================\n",
      "🔍 Search pattern: layer_coherence_summary_k5.csv\n",
      "🔍 Found 17 layer summary files\n",
      "======================================================================\n",
      "Coherence layer summary statistics by ALPHA value (k=5)\n",
      "======================================================================\n",
      "\n",
      "Processing Alpha=0.01 (k=5)\n",
      "  Summary file saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/alpha_0.01_coherence_layer_summary_k5.csv\n",
      "  Number of layers: 3\n",
      "    Layer 0: W_NPMI=0.0088, W_C_V=0.4888, W_U_Mass=-0.5848, runs=3\n",
      "    Layer 1: W_NPMI=0.0322, W_C_V=0.5295, W_U_Mass=-1.7487, runs=3\n",
      "    Layer 2: W_NPMI=0.0646, W_C_V=0.5643, W_U_Mass=-2.6193, runs=3\n",
      "\n",
      "Processing Alpha=0.05 (k=5)\n",
      "  Summary file saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/alpha_0.05_coherence_layer_summary_k5.csv\n",
      "  Number of layers: 3\n",
      "    Layer 0: W_NPMI=0.0043, W_C_V=0.4838, W_U_Mass=-0.6072, runs=3\n",
      "    Layer 1: W_NPMI=0.0450, W_C_V=0.5856, W_U_Mass=-1.9528, runs=3\n",
      "    Layer 2: W_NPMI=0.0705, W_C_V=0.5615, W_U_Mass=-2.5096, runs=3\n",
      "\n",
      "Processing Alpha=0.1 (k=5)\n",
      "  Summary file saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/alpha_0.1_coherence_layer_summary_k5.csv\n",
      "  Number of layers: 3\n",
      "    Layer 0: W_NPMI=0.0110, W_C_V=0.4963, W_U_Mass=-0.5897, runs=3\n",
      "    Layer 1: W_NPMI=0.0573, W_C_V=0.5571, W_U_Mass=-2.1607, runs=3\n",
      "    Layer 2: W_NPMI=0.0766, W_C_V=0.5701, W_U_Mass=-2.7923, runs=3\n",
      "\n",
      "Processing Alpha=0.2 (k=5)\n",
      "  Summary file saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/alpha_0.2_coherence_layer_summary_k5.csv\n",
      "  Number of layers: 3\n",
      "    Layer 0: W_NPMI=0.0059, W_C_V=0.4993, W_U_Mass=-0.5853, runs=3\n",
      "    Layer 1: W_NPMI=0.0716, W_C_V=0.5866, W_U_Mass=-2.1948, runs=3\n",
      "    Layer 2: W_NPMI=0.0635, W_C_V=0.5633, W_U_Mass=-2.8816, runs=3\n",
      "\n",
      "Processing Alpha=0.5 (k=5)\n",
      "  Summary file saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/alpha_0.5_coherence_layer_summary_k5.csv\n",
      "  Number of layers: 3\n",
      "    Layer 0: W_NPMI=-0.0020, W_C_V=0.4948, W_U_Mass=-0.6723, runs=3\n",
      "    Layer 1: W_NPMI=0.0563, W_C_V=0.5767, W_U_Mass=-1.5515, runs=3\n",
      "    Layer 2: W_NPMI=0.0641, W_C_V=0.5516, W_U_Mass=-2.8964, runs=3\n",
      "\n",
      "Processing Alpha=1.0 (k=5)\n",
      "  Summary file saved: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/alpha_1.0_coherence_layer_summary_k5.csv\n",
      "  Number of layers: 3\n",
      "    Layer 0: W_NPMI=0.0019, W_C_V=0.5061, W_U_Mass=-0.5754, runs=2\n",
      "    Layer 1: W_NPMI=0.0732, W_C_V=0.5866, W_U_Mass=-1.8243, runs=2\n",
      "    Layer 2: W_NPMI=0.0523, W_C_V=0.5481, W_U_Mass=-2.7396, runs=2\n",
      "\n",
      "Overall comparison file saved: /Volumes/My Passport/收敛结果/step3/alpha_coherence_layer_comparison_k5.csv\n",
      "================================================================================\n",
      "✅ Coherence hierarchical analysis completed! (k=5)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Execute streamlined coherence hierarchical analysis (filename includes k value)\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step3\"\n",
    "top_k = 5\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Starting node coherence metric calculation and hierarchical analysis (k={top_k})...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate node coherence and layer summary\n",
    "calculate_coherence_layered_analysis(base_path, corpus, top_k)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Starting aggregation of layer coherence statistics by eta value (k={top_k})...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Aggregate by eta (pass top_k parameter)\n",
    "aggregate_coherence_by_alpha(base_path, top_k)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"✅ Coherence hierarchical analysis completed! (k={top_k})\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4ccd44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f1602ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Starting Step3 perplexity calculation...\n",
      "================================================================================\n",
      "📊 Dataset split:\n",
      "   Total documents: 970\n",
      "   Training set: 776 documents\n",
      "   Test set: 194 documents\n",
      "🔍 Found 17 model result files to process\n",
      "\n",
      "================================================================================\n",
      "[1/17] Calculating perplexity: depth_3_gamma_0.05_eta_0.05_alpha_1_run_3\n",
      "Parameters - Alpha: 1.0, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 265\n",
      "📈 Node count: 307\n",
      "📈 Path mapping count: 970\n",
      "🔄 Starting perplexity calculation...\n",
      "   Test document count: 194\n",
      "   Path mapping count: 970\n",
      "   Smoothing parameter: 0.05\n",
      "   Vocabulary size: 1490\n",
      "   Built word distributions for 307 nodes\n",
      "✅ Perplexity calculation completed:\n",
      "   Matched documents: 194/194\n",
      "   Valid documents: 194\n",
      "   Total words: 16508\n",
      "   Perplexity: 1087.6382\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_3/perplexity_results_step3.csv\n",
      "📊 Perplexity results:\n",
      "   - Overall perplexity: 1087.6382\n",
      "   - Average document perplexity: 1569.2452\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test documents: 194/194\n",
      "\n",
      "================================================================================\n",
      "[2/17] Calculating perplexity: depth_3_gamma_0.05_eta_0.05_alpha_1_run_2\n",
      "Parameters - Alpha: 1.0, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 245\n",
      "📈 Node count: 321\n",
      "📈 Path mapping count: 970\n",
      "🔄 Starting perplexity calculation...\n",
      "   Test document count: 194\n",
      "   Path mapping count: 970\n",
      "   Smoothing parameter: 0.05\n",
      "   Vocabulary size: 1490\n",
      "   Built word distributions for 321 nodes\n",
      "✅ Perplexity calculation completed:\n",
      "   Matched documents: 194/194\n",
      "   Valid documents: 194\n",
      "   Total words: 16508\n",
      "   Perplexity: 865.2915\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_2/perplexity_results_step3.csv\n",
      "📊 Perplexity results:\n",
      "   - Overall perplexity: 865.2915\n",
      "   - Average document perplexity: 1187.1261\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test documents: 194/194\n",
      "\n",
      "================================================================================\n",
      "[3/17] Calculating perplexity: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_3\n",
      "Parameters - Alpha: 0.2, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 170\n",
      "📈 Node count: 325\n",
      "📈 Path mapping count: 970\n",
      "🔄 Starting perplexity calculation...\n",
      "   Test document count: 194\n",
      "   Path mapping count: 970\n",
      "   Smoothing parameter: 0.05\n",
      "   Vocabulary size: 1490\n",
      "   Built word distributions for 325 nodes\n",
      "✅ Perplexity calculation completed:\n",
      "   Matched documents: 194/194\n",
      "   Valid documents: 194\n",
      "   Total words: 16508\n",
      "   Perplexity: 920.8281\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_3/perplexity_results_step3.csv\n",
      "📊 Perplexity results:\n",
      "   - Overall perplexity: 920.8281\n",
      "   - Average document perplexity: 1376.2314\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test documents: 194/194\n",
      "\n",
      "================================================================================\n",
      "[4/17] Calculating perplexity: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_1\n",
      "Parameters - Alpha: 0.2, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 170\n",
      "📈 Node count: 325\n",
      "📈 Path mapping count: 970\n",
      "🔄 Starting perplexity calculation...\n",
      "   Test document count: 194\n",
      "   Path mapping count: 970\n",
      "   Smoothing parameter: 0.05\n",
      "   Vocabulary size: 1490\n",
      "   Built word distributions for 325 nodes\n",
      "✅ Perplexity calculation completed:\n",
      "   Matched documents: 194/194\n",
      "   Valid documents: 194\n",
      "   Total words: 16508\n",
      "   Perplexity: 838.6609\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_1/perplexity_results_step3.csv\n",
      "📊 Perplexity results:\n",
      "   - Overall perplexity: 838.6609\n",
      "   - Average document perplexity: 1211.7534\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test documents: 194/194\n",
      "\n",
      "================================================================================\n",
      "[5/17] Calculating perplexity: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_2\n",
      "Parameters - Alpha: 0.2, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 170\n",
      "📈 Node count: 333\n",
      "📈 Path mapping count: 970\n",
      "🔄 Starting perplexity calculation...\n",
      "   Test document count: 194\n",
      "   Path mapping count: 970\n",
      "   Smoothing parameter: 0.05\n",
      "   Vocabulary size: 1490\n",
      "   Built word distributions for 333 nodes\n",
      "✅ Perplexity calculation completed:\n",
      "   Matched documents: 194/194\n",
      "   Valid documents: 194\n",
      "   Total words: 16508\n",
      "   Perplexity: 882.0026\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_2/perplexity_results_step3.csv\n",
      "📊 Perplexity results:\n",
      "   - Overall perplexity: 882.0026\n",
      "   - Average document perplexity: 1209.3833\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test documents: 194/194\n",
      "\n",
      "================================================================================\n",
      "[6/17] Calculating perplexity: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_3\n",
      "Parameters - Alpha: 0.5, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 275\n",
      "📈 Node count: 292\n",
      "📈 Path mapping count: 970\n",
      "🔄 Starting perplexity calculation...\n",
      "   Test document count: 194\n",
      "   Path mapping count: 970\n",
      "   Smoothing parameter: 0.05\n",
      "   Vocabulary size: 1490\n",
      "   Built word distributions for 292 nodes\n",
      "✅ Perplexity calculation completed:\n",
      "   Matched documents: 194/194\n",
      "   Valid documents: 194\n",
      "   Total words: 16508\n",
      "   Perplexity: 1123.9603\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_3/perplexity_results_step3.csv\n",
      "📊 Perplexity results:\n",
      "   - Overall perplexity: 1123.9603\n",
      "   - Average document perplexity: 1874.6313\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test documents: 194/194\n",
      "\n",
      "================================================================================\n",
      "[7/17] Calculating perplexity: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_1\n",
      "Parameters - Alpha: 0.5, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 275\n",
      "📈 Node count: 282\n",
      "📈 Path mapping count: 970\n",
      "🔄 Starting perplexity calculation...\n",
      "   Test document count: 194\n",
      "   Path mapping count: 970\n",
      "   Smoothing parameter: 0.05\n",
      "   Vocabulary size: 1490\n",
      "   Built word distributions for 282 nodes\n",
      "✅ Perplexity calculation completed:\n",
      "   Matched documents: 194/194\n",
      "   Valid documents: 194\n",
      "   Total words: 16508\n",
      "   Perplexity: 930.5546\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_1/perplexity_results_step3.csv\n",
      "📊 Perplexity results:\n",
      "   - Overall perplexity: 930.5546\n",
      "   - Average document perplexity: 1245.8512\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test documents: 194/194\n",
      "\n",
      "================================================================================\n",
      "[8/17] Calculating perplexity: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_2\n",
      "Parameters - Alpha: 0.5, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 275\n",
      "📈 Node count: 292\n",
      "📈 Path mapping count: 970\n",
      "🔄 Starting perplexity calculation...\n",
      "   Test document count: 194\n",
      "   Path mapping count: 970\n",
      "   Smoothing parameter: 0.05\n",
      "   Vocabulary size: 1490\n",
      "   Built word distributions for 292 nodes\n",
      "✅ Perplexity calculation completed:\n",
      "   Matched documents: 194/194\n",
      "   Valid documents: 194\n",
      "   Total words: 16508\n",
      "   Perplexity: 845.5838\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_2/perplexity_results_step3.csv\n",
      "📊 Perplexity results:\n",
      "   - Overall perplexity: 845.5838\n",
      "   - Average document perplexity: 1132.1705\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test documents: 194/194\n",
      "\n",
      "================================================================================\n",
      "[9/17] Calculating perplexity: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_3\n",
      "Parameters - Alpha: 0.05, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 175\n",
      "📈 Node count: 275\n",
      "📈 Path mapping count: 970\n",
      "🔄 Starting perplexity calculation...\n",
      "   Test document count: 194\n",
      "   Path mapping count: 970\n",
      "   Smoothing parameter: 0.05\n",
      "   Vocabulary size: 1490\n",
      "   Built word distributions for 275 nodes\n",
      "✅ Perplexity calculation completed:\n",
      "   Matched documents: 194/194\n",
      "   Valid documents: 194\n",
      "   Total words: 16508\n",
      "   Perplexity: 895.4895\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_3/perplexity_results_step3.csv\n",
      "📊 Perplexity results:\n",
      "   - Overall perplexity: 895.4895\n",
      "   - Average document perplexity: 1421.5232\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test documents: 194/194\n",
      "\n",
      "================================================================================\n",
      "[10/17] Calculating perplexity: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_1\n",
      "Parameters - Alpha: 0.05, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 175\n",
      "📈 Node count: 316\n",
      "📈 Path mapping count: 970\n",
      "🔄 Starting perplexity calculation...\n",
      "   Test document count: 194\n",
      "   Path mapping count: 970\n",
      "   Smoothing parameter: 0.05\n",
      "   Vocabulary size: 1490\n",
      "   Built word distributions for 316 nodes\n",
      "✅ Perplexity calculation completed:\n",
      "   Matched documents: 194/194\n",
      "   Valid documents: 194\n",
      "   Total words: 16508\n",
      "   Perplexity: 876.3937\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_1/perplexity_results_step3.csv\n",
      "📊 Perplexity results:\n",
      "   - Overall perplexity: 876.3937\n",
      "   - Average document perplexity: 1413.9298\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test documents: 194/194\n",
      "\n",
      "================================================================================\n",
      "[11/17] Calculating perplexity: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_2\n",
      "Parameters - Alpha: 0.05, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 175\n",
      "📈 Node count: 305\n",
      "📈 Path mapping count: 970\n",
      "🔄 Starting perplexity calculation...\n",
      "   Test document count: 194\n",
      "   Path mapping count: 970\n",
      "   Smoothing parameter: 0.05\n",
      "   Vocabulary size: 1490\n",
      "   Built word distributions for 305 nodes\n",
      "✅ Perplexity calculation completed:\n",
      "   Matched documents: 194/194\n",
      "   Valid documents: 194\n",
      "   Total words: 16508\n",
      "   Perplexity: 1099.8439\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_2/perplexity_results_step3.csv\n",
      "📊 Perplexity results:\n",
      "   - Overall perplexity: 1099.8439\n",
      "   - Average document perplexity: 1778.1760\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test documents: 194/194\n",
      "\n",
      "================================================================================\n",
      "[12/17] Calculating perplexity: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_2\n",
      "Parameters - Alpha: 0.1, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 90\n",
      "📈 Node count: 299\n",
      "📈 Path mapping count: 970\n",
      "🔄 Starting perplexity calculation...\n",
      "   Test document count: 194\n",
      "   Path mapping count: 970\n",
      "   Smoothing parameter: 0.05\n",
      "   Vocabulary size: 1490\n",
      "   Built word distributions for 299 nodes\n",
      "✅ Perplexity calculation completed:\n",
      "   Matched documents: 194/194\n",
      "   Valid documents: 194\n",
      "   Total words: 16508\n",
      "   Perplexity: 846.0183\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_2/perplexity_results_step3.csv\n",
      "📊 Perplexity results:\n",
      "   - Overall perplexity: 846.0183\n",
      "   - Average document perplexity: 1248.2811\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test documents: 194/194\n",
      "\n",
      "================================================================================\n",
      "[13/17] Calculating perplexity: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_3\n",
      "Parameters - Alpha: 0.1, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 90\n",
      "📈 Node count: 322\n",
      "📈 Path mapping count: 970\n",
      "🔄 Starting perplexity calculation...\n",
      "   Test document count: 194\n",
      "   Path mapping count: 970\n",
      "   Smoothing parameter: 0.05\n",
      "   Vocabulary size: 1490\n",
      "   Built word distributions for 322 nodes\n",
      "✅ Perplexity calculation completed:\n",
      "   Matched documents: 194/194\n",
      "   Valid documents: 194\n",
      "   Total words: 16508\n",
      "   Perplexity: 859.2048\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_3/perplexity_results_step3.csv\n",
      "📊 Perplexity results:\n",
      "   - Overall perplexity: 859.2048\n",
      "   - Average document perplexity: 1175.9878\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test documents: 194/194\n",
      "\n",
      "================================================================================\n",
      "[14/17] Calculating perplexity: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1\n",
      "Parameters - Alpha: 0.1, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 90\n",
      "📈 Node count: 315\n",
      "📈 Path mapping count: 970\n",
      "🔄 Starting perplexity calculation...\n",
      "   Test document count: 194\n",
      "   Path mapping count: 970\n",
      "   Smoothing parameter: 0.05\n",
      "   Vocabulary size: 1490\n",
      "   Built word distributions for 315 nodes\n",
      "✅ Perplexity calculation completed:\n",
      "   Matched documents: 194/194\n",
      "   Valid documents: 194\n",
      "   Total words: 16508\n",
      "   Perplexity: 880.3369\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1/perplexity_results_step3.csv\n",
      "📊 Perplexity results:\n",
      "   - Overall perplexity: 880.3369\n",
      "   - Average document perplexity: 1247.5163\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test documents: 194/194\n",
      "\n",
      "================================================================================\n",
      "[15/17] Calculating perplexity: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_3\n",
      "Parameters - Alpha: 0.01, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 285\n",
      "📈 Node count: 296\n",
      "📈 Path mapping count: 970\n",
      "🔄 Starting perplexity calculation...\n",
      "   Test document count: 194\n",
      "   Path mapping count: 970\n",
      "   Smoothing parameter: 0.05\n",
      "   Vocabulary size: 1490\n",
      "   Built word distributions for 296 nodes\n",
      "✅ Perplexity calculation completed:\n",
      "   Matched documents: 194/194\n",
      "   Valid documents: 194\n",
      "   Total words: 16508\n",
      "   Perplexity: 950.8714\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_3/perplexity_results_step3.csv\n",
      "📊 Perplexity results:\n",
      "   - Overall perplexity: 950.8714\n",
      "   - Average document perplexity: 1655.6105\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test documents: 194/194\n",
      "\n",
      "================================================================================\n",
      "[16/17] Calculating perplexity: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_2\n",
      "Parameters - Alpha: 0.01, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 285\n",
      "📈 Node count: 313\n",
      "📈 Path mapping count: 970\n",
      "🔄 Starting perplexity calculation...\n",
      "   Test document count: 194\n",
      "   Path mapping count: 970\n",
      "   Smoothing parameter: 0.05\n",
      "   Vocabulary size: 1490\n",
      "   Built word distributions for 313 nodes\n",
      "✅ Perplexity calculation completed:\n",
      "   Matched documents: 194/194\n",
      "   Valid documents: 194\n",
      "   Total words: 16508\n",
      "   Perplexity: 852.6347\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_2/perplexity_results_step3.csv\n",
      "📊 Perplexity results:\n",
      "   - Overall perplexity: 852.6347\n",
      "   - Average document perplexity: 1313.8139\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test documents: 194/194\n",
      "\n",
      "================================================================================\n",
      "[17/17] Calculating perplexity: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_1\n",
      "Parameters - Alpha: 0.01, Eta: 0.05, Gamma: 0.05, Depth: 3\n",
      "================================================================================\n",
      "📈 Last iteration: 285\n",
      "📈 Node count: 312\n",
      "📈 Path mapping count: 970\n",
      "🔄 Starting perplexity calculation...\n",
      "   Test document count: 194\n",
      "   Path mapping count: 970\n",
      "   Smoothing parameter: 0.05\n",
      "   Vocabulary size: 1490\n",
      "   Built word distributions for 312 nodes\n",
      "✅ Perplexity calculation completed:\n",
      "   Matched documents: 194/194\n",
      "   Valid documents: 194\n",
      "   Total words: 16508\n",
      "   Perplexity: 886.3878\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_1/perplexity_results_step3.csv\n",
      "📊 Perplexity results:\n",
      "   - Overall perplexity: 886.3878\n",
      "   - Average document perplexity: 1446.6561\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test documents: 194/194\n",
      "\n",
      "✅ Perplexity calculation for all files completed! (Step3)\n",
      "\n",
      "================================================================================\n",
      "Starting aggregation of perplexity statistics by alpha value...\n",
      "================================================================================\n",
      "🔍 Found 17 Step3 perplexity result files\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.05_alpha_1_run_3 - 1 rows of data\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.05_alpha_1_run_2 - 1 rows of data\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_3 - 1 rows of data\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_1 - 1 rows of data\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_2 - 1 rows of data\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_3 - 1 rows of data\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_1 - 1 rows of data\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_2 - 1 rows of data\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_3 - 1 rows of data\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_1 - 1 rows of data\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_2 - 1 rows of data\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_2 - 1 rows of data\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_3 - 1 rows of data\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1 - 1 rows of data\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_3 - 1 rows of data\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_2 - 1 rows of data\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_1 - 1 rows of data\n",
      "📊 Data summary:\n",
      "   Total data rows: 17\n",
      "   Unique alpha values: [0.01, 0.05, 0.1, 0.2, 0.5, 1.0]\n",
      "   Data count per alpha: {0.01: 3, 0.05: 3, 0.1: 3, 0.2: 3, 0.5: 3, 1.0: 2}\n",
      "================================================================================\n",
      "Perplexity summary statistics by ALPHA value (Step3)\n",
      "================================================================================\n",
      "\n",
      "Processing Alpha=0.01\n",
      "Output directory: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001\n",
      "Data count for this group: 3\n",
      "   Aggregation dictionary: ['perplexity', 'avg_doc_perplexity', 'valid_test_docs', 'total_test_words', 'doc_match_rate', 'avg_path_length', 'log_likelihood', 'run_id', 'eta', 'gamma', 'depth']\n",
      "❌ Error processing Alpha=0.01: no results\n",
      "\n",
      "Processing Alpha=0.05\n",
      "Output directory: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005\n",
      "Data count for this group: 3\n",
      "   Aggregation dictionary: ['perplexity', 'avg_doc_perplexity', 'valid_test_docs', 'total_test_words', 'doc_match_rate', 'avg_path_length', 'log_likelihood', 'run_id', 'eta', 'gamma', 'depth']\n",
      "❌ Error processing Alpha=0.05: no results\n",
      "\n",
      "Processing Alpha=0.1\n",
      "Output directory: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01\n",
      "Data count for this group: 3\n",
      "   Aggregation dictionary: ['perplexity', 'avg_doc_perplexity', 'valid_test_docs', 'total_test_words', 'doc_match_rate', 'avg_path_length', 'log_likelihood', 'run_id', 'eta', 'gamma', 'depth']\n",
      "❌ Error processing Alpha=0.1: no results\n",
      "\n",
      "Processing Alpha=0.2\n",
      "Output directory: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02\n",
      "Data count for this group: 3\n",
      "   Aggregation dictionary: ['perplexity', 'avg_doc_perplexity', 'valid_test_docs', 'total_test_words', 'doc_match_rate', 'avg_path_length', 'log_likelihood', 'run_id', 'eta', 'gamma', 'depth']\n",
      "❌ Error processing Alpha=0.2: no results\n",
      "\n",
      "Processing Alpha=0.5\n",
      "Output directory: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05\n",
      "Data count for this group: 3\n",
      "   Aggregation dictionary: ['perplexity', 'avg_doc_perplexity', 'valid_test_docs', 'total_test_words', 'doc_match_rate', 'avg_path_length', 'log_likelihood', 'run_id', 'eta', 'gamma', 'depth']\n",
      "❌ Error processing Alpha=0.5: no results\n",
      "\n",
      "Processing Alpha=1.0\n",
      "Output directory: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1\n",
      "Data count for this group: 2\n",
      "   Aggregation dictionary: ['perplexity', 'avg_doc_perplexity', 'valid_test_docs', 'total_test_words', 'doc_match_rate', 'avg_path_length', 'log_likelihood', 'run_id', 'eta', 'gamma', 'depth']\n",
      "❌ Error processing Alpha=1.0: no results\n",
      "\n",
      "================================================================================\n",
      "Generating overall comparison file\n",
      "================================================================================\n",
      "✓ Overall comparison file saved to: /Volumes/My Passport/收敛结果/step3/alpha_perplexity_comparison.csv\n",
      "\n",
      "Cross-Alpha Perplexity Comparison:\n",
      "Alpha Value     Average Perplexity(±std)     Run Count\n",
      "--------------------------------------------------\n",
      "  0.010    896.6313(±49.9130)           3\n",
      "  0.050    957.2424(±123.8651)           3\n",
      "  0.100    861.8533(±17.3119)           3\n",
      "  0.200    880.4972(±41.1043)           3\n",
      "  0.500    966.6996(±142.6647)           3\n",
      "  1.000    976.4649(±157.2229)           2\n",
      "================================================================================\n",
      "✅ Step3 perplexity calculation and aggregation completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/v5/6mdkg5713kxgwg5xs24g8rvr0000gn/T/ipykernel_55394/1702905664.py\", line 464, in aggregate_perplexity_by_alpha_step3\n",
      "    alpha_summary = group_data.agg(agg_dict).round(4)\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/frame.py\", line 9342, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 776, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 172, in agg\n",
      "    return self.agg_dict_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 504, in agg_dict_like\n",
      "    results = {\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 505, in <dictcomp>\n",
      "    key: obj._gotitem(key, ndim=1).agg(how) for key, how in arg.items()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/series.py\", line 4605, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 1126, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 175, in agg\n",
      "    return self.agg_list_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 438, in agg_list_like\n",
      "    raise ValueError(\"no results\")\n",
      "ValueError: no results\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/v5/6mdkg5713kxgwg5xs24g8rvr0000gn/T/ipykernel_55394/1702905664.py\", line 464, in aggregate_perplexity_by_alpha_step3\n",
      "    alpha_summary = group_data.agg(agg_dict).round(4)\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/frame.py\", line 9342, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 776, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 172, in agg\n",
      "    return self.agg_dict_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 504, in agg_dict_like\n",
      "    results = {\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 505, in <dictcomp>\n",
      "    key: obj._gotitem(key, ndim=1).agg(how) for key, how in arg.items()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/series.py\", line 4605, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 1126, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 175, in agg\n",
      "    return self.agg_list_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 438, in agg_list_like\n",
      "    raise ValueError(\"no results\")\n",
      "ValueError: no results\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/v5/6mdkg5713kxgwg5xs24g8rvr0000gn/T/ipykernel_55394/1702905664.py\", line 464, in aggregate_perplexity_by_alpha_step3\n",
      "    alpha_summary = group_data.agg(agg_dict).round(4)\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/frame.py\", line 9342, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 776, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 172, in agg\n",
      "    return self.agg_dict_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 504, in agg_dict_like\n",
      "    results = {\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 505, in <dictcomp>\n",
      "    key: obj._gotitem(key, ndim=1).agg(how) for key, how in arg.items()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/series.py\", line 4605, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 1126, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 175, in agg\n",
      "    return self.agg_list_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 438, in agg_list_like\n",
      "    raise ValueError(\"no results\")\n",
      "ValueError: no results\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/v5/6mdkg5713kxgwg5xs24g8rvr0000gn/T/ipykernel_55394/1702905664.py\", line 464, in aggregate_perplexity_by_alpha_step3\n",
      "    alpha_summary = group_data.agg(agg_dict).round(4)\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/frame.py\", line 9342, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 776, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 172, in agg\n",
      "    return self.agg_dict_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 504, in agg_dict_like\n",
      "    results = {\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 505, in <dictcomp>\n",
      "    key: obj._gotitem(key, ndim=1).agg(how) for key, how in arg.items()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/series.py\", line 4605, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 1126, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 175, in agg\n",
      "    return self.agg_list_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 438, in agg_list_like\n",
      "    raise ValueError(\"no results\")\n",
      "ValueError: no results\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/v5/6mdkg5713kxgwg5xs24g8rvr0000gn/T/ipykernel_55394/1702905664.py\", line 464, in aggregate_perplexity_by_alpha_step3\n",
      "    alpha_summary = group_data.agg(agg_dict).round(4)\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/frame.py\", line 9342, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 776, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 172, in agg\n",
      "    return self.agg_dict_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 504, in agg_dict_like\n",
      "    results = {\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 505, in <dictcomp>\n",
      "    key: obj._gotitem(key, ndim=1).agg(how) for key, how in arg.items()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/series.py\", line 4605, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 1126, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 175, in agg\n",
      "    return self.agg_list_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 438, in agg_list_like\n",
      "    raise ValueError(\"no results\")\n",
      "ValueError: no results\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/v5/6mdkg5713kxgwg5xs24g8rvr0000gn/T/ipykernel_55394/1702905664.py\", line 464, in aggregate_perplexity_by_alpha_step3\n",
      "    alpha_summary = group_data.agg(agg_dict).round(4)\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/frame.py\", line 9342, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 776, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 172, in agg\n",
      "    return self.agg_dict_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 504, in agg_dict_like\n",
      "    results = {\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 505, in <dictcomp>\n",
      "    key: obj._gotitem(key, ndim=1).agg(how) for key, how in arg.items()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/series.py\", line 4605, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 1126, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 175, in agg\n",
      "    return self.agg_list_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 438, in agg_list_like\n",
      "    raise ValueError(\"no results\")\n",
      "ValueError: no results\n"
     ]
    }
   ],
   "source": [
    "# First run complete perplexity calculation (adapted for step3)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def compute_perplexity_with_path_mapping_fixed(word_data, path_mapping_data, corpus, test_doc_ids, eta_smoothing=0.05):\n",
    "    \"\"\"\n",
    "    Corrected version: Calculate perplexity using path mapping\n",
    "    \n",
    "    Parameters:\n",
    "    word_data: DataFrame, node word distribution data\n",
    "    path_mapping_data: DataFrame, document path mapping data\n",
    "    corpus: dict, original corpus {doc_id: [words]}\n",
    "    test_doc_ids: list, test document ID list\n",
    "    eta_smoothing: float, smoothing parameter\n",
    "    \n",
    "    Returns:\n",
    "    dict: perplexity calculation results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🔄 Starting perplexity calculation...\")\n",
    "    print(f\"   Test document count: {len(test_doc_ids)}\")\n",
    "    print(f\"   Path mapping count: {len(path_mapping_data)}\")\n",
    "    print(f\"   Smoothing parameter: {eta_smoothing}\")\n",
    "    \n",
    "    # Get vocabulary\n",
    "    vocabulary = sorted(list(word_data['word'].dropna().unique()))\n",
    "    vocab_size = len(vocabulary)\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "    \n",
    "    print(f\"   Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    # Build node word distribution dictionary\n",
    "    node_word_dist = {}\n",
    "    for node_id in word_data['node_id'].unique():\n",
    "        node_words = word_data[word_data['node_id'] == node_id]\n",
    "        \n",
    "        # Initialize count vector\n",
    "        counts = np.zeros(vocab_size)\n",
    "        \n",
    "        # Fill word counts\n",
    "        for _, row in node_words.iterrows():\n",
    "            word = row['word']\n",
    "            if pd.notna(word) and word in word_to_idx:\n",
    "                counts[word_to_idx[word]] = row['count']\n",
    "        \n",
    "        # Add smoothing\n",
    "        smoothed_counts = counts + eta_smoothing\n",
    "        \n",
    "        # Calculate probability distribution\n",
    "        probabilities = smoothed_counts / np.sum(smoothed_counts)\n",
    "        node_word_dist[node_id] = probabilities\n",
    "    \n",
    "    print(f\"   Built word distributions for {len(node_word_dist)} nodes\")\n",
    "    \n",
    "    # Calculate perplexity for test documents\n",
    "    total_log_likelihood = 0.0\n",
    "    total_words = 0\n",
    "    valid_docs = 0\n",
    "    matched_docs = 0\n",
    "    doc_perplexities = []\n",
    "    path_lengths = []\n",
    "    \n",
    "    for doc_id in test_doc_ids:\n",
    "        if doc_id not in corpus:\n",
    "            continue\n",
    "            \n",
    "        # Get document path mapping\n",
    "        doc_path_mapping = path_mapping_data[path_mapping_data['document_id'] == doc_id]\n",
    "        \n",
    "        if len(doc_path_mapping) == 0:\n",
    "            continue\n",
    "        \n",
    "        matched_docs += 1\n",
    "        \n",
    "        # Get document words\n",
    "        doc_words = corpus[doc_id]\n",
    "        \n",
    "        if len(doc_words) == 0:\n",
    "            continue\n",
    "        \n",
    "        valid_docs += 1\n",
    "        \n",
    "        # Get document path (assume taking the first path)\n",
    "        if len(doc_path_mapping) > 0:\n",
    "            path_row = doc_path_mapping.iloc[0]\n",
    "            \n",
    "            # Get leaf node from path\n",
    "            leaf_node_id = path_row['leaf_node_id']\n",
    "            \n",
    "            # Record path length\n",
    "            path_length = 0\n",
    "            layer_cols = [col for col in path_row.index if col.startswith('layer_') and col.endswith('_node_id')]\n",
    "            for col in layer_cols:\n",
    "                if pd.notna(path_row[col]):\n",
    "                    path_length += 1\n",
    "            path_lengths.append(path_length)\n",
    "            \n",
    "            if leaf_node_id in node_word_dist:\n",
    "                # Use leaf node word distribution to calculate likelihood\n",
    "                node_probs = node_word_dist[leaf_node_id]\n",
    "                \n",
    "                doc_log_likelihood = 0.0\n",
    "                doc_word_count = 0\n",
    "                \n",
    "                for word in doc_words:\n",
    "                    if word in word_to_idx:\n",
    "                        word_idx = word_to_idx[word]\n",
    "                        word_prob = node_probs[word_idx]\n",
    "                        \n",
    "                        if word_prob > 0:\n",
    "                            doc_log_likelihood += np.log(word_prob)\n",
    "                            doc_word_count += 1\n",
    "                \n",
    "                if doc_word_count > 0:\n",
    "                    total_log_likelihood += doc_log_likelihood\n",
    "                    total_words += doc_word_count\n",
    "                    \n",
    "                    # Calculate document perplexity\n",
    "                    doc_perplexity = np.exp(-doc_log_likelihood / doc_word_count)\n",
    "                    doc_perplexities.append(doc_perplexity)\n",
    "    \n",
    "    # Calculate overall perplexity\n",
    "    if total_words > 0:\n",
    "        perplexity = np.exp(-total_log_likelihood / total_words)\n",
    "        avg_doc_perplexity = np.mean(doc_perplexities) if doc_perplexities else 0.0\n",
    "        match_rate = matched_docs / len(test_doc_ids) if len(test_doc_ids) > 0 else 0.0\n",
    "        avg_path_length = np.mean(path_lengths) if path_lengths else 0.0\n",
    "        \n",
    "        results = {\n",
    "            'perplexity': perplexity,\n",
    "            'avg_doc_perplexity': avg_doc_perplexity,\n",
    "            'log_likelihood': total_log_likelihood,\n",
    "            'total_words': total_words,\n",
    "            'valid_docs': valid_docs,\n",
    "            'matched_docs': matched_docs,\n",
    "            'match_rate': match_rate,\n",
    "            'avg_path_length': avg_path_length\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ Perplexity calculation completed:\")\n",
    "        print(f\"   Matched documents: {matched_docs}/{len(test_doc_ids)}\")\n",
    "        print(f\"   Valid documents: {valid_docs}\")\n",
    "        print(f\"   Total words: {total_words}\")\n",
    "        print(f\"   Perplexity: {perplexity:.4f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    else:\n",
    "        print(\"❌ No valid words for perplexity calculation\")\n",
    "        return None\n",
    "\n",
    "def calculate_hlda_perplexity_with_path_mapping_step3(base_path=\".\", corpus=None, test_ratio=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Step3 version: hLDA perplexity calculation based on iteration_path_document_mapping.csv\n",
    "    Adapted for step3 alpha parameter instead of eta parameter\n",
    "    \"\"\"\n",
    "    \n",
    "    if corpus is None:\n",
    "        print(\"❌ Must provide original corpus\")\n",
    "        return\n",
    "    \n",
    "    # Split training and test sets\n",
    "    doc_ids = list(corpus.keys())\n",
    "    train_ids, test_ids = train_test_split(doc_ids, test_size=test_ratio, random_state=random_state)\n",
    "    \n",
    "    print(f\"📊 Dataset split:\")\n",
    "    print(f\"   Total documents: {len(doc_ids)}\")\n",
    "    print(f\"   Training set: {len(train_ids)} documents\")\n",
    "    print(f\"   Test set: {len(test_ids)} documents\")\n",
    "    \n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_node_word_distributions.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    print(f\"🔍 Found {len(files)} model result files to process\")\n",
    "    \n",
    "    for idx, file_path in enumerate(files, 1):\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        \n",
    "        # Step3 parameter extraction (mainly alpha parameter)\n",
    "        eta = 0.05  # eta is fixed at 0.05 in step3\n",
    "        gamma = 0.05  # fixed value\n",
    "        depth = 3  # fixed value\n",
    "        alpha = 0.1  # main varying parameter\n",
    "        \n",
    "        # Extract alpha value from folder name (adapted for step3)\n",
    "        if 'alpha_' in folder_name:\n",
    "            try:\n",
    "                alpha_part = folder_name.split('alpha_')[1].split('_')[0]\n",
    "                alpha = float(alpha_part)\n",
    "            except (IndexError, ValueError):\n",
    "                # Pattern matching through folder name\n",
    "                if 'a001' in folder_name:\n",
    "                    alpha = 0.01\n",
    "                elif 'a005' in folder_name:\n",
    "                    alpha = 0.05\n",
    "                elif 'a02' in folder_name:\n",
    "                    alpha = 0.2\n",
    "                elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                    alpha = 0.5\n",
    "                elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                    alpha = 1.0\n",
    "                elif 'a01' in folder_name:\n",
    "                    alpha = 0.1\n",
    "        else:\n",
    "            # Pattern matching through folder name\n",
    "            if 'a001' in folder_name:\n",
    "                alpha = 0.01\n",
    "            elif 'a005' in folder_name:\n",
    "                alpha = 0.05\n",
    "            elif 'a02' in folder_name:\n",
    "                alpha = 0.2\n",
    "            elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                alpha = 0.5\n",
    "            elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                alpha = 1.0\n",
    "            elif 'a01' in folder_name:\n",
    "                alpha = 0.1\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"[{idx}/{len(files)}] Calculating perplexity: {folder_name}\")\n",
    "        print(f\"Parameters - Alpha: {alpha}, Eta: {eta}, Gamma: {gamma}, Depth: {depth}\")  # Corrected: highlight alpha parameter\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            # Read word distribution data\n",
    "            word_df = pd.read_csv(file_path)\n",
    "            word_df.columns = [col.strip(\"'\\\" \") for col in word_df.columns]\n",
    "            \n",
    "            # Read path mapping data\n",
    "            path_mapping_file = os.path.join(folder_path, 'iteration_path_document_mapping.csv')\n",
    "            if not os.path.exists(path_mapping_file):\n",
    "                print(\"⚠️ Path mapping file not found, skipping this file\")\n",
    "                continue\n",
    "                \n",
    "            path_mapping_df = pd.read_csv(path_mapping_file)\n",
    "            path_mapping_df.columns = [col.strip(\"'\\\" \") for col in path_mapping_df.columns]\n",
    "            \n",
    "            # Get last iteration data\n",
    "            max_iteration = word_df['iteration'].max()\n",
    "            last_word_data = word_df[word_df['iteration'] == max_iteration]\n",
    "            last_path_mapping_data = path_mapping_df[path_mapping_df['iteration'] == max_iteration]\n",
    "            \n",
    "            print(f\"📈 Last iteration: {max_iteration}\")\n",
    "            print(f\"📈 Node count: {last_word_data['node_id'].nunique()}\")\n",
    "            print(f\"📈 Path mapping count: {len(last_path_mapping_data)}\")\n",
    "            \n",
    "            # Use corrected version to calculate perplexity (using fixed eta=0.05 as smoothing parameter)\n",
    "            perplexity_results = compute_perplexity_with_path_mapping_fixed(\n",
    "                last_word_data, \n",
    "                last_path_mapping_data, \n",
    "                corpus, \n",
    "                test_ids, \n",
    "                eta  # Use fixed eta=0.05 as smoothing parameter\n",
    "            )\n",
    "            \n",
    "            if perplexity_results is not None:\n",
    "                # Save perplexity results (corrected: use alpha as main parameter)\n",
    "                perplexity_data = [{\n",
    "                    'alpha': alpha,  # Corrected: use alpha instead of eta as main parameter\n",
    "                    'eta': eta,  # Record fixed eta value\n",
    "                    'gamma': gamma,\n",
    "                    'depth': depth,\n",
    "                    'iteration': max_iteration,\n",
    "                    'test_docs_count': len(test_ids),\n",
    "                    'valid_test_docs': perplexity_results['valid_docs'],\n",
    "                    'matched_docs': perplexity_results['matched_docs'],\n",
    "                    'total_test_words': perplexity_results['total_words'],\n",
    "                    'log_likelihood': perplexity_results['log_likelihood'],\n",
    "                    'perplexity': perplexity_results['perplexity'],\n",
    "                    'avg_doc_perplexity': perplexity_results['avg_doc_perplexity'],\n",
    "                    'doc_match_rate': perplexity_results['match_rate'],\n",
    "                    'avg_path_length': perplexity_results['avg_path_length']\n",
    "                }]\n",
    "                \n",
    "                perplexity_df = pd.DataFrame(perplexity_data)\n",
    "                output_path = os.path.join(folder_path, 'perplexity_results_step3.csv')\n",
    "                perplexity_df.to_csv(output_path, index=False)\n",
    "                \n",
    "                print(f\"💾 Perplexity results saved to: {output_path}\")\n",
    "                print(f\"📊 Perplexity results:\")\n",
    "                print(f\"   - Overall perplexity: {perplexity_results['perplexity']:.4f}\")\n",
    "                print(f\"   - Average document perplexity: {perplexity_results['avg_doc_perplexity']:.4f}\")\n",
    "                print(f\"   - Document match rate: {perplexity_results['match_rate']:.1%}\")\n",
    "                print(f\"   - Average path length: {perplexity_results['avg_path_length']:.1f}\")\n",
    "                print(f\"   - Valid test documents: {perplexity_results['valid_docs']}/{len(test_ids)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"❌ Error processing file {file_path}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    print(f\"\\n✅ Perplexity calculation for all files completed! (Step3)\")\n",
    "\n",
    "def aggregate_perplexity_by_alpha_step3(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    Step3 version: Aggregate average perplexity and other metrics by alpha value across multiple runs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find all perplexity_results_step3.csv files\n",
    "    pattern = os.path.join(base_path, \"**\", \"perplexity_results_step3.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    print(f\"🔍 Found {len(files)} Step3 perplexity result files\")\n",
    "    \n",
    "    if len(files) == 0:\n",
    "        print(\"❌ No Step3 perplexity result files found\")\n",
    "        return\n",
    "    \n",
    "    all_data = []\n",
    "    alpha_groups = {}\n",
    "    \n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        parent_folder = os.path.dirname(folder_path)\n",
    "        \n",
    "        # Extract alpha value (adapted for step3)\n",
    "        alpha = None\n",
    "        if 'alpha_' in folder_name:\n",
    "            try:\n",
    "                alpha_part = folder_name.split('alpha_')[1].split('_')[0]\n",
    "                alpha = float(alpha_part)\n",
    "            except:\n",
    "                # Pattern matching through folder name\n",
    "                if 'a001' in folder_name:\n",
    "                    alpha = 0.01\n",
    "                elif 'a005' in folder_name:\n",
    "                    alpha = 0.05\n",
    "                elif 'a02' in folder_name:\n",
    "                    alpha = 0.2\n",
    "                elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                    alpha = 0.5\n",
    "                elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                    alpha = 1.0\n",
    "                elif 'a01' in folder_name:\n",
    "                    alpha = 0.1\n",
    "        else:\n",
    "            # Pattern matching through folder name\n",
    "            if 'a001' in folder_name:\n",
    "                alpha = 0.01\n",
    "            elif 'a005' in folder_name:\n",
    "                alpha = 0.05\n",
    "            elif 'a02' in folder_name:\n",
    "                alpha = 0.2\n",
    "            elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                alpha = 0.5\n",
    "            elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                alpha = 1.0\n",
    "            elif 'a01' in folder_name:\n",
    "                alpha = 0.1\n",
    "        \n",
    "        if alpha is None:\n",
    "            print(f\"Warning: Cannot extract alpha value from folder name {folder_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Extract run number\n",
    "        run_match = folder_name.split('_run_')\n",
    "        if len(run_match) > 1:\n",
    "            run_id = run_match[1]\n",
    "        else:\n",
    "            print(f\"Warning: Cannot extract run number from folder name {folder_name}\")\n",
    "            run_id = \"unknown\"\n",
    "        \n",
    "        if alpha not in alpha_groups:\n",
    "            alpha_groups[alpha] = parent_folder\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"📖 Reading file: {folder_name} - {len(df)} rows of data\")\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                # Check if required fields exist\n",
    "                if 'perplexity' not in row:\n",
    "                    print(f\"Warning: {file_path} missing perplexity column\")\n",
    "                    continue\n",
    "                    \n",
    "                all_data.append({\n",
    "                    'alpha': alpha,  # Corrected: use alpha as main parameter\n",
    "                    'run_id': run_id,\n",
    "                    'eta': row.get('eta', 0.05),  # Record fixed eta value\n",
    "                    'gamma': row.get('gamma', 0.05),\n",
    "                    'depth': row.get('depth', 3),\n",
    "                    'perplexity': row.get('perplexity', 0),\n",
    "                    'avg_doc_perplexity': row.get('avg_doc_perplexity', row.get('perplexity', 0)),\n",
    "                    'valid_test_docs': row.get('valid_test_docs', 0),\n",
    "                    'total_test_words': row.get('total_test_words', 0),\n",
    "                    'doc_match_rate': row.get('doc_match_rate', 0),\n",
    "                    'avg_path_length': row.get('avg_path_length', 0),\n",
    "                    'log_likelihood': row.get('log_likelihood', 0),\n",
    "                    'parent_folder': parent_folder\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_path}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    summary_df = pd.DataFrame(all_data)\n",
    "    \n",
    "    if summary_df.empty:\n",
    "        print(\"No valid data found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"📊 Data summary:\")\n",
    "    print(f\"   Total data rows: {len(summary_df)}\")\n",
    "    print(f\"   Unique alpha values: {sorted(summary_df['alpha'].unique())}\")\n",
    "    print(f\"   Data count per alpha: {summary_df['alpha'].value_counts().sort_index().to_dict()}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"Perplexity summary statistics by ALPHA value (Step3)\")  # Corrected: show ALPHA\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Generate summary files grouped by alpha\n",
    "    for alpha, group_data in summary_df.groupby('alpha'):\n",
    "        parent_folder = group_data['parent_folder'].iloc[0]\n",
    "        \n",
    "        print(f\"\\nProcessing Alpha={alpha}\")  # Corrected: show Alpha\n",
    "        print(f\"Output directory: {parent_folder}\")\n",
    "        print(f\"Data count for this group: {len(group_data)}\")\n",
    "        \n",
    "        # Check if group_data is empty\n",
    "        if len(group_data) == 0:\n",
    "            print(f\"Warning: Alpha={alpha} group has no data, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Build aggregation dictionary\n",
    "        agg_dict = {}\n",
    "        \n",
    "        # Check if each column has valid data\n",
    "        numeric_cols = ['perplexity', 'avg_doc_perplexity', 'valid_test_docs', \n",
    "                       'total_test_words', 'doc_match_rate', 'avg_path_length', 'log_likelihood']\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if col in group_data.columns:\n",
    "                valid_data = group_data[col].dropna()\n",
    "                if len(valid_data) > 0:\n",
    "                    if col in ['perplexity', 'avg_doc_perplexity', 'doc_match_rate', 'avg_path_length', 'log_likelihood']:\n",
    "                        agg_dict[col] = ['mean', 'std', 'min', 'max']\n",
    "                    else:\n",
    "                        agg_dict[col] = ['mean', 'std']\n",
    "                else:\n",
    "                    print(f\"   Warning: {col} column has no valid data\")\n",
    "        \n",
    "        # Add count\n",
    "        if 'run_id' in group_data.columns:\n",
    "            agg_dict['run_id'] = 'count'\n",
    "        \n",
    "        # Parameter columns\n",
    "        for col in ['eta', 'gamma', 'depth']:\n",
    "            if col in group_data.columns:\n",
    "                agg_dict[col] = 'first'\n",
    "        \n",
    "        if not agg_dict:\n",
    "            print(f\"Warning: Alpha={alpha} group has no aggregatable columns, skipping\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Perform aggregation operation\n",
    "            print(f\"   Aggregation dictionary: {list(agg_dict.keys())}\")\n",
    "            alpha_summary = group_data.agg(agg_dict).round(4)\n",
    "            \n",
    "            # Flatten column names\n",
    "            alpha_summary.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in alpha_summary.columns]\n",
    "            alpha_summary = alpha_summary.reset_index()\n",
    "            alpha_summary.insert(0, 'alpha', alpha)  # Corrected: insert alpha column\n",
    "            \n",
    "            # Add run_id list\n",
    "            run_ids = ', '.join(sorted(group_data['run_id'].unique()))\n",
    "            alpha_summary['run_ids'] = run_ids\n",
    "            \n",
    "            # Save summary results (corrected: filename uses alpha)\n",
    "            output_filename = f'alpha_{alpha}_perplexity_summary.csv'\n",
    "            output_path = os.path.join(parent_folder, output_filename)\n",
    "            alpha_summary.to_csv(output_path, index=False)\n",
    "            \n",
    "            print(f\"  ✓ Saved summary file: {output_path}\")\n",
    "            \n",
    "            # Show statistics\n",
    "            if 'run_id_count' in alpha_summary.columns:\n",
    "                print(f\"  Run count: {int(alpha_summary['run_id_count'].iloc[0])}\")\n",
    "            \n",
    "            if 'perplexity_mean' in alpha_summary.columns:\n",
    "                mean_perp = alpha_summary['perplexity_mean'].iloc[0]\n",
    "                std_perp = alpha_summary.get('perplexity_std', pd.Series([0])).iloc[0]\n",
    "                print(f\"  Average perplexity: {mean_perp:.4f} (±{std_perp:.4f})\")\n",
    "            \n",
    "            print(f\"  Included runs: {run_ids}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing Alpha={alpha}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Generate overall comparison file (corrected: filename uses alpha)\n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(\"Generating overall comparison file\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Build overall aggregation dictionary\n",
    "        overall_agg_dict = {}\n",
    "        \n",
    "        for col in ['perplexity', 'avg_doc_perplexity', 'doc_match_rate', 'avg_path_length', \n",
    "                   'valid_test_docs', 'total_test_words', 'log_likelihood']:\n",
    "            if col in summary_df.columns:\n",
    "                valid_data = summary_df[col].dropna()\n",
    "                if len(valid_data) > 0:\n",
    "                    overall_agg_dict[col] = ['mean', 'std']\n",
    "                    if col in ['perplexity', 'avg_doc_perplexity']:\n",
    "                        overall_agg_dict[col].extend(['min', 'max'])\n",
    "        \n",
    "        if 'run_id' in summary_df.columns:\n",
    "            overall_agg_dict['run_id'] = 'count'\n",
    "        \n",
    "        if not overall_agg_dict:\n",
    "            print(\"Warning: No aggregatable columns for overall comparison\")\n",
    "            return None\n",
    "        \n",
    "        overall_summary = summary_df.groupby('alpha').agg(overall_agg_dict).round(4)  # Corrected: group by alpha\n",
    "        \n",
    "        # Flatten column names\n",
    "        overall_summary.columns = ['_'.join(col).strip() for col in overall_summary.columns]\n",
    "        overall_summary = overall_summary.reset_index()\n",
    "        \n",
    "        overall_output_path = os.path.join(base_path, 'alpha_perplexity_comparison.csv')  # Corrected: filename uses alpha\n",
    "        overall_summary.to_csv(overall_output_path, index=False)\n",
    "        print(f\"✓ Overall comparison file saved to: {overall_output_path}\")\n",
    "        \n",
    "        # Show cross-alpha comparison\n",
    "        print(f\"\\nCross-Alpha Perplexity Comparison:\")  # Corrected: show Alpha\n",
    "        print(\"Alpha Value     Average Perplexity(±std)     Run Count\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for _, row in overall_summary.iterrows():\n",
    "            alpha = row['alpha']  # Corrected: use alpha\n",
    "            run_count = int(row.get('run_id_count', 0))\n",
    "            \n",
    "            if 'perplexity_mean' in row:\n",
    "                mean_perp = row['perplexity_mean']\n",
    "                std_perp = row.get('perplexity_std', 0)\n",
    "                print(f\"{alpha:7.3f}    {mean_perp:8.4f}(±{std_perp:6.4f})        {run_count:4d}\")\n",
    "            else:\n",
    "                print(f\"{alpha:7.3f}    Missing data                    {run_count:4d}\")\n",
    "        \n",
    "        return overall_summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating overall comparison: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Execute Step3 perplexity calculation and aggregation\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step3\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Starting Step3 perplexity calculation...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Calculate perplexity (Step3 version)\n",
    "calculate_hlda_perplexity_with_path_mapping_step3(base_path, corpus, test_ratio=0.2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Starting aggregation of perplexity statistics by alpha value...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 2. Aggregate by alpha (Step3 version)\n",
    "overall_summary = aggregate_perplexity_by_alpha_step3(base_path)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"✅ Step3 perplexity calculation and aggregation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fce20bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "611fa80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Starting calculation of Step3 branching factor and Gini coefficient metrics (layer only)...\n",
      "================================================================================\n",
      "🔍 Found 17 entropy files to process (Step3 - Layer only)\n",
      "\n",
      "[1/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_1_run_3 (Alpha=1.0)\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_3/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary (Alpha=1.0):\n",
      "   Total nodes: 307\n",
      "   Total layers: 3\n",
      "   Layer 0: Avg branching=64.00, Doc Gini=0.0000\n",
      "   Layer 2: Avg branching=0.00, Doc Gini=0.4731\n",
      "   Layer 1: Avg branching=3.78, Doc Gini=0.5599\n",
      "\n",
      "[2/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_1_run_2 (Alpha=1.0)\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_2/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary (Alpha=1.0):\n",
      "   Total nodes: 321\n",
      "   Total layers: 3\n",
      "   Layer 0: Avg branching=57.00, Doc Gini=0.0000\n",
      "   Layer 1: Avg branching=4.61, Doc Gini=0.6218\n",
      "   Layer 2: Avg branching=0.00, Doc Gini=0.3906\n",
      "\n",
      "[3/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_3 (Alpha=0.2)\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_3/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary (Alpha=0.2):\n",
      "   Total nodes: 325\n",
      "   Total layers: 3\n",
      "   Layer 0: Avg branching=78.00, Doc Gini=0.0000\n",
      "   Layer 1: Avg branching=3.15, Doc Gini=0.4636\n",
      "   Layer 2: Avg branching=0.00, Doc Gini=0.3704\n",
      "\n",
      "[4/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_1 (Alpha=0.2)\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_1/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary (Alpha=0.2):\n",
      "   Total nodes: 325\n",
      "   Total layers: 3\n",
      "   Layer 0: Avg branching=67.00, Doc Gini=0.0000\n",
      "   Layer 1: Avg branching=3.84, Doc Gini=0.5094\n",
      "   Layer 2: Avg branching=0.00, Doc Gini=0.3899\n",
      "\n",
      "[5/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_2 (Alpha=0.2)\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_2/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary (Alpha=0.2):\n",
      "   Total nodes: 333\n",
      "   Total layers: 3\n",
      "   Layer 0: Avg branching=71.00, Doc Gini=0.0000\n",
      "   Layer 1: Avg branching=3.68, Doc Gini=0.5250\n",
      "   Layer 2: Avg branching=0.00, Doc Gini=0.3668\n",
      "\n",
      "[6/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_3 (Alpha=0.5)\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_3/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary (Alpha=0.5):\n",
      "   Total nodes: 292\n",
      "   Total layers: 3\n",
      "   Layer 0: Avg branching=55.00, Doc Gini=0.0000\n",
      "   Layer 1: Avg branching=4.29, Doc Gini=0.6915\n",
      "   Layer 2: Avg branching=0.00, Doc Gini=0.4464\n",
      "\n",
      "[7/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_1 (Alpha=0.5)\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_1/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary (Alpha=0.5):\n",
      "   Total nodes: 282\n",
      "   Total layers: 3\n",
      "   Layer 0: Avg branching=45.00, Doc Gini=0.0000\n",
      "   Layer 1: Avg branching=5.24, Doc Gini=0.7496\n",
      "   Layer 2: Avg branching=0.00, Doc Gini=0.3903\n",
      "\n",
      "[8/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_2 (Alpha=0.5)\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_2/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary (Alpha=0.5):\n",
      "   Total nodes: 292\n",
      "   Total layers: 3\n",
      "   Layer 0: Avg branching=36.00, Doc Gini=0.0000\n",
      "   Layer 1: Avg branching=7.08, Doc Gini=0.7325\n",
      "   Layer 2: Avg branching=0.00, Doc Gini=0.3827\n",
      "\n",
      "[9/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_3 (Alpha=0.05)\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_3/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary (Alpha=0.05):\n",
      "   Total nodes: 275\n",
      "   Total layers: 3\n",
      "   Layer 0: Avg branching=62.00, Doc Gini=0.0000\n",
      "   Layer 1: Avg branching=3.42, Doc Gini=0.6007\n",
      "   Layer 2: Avg branching=0.00, Doc Gini=0.5436\n",
      "\n",
      "[10/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_1 (Alpha=0.05)\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_1/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary (Alpha=0.05):\n",
      "   Total nodes: 316\n",
      "   Total layers: 3\n",
      "   Layer 0: Avg branching=60.00, Doc Gini=0.0000\n",
      "   Layer 1: Avg branching=4.25, Doc Gini=0.5658\n",
      "   Layer 2: Avg branching=0.00, Doc Gini=0.4017\n",
      "\n",
      "[11/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_2 (Alpha=0.05)\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_2/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary (Alpha=0.05):\n",
      "   Total nodes: 305\n",
      "   Total layers: 3\n",
      "   Layer 0: Avg branching=60.00, Doc Gini=0.0000\n",
      "   Layer 1: Avg branching=4.07, Doc Gini=0.5992\n",
      "   Layer 2: Avg branching=0.00, Doc Gini=0.4337\n",
      "\n",
      "[12/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_2 (Alpha=0.1)\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_2/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary (Alpha=0.1):\n",
      "   Total nodes: 299\n",
      "   Total layers: 3\n",
      "   Layer 0: Avg branching=55.00, Doc Gini=0.0000\n",
      "   Layer 1: Avg branching=4.42, Doc Gini=0.5330\n",
      "   Layer 2: Avg branching=0.00, Doc Gini=0.4104\n",
      "\n",
      "[13/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_3 (Alpha=0.1)\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_3/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary (Alpha=0.1):\n",
      "   Total nodes: 322\n",
      "   Total layers: 3\n",
      "   Layer 0: Avg branching=59.00, Doc Gini=0.0000\n",
      "   Layer 2: Avg branching=0.00, Doc Gini=0.3689\n",
      "   Layer 1: Avg branching=4.44, Doc Gini=0.5429\n",
      "\n",
      "[14/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1 (Alpha=0.1)\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary (Alpha=0.1):\n",
      "   Total nodes: 315\n",
      "   Total layers: 3\n",
      "   Layer 0: Avg branching=62.00, Doc Gini=0.0000\n",
      "   Layer 1: Avg branching=4.06, Doc Gini=0.5204\n",
      "   Layer 2: Avg branching=0.00, Doc Gini=0.3778\n",
      "\n",
      "[15/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_3 (Alpha=0.01)\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_3/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary (Alpha=0.01):\n",
      "   Total nodes: 296\n",
      "   Total layers: 3\n",
      "   Layer 0: Avg branching=66.00, Doc Gini=0.0000\n",
      "   Layer 1: Avg branching=3.47, Doc Gini=0.5692\n",
      "   Layer 2: Avg branching=0.00, Doc Gini=0.4197\n",
      "\n",
      "[16/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_2 (Alpha=0.01)\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_2/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary (Alpha=0.01):\n",
      "   Total nodes: 313\n",
      "   Total layers: 3\n",
      "   Layer 0: Avg branching=59.00, Doc Gini=0.0000\n",
      "   Layer 1: Avg branching=4.29, Doc Gini=0.6102\n",
      "   Layer 2: Avg branching=0.00, Doc Gini=0.3800\n",
      "\n",
      "[17/17] Processing folder: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_1 (Alpha=0.01)\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_1/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary (Alpha=0.01):\n",
      "   Total nodes: 312\n",
      "   Total layers: 3\n",
      "   Layer 0: Avg branching=65.00, Doc Gini=0.0000\n",
      "   Layer 1: Avg branching=3.78, Doc Gini=0.5393\n",
      "   Layer 2: Avg branching=0.00, Doc Gini=0.4497\n",
      "\n",
      "================================================================================\n",
      "Starting aggregation of branching factor and Gini coefficient statistics by alpha value (layer only)...\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Aggregating layer branching factor and Gini coefficient metrics... (Step3 - Layer only)\n",
      "================================================================================\n",
      "🔍 Found 17 layer metrics files\n",
      "\n",
      "📊 Layer data check:\n",
      "   Total data rows: 51\n",
      "   Alpha value distribution: {0.2: 9, 0.5: 9, 0.05: 9, 0.1: 9, 0.01: 9, 1.0: 6}\n",
      "Layer branching factor and Gini coefficient aggregated statistics by ALPHA value\n",
      "================================================================================\n",
      "\n",
      "Processing Alpha=0.01 layer metrics\n",
      "   Data count: 9\n",
      "   Run IDs: ['3', '2', '1']\n",
      "  ✅ Saved layer summary file: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/alpha_0.01_layer_branching_gini_summary.csv\n",
      "  Number of layers: 3\n",
      "\n",
      "Processing Alpha=0.05 layer metrics\n",
      "   Data count: 9\n",
      "   Run IDs: ['3', '1', '2']\n",
      "  ✅ Saved layer summary file: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/alpha_0.05_layer_branching_gini_summary.csv\n",
      "  Number of layers: 3\n",
      "\n",
      "Processing Alpha=0.1 layer metrics\n",
      "   Data count: 9\n",
      "   Run IDs: ['2', '3', '1']\n",
      "  ✅ Saved layer summary file: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/alpha_0.1_layer_branching_gini_summary.csv\n",
      "  Number of layers: 3\n",
      "\n",
      "Processing Alpha=0.2 layer metrics\n",
      "   Data count: 9\n",
      "   Run IDs: ['3', '1', '2']\n",
      "  ✅ Saved layer summary file: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/alpha_0.2_layer_branching_gini_summary.csv\n",
      "  Number of layers: 3\n",
      "\n",
      "Processing Alpha=0.5 layer metrics\n",
      "   Data count: 9\n",
      "   Run IDs: ['3', '1', '2']\n",
      "  ✅ Saved layer summary file: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/alpha_0.5_layer_branching_gini_summary.csv\n",
      "  Number of layers: 3\n",
      "\n",
      "Processing Alpha=1.0 layer metrics\n",
      "   Data count: 6\n",
      "   Run IDs: ['3', '2']\n",
      "  ✅ Saved layer summary file: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/alpha_1.0_layer_branching_gini_summary.csv\n",
      "  Number of layers: 3\n",
      "\n",
      "Overall layer comparison file saved to: /Volumes/My Passport/收敛结果/step3/alpha_layer_branching_gini_comparison.csv\n",
      "\n",
      "================================================================================\n",
      "Displaying Step3 branching factor and Gini coefficient summary report (layer only)...\n",
      "================================================================================\n",
      "====================================================================================================\n",
      "Branching Factor and Gini Coefficient Analysis Summary Report (Step3 - Alpha Parameter, Layer Only)\n",
      "====================================================================================================\n",
      "\n",
      "📊 Layer-wise branching factor and Gini coefficient analysis:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Layer 0 Cross-Alpha Comparison:\n",
      "Alpha Value     Avg Branching(±std)     Doc Gini(±std)     Branch Gini(±std)     Run Count\n",
      "---------------------------------------------------------------------------\n",
      "  0.010     63.33(±3.79)     0.0000(±0.0000)     0.0000(±0.0000)        3\n",
      "  0.050     60.67(±1.15)     0.0000(±0.0000)     0.0000(±0.0000)        3\n",
      "  0.100     58.67(±3.51)     0.0000(±0.0000)     0.0000(±0.0000)        3\n",
      "  0.200     72.00(±5.57)     0.0000(±0.0000)     0.0000(±0.0000)        3\n",
      "  0.500     45.33(±9.50)     0.0000(±0.0000)     0.0000(±0.0000)        3\n",
      "  1.000     60.50(±4.95)     0.0000(±0.0000)     0.0000(±0.0000)        2\n",
      "\n",
      "Layer 1 Cross-Alpha Comparison:\n",
      "Alpha Value     Avg Branching(±std)     Doc Gini(±std)     Branch Gini(±std)     Run Count\n",
      "---------------------------------------------------------------------------\n",
      "  0.010      3.85(±0.41)     0.5729(±0.0356)     0.4183(±0.0451)        3\n",
      "  0.050      3.91(±0.44)     0.5886(±0.0197)     0.4031(±0.0629)        3\n",
      "  0.100      4.31(±0.21)     0.5321(±0.0113)     0.4328(±0.0121)        3\n",
      "  0.200      3.56(±0.36)     0.4993(±0.0319)     0.4354(±0.0179)        3\n",
      "  0.500      5.54(±1.42)     0.7245(±0.0298)     0.5911(±0.0421)        3\n",
      "  1.000      4.20(±0.59)     0.5908(±0.0438)     0.4695(±0.0581)        2\n",
      "\n",
      "Layer 2 Cross-Alpha Comparison:\n",
      "Alpha Value     Avg Branching(±std)     Doc Gini(±std)     Branch Gini(±std)     Run Count\n",
      "---------------------------------------------------------------------------\n",
      "  0.010      0.00(±0.00)     0.4164(±0.0349)     0.0000(±0.0000)        3\n",
      "  0.050      0.00(±0.00)     0.4597(±0.0744)     0.0000(±0.0000)        3\n",
      "  0.100      0.00(±0.00)     0.3857(±0.0219)     0.0000(±0.0000)        3\n",
      "  0.200      0.00(±0.00)     0.3757(±0.0124)     0.0000(±0.0000)        3\n",
      "  0.500      0.00(±0.00)     0.4065(±0.0348)     0.0000(±0.0000)        3\n",
      "  1.000      0.00(±0.00)     0.4318(±0.0583)     0.0000(±0.0000)        2\n",
      "\n",
      "====================================================================================================\n",
      "✅ Step3 branching factor and Gini coefficient analysis completed! (Layer metrics only)\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def calculate_branching_and_gini_metrics_step3_no_global(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    Step3 version: Calculate branching factor and Gini coefficient metrics for each model (adapted for alpha parameter)\n",
    "    Only generates layer-level metrics, no global metrics\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(base_path, \"**\", \"corrected_renyi_entropy.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    print(f\"🔍 Found {len(files)} entropy files to process (Step3 - Layer only)\")\n",
    "    \n",
    "    for idx, file_path in enumerate(files, 1):\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        \n",
    "        # Step3 parameter extraction (mainly alpha parameter)\n",
    "        alpha = 0.1  # Default value\n",
    "        if 'alpha_' in folder_name:\n",
    "            try:\n",
    "                alpha_part = folder_name.split('alpha_')[1].split('_')[0]\n",
    "                alpha = float(alpha_part)\n",
    "            except (IndexError, ValueError):\n",
    "                # Pattern matching through folder name\n",
    "                if 'a001' in folder_name:\n",
    "                    alpha = 0.01\n",
    "                elif 'a005' in folder_name:\n",
    "                    alpha = 0.05\n",
    "                elif 'a02' in folder_name:\n",
    "                    alpha = 0.2\n",
    "                elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                    alpha = 0.5\n",
    "                elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                    alpha = 1.0\n",
    "                elif 'a01' in folder_name:\n",
    "                    alpha = 0.1\n",
    "        else:\n",
    "            # Pattern matching through folder name\n",
    "            if 'a001' in folder_name:\n",
    "                alpha = 0.01\n",
    "            elif 'a005' in folder_name:\n",
    "                alpha = 0.05\n",
    "            elif 'a02' in folder_name:\n",
    "                alpha = 0.2\n",
    "            elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                alpha = 0.5\n",
    "            elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                alpha = 1.0\n",
    "            elif 'a01' in folder_name:\n",
    "                alpha = 0.1\n",
    "        \n",
    "        print(f\"\\n[{idx}/{len(files)}] Processing folder: {folder_name} (Alpha={alpha})\")\n",
    "        \n",
    "        try:\n",
    "            # Read entropy file\n",
    "            entropy_df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Check if necessary columns exist\n",
    "            required_cols = ['node_id', 'layer', 'document_count', 'child_count']\n",
    "            missing_cols = [col for col in required_cols if col not in entropy_df.columns]\n",
    "            \n",
    "            if missing_cols:\n",
    "                print(f\"⚠️ Missing required columns: {missing_cols}, skipping this file\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate layer-wise branching factor and Gini coefficient metrics only\n",
    "            layer_metrics = []\n",
    "            \n",
    "            for layer in entropy_df['layer'].unique():\n",
    "                if layer == -1:  # Skip invalid layers\n",
    "                    continue\n",
    "                    \n",
    "                layer_nodes = entropy_df[entropy_df['layer'] == layer]\n",
    "                \n",
    "                # Basic statistics\n",
    "                node_count = len(layer_nodes)\n",
    "                total_documents = layer_nodes['document_count'].sum()\n",
    "                \n",
    "                # Branching factor statistics\n",
    "                child_counts = layer_nodes['child_count'].values\n",
    "                total_branches = child_counts.sum()\n",
    "                \n",
    "                # Non-leaf node statistics\n",
    "                non_leaf_nodes = (child_counts > 0).sum()\n",
    "                non_leaf_counts = child_counts[child_counts > 0]\n",
    "                \n",
    "                # Branching factor statistics\n",
    "                if len(non_leaf_counts) > 0:\n",
    "                    avg_branching_factor = non_leaf_counts.mean()\n",
    "                    std_branching_factor = non_leaf_counts.std()\n",
    "                    non_leaf_avg_branching = non_leaf_counts.mean()\n",
    "                else:\n",
    "                    avg_branching_factor = 0.0\n",
    "                    std_branching_factor = 0.0\n",
    "                    non_leaf_avg_branching = 0.0\n",
    "                \n",
    "                # Gini coefficient calculation\n",
    "                def gini_coefficient(values):\n",
    "                    \"\"\"Calculate Gini coefficient\"\"\"\n",
    "                    if len(values) == 0:\n",
    "                        return 0.0\n",
    "                    values = np.array(values)\n",
    "                    values = values[values > 0]  # Only consider positive values\n",
    "                    if len(values) <= 1:\n",
    "                        return 0.0\n",
    "                    \n",
    "                    values = np.sort(values)\n",
    "                    n = len(values)\n",
    "                    cumsum = np.cumsum(values)\n",
    "                    return (n + 1 - 2 * np.sum(cumsum) / cumsum[-1]) / n\n",
    "                \n",
    "                # Document distribution Gini coefficient\n",
    "                doc_counts = layer_nodes['document_count'].values\n",
    "                gini_doc_distribution = gini_coefficient(doc_counts)\n",
    "                \n",
    "                # Branching distribution Gini coefficient\n",
    "                gini_branch_distribution = gini_coefficient(child_counts)\n",
    "                \n",
    "                layer_metrics.append({\n",
    "                    'layer': layer,\n",
    "                    'node_count': node_count,\n",
    "                    'total_branches': total_branches,\n",
    "                    'avg_branching_factor': avg_branching_factor,\n",
    "                    'std_branching_factor': std_branching_factor,\n",
    "                    'non_leaf_nodes': non_leaf_nodes,\n",
    "                    'non_leaf_avg_branching': non_leaf_avg_branching,\n",
    "                    'total_documents': total_documents,\n",
    "                    'gini_doc_distribution': gini_doc_distribution,\n",
    "                    'gini_branch_distribution': gini_branch_distribution,\n",
    "                    'alpha': alpha  # Add alpha parameter record\n",
    "                })\n",
    "            \n",
    "            # Save layer metrics only\n",
    "            if layer_metrics:\n",
    "                layer_df = pd.DataFrame(layer_metrics)\n",
    "                layer_output_path = os.path.join(folder_path, 'layer_branching_gini_metrics.csv')\n",
    "                layer_df.to_csv(layer_output_path, index=False)\n",
    "                print(f\"✓ Layer metrics saved to: {layer_output_path}\")\n",
    "            \n",
    "            # Display brief statistics for layers only\n",
    "            print(f\"📊 Layer metrics summary (Alpha={alpha}):\")\n",
    "            total_nodes = len(entropy_df)\n",
    "            total_layers = len(entropy_df['layer'].unique()) - (1 if -1 in entropy_df['layer'].unique() else 0)\n",
    "            print(f\"   Total nodes: {total_nodes}\")\n",
    "            print(f\"   Total layers: {total_layers}\")\n",
    "            \n",
    "            if layer_metrics:\n",
    "                for layer_metric in layer_metrics:\n",
    "                    layer_num = layer_metric['layer']\n",
    "                    layer_branching = layer_metric['avg_branching_factor']\n",
    "                    layer_doc_gini = layer_metric['gini_doc_distribution']\n",
    "                    print(f\"   Layer {layer_num}: Avg branching={layer_branching:.2f}, Doc Gini={layer_doc_gini:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"❌ Error processing file {file_path}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "def aggregate_branching_gini_by_alpha_step3_layer_only(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    Step3 version: Aggregate branching factor and Gini coefficient statistics by alpha value (layer metrics only)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Aggregating layer branching factor and Gini coefficient metrics... (Step3 - Layer only)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    pattern = os.path.join(base_path, \"**\", \"layer_branching_gini_metrics.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    print(f\"🔍 Found {len(files)} layer metrics files\")\n",
    "    \n",
    "    all_layer_data = []\n",
    "    \n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        parent_folder = os.path.dirname(folder_path)\n",
    "        \n",
    "        # Extract alpha value (adapted for step3)\n",
    "        alpha = None\n",
    "        if 'alpha_' in folder_name:\n",
    "            try:\n",
    "                alpha_part = folder_name.split('alpha_')[1].split('_')[0]\n",
    "                alpha = float(alpha_part)\n",
    "            except:\n",
    "                # Pattern matching through folder name\n",
    "                if 'a001' in folder_name:\n",
    "                    alpha = 0.01\n",
    "                elif 'a005' in folder_name:\n",
    "                    alpha = 0.05\n",
    "                elif 'a02' in folder_name:\n",
    "                    alpha = 0.2\n",
    "                elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                    alpha = 0.5\n",
    "                elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                    alpha = 1.0\n",
    "                elif 'a01' in folder_name:\n",
    "                    alpha = 0.1\n",
    "        else:\n",
    "            # Pattern matching through folder name\n",
    "            if 'a001' in folder_name:\n",
    "                alpha = 0.01\n",
    "            elif 'a005' in folder_name:\n",
    "                alpha = 0.05\n",
    "            elif 'a02' in folder_name:\n",
    "                alpha = 0.2\n",
    "            elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                alpha = 0.5\n",
    "            elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                alpha = 1.0\n",
    "            elif 'a01' in folder_name:\n",
    "                alpha = 0.1\n",
    "        \n",
    "        if alpha is None:\n",
    "            print(f\"⚠️ Unable to extract alpha value: {folder_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Extract run number\n",
    "        run_match = folder_name.split('_run_')\n",
    "        if len(run_match) > 1:\n",
    "            run_id = run_match[1]\n",
    "        else:\n",
    "            print(f\"⚠️ Unable to extract run number: {folder_name}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                all_layer_data.append({\n",
    "                    'alpha': alpha,\n",
    "                    'run_id': run_id,\n",
    "                    'layer': row.get('layer', -1),\n",
    "                    'node_count': row.get('node_count', 0),\n",
    "                    'total_branches': row.get('total_branches', 0),\n",
    "                    'avg_branching_factor': row.get('avg_branching_factor', 0),\n",
    "                    'std_branching_factor': row.get('std_branching_factor', 0),\n",
    "                    'non_leaf_nodes': row.get('non_leaf_nodes', 0),\n",
    "                    'non_leaf_avg_branching': row.get('non_leaf_avg_branching', 0),\n",
    "                    'total_documents': row.get('total_documents', 0),\n",
    "                    'gini_doc_distribution': row.get('gini_doc_distribution', 0),\n",
    "                    'gini_branch_distribution': row.get('gini_branch_distribution', 0),\n",
    "                    'parent_folder': parent_folder\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error reading file {file_path}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame and aggregate by alpha groups\n",
    "    if all_layer_data:\n",
    "        layer_summary_df = pd.DataFrame(all_layer_data)\n",
    "        \n",
    "        print(f\"\\n📊 Layer data check:\")\n",
    "        print(f\"   Total data rows: {len(layer_summary_df)}\")\n",
    "        print(f\"   Alpha value distribution: {layer_summary_df['alpha'].value_counts().to_dict()}\")\n",
    "        \n",
    "        print(\"Layer branching factor and Gini coefficient aggregated statistics by ALPHA value\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Generate layer summary files grouped by alpha\n",
    "        for alpha, group_data in layer_summary_df.groupby('alpha'):\n",
    "            parent_folder = group_data['parent_folder'].iloc[0]\n",
    "            \n",
    "            print(f\"\\nProcessing Alpha={alpha} layer metrics\")\n",
    "            print(f\"   Data count: {len(group_data)}\")\n",
    "            print(f\"   Run IDs: {list(group_data['run_id'].unique())}\")\n",
    "            \n",
    "            try:\n",
    "                # Calculate layer summary statistics\n",
    "                layer_summary = group_data.groupby('layer').agg({\n",
    "                    'avg_branching_factor': ['mean', 'std', 'count'],\n",
    "                    'gini_doc_distribution': ['mean', 'std', 'count'],\n",
    "                    'gini_branch_distribution': ['mean', 'std', 'count'],\n",
    "                    'node_count': ['mean', 'std'],\n",
    "                    'total_documents': 'mean',\n",
    "                    'run_id': lambda x: ', '.join(sorted(x.unique()))\n",
    "                }).round(4)\n",
    "                \n",
    "                # Flatten column names\n",
    "                layer_summary.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in layer_summary.columns]\n",
    "                layer_summary = layer_summary.reset_index()\n",
    "                layer_summary.insert(0, 'alpha', alpha)\n",
    "                \n",
    "                # Save layer summary results\n",
    "                output_filename = f'alpha_{alpha}_layer_branching_gini_summary.csv'\n",
    "                output_path = os.path.join(parent_folder, output_filename)\n",
    "                layer_summary.to_csv(output_path, index=False)\n",
    "                \n",
    "                print(f\"  ✅ Saved layer summary file: {output_path}\")\n",
    "                print(f\"  Number of layers: {len(layer_summary)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error processing Alpha={alpha}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                \n",
    "        # Generate overall layer comparison file\n",
    "        overall_layer_summary = layer_summary_df.groupby(['alpha', 'layer']).agg({\n",
    "            'avg_branching_factor': ['mean', 'std'],\n",
    "            'gini_doc_distribution': ['mean', 'std'],\n",
    "            'gini_branch_distribution': ['mean', 'std'],\n",
    "            'node_count': ['mean', 'std'],\n",
    "            'run_id': 'count'\n",
    "        }).round(4)\n",
    "        \n",
    "        # Flatten column names\n",
    "        overall_layer_summary.columns = ['_'.join(col).strip() for col in overall_layer_summary.columns]\n",
    "        overall_layer_summary = overall_layer_summary.reset_index()\n",
    "        \n",
    "        overall_output_path = os.path.join(base_path, 'alpha_layer_branching_gini_comparison.csv')\n",
    "        overall_layer_summary.to_csv(overall_output_path, index=False)\n",
    "        print(f\"\\nOverall layer comparison file saved to: {overall_output_path}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ No valid layer data found\")\n",
    "\n",
    "def display_branching_gini_summary_step3_layer_only(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    Step3 version: Display branching factor and Gini coefficient summary report (layer metrics only)\n",
    "    \"\"\"\n",
    "    print(\"=\" * 100)\n",
    "    print(\"Branching Factor and Gini Coefficient Analysis Summary Report (Step3 - Alpha Parameter, Layer Only)\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Read layer comparison file only\n",
    "    layer_comparison_file = os.path.join(base_path, 'alpha_layer_branching_gini_comparison.csv')\n",
    "    \n",
    "    if os.path.exists(layer_comparison_file):\n",
    "        print(\"\\n📊 Layer-wise branching factor and Gini coefficient analysis:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        df = pd.read_csv(layer_comparison_file)\n",
    "        \n",
    "        # Find correct count column name\n",
    "        count_col = None\n",
    "        for col in df.columns:\n",
    "            if 'run_id' in col and ('count' in col or col.endswith('_count')):\n",
    "                count_col = col\n",
    "                break\n",
    "        \n",
    "        for layer in sorted(df['layer'].unique()):\n",
    "            print(f\"\\nLayer {int(layer)} Cross-Alpha Comparison:\")\n",
    "            print(\"Alpha Value     Avg Branching(±std)     Doc Gini(±std)     Branch Gini(±std)     Run Count\")\n",
    "            print(\"-\" * 75)\n",
    "            \n",
    "            layer_data = df[df['layer'] == layer]\n",
    "            for _, row in layer_data.iterrows():\n",
    "                alpha = row['alpha']\n",
    "                avg_branch = row['avg_branching_factor_mean']\n",
    "                branch_std = row['avg_branching_factor_std']\n",
    "                doc_gini = row['gini_doc_distribution_mean']\n",
    "                doc_gini_std = row['gini_doc_distribution_std']\n",
    "                branch_gini = row['gini_branch_distribution_mean']\n",
    "                branch_gini_std = row['gini_branch_distribution_std']\n",
    "                run_count = int(row[count_col]) if count_col else 0\n",
    "                \n",
    "                print(f\"{alpha:7.3f}    {avg_branch:6.2f}(±{branch_std:4.2f})     {doc_gini:6.4f}(±{doc_gini_std:5.4f})     {branch_gini:6.4f}(±{branch_gini_std:5.4f})     {run_count:4d}\")\n",
    "    else:\n",
    "        print(\"⚠️ Layer comparison file not found\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"✅ Step3 branching factor and Gini coefficient analysis completed! (Layer metrics only)\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "# Execute Step3 branching factor and Gini coefficient analysis (layer metrics only)\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step3\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Starting calculation of Step3 branching factor and Gini coefficient metrics (layer only)...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Calculate branching factor and Gini coefficient for each model (Step3 version, layer only)\n",
    "calculate_branching_and_gini_metrics_step3_no_global(base_path)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Starting aggregation of branching factor and Gini coefficient statistics by alpha value (layer only)...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 2. Aggregate by alpha (Step3 version, layer only)\n",
    "aggregate_branching_gini_by_alpha_step3_layer_only(base_path)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Displaying Step3 branching factor and Gini coefficient summary report (layer only)...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 3. Display summary (Step3 version, layer only)\n",
    "display_branching_gini_summary_step3_layer_only(base_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
