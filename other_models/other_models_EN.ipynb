{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2044b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 0. set-up part:  import necessary libraries and set up environment \"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from threading import Thread\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "import json\n",
    "import cProfile\n",
    "\n",
    "import gensim\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# download nltk data once time\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "#  chinese character support in matplotlib\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS' 'SimHei' 'DejaVu Sans']  \n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a654762",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 1.1 Data Preprocessing: load data, clean text, lemmatization, remove low-frequency words\"\"\"\n",
    "\n",
    "# Map POS tags to WordNet format， Penn Treebank annotation: fine-grained (45 tags), WordNet annotation: coarse-grained (4 tags: a, v, n, r)\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'  # 形容词\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'  # 动词\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n'  # 名词\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'  # 副词\n",
    "    else:\n",
    "        return 'n'  # 默认名词\n",
    "\n",
    "# Text cleaning and lemmatization preprocessing function\n",
    "def clean_and_lemmatize(text):\n",
    "    if pd.isnull(text):\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove non-alphabetic characters using regex\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmatized = [lemmatizer.lemmatize(w, get_wordnet_pos(pos)) for w, pos in pos_tags]\n",
    "    return lemmatized  \n",
    "\n",
    "#-----------------Load data----------------\n",
    "data = pd.read_excel('./data/raw/papers_CM.xlsx', usecols=['PaperID', 'Abstract', 'Keywords', 'Year'])\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# clean and lemmatize the abstracts\n",
    "data['Lemmatized_Tokens'] = data['Abstract'].apply(clean_and_lemmatize)\n",
    "\n",
    "# count word frequencies\n",
    "all_tokens = [word for tokens in data['Lemmatized_Tokens'] for word in tokens]\n",
    "word_counts = Counter(all_tokens)\n",
    "\n",
    "# set a minimum frequency threshold for valid words\n",
    "min_freq = 10\n",
    "valid_words = set([word for word, freq in word_counts.items() if freq >= min_freq])\n",
    "\n",
    "# remove rare words based on frequency threshold\n",
    "def remove_rare_words(tokens):\n",
    "    return [word for word in tokens if word in valid_words]\n",
    "\n",
    "data['Filtered_Tokens'] = data['Lemmatized_Tokens'].apply(remove_rare_words)\n",
    "\n",
    "# join tokens back into cleaned abstracts\n",
    "data['Cleaned_Abstract'] = data['Filtered_Tokens'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "# create a cleaned DataFrame with relevant columns\n",
    "cleaned_data = data[['PaperID', 'Year', 'Cleaned_Abstract']]\n",
    "cleaned_data = cleaned_data[~(cleaned_data['PaperID'] == 57188)] # this paper has no abstract\n",
    "cleaned_data = cleaned_data.reset_index(drop=True) \n",
    "cleaned_data.insert(0, 'Document_ID', range(len(cleaned_data))) \n",
    "abstract_list = cleaned_data['Cleaned_Abstract'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "corpus = {doc_id: abstract_list for doc_id, abstract_list in enumerate(abstract_list)}\n",
    "# cleaned_data.to_csv('./data/processed/cleaned_data.xlsx', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b1b2573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 General Topic Model Function Library\n",
      "============================================================\n",
      "✅ General parameters defined: K=252, α=0.1, η=0.05\n"
     ]
    }
   ],
   "source": [
    "# ===== General Function Library: Applicable to All Topic Models =====\n",
    "\n",
    "import math, random\n",
    "import tomotopy as tp\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "print(\"🔧 General Topic Model Function Library\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ===== General Parameter Definitions =====\n",
    "# These parameters can be shared by all models\n",
    "ALPHA = 0.1          # Dirichlet prior parameter for document-topic distribution\n",
    "ETA = 0.05           # Dirichlet prior parameter for topic-word distribution\n",
    "K_LEAF = 252         # Number of topics (aligned with the number of hLDA leaf nodes)\n",
    "\n",
    "MAX_ITERS = 1000     # Maximum number of training iterations\n",
    "INTERVAL = 100       # Interval for checking convergence\n",
    "PPL_TOL = 0.01       # Perplexity change tolerance (1%)\n",
    "COH_TOL = 0.005      # Coherence change tolerance\n",
    "STABLE_K = 5         # Number of consecutive times the early stopping condition is met\n",
    "B1, B2 = 59, 4       # Branching factor parameters for PAM/hPAM models\n",
    "\n",
    "print(f\"✅ General parameters defined: K={K_LEAF}, α={ALPHA}, η={ETA}\")\n",
    "\n",
    "# ===== Enhanced Coherence Calculation Function (Supports Multiple Metrics Including NPMI) =====\n",
    "def calculate_multiple_coherence_metrics(mdl, corpus_docs, metrics=['c_v', 'c_npmi'], fast_mode=True, top_n=5):\n",
    "    \"\"\"\n",
    "    Enhanced coherence calculation function - supports multiple coherence metrics including NPMI\n",
    "    \n",
    "    Supported coherence metrics:\n",
    "    - c_v: Vector space-based coherence (default)\n",
    "    - c_npmi: Normalized Pointwise Mutual Information (NPMI)\n",
    "    \n",
    "    Supported model types:\n",
    "    - Single-layer models: LDA, CTM\n",
    "    - Hierarchical models: hLDA, PAM, hPAM\n",
    "    - Non-parametric models: HDP\n",
    "    \n",
    "    Returns: dict containing various coherence metrics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        docs = list(corpus_docs)\n",
    "        dictionary = Dictionary(docs)\n",
    "        \n",
    "        # Check if it is a hierarchical model\n",
    "        model_type_str = str(type(mdl))\n",
    "        is_hierarchical = hasattr(mdl, 'depth') or 'HLDA' in model_type_str or 'PAM' in model_type_str\n",
    "        \n",
    "        if is_hierarchical:\n",
    "            # Hierarchical model: calculate coherence by level, weighted average\n",
    "            metrics_results = {metric: [] for metric in metrics}\n",
    "            layer_weights = []\n",
    "            \n",
    "            try:\n",
    "                for level in range(getattr(mdl, 'depth', 3)):\n",
    "                    level_topics = []\n",
    "                    level_doc_count = 0\n",
    "                    \n",
    "                    # Iterate through all nodes of this level\n",
    "                    for k in range(getattr(mdl, 'k', 100)):\n",
    "                        try:\n",
    "                            topic_words = mdl.get_topic_words(k, top_n=top_n)\n",
    "                            if topic_words:\n",
    "                                words = [word for word, prob in topic_words]\n",
    "                                level_topics.append(words)\n",
    "                                level_doc_count += 1\n",
    "                        except:\n",
    "                            continue\n",
    "                    \n",
    "                    if level_topics:\n",
    "                        # Calculate coherence for each metric\n",
    "                        for metric in metrics:\n",
    "                            try:\n",
    "                                cm = CoherenceModel(\n",
    "                                    topics=level_topics,\n",
    "                                    texts=docs,\n",
    "                                    dictionary=dictionary,\n",
    "                                    coherence=metric,\n",
    "                                    processes=1\n",
    "                                )\n",
    "                                score = cm.get_coherence()\n",
    "                                if score and not math.isnan(score):\n",
    "                                    metrics_results[metric].append(score)\n",
    "                                else:\n",
    "                                    metrics_results[metric].append(0.1)\n",
    "                            except Exception as e:\n",
    "                                print(f\"Hierarchical model {metric} calculation warning: {e}\")\n",
    "                                metrics_results[metric].append(0.1)\n",
    "                        \n",
    "                        layer_weights.append(level_doc_count)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Calculate weighted average\n",
    "            final_results = {}\n",
    "            for metric in metrics:\n",
    "                if metrics_results[metric] and layer_weights:\n",
    "                    total_weight = sum(layer_weights)\n",
    "                    if total_weight > 0:\n",
    "                        weighted_avg = sum(score * w for score, w in zip(metrics_results[metric], layer_weights)) / total_weight\n",
    "                        final_results[metric] = weighted_avg\n",
    "                    else:\n",
    "                        final_results[metric] = 0.1\n",
    "                else:\n",
    "                    final_results[metric] = 0.1\n",
    "            \n",
    "            return final_results\n",
    "        \n",
    "        # Single-layer model: calculate coherence for all topics\n",
    "        num_topics = getattr(mdl, 'k', None) or getattr(mdl, 'num_topics', None) or 100\n",
    "        topics = []\n",
    "        \n",
    "        for k in range(num_topics):\n",
    "            try:\n",
    "                topic_words = mdl.get_topic_words(k, top_n=top_n)\n",
    "                if topic_words:\n",
    "                    words = [word for word, prob in topic_words]\n",
    "                    topics.append(words)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if not topics:\n",
    "            return {metric: 0.1 for metric in metrics}\n",
    "        \n",
    "        # Calculate coherence for each metric\n",
    "        final_results = {}\n",
    "        for metric in metrics:\n",
    "            try:\n",
    "                cm = CoherenceModel(\n",
    "                    topics=topics,\n",
    "                    texts=docs,\n",
    "                    dictionary=dictionary,\n",
    "                    coherence=metric,\n",
    "                    processes=1\n",
    "                )\n",
    "                score = cm.get_coherence()\n",
    "                final_results[metric] = score if score and not math.isnan(score) else 0.1\n",
    "            except Exception as e:\n",
    "                print(f\"Single-layer model {metric} calculation warning: {e}\")\n",
    "                final_results[metric] = 0.1\n",
    "        \n",
    "        return final_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Coherence calculation error: {e}\")\n",
    "        return {metric: 0.1 for metric in metrics}\n",
    "\n",
    "def calculate_gensim_coherence(mdl, corpus_docs, coherence='c_v', fast_mode=True):\n",
    "    \"\"\"\n",
    "    Backward-compatible coherence calculation function\n",
    "    \n",
    "    Now internally calls the enhanced version but maintains the original interface\n",
    "    \"\"\"\n",
    "    results = calculate_multiple_coherence_metrics(mdl, corpus_docs, [coherence], fast_mode)\n",
    "    return results.get(coherence, 0.1)\n",
    "\n",
    "# ===== General Effective Topic Count Calculation Function =====\n",
    "def effective_k(mdl, top_n=1, min_prob=1e-12):\n",
    "    \"\"\"\n",
    "    General effective topic count estimation function - applicable to all topic models\n",
    "    \n",
    "    Supported model types:\n",
    "    - Parametric models: LDA, CTM, hLDA, PAM, hPAM (fixed number of topics)\n",
    "    - Non-parametric models: HDP (dynamic number of topics)\n",
    "    \"\"\"\n",
    "    kmax = getattr(mdl, 'k', None) or getattr(mdl, 'num_topics', None) or 0\n",
    "    eff = 0\n",
    "    for k in range(kmax):\n",
    "        try:\n",
    "            tw = mdl.get_topic_words(k, top_n=top_n)\n",
    "            if tw and tw[0][1] > min_prob: \n",
    "                eff += 1\n",
    "        except:\n",
    "            continue\n",
    "    return eff\n",
    "\n",
    "# ===== Enhanced Auto-Training Function (Supports Multiple Coherence Metrics) =====\n",
    "def auto_train_with_multiple_metrics(mdl, docs, seed=0, max_iters=MAX_ITERS, interval=INTERVAL,\n",
    "                                   ppl_tol=PPL_TOL, coh_tol=COH_TOL, stable_k=STABLE_K,\n",
    "                                   coherence_measures=['c_v', 'c_npmi'], name=None, fast_coherence=True):\n",
    "    \"\"\"\n",
    "    Enhanced auto-training function - supports multiple coherence metrics\n",
    "    \n",
    "    Features:\n",
    "    - Early stopping mechanism (based on perplexity and primary coherence stability)\n",
    "    - Training history logging\n",
    "    - Multiple coherence metric calculation (C_v, NPMI)\n",
    "    - Supports custom training parameters\n",
    "    \n",
    "    Parameters:\n",
    "    - coherence_measures: List of coherence metrics, e.g., ['c_v', 'c_npmi']\n",
    "    \n",
    "    Returns:\n",
    "    - model: Trained model\n",
    "    - history: Training history [(iter, llpw, ppl, coherence_dict), ...]\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    for d in docs: \n",
    "        mdl.add_doc(d)\n",
    "\n",
    "    prev_ppl = prev_coh = None\n",
    "    hold_ppl = hold_coh = 0\n",
    "    hist = []  # (iter, ll_per_word, ppl, coherence_dict)\n",
    "    \n",
    "    primary_coherence = coherence_measures[0]  # Use the first metric as the criterion for early stopping\n",
    "\n",
    "    for it in range(interval, max_iters + 1, interval):\n",
    "        mdl.train(interval)\n",
    "        llpw = mdl.ll_per_word\n",
    "        ppl  = math.exp(-llpw)\n",
    "        \n",
    "        print(f\"[{name or mdl.__class__.__name__}] iter={it:4d} llpw={llpw:.4f} ppl={ppl:.2f} Calculating multiple coherence metrics...\")\n",
    "        \n",
    "        # Calculate multiple coherence metrics\n",
    "        coherence_dict = calculate_multiple_coherence_metrics(mdl, docs, coherence_measures, fast_mode=fast_coherence)\n",
    "        primary_coh = coherence_dict.get(primary_coherence, 0.1)\n",
    "        \n",
    "        hist.append((it, llpw, ppl, coherence_dict))\n",
    "        \n",
    "        # Print results\n",
    "        coh_str = \", \".join([f\"{metric}={score:.4f}\" for metric, score in coherence_dict.items()])\n",
    "        print(f\"[{name or mdl.__class__.__name__}] iter={it:4d} llpw={llpw:.4f} ppl={ppl:.2f} {coh_str}\")\n",
    "\n",
    "        # Early stopping criterion (based on the primary coherence metric)\n",
    "        if prev_ppl is not None:\n",
    "            if abs(prev_ppl - ppl)/max(prev_ppl, 1e-12) < ppl_tol:  \n",
    "                hold_ppl += 1\n",
    "            else:\n",
    "                hold_ppl = 0\n",
    "        if prev_coh is not None:\n",
    "            if abs(prev_coh - primary_coh) < coh_tol: \n",
    "                hold_coh += 1\n",
    "            else: \n",
    "                hold_coh = 0\n",
    "        prev_ppl, prev_coh = ppl, primary_coh\n",
    "\n",
    "        if hold_ppl >= stable_k and hold_coh >= stable_k:\n",
    "            print(f\"[{name or mdl.__class__.__name__}] early stop at iter {it}\")\n",
    "            break\n",
    "    return mdl, hist\n",
    "\n",
    "def auto_train(mdl, docs, seed=0, max_iters=MAX_ITERS, interval=INTERVAL,\n",
    "               ppl_tol=PPL_TOL, coh_tol=COH_TOL, stable_k=STABLE_K,\n",
    "               coherence_measure='c_v', name=None, fast_coherence=True):\n",
    "    \"\"\"\n",
    "    Backward-compatible auto-training function\n",
    "    \n",
    "    Now internally calls the enhanced version but maintains the original interface\n",
    "    \"\"\"\n",
    "    enhanced_model, enhanced_hist = auto_train_with_multiple_metrics(\n",
    "        mdl, docs, seed, max_iters, interval, ppl_tol, coh_tol, stable_k,\n",
    "        [coherence_measure], name, fast_coherence\n",
    "    )\n",
    "    \n",
    "    # Convert history format for compatibility\n",
    "    compatible_hist = []\n",
    "    for it, llpw, ppl, coh_dict in enhanced_hist:\n",
    "        primary_coh = coh_dict.get(coherence_measure, 0.1)\n",
    "        compatible_hist.append((it, llpw, ppl, primary_coh))\n",
    "    \n",
    "    return enhanced_model, compatible_hist\n",
    "\n",
    "# ===== General Topic Analysis Function =====\n",
    "def analyze_model_topics(model, model_name=\"Model\", top_words=5, min_prob=0.01, max_display=10):\n",
    "    \"\"\"\n",
    "    General topic analysis function - applicable to all topic models\n",
    "    \n",
    "    Functionality:\n",
    "    - Extracts active topics\n",
    "    - Displays topic words and weights\n",
    "    - Calculates topic activity rate\n",
    "    \"\"\"\n",
    "    print(f\"\\n🔍 {model_name} Topic Analysis (showing top {top_words} words):\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    active_topics = 0\n",
    "    topic_info = []\n",
    "    \n",
    "    num_topics = getattr(model, 'k', None) or getattr(model, 'num_topics', None) or 0\n",
    "    \n",
    "    for k in range(num_topics):\n",
    "        try:\n",
    "            topic_words = model.get_topic_words(k, top_n=top_words)\n",
    "            if topic_words and topic_words[0][1] > min_prob:\n",
    "                active_topics += 1\n",
    "                words_str = \", \".join([f\"{word}({prob:.3f})\" for word, prob in topic_words[:5]])\n",
    "                topic_info.append((k, topic_words[0][1], words_str))\n",
    "                \n",
    "                if active_topics <= max_display:\n",
    "                    print(f\"Topic {k:3d} (weight:{topic_words[0][1]:.3f}): {words_str}\")\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if active_topics > max_display:\n",
    "        print(f\"... (and {active_topics - max_display} more active topics)\")\n",
    "    \n",
    "    print(f\"\\n📊 {model_name} Topic Statistics:\")\n",
    "    print(f\"   - Active topics: {active_topics}/{num_topics}\")\n",
    "    print(f\"   - Topic activity rate: {active_topics/num_topics*100:.1f}%\")\n",
    "    \n",
    "    return topic_info\n",
    "\n",
    "# ===== Multiple Coherence Metrics Evaluation Function =====\n",
    "def evaluate_model_with_multiple_coherence(model, docs, model_name=\"Model\", \n",
    "                                         coherence_metrics=['c_v', 'c_npmi'],\n",
    "                                         top_words_for_coherence=5):\n",
    "    \"\"\"\n",
    "    Evaluate a model using multiple coherence metrics\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained topic model\n",
    "    - docs: List of documents\n",
    "    - model_name: Name of the model\n",
    "    - coherence_metrics: List of coherence metrics to calculate\n",
    "    - top_words_for_coherence: Number of topic words to use for coherence calculation\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary containing all coherence metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n🔍 {model_name} Multiple Coherence Metrics Evaluation:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "\n",
    "    # Calculate multiple coherence metrics\n",
    "    coherence_results = calculate_multiple_coherence_metrics(\n",
    "        model, docs, coherence_metrics, top_n=top_words_for_coherence\n",
    "    )\n",
    "    \n",
    "    print(f\"📊 Coherence Metrics Results:\")\n",
    "    for metric, score in coherence_results.items():\n",
    "        metric_desc = {\n",
    "            'c_v': 'C_v (Vector Space Coherence)',\n",
    "            'c_npmi': 'NPMI (Normalized Pointwise Mutual Information)'\n",
    "        }\n",
    "        description = metric_desc.get(metric, metric)\n",
    "        print(f\"   📈 {description}: {score:.6f}\")\n",
    "    \n",
    "    # Calculate basic metrics\n",
    "    effective_topics = effective_k(model)\n",
    "    total_topics = getattr(model, 'k', None) or getattr(model, 'num_topics', None) or 0\n",
    "    \n",
    "    print(f\"\\n📊 Basic Metrics:\")\n",
    "    print(f\"   📈 Effective topics: {effective_topics}\")\n",
    "    print(f\"   📈 Total topics: {total_topics}\")\n",
    "    if total_topics > 0:\n",
    "        print(f\"   📈 Topic utilization: {effective_topics/total_topics*100:.1f}%\")\n",
    "    \n",
    "    # Metric explanations\n",
    "    print(f\"\\n💡 Coherence Metric Explanations:\")\n",
    "    print(f\"   🎯 C_v: Range [0,1], higher is better, based on word vector similarity\")\n",
    "    print(f\"   🎯 NPMI: Range [-1,1], higher is better, Normalized Pointwise Mutual Information\")\n",
    "    \n",
    "    # Composite score (optional)\n",
    "    # Normalize each metric and calculate a weighted average\n",
    "    normalized_scores = {}\n",
    "    \n",
    "    # C_v and NPMI: higher is better\n",
    "    normalized_scores['c_v'] = max(0, coherence_results.get('c_v', 0))\n",
    "    if 'c_npmi' in coherence_results:\n",
    "        # NPMI range is [-1,1], convert to [0,1]\n",
    "        npmi_score = coherence_results['c_npmi']\n",
    "        normalized_scores['c_npmi'] = (npmi_score + 1) / 2\n",
    "    \n",
    "    \n",
    "    if normalized_scores:\n",
    "        composite_score = np.mean(list(normalized_scores.values()))\n",
    "        print(f\"\\n🏆 Composite Coherence Score: {composite_score:.4f} (average of normalized scores)\")\n",
    "    \n",
    "    # Return full results\n",
    "    result = {\n",
    "        'coherence_metrics': coherence_results,\n",
    "        'effective_topics': effective_topics,\n",
    "        'total_topics': total_topics,\n",
    "        'topic_utilization': effective_topics/total_topics if total_topics > 0 else 0,\n",
    "        'normalized_scores': normalized_scores,\n",
    "        'composite_score': composite_score if normalized_scores else 0\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fadcef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Enhanced LDA training function defined\n",
      "💡 Features:\n",
      "   - Supports simultaneous calculation of multiple coherence metrics\n",
      "   - Includes metrics like NPMI, C_v\n",
      "   - Provides a composite coherence score\n",
      "   - Detailed training process tracking\n",
      "   - Maintains compatibility with the original interface\n",
      "\n",
      "📋 Usage:\n",
      "   model, metrics, history = train_lda_with_multiple_coherence_metrics(docs, K=252)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== Enhanced LDA Model Training (with Multiple Coherence Metrics Support) =====\n",
    "\n",
    "def train_lda_with_multiple_coherence_metrics(docs, seed=42, max_iters=1000, K=252, detailed_output=True):\n",
    "    \"\"\"\n",
    "    Train an LDA model using multiple coherence metrics.\n",
    "    \n",
    "    This function demonstrates how to use multiple coherence metrics (including NPMI)\n",
    "    during the training process.\n",
    "    \n",
    "    Parameters:\n",
    "    - docs: List of documents\n",
    "    - seed: Random seed\n",
    "    - max_iters: Maximum number of training iterations\n",
    "    - K: Number of topics\n",
    "    - detailed_output: Whether to output detailed information\n",
    "    \n",
    "    Returns:\n",
    "    - model: Trained LDA model\n",
    "    - metrics: A dictionary of evaluation metrics including multiple coherence scores\n",
    "    - history: Training history\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🚀 Training LDA model with multiple coherence metrics (K={K}, seed={seed})\")\n",
    "    print(f\"📊 Data Overview:\")\n",
    "    print(f\"   - Number of documents: {len(docs)}\")\n",
    "    print(f\"   - Average document length: {np.mean([len(doc) for doc in docs]):.1f} words\")\n",
    "    print(f\"   - Vocabulary size: {len(set(word for doc in docs for word in doc))}\")\n",
    "    \n",
    "    print(f\"\\n🔧 Model Parameters:\")\n",
    "    print(f\"   - Number of topics K = {K}\")\n",
    "    print(f\"   - Alpha = {ALPHA}\")\n",
    "    print(f\"   - Eta = {ETA}\")\n",
    "    print(f\"   - Max iterations = {max_iters}\")\n",
    "    print(f\"   - Coherence metrics = ['c_v', 'c_npmi']\")\n",
    "    \n",
    "    # Create LDA model\n",
    "    print(f\"\\n🚀 Creating LDA model...\")\n",
    "    lda_model = tp.LDAModel(k=K, alpha=ALPHA, eta=ETA, seed=seed)\n",
    "    \n",
    "    # Use the enhanced multi-metric training function\n",
    "    print(f\"\\n🎯 Starting training with multiple coherence metrics...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    trained_model, training_history = auto_train_with_multiple_metrics(\n",
    "        lda_model, docs, \n",
    "        seed=seed, \n",
    "        max_iters=max_iters,\n",
    "        interval=100,  # Check every 100 iterations\n",
    "        coherence_measures=['c_v', 'c_npmi'],  # Use multiple coherence metrics\n",
    "        name='LDA-Multi-Coherence'\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\n✅ Training complete! Time taken: {training_time:.1f} seconds\")\n",
    "    \n",
    "    # Extract final metrics\n",
    "    if training_history:\n",
    "        final_iter, final_llpw, final_ppl, final_coherence_dict = training_history[-1]\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        effective_topics = effective_k(trained_model)\n",
    "        \n",
    "        # Perform a full evaluation with multiple coherence metrics\n",
    "        print(f\"\\n🔍 Performing full evaluation with multiple coherence metrics...\")\n",
    "        full_evaluation = evaluate_model_with_multiple_coherence(\n",
    "            trained_model, docs, \"LDA\", \n",
    "            coherence_metrics=['c_v', 'c_npmi'],\n",
    "            top_words_for_coherence=5\n",
    "        )\n",
    "        \n",
    "        # Build the complete evaluation metrics\n",
    "        metrics = {\n",
    "            'iterations': final_iter,\n",
    "            'log_likelihood_per_word': final_llpw,\n",
    "            'perplexity': final_ppl,\n",
    "            'effective_topics': effective_topics,\n",
    "            'training_time_seconds': training_time,\n",
    "            'convergence_iterations': len(training_history),\n",
    "            \n",
    "            # Multiple coherence metrics\n",
    "            'coherence_c_v': full_evaluation['coherence_metrics'].get('c_v', 0),\n",
    "            'coherence_npmi': full_evaluation['coherence_metrics'].get('c_npmi', 0),\n",
    "            \n",
    "            # Composite score\n",
    "            'composite_coherence_score': full_evaluation.get('composite_score', 0),\n",
    "            'topic_utilization': full_evaluation.get('topic_utilization', 0),\n",
    "            \n",
    "            # Full evaluation results\n",
    "            'full_evaluation': full_evaluation\n",
    "        }\n",
    "        \n",
    "        if detailed_output:\n",
    "            print(f\"\\n📈 Final Evaluation Results:\")\n",
    "            print(f\"   - Training iterations: {final_iter}\")\n",
    "            print(f\"   - Log-likelihood per word: {final_llpw:.6f}\")\n",
    "            print(f\"   - Perplexity: {final_ppl:.2f}\")\n",
    "            print(f\"   - Effective topics: {effective_topics}/{K}\")\n",
    "            print(f\"   - Topic utilization: {metrics['topic_utilization']*100:.1f}%\")\n",
    "            \n",
    "            print(f\"\\n📊 Multiple Coherence Metrics Comparison:\")\n",
    "            print(f\"   - C_v (Vector Space): {metrics['coherence_c_v']:.6f}\")\n",
    "            print(f\"   - NPMI (Pointwise Mutual Information): {metrics['coherence_npmi']:.6f}\")\n",
    "            print(f\"   - Composite Score: {metrics['composite_coherence_score']:.6f}\")\n",
    "            \n",
    "            # Display training process overview\n",
    "            if len(training_history) > 1:\n",
    "                first_iter, first_llpw, first_ppl, first_coh_dict = training_history[0]\n",
    "                first_cv = first_coh_dict.get('c_v', 0)\n",
    "                first_npmi = first_coh_dict.get('c_npmi', 0)\n",
    "                final_cv = final_coherence_dict.get('c_v', 0)\n",
    "                final_npmi = final_coherence_dict.get('c_npmi', 0)\n",
    "                \n",
    "                print(f\"\\n📊 Training Improvement:\")\n",
    "                print(f\"   - Perplexity improvement: {first_ppl:.2f} → {final_ppl:.2f} (↓{first_ppl-final_ppl:.2f})\")\n",
    "                print(f\"   - C_v improvement: {first_cv:.4f} → {final_cv:.4f} (↑{final_cv-first_cv:.4f})\")\n",
    "                print(f\"   - NPMI improvement: {first_npmi:.4f} → {final_npmi:.4f} (↑{final_npmi-first_npmi:.4f})\")\n",
    "                print(f\"   - Log-likelihood improvement: {first_llpw:.6f} → {final_llpw:.6f} (↑{final_llpw-first_llpw:.6f})\")\n",
    "    else:\n",
    "        print(\"❌ No training history recorded\")\n",
    "        metrics = {}\n",
    "    \n",
    "    return trained_model, metrics, training_history\n",
    "\n",
    "print(\"🎯 Enhanced LDA training function defined\")\n",
    "print(\"💡 Features:\")\n",
    "print(\"   - Supports simultaneous calculation of multiple coherence metrics\")\n",
    "print(\"   - Includes metrics like NPMI, C_v\")\n",
    "print(\"   - Provides a composite coherence score\")\n",
    "print(\"   - Detailed training process tracking\")\n",
    "print(\"   - Maintains compatibility with the original interface\")\n",
    "\n",
    "print(f\"\\n📋 Usage:\")\n",
    "print(f\"   model, metrics, history = train_lda_with_multiple_coherence_metrics(docs, K=252)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "095a32fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Guide to Using NPMI and Multiple Coherence Metrics in hLDA Comparative Studies\n",
      "================================================================================\n",
      "📋 Guide for Comparative Study Based on Your hLDA Parameters:\n",
      "============================================================\n",
      "🎯 Your hLDA Configuration:\n",
      "   - depth: 3\n",
      "   - gamma: 0.05\n",
      "   - eta: 0.05\n",
      "   - alpha: 0.1\n",
      "   - leaf_nodes: 252\n",
      "   - branching: Layer 0-1: 59, Layer 1-2: 4\n",
      "\n",
      "📊 Recommended Coherence Metric Comparison Schemes:\n",
      "\n",
      "📈 Primary Comparison Metrics:\n",
      "   - Metrics: ['c_v', 'c_npmi']\n",
      "   - Rationale: C_v is a standard metric, NPMI provides a pointwise mutual information perspective\n",
      "\n",
      "📈 Comprehensive Comparison Metrics:\n",
      "   - Metrics: ['c_v', 'c_npmi']\n",
      "   - Rationale: Covers coherence measures with different theoretical foundations\n",
      "\n",
      "📈 Research-Focused Metrics:\n",
      "   - Metrics: ['c_v', 'c_npmi']\n",
      "   - Rationale: Balances computational efficiency and evaluation comprehensiveness\n",
      "\n",
      "🔧 Implementation Recommendations:\n",
      "   1. Evaluate all models (LDA, PAM, CTM) using the same coherence metrics.\n",
      "   2. Focus on the comparative results of C_v and NPMI.\n",
      "   3. Analyze the consistency of model rankings under different metrics.\n",
      "   4. Consider the trade-off between coherence metrics and perplexity.\n",
      "   5. Record the performance differences of each model under different metrics.\n",
      "\n",
      "💡 Comparative Analysis Suggestions:\n",
      "   📊 Multi-angle evaluation: Use both C_v and NPMI to avoid single-metric bias.\n",
      "   🎯 Hierarchical comparison: Compare the coherence differences between hLDA's structure and other models.\n",
      "   ⚖️  Trade-off analysis: Analyze the relationship between coherence improvement and computational complexity.\n",
      "   📈 Stability check: Use multiple random seeds to verify the stability of coherence metrics.\n",
      "   🔍 Topic quality: Combine with qualitative analysis to validate the practical significance of coherence metrics.\n",
      "\n",
      "🎯 Specific Implementation Code Template:\n",
      "\n",
      "# Compare multiple coherence metrics across different models\n",
      "models_to_compare = ['LDA', 'PAM', 'CTM', 'hLDA']\n",
      "coherence_results = {}\n",
      "\n",
      "for model_name in models_to_compare:\n",
      "    if model_name == 'hLDA':\n",
      "        # Get hLDA results from step3\n",
      "        coherence_results[model_name] = get_hlda_coherence_from_step3()\n",
      "    else:\n",
      "        # Calculate coherence for other models\n",
      "        model = globals()[f'{model_name.lower()}_model']\n",
      "        evaluation = evaluate_model_with_multiple_coherence(\n",
      "            model, docs, model_name,\n",
      "            coherence_metrics=['c_v', 'c_npmi']\n",
      "        )\n",
      "        coherence_results[model_name] = evaluation['coherence_metrics']\n",
      "\n",
      "# Generate a comparison report\n",
      "generate_coherence_comparison_report(coherence_results)\n",
      "\n",
      "\n",
      "📋 Research Value:\n",
      "   🔬 Methodological contribution: Multiple coherence metrics provide a more comprehensive evaluation of topic quality.\n",
      "   📊 Result credibility: Metrics like NPMI enhance the persuasiveness of research findings.\n",
      "   🎯 Comparison fairness: A unified evaluation framework ensures fairness in model comparison.\n",
      "   💡 Theoretical depth: Different coherence metrics reflect different theoretical perspectives on topic modeling.\n",
      "   🚀 Practical application: Multi-metric evaluation better guides model selection in practical applications.\n"
     ]
    }
   ],
   "source": [
    "# ===== Guide to Using NPMI and Multiple Coherence Metrics in hLDA Comparative Studies =====\n",
    "\n",
    "print(\"🎯 Guide to Using NPMI and Multiple Coherence Metrics in hLDA Comparative Studies\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def demonstrate_npmi_usage_for_hlda_comparison():\n",
    "    \"\"\"\n",
    "    Demonstrates how to use NPMI and other coherence metrics in hLDA comparative studies.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"📋 Guide for Comparative Study Based on Your hLDA Parameters:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    hlda_params = {\n",
    "        'depth': 3,\n",
    "        'gamma': 0.05,\n",
    "        'eta': 0.05,\n",
    "        'alpha': 0.1,\n",
    "        'leaf_nodes': 252,\n",
    "        'branching': 'Layer 0-1: 59, Layer 1-2: 4'\n",
    "    }\n",
    "    \n",
    "    print(\"🎯 Your hLDA Configuration:\")\n",
    "    for param, value in hlda_params.items():\n",
    "        print(f\"   - {param}: {value}\")\n",
    "    \n",
    "    print(f\"\\n📊 Recommended Coherence Metric Comparison Schemes:\")\n",
    "    \n",
    "    coherence_strategies = {\n",
    "        'primary_metrics': {\n",
    "            'description': 'Primary Comparison Metrics',\n",
    "            'metrics': ['c_v', 'c_npmi'],\n",
    "            'reason': 'C_v is a standard metric, NPMI provides a pointwise mutual information perspective'\n",
    "        },\n",
    "        'comprehensive_metrics': {\n",
    "            'description': 'Comprehensive Comparison Metrics',\n",
    "            'metrics': ['c_v', 'c_npmi'],\n",
    "            'reason': 'Covers coherence measures with different theoretical foundations'\n",
    "        },\n",
    "        'research_focused': {\n",
    "            'description': 'Research-Focused Metrics',\n",
    "            'metrics': ['c_v', 'c_npmi'],\n",
    "            'reason': 'Balances computational efficiency and evaluation comprehensiveness'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for strategy_name, details in coherence_strategies.items():\n",
    "        print(f\"\\n📈 {details['description']}:\")\n",
    "        print(f\"   - Metrics: {details['metrics']}\")\n",
    "        print(f\"   - Rationale: {details['reason']}\")\n",
    "    \n",
    "    print(f\"\\n🔧 Implementation Recommendations:\")\n",
    "    \n",
    "    implementation_steps = [\n",
    "        \"1. Evaluate all models (LDA, PAM, CTM) using the same coherence metrics.\",\n",
    "        \"2. Focus on the comparative results of C_v and NPMI.\",\n",
    "        \"3. Analyze the consistency of model rankings under different metrics.\",\n",
    "        \"4. Consider the trade-off between coherence metrics and perplexity.\",\n",
    "        \"5. Record the performance differences of each model under different metrics.\"\n",
    "    ]\n",
    "    \n",
    "    for step in implementation_steps:\n",
    "        print(f\"   {step}\")\n",
    "    \n",
    "    print(f\"\\n💡 Comparative Analysis Suggestions:\")\n",
    "    \n",
    "    analysis_suggestions = [\n",
    "        \"📊 Multi-angle evaluation: Use both C_v and NPMI to avoid single-metric bias.\",\n",
    "        \"🎯 Hierarchical comparison: Compare the coherence differences between hLDA's structure and other models.\",\n",
    "        \"⚖️  Trade-off analysis: Analyze the relationship between coherence improvement and computational complexity.\",\n",
    "        \"📈 Stability check: Use multiple random seeds to verify the stability of coherence metrics.\",\n",
    "        \"🔍 Topic quality: Combine with qualitative analysis to validate the practical significance of coherence metrics.\"\n",
    "    ]\n",
    "    \n",
    "    for suggestion in analysis_suggestions:\n",
    "        print(f\"   {suggestion}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Specific Implementation Code Template:\")\n",
    "    \n",
    "    code_template = '''\n",
    "# Compare multiple coherence metrics across different models\n",
    "models_to_compare = ['LDA', 'PAM', 'CTM', 'hLDA']\n",
    "coherence_results = {}\n",
    "\n",
    "for model_name in models_to_compare:\n",
    "    if model_name == 'hLDA':\n",
    "        # Get hLDA results from step3\n",
    "        coherence_results[model_name] = get_hlda_coherence_from_step3()\n",
    "    else:\n",
    "        # Calculate coherence for other models\n",
    "        model = globals()[f'{model_name.lower()}_model']\n",
    "        evaluation = evaluate_model_with_multiple_coherence(\n",
    "            model, docs, model_name,\n",
    "            coherence_metrics=['c_v', 'c_npmi']\n",
    "        )\n",
    "        coherence_results[model_name] = evaluation['coherence_metrics']\n",
    "\n",
    "# Generate a comparison report\n",
    "generate_coherence_comparison_report(coherence_results)\n",
    "'''\n",
    "    \n",
    "    print(code_template)\n",
    "    \n",
    "    print(f\"\\n📋 Research Value:\")\n",
    "    \n",
    "    research_values = [\n",
    "        \"🔬 Methodological contribution: Multiple coherence metrics provide a more comprehensive evaluation of topic quality.\",\n",
    "        \"📊 Result credibility: Metrics like NPMI enhance the persuasiveness of research findings.\",\n",
    "        \"🎯 Comparison fairness: A unified evaluation framework ensures fairness in model comparison.\",\n",
    "        \"💡 Theoretical depth: Different coherence metrics reflect different theoretical perspectives on topic modeling.\",\n",
    "        \"🚀 Practical application: Multi-metric evaluation better guides model selection in practical applications.\"\n",
    "    ]\n",
    "    \n",
    "    for value in research_values:\n",
    "        print(f\"   {value}\")\n",
    "\n",
    "# Execute demonstration\n",
    "demonstrate_npmi_usage_for_hlda_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87c933e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Starting LDA model training\n",
      "============================================================\n",
      "\n",
      "🎯 Training LDA model with the full dataset...\n",
      "📚 Using full dataset: 970 documents\n",
      "🚀 Starting to train LDA model (seed=42)\n",
      "📊 Data Overview:\n",
      "   - Number of documents: 970\n",
      "   - Average document length: 85.8 words\n",
      "   - Vocabulary size: 1490\n",
      "\n",
      "🔧 Model Parameters:\n",
      "   - Number of topics K = 252\n",
      "   - Alpha = 0.1\n",
      "   - Eta = 0.05\n",
      "   - Max iterations = 300\n",
      "   - Check interval = 100\n",
      "   - Coherence metrics = ['c_v', 'c_npmi']\n",
      "🚀 Creating LDA model (K=252, α=0.1, η=0.05, seed=42)\n",
      "[LDA] iter= 100 llpw=-7.5244 ppl=1852.68 Calculating multiple coherence metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v5/6mdkg5713kxgwg5xs24g8rvr0000gn/T/ipykernel_55006/1551040497.py:208: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  mdl.train(interval)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LDA] iter= 100 llpw=-7.5244 ppl=1852.68 c_v=0.5816, c_npmi=0.0445\n",
      "[LDA] iter= 200 llpw=-7.3837 ppl=1609.54 Calculating multiple coherence metrics...\n",
      "[LDA] iter= 200 llpw=-7.3837 ppl=1609.54 c_v=0.5787, c_npmi=0.0561\n",
      "[LDA] iter= 300 llpw=-7.3257 ppl=1518.89 Calculating multiple coherence metrics...\n",
      "[LDA] iter= 300 llpw=-7.3257 ppl=1518.89 c_v=0.5757, c_npmi=0.0554\n",
      "\n",
      "✅ Training complete! Time taken: 25.9 seconds\n",
      "\n",
      "🔍 Performing full evaluation with multiple coherence metrics...\n",
      "\n",
      "🔍 LDA Multiple Coherence Metrics Evaluation:\n",
      "================================================================================\n",
      "📊 Coherence Metrics Results:\n",
      "   📈 C_v (Vector Space Coherence): 0.575673\n",
      "   📈 NPMI (Normalized Pointwise Mutual Information): 0.055425\n",
      "\n",
      "📊 Basic Metrics:\n",
      "   📈 Effective topics: 252\n",
      "   📈 Total topics: 252\n",
      "   📈 Topic utilization: 100.0%\n",
      "\n",
      "💡 Coherence Metric Explanations:\n",
      "   🎯 C_v: Range [0,1], higher is better, based on word vector similarity\n",
      "   🎯 NPMI: Range [-1,1], higher is better, Normalized Pointwise Mutual Information\n",
      "\n",
      "🏆 Composite Coherence Score: 0.5517 (average of normalized scores)\n",
      "\n",
      "📈 Final Evaluation Results:\n",
      "   - Training iterations: 300\n",
      "   - Log-likelihood per word: -7.325733\n",
      "   - Perplexity: 1518.89\n",
      "   - Effective topics: 252/252\n",
      "\n",
      "📊 Multiple Coherence Metrics:\n",
      "   - C_v (Vector Space): 0.575673\n",
      "   - NPMI (Pointwise Mutual Information): 0.055425\n",
      "   - Composite Score: 0.551693\n",
      "   - Topic Utilization: 100.0%\n",
      "   - Convergence history length: 3\n",
      "\n",
      "📊 Training Improvement:\n",
      "   - Perplexity improvement: 1852.68 → 1518.89 (↓333.79)\n",
      "   - C_v improvement: 0.5816 → 0.5757 (↑-0.0059)\n",
      "   - NPMI improvement: 0.0445 → 0.0554 (↑0.0110)\n",
      "   - Log-likelihood improvement: -7.524389 → -7.325733 (↑0.198656)\n",
      "\n",
      "🔍 LDA Topic Analysis (showing top 5 words):\n",
      "================================================================================\n",
      "Topic   0 (weight:0.050): refinement(0.050), fcm(0.034), local(0.027), criterion(0.023), automatically(0.023)\n",
      "Topic   1 (weight:0.030): frequency(0.030), poroelastic(0.030), multiphysics(0.030), ratio(0.023), soil(0.023)\n",
      "Topic   2 (weight:0.257): model(0.257), framework(0.052), computational(0.038), couple(0.035), work(0.035)\n",
      "Topic   3 (weight:0.199): boundary(0.199), condition(0.175), displacement(0.028), give(0.026), impose(0.020)\n",
      "Topic   4 (weight:0.081): isogeometric(0.081), analysis(0.046), nurbs(0.042), iga(0.035), bsplines(0.033)\n",
      "Topic   5 (weight:0.153): fractional(0.153), equation(0.082), diffusion(0.062), derivative(0.054), order(0.027)\n",
      "Topic   6 (weight:0.041): computation(0.041), nonlinear(0.038), procedure(0.035), process(0.035), reducedorder(0.035)\n",
      "Topic   7 (weight:0.075): system(0.075), develop(0.043), provide(0.041), framework(0.034), energy(0.030)\n",
      "Topic   8 (weight:0.066): flow(0.066), simulation(0.057), blood(0.048), mesoscopic(0.044), method(0.018)\n",
      "Topic   9 (weight:0.058): instability(0.058), stability(0.047), membrane(0.032), symmetry(0.032), mode(0.021)\n",
      "... (and 229 more active topics)\n",
      "\n",
      "📊 LDA Topic Statistics:\n",
      "   - Active topics: 239/252\n",
      "   - Topic activity rate: 94.8%\n",
      "\n",
      "🎯 LDA model evaluation complete!\n",
      "📊 Key Metrics:\n",
      "   - iterations: 300\n",
      "   - log_likelihood_per_word: -7.3257\n",
      "   - perplexity: 1518.8861\n",
      "   - coherence_c_v: 0.5757\n",
      "   - coherence_npmi: 0.0554\n",
      "   - effective_topics: 252\n",
      "   - training_time_seconds: 25.8515\n",
      "   - convergence_iterations: 3\n",
      "   - composite_score: 0.5517\n",
      "   - topic_utilization: 1.0000\n",
      "\n",
      "💾 Model is trained and can be used for:\n",
      "   1. Topic word extraction\n",
      "   2. Document-topic distribution analysis\n",
      "   3. Comparison with other models\n",
      "   4. Topic evolution analysis\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== LDA Model Construction and Training =====\n",
    "\n",
    "print(\"🎯 Starting LDA model training\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def build_lda(docs, seed=0, max_iters=MAX_ITERS, use_multiple_coherence=True):\n",
    "    \"\"\"Build and train an LDA model - supports multiple coherence metrics\"\"\"\n",
    "    print(f\"🚀 Creating LDA model (K={K_LEAF}, α={ALPHA}, η={ETA}, seed={seed})\")\n",
    "    mdl = tp.LDAModel(k=K_LEAF, alpha=ALPHA, eta=ETA, seed=seed)\n",
    "    \n",
    "    if use_multiple_coherence:\n",
    "        # Train with multiple coherence metrics\n",
    "        return auto_train_with_multiple_metrics(\n",
    "            mdl, docs, seed=seed, max_iters=max_iters, \n",
    "            coherence_measures=['c_v', 'c_npmi'], \n",
    "            name='LDA'\n",
    "        )\n",
    "    else:\n",
    "        # Train with a traditional single metric\n",
    "        return auto_train(mdl, docs, seed=seed, max_iters=max_iters, name='LDA')\n",
    "\n",
    "def train_and_evaluate_lda(docs, seed=42, max_iters=2000, detailed_output=True, use_multiple_coherence=True):\n",
    "    \"\"\"\n",
    "    Train an LDA model and perform a full evaluation - supports multiple coherence metrics\n",
    "    \n",
    "    Parameters:\n",
    "    - docs: List of documents\n",
    "    - seed: Random seed\n",
    "    - max_iters: Maximum number of training iterations\n",
    "    - detailed_output: Whether to output detailed information\n",
    "    - use_multiple_coherence: Whether to use multiple coherence metrics\n",
    "    \n",
    "    Returns:\n",
    "    - model: Trained LDA model\n",
    "    - metrics: Dictionary of evaluation metrics\n",
    "    - history: Training history\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🚀 Starting to train LDA model (seed={seed})\")\n",
    "    print(f\"📊 Data Overview:\")\n",
    "    print(f\"   - Number of documents: {len(docs)}\")\n",
    "    print(f\"   - Average document length: {np.mean([len(doc) for doc in docs]):.1f} words\")\n",
    "    print(f\"   - Vocabulary size: {len(set(word for doc in docs for word in doc))}\")\n",
    "    \n",
    "    print(f\"\\n🔧 Model Parameters:\")\n",
    "    print(f\"   - Number of topics K = {K_LEAF}\")\n",
    "    print(f\"   - Alpha = {ALPHA}\")\n",
    "    print(f\"   - Eta = {ETA}\")\n",
    "    print(f\"   - Max iterations = {max_iters}\")\n",
    "    print(f\"   - Check interval = {INTERVAL}\")\n",
    "    if use_multiple_coherence:\n",
    "        print(f\"   - Coherence metrics = ['c_v', 'c_npmi']\")\n",
    "    \n",
    "    # Train the model\n",
    "    start_time = time.time()\n",
    "    lda_model, training_history = build_lda(docs, seed=seed, max_iters=max_iters, \n",
    "                                           use_multiple_coherence=use_multiple_coherence)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n✅ Training complete! Time taken: {training_time:.1f} seconds\")\n",
    "    \n",
    "    # Extract final metrics\n",
    "    if training_history:\n",
    "        if use_multiple_coherence:\n",
    "            # Process multiple coherence metric results\n",
    "            final_iter, final_llpw, final_ppl, final_coherence_dict = training_history[-1]\n",
    "            final_coh_cv = final_coherence_dict.get('c_v', 0)\n",
    "            final_coh_npmi = final_coherence_dict.get('c_npmi', 0)\n",
    "        else:\n",
    "            # Process single coherence metric results\n",
    "            final_iter, final_llpw, final_ppl, final_coh_cv = training_history[-1]\n",
    "            final_coh_npmi = 0\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        effective_topics = effective_k(lda_model)\n",
    "        \n",
    "        # Perform a full evaluation with multiple coherence metrics\n",
    "        if use_multiple_coherence:\n",
    "            print(f\"\\n🔍 Performing full evaluation with multiple coherence metrics...\")\n",
    "            full_evaluation = evaluate_model_with_multiple_coherence(\n",
    "                lda_model, docs, \"LDA\", \n",
    "                coherence_metrics=['c_v', 'c_npmi'],\n",
    "                top_words_for_coherence=5\n",
    "            )\n",
    "            \n",
    "            metrics = {\n",
    "                'iterations': final_iter,\n",
    "                'log_likelihood_per_word': final_llpw,\n",
    "                'perplexity': final_ppl,\n",
    "                'coherence_c_v': full_evaluation['coherence_metrics'].get('c_v', final_coh_cv),\n",
    "                'coherence_npmi': full_evaluation['coherence_metrics'].get('c_npmi', final_coh_npmi),\n",
    "\n",
    "                'effective_topics': effective_topics,\n",
    "                'training_time_seconds': training_time,\n",
    "                'convergence_iterations': len(training_history),\n",
    "                'composite_score': full_evaluation.get('composite_score', 0),\n",
    "                'topic_utilization': full_evaluation.get('topic_utilization', 0)\n",
    "            }\n",
    "        else:\n",
    "            metrics = {\n",
    "                'iterations': final_iter,\n",
    "                'log_likelihood_per_word': final_llpw,\n",
    "                'perplexity': final_ppl,\n",
    "                'coherence_c_v': final_coh_cv,\n",
    "                'effective_topics': effective_topics,\n",
    "                'training_time_seconds': training_time,\n",
    "                'convergence_iterations': len(training_history)\n",
    "            }\n",
    "        \n",
    "        if detailed_output:\n",
    "            print(f\"\\n📈 Final Evaluation Results:\")\n",
    "            print(f\"   - Training iterations: {final_iter}\")\n",
    "            print(f\"   - Log-likelihood per word: {final_llpw:.6f}\")\n",
    "            print(f\"   - Perplexity: {final_ppl:.2f}\")\n",
    "            print(f\"   - Effective topics: {effective_topics}/{K_LEAF}\")\n",
    "            \n",
    "            if use_multiple_coherence:\n",
    "                print(f\"\\n📊 Multiple Coherence Metrics:\")\n",
    "                print(f\"   - C_v (Vector Space): {metrics['coherence_c_v']:.6f}\")\n",
    "                print(f\"   - NPMI (Pointwise Mutual Information): {metrics['coherence_npmi']:.6f}\")\n",
    "                print(f\"   - Composite Score: {metrics['composite_score']:.6f}\")\n",
    "                print(f\"   - Topic Utilization: {metrics['topic_utilization']*100:.1f}%\")\n",
    "            else:\n",
    "                print(f\"   - Coherence (C_v): {metrics['coherence_c_v']:.4f}\")\n",
    "            \n",
    "            print(f\"   - Convergence history length: {len(training_history)}\")\n",
    "            \n",
    "            # Display training process overview\n",
    "            if len(training_history) > 1:\n",
    "                if use_multiple_coherence:\n",
    "                    first_iter, first_llpw, first_ppl, first_coh_dict = training_history[0]\n",
    "                    final_iter, final_llpw, final_ppl, final_coh_dict = training_history[-1]\n",
    "                    \n",
    "                    first_cv = first_coh_dict.get('c_v', 0)\n",
    "                    first_npmi = first_coh_dict.get('c_npmi', 0)\n",
    "                    final_cv = final_coh_dict.get('c_v', 0)\n",
    "                    final_npmi = final_coh_dict.get('c_npmi', 0)\n",
    "                    \n",
    "                    print(f\"\\n📊 Training Improvement:\")\n",
    "                    print(f\"   - Perplexity improvement: {first_ppl:.2f} → {final_ppl:.2f} (↓{first_ppl-final_ppl:.2f})\")\n",
    "                    print(f\"   - C_v improvement: {first_cv:.4f} → {final_cv:.4f} (↑{final_cv-first_cv:.4f})\")\n",
    "                    print(f\"   - NPMI improvement: {first_npmi:.4f} → {final_npmi:.4f} (↑{final_npmi-first_npmi:.4f})\")\n",
    "                    print(f\"   - Log-likelihood improvement: {first_llpw:.6f} → {final_llpw:.6f} (↑{final_llpw-first_llpw:.6f})\")\n",
    "                else:\n",
    "                    first_iter, first_llpw, first_ppl, first_coh = training_history[0]\n",
    "                    print(f\"\\n📊 Training Improvement:\")\n",
    "                    print(f\"   - Perplexity improvement: {first_ppl:.2f} → {final_ppl:.2f} (↓{first_ppl-final_ppl:.2f})\")\n",
    "                    print(f\"   - Coherence improvement: {first_coh:.4f} → {final_coh_cv:.4f} (↑{final_coh_cv-first_coh:.4f})\")\n",
    "                    print(f\"   - Log-likelihood improvement: {first_llpw:.6f} → {final_llpw:.6f} (↑{final_llpw-first_llpw:.6f})\")\n",
    "    else:\n",
    "        print(\"❌ No training history recorded\")\n",
    "        metrics = {}\n",
    "    \n",
    "    return lda_model, metrics, training_history\n",
    "\n",
    "# ===== Execute LDA Modeling and Evaluation =====\n",
    "print(\"\\n🎯 Training LDA model with the full dataset...\")\n",
    "\n",
    "# Use the full corpus\n",
    "full_docs = list(corpus.values())\n",
    "print(f\"📚 Using full dataset: {len(full_docs)} documents\")\n",
    "\n",
    "# Train and evaluate - using multiple coherence metrics (including NPMI)\n",
    "lda_model, lda_metrics, lda_history = train_and_evaluate_lda(\n",
    "    docs=full_docs,\n",
    "    seed=42,\n",
    "    max_iters=300,  # 🔧 You can modify this value to control the max iterations\n",
    "    detailed_output=True,\n",
    "    use_multiple_coherence=True  # 🆕 Enable multiple coherence metrics (including NPMI)\n",
    ")\n",
    "\n",
    "# Topic analysis\n",
    "if lda_model:\n",
    "    topic_info = analyze_model_topics(lda_model, model_name=\"LDA\", top_words=5)\n",
    "    \n",
    "    print(f\"\\n🎯 LDA model evaluation complete!\")\n",
    "    print(f\"📊 Key Metrics:\")\n",
    "    if lda_metrics:\n",
    "        for key, value in lda_metrics.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"   - {key}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"   - {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\n💾 Model is trained and can be used for:\")\n",
    "    print(f\"   1. Topic word extraction\")\n",
    "    print(f\"   2. Document-topic distribution analysis\") \n",
    "    print(f\"   3. Comparison with other models\")\n",
    "    print(f\"   4. Topic evolution analysis\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ LDA model training failed\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36355395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_topic_doc_counts(model, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Get the number of documents covered by each topic (i.e., in how many documents each topic appears)\n",
    "    \"\"\"\n",
    "    num_topics = getattr(model, 'k', None) or getattr(model, 'num_topics', None) or 0\n",
    "    topic_doc_counts = [0] * num_topics\n",
    "    for doc in model.docs:\n",
    "        # Get the topic distribution for this document\n",
    "        try:\n",
    "            topic_dist = doc.get_topic_dist()\n",
    "        except:\n",
    "            continue\n",
    "        for k, prob in enumerate(topic_dist):\n",
    "            if prob > threshold:\n",
    "                topic_doc_counts[k] += 1\n",
    "    return topic_doc_counts\n",
    "\n",
    "def calculate_weighted_renyi_entropy_full_weighted(model, alpha=2):\n",
    "    \"\"\"\n",
    "    Renyi entropy weighted by the number of documents covered by the topic\n",
    "    \"\"\"\n",
    "    entropies = []\n",
    "    num_topics = getattr(model, 'k', None) or getattr(model, 'num_topics', None) or 0\n",
    "    for k in range(num_topics):\n",
    "        try:\n",
    "            topic_probs = np.array([prob for word, prob in model.get_topic_words(k, top_n=-1)])\n",
    "            topic_probs = topic_probs / topic_probs.sum()\n",
    "            if len(topic_probs) > 0:\n",
    "                renyi = (1/(1-alpha)) * np.log(np.sum(topic_probs**alpha))\n",
    "                entropies.append(renyi)\n",
    "            else:\n",
    "                entropies.append(0)\n",
    "        except:\n",
    "            entropies.append(0)\n",
    "    # Get the document count for each topic\n",
    "    topic_doc_counts = get_topic_doc_counts(model)\n",
    "    total = sum(topic_doc_counts)\n",
    "    if total > 0:\n",
    "        return np.average(entropies, weights=topic_doc_counts)\n",
    "    else:\n",
    "        return np.mean(entropies)\n",
    "    \n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "def calculate_weighted_jsd_full(model, top_n=-1):\n",
    "    \"\"\"\n",
    "    Calculate the pairwise JSD distance for all topic distributions (unweighted), return the average JSD\n",
    "    - model: topic model object (e.g., tomotopy LDA/CTM/PAM/hLDA)\n",
    "    - top_n: how many words to take for each topic (-1 for all)\n",
    "    \"\"\"\n",
    "    num_topics = getattr(model, 'k', None) or getattr(model, 'num_topics', None) or 0\n",
    "    topic_distributions = []\n",
    "    for k in range(num_topics):\n",
    "        try:\n",
    "            topic_words = model.get_topic_words(k, top_n=top_n)\n",
    "            probs = np.array([prob for word, prob in topic_words])\n",
    "            probs = probs / probs.sum() if probs.sum() > 0 else np.ones_like(probs) / len(probs)\n",
    "            topic_distributions.append(probs)\n",
    "        except:\n",
    "            continue\n",
    "    # Calculate pairwise JSD\n",
    "    jsd_values = []\n",
    "    for i in range(len(topic_distributions)):\n",
    "        for j in range(i+1, len(topic_distributions)):\n",
    "            # Align length\n",
    "            p = topic_distributions[i]\n",
    "            q = topic_distributions[j]\n",
    "            min_len = min(len(p), len(q))\n",
    "            jsd = jensenshannon(p[:min_len], q[:min_len])\n",
    "            jsd_values.append(jsd)\n",
    "    if jsd_values:\n",
    "        return float(np.mean(jsd_values))\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "def calculate_renyi_entropy_unweighted(model, alpha=2):\n",
    "    \"\"\"\n",
    "    Calculate the Renyi entropy for all topics (unweighted version, direct average)\n",
    "    - model: topic model object (e.g., tomotopy LDA/CTM/PAM/hLDA)\n",
    "    - alpha: order of Renyi entropy (commonly 2)\n",
    "    Returns: the average of Renyi entropies for all topics\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    entropies = []\n",
    "    num_topics = getattr(model, 'k', None) or getattr(model, 'num_topics', None) or 0\n",
    "    for k in range(num_topics):\n",
    "        try:\n",
    "            topic_probs = np.array([prob for word, prob in model.get_topic_words(k, top_n=-1)])\n",
    "            topic_probs = topic_probs / topic_probs.sum()\n",
    "            if len(topic_probs) > 0:\n",
    "                renyi = (1/(1-alpha)) * np.log(np.sum(topic_probs**alpha))\n",
    "                entropies.append(renyi)\n",
    "            else:\n",
    "                entropies.append(0)\n",
    "        except:\n",
    "            entropies.append(0)\n",
    "    if entropies:\n",
    "        return float(np.mean(entropies))\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f54ce14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔬 Renyi entropy weighted by document count: 3.6679\n",
      "🔬 Average JSD (full distribution): 0.2599\n",
      "Unweighted Renyi entropy: 4.1623\n"
     ]
    }
   ],
   "source": [
    "# Called after LDA model training and evaluation\n",
    "if lda_model:\n",
    "    renyi_entropy = calculate_weighted_renyi_entropy_full_weighted(lda_model, alpha=2)\n",
    "    jsd_distance = calculate_weighted_jsd_full(lda_model)\n",
    "    print(f\"\\n🔬 Renyi entropy weighted by document count: {renyi_entropy:.4f}\")\n",
    "    print(f\"🔬 Average JSD (full distribution): {jsd_distance:.4f}\")\n",
    "renyi_entropy_unweighted = calculate_renyi_entropy_unweighted(lda_model, alpha=2)\n",
    "print(f\"Unweighted Renyi entropy: {renyi_entropy_unweighted:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16db1f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Starting CTM model training\n",
      "============================================================\n",
      "\n",
      "🎯 Training CTM model with the full dataset...\n",
      "📚 Using full dataset: 970 documents\n",
      "🚀 Starting to train CTM model (seed=42)\n",
      "📊 Data Overview:\n",
      "   - Number of documents: 970\n",
      "   - Average document length: 85.8 words\n",
      "   - Vocabulary size: 1490\n",
      "\n",
      "🔧 Model Parameters:\n",
      "   - Number of topics K = 20\n",
      "   - Alpha = 0.1\n",
      "   - Eta = 0.05\n",
      "   - Max iterations = 100\n",
      "   - Check interval = 100\n",
      "   - Multiple coherence metrics: Enabled (C_v, NPMI)\n",
      "🚀 Creating CTM model (K=20, α=0.1, η=0.05, seed=42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v5/6mdkg5713kxgwg5xs24g8rvr0000gn/T/ipykernel_55006/1551040497.py:208: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  mdl.train(interval)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CTM] iter= 100 llpw=-5.5186 ppl=249.29 Calculating multiple coherence metrics...\n",
      "[CTM] iter= 100 llpw=-5.5186 ppl=249.29 c_v=0.5187, c_npmi=0.0089\n",
      "\n",
      "✅ Training complete! Time taken: 7.6 seconds\n",
      "\n",
      "📈 Final Evaluation Results:\n",
      "   - Training iterations: 100\n",
      "   - Log-likelihood per word: -5.518621\n",
      "   - Perplexity: 249.29\n",
      "   - Coherence (C_v): 0.5187\n",
      "   - Effective topics: 20/20\n",
      "   - Convergence history length: 1\n",
      "\n",
      "🔍 CTM Multiple Coherence Metrics Evaluation:\n",
      "================================================================================\n",
      "📊 Coherence Metrics Results:\n",
      "   📈 C_v (Vector Space Coherence): 0.518657\n",
      "   📈 NPMI (Normalized Pointwise Mutual Information): 0.008891\n",
      "\n",
      "📊 Basic Metrics:\n",
      "   📈 Effective topics: 20\n",
      "   📈 Total topics: 20\n",
      "   📈 Topic utilization: 100.0%\n",
      "\n",
      "💡 Coherence Metric Explanations:\n",
      "   🎯 C_v: Range [0,1], higher is better, based on word vector similarity\n",
      "   🎯 NPMI: Range [-1,1], higher is better, Normalized Pointwise Mutual Information\n",
      "\n",
      "🏆 Composite Coherence Score: 0.5116 (average of normalized scores)\n",
      "\n",
      "🔍 CTM Topic Analysis (showing top 5 words):\n",
      "================================================================================\n",
      "Topic   0 (weight:0.091): flow(0.091), fiber(0.081), st(0.067), tire(0.044), computation(0.038)\n",
      "Topic   1 (weight:0.093): element(0.093), function(0.088), approximation(0.039), shape(0.038), displacement(0.037)\n",
      "Topic   2 (weight:0.068): particle(0.068), equation(0.042), fluid(0.040), meshfree(0.037), diffusion(0.035)\n",
      "Topic   3 (weight:0.068): damage(0.068), stochastic(0.042), failure(0.036), plate(0.031), shear(0.026)\n",
      "Topic   4 (weight:0.096): method(0.096), computational(0.033), accuracy(0.031), high(0.024), integration(0.022)\n",
      "Topic   5 (weight:0.075): time(0.075), algorithm(0.045), step(0.030), compute(0.026), interaction(0.026)\n",
      "Topic   6 (weight:0.038): computation(0.038), analysis(0.037), representation(0.031), discretization(0.028), system(0.027)\n",
      "Topic   7 (weight:0.032): uncertainty(0.032), model(0.030), grain(0.018), crystal(0.017), response(0.016)\n",
      "Topic   8 (weight:0.037): design(0.037), mechanical(0.034), framework(0.032), model(0.031), thermal(0.029)\n",
      "Topic   9 (weight:0.049): problem(0.049), reduce(0.043), domain(0.041), order(0.032), approach(0.031)\n",
      "... (and 10 more active topics)\n",
      "\n",
      "📊 CTM Topic Statistics:\n",
      "   - Active topics: 20/20\n",
      "   - Topic activity rate: 100.0%\n",
      "\n",
      "🎯 CTM model evaluation complete!\n",
      "📊 Key Metrics:\n",
      "   - iterations: 100\n",
      "   - log_likelihood_per_word: -5.5186\n",
      "   - perplexity: 249.2909\n",
      "   - coherence_c_v: {'c_v': 0.5186574662953594, 'c_npmi': 0.008890875527411798}\n",
      "   - effective_topics: 20\n",
      "   - training_time_seconds: 7.6465\n",
      "   - convergence_iterations: 1\n",
      "   - coherence_metrics: {'c_v': 0.5186574662953594, 'c_npmi': 0.008890875527411798}\n",
      "   - total_topics: 20\n",
      "   - topic_utilization: 1.0000\n",
      "   - normalized_scores: {'c_v': 0.5186574662953594, 'c_npmi': 0.5044454377637059}\n",
      "   - composite_score: 0.5116\n",
      "\n",
      "💾 CTM model is trained, features:\n",
      "   1. Correlated Topic Model\n",
      "   2. Models correlations between topics\n",
      "   3. More realistic topic relationships\n",
      "   4. Suitable for complex document collections\n",
      "\n",
      "🔬 Renyi entropy weighted by document count (CTM): 3.8798\n",
      "🔬 Average JSD (full distribution) (CTM): 0.1328\n",
      "🔬 Unweighted Renyi entropy (CTM): 3.9362\n"
     ]
    }
   ],
   "source": [
    "# # ===== CTM Model Construction and Training =====\n",
    "\n",
    "# print(\"\\n🎯 Starting CTM model training\")\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# def build_ctm(docs, seed=0, max_iters=2000, K_LEAF=252, use_multiple_coherence=True):\n",
    "#     \"\"\"Build and train a CTM model - supports multiple coherence metrics\"\"\"\n",
    "#     print(f\"🚀 Creating CTM model (K={K_LEAF}, α={ALPHA}, η={ETA}, seed={seed})\")\n",
    "#     # The correct class name is CTModel\n",
    "#     mdl = tp.CTModel(k=K_LEAF, smoothing_alpha=ALPHA, eta=ETA, seed=seed)\n",
    "    \n",
    "#     if use_multiple_coherence:\n",
    "#         # Train with multiple coherence metrics\n",
    "#         return auto_train_with_multiple_metrics(\n",
    "#             mdl, docs, seed=seed, max_iters=max_iters, \n",
    "#             coherence_measures=['c_v', 'c_npmi'], \n",
    "#             name='CTM'\n",
    "#         )\n",
    "#     else:\n",
    "#         # Train with a traditional single metric\n",
    "#         return auto_train(mdl, docs, seed=seed, max_iters=max_iters, name='CTM')   \n",
    "\n",
    "# def train_and_evaluate_ctm(docs, seed=42, max_iters=2000, K_LEAF=252, detailed_output=True, use_multiple_coherence=True):\n",
    "#     \"\"\"\n",
    "#     Train a CTM model and perform a full evaluation - supports multiple coherence metrics\n",
    "    \n",
    "#     Parameters:\n",
    "#     - docs: List of documents\n",
    "#     - seed: Random seed\n",
    "#     - max_iters: Maximum number of training iterations\n",
    "#     - detailed_output: Whether to output detailed information\n",
    "#     - use_multiple_coherence: Whether to use multiple coherence metrics\n",
    "    \n",
    "#     Returns:\n",
    "#     - model: Trained CTM model\n",
    "#     - metrics: Dictionary of evaluation metrics\n",
    "#     - history: Training history\n",
    "#     \"\"\"\n",
    "    \n",
    "#     print(f\"🚀 Starting to train CTM model (seed={seed})\")\n",
    "#     print(f\"📊 Data Overview:\")\n",
    "#     print(f\"   - Number of documents: {len(docs)}\")\n",
    "#     print(f\"   - Average document length: {np.mean([len(doc) for doc in docs]):.1f} words\")\n",
    "#     print(f\"   - Vocabulary size: {len(set(word for doc in docs for word in doc))}\")\n",
    "    \n",
    "#     print(f\"\\n🔧 Model Parameters:\")\n",
    "#     print(f\"   - Number of topics K = {K_LEAF}\")\n",
    "#     print(f\"   - Alpha = {ALPHA}\")\n",
    "#     print(f\"   - Eta = {ETA}\")\n",
    "#     print(f\"   - Max iterations = {max_iters}\")\n",
    "#     print(f\"   - Check interval = {INTERVAL}\")\n",
    "#     if use_multiple_coherence:\n",
    "#         print(f\"   - Multiple coherence metrics: Enabled (C_v, NPMI)\")\n",
    "    \n",
    "#     # Train the model\n",
    "#     start_time = time.time()\n",
    "#     if use_multiple_coherence:\n",
    "#         # Train with multiple coherence metrics\n",
    "#         ctm_model, training_history = build_ctm(docs, seed=seed, max_iters=max_iters, \n",
    "#                                               K_LEAF=K_LEAF, use_multiple_coherence=True)\n",
    "#     else:\n",
    "#         # Train with a traditional single metric\n",
    "#         ctm_model, training_history = build_ctm(docs, seed=seed, max_iters=max_iters, \n",
    "#                                               K_LEAF=K_LEAF, use_multiple_coherence=False)\n",
    "    \n",
    "#     training_time = time.time() - start_time\n",
    "    \n",
    "#     print(f\"\\n✅ Training complete! Time taken: {training_time:.1f} seconds\")\n",
    "    \n",
    "#     # Extract final metrics\n",
    "#     if training_history:\n",
    "#         if use_multiple_coherence and isinstance(training_history[-1], dict):\n",
    "#             # Multiple coherence metrics mode\n",
    "#             final_metrics = training_history[-1]\n",
    "#             final_iter = final_metrics['iteration']\n",
    "#             final_llpw = final_metrics['log_likelihood']\n",
    "#             final_ppl = final_metrics.get('perplexity', 0)\n",
    "            \n",
    "#             # Calculate additional metrics\n",
    "#             effective_topics = effective_k(ctm_model)\n",
    "            \n",
    "#             metrics = {\n",
    "#                 'iterations': final_iter,\n",
    "#                 'log_likelihood_per_word': final_llpw,\n",
    "#                 'perplexity': final_ppl,\n",
    "#                 'coherence_c_v': final_metrics.get('c_v', 0),\n",
    "#                 'coherence_c_npmi': final_metrics.get('c_npmi', 0),\n",
    "#                 'effective_topics': effective_topics,\n",
    "#                 'training_time_seconds': training_time,\n",
    "#                 'convergence_iterations': len(training_history)\n",
    "#             }\n",
    "            \n",
    "#             if detailed_output:\n",
    "#                 print(f\"\\n📈 Final Evaluation Results:\")\n",
    "#                 print(f\"   - Training iterations: {final_iter}\")\n",
    "#                 print(f\"   - Log-likelihood per word: {final_llpw:.6f}\")\n",
    "#                 print(f\"   - Perplexity: {final_ppl:.2f}\")\n",
    "#                 print(f\"   - C_v Coherence: {final_metrics.get('c_v', 'N/A')}\")\n",
    "#                 print(f\"   - NPMI Coherence: {final_metrics.get('c_npmi', 'N/A')}\")\n",
    "#                 print(f\"   - Effective topics: {effective_topics}/{K_LEAF}\")\n",
    "#                 print(f\"   - Convergence history length: {len(training_history)}\")\n",
    "#         else:\n",
    "#             # Traditional single metric mode\n",
    "#             final_iter, final_llpw, final_ppl, final_coh = training_history[-1]\n",
    "            \n",
    "#             # Calculate additional metrics\n",
    "#             effective_topics = effective_k(ctm_model)\n",
    "            \n",
    "#             metrics = {\n",
    "#                 'iterations': final_iter,\n",
    "#                 'log_likelihood_per_word': final_llpw,\n",
    "#                 'perplexity': final_ppl,\n",
    "#                 'coherence_c_v': final_coh,\n",
    "#                 'effective_topics': effective_topics,\n",
    "#                 'training_time_seconds': training_time,\n",
    "#                 'convergence_iterations': len(training_history)\n",
    "#             }\n",
    "            \n",
    "#             if detailed_output:\n",
    "#                 print(f\"\\n📈 Final Evaluation Results:\")\n",
    "#                 print(f\"   - Training iterations: {final_iter}\")\n",
    "#                 print(f\"   - Log-likelihood per word: {final_llpw:.6f}\")\n",
    "#                 print(f\"   - Perplexity: {final_ppl:.2f}\")\n",
    "#                 print(f\"   - Coherence (C_v): {final_coh.get('c_v', 0):.4f}\")\n",
    "#                 print(f\"   - Effective topics: {effective_topics}/{K_LEAF}\")\n",
    "#                 print(f\"   - Convergence history length: {len(training_history)}\")\n",
    "                \n",
    "#                 # Display training process overview\n",
    "#                 if len(training_history) > 1:\n",
    "#                     first_iter, first_llpw, first_ppl, first_coh = training_history[0]\n",
    "#                     print(f\"\\n📊 Training Improvement:\")\n",
    "#                     print(f\"   - Perplexity improvement: {first_ppl:.2f} → {final_ppl:.2f} (↓{first_ppl-final_ppl:.2f})\")\n",
    "#                     print(f\"   - Coherence improvement: {first_coh:.4f} → {final_coh:.4f} (↑{final_coh-first_coh:.4f})\")\n",
    "#                     print(f\"   - Log-likelihood improvement: {first_llpw:.6f} → {final_llpw:.6f} (↑{final_llpw-first_llpw:.6f})\")\n",
    "#     else:\n",
    "#         print(\"❌ No training history recorded\")\n",
    "#         metrics = {}\n",
    "    \n",
    "#     # If multiple coherence is enabled, perform a full evaluation\n",
    "#     if use_multiple_coherence:\n",
    "#         evaluation_results = evaluate_model_with_multiple_coherence(\n",
    "#             ctm_model, docs, model_name=\"CTM\"\n",
    "#         )\n",
    "#         metrics.update(evaluation_results)\n",
    "    \n",
    "#     return ctm_model, metrics, training_history\n",
    "\n",
    "# # ===== Execute CTM Modeling and Evaluation =====\n",
    "# print(\"\\n🎯 Training CTM model with the full dataset...\")\n",
    "\n",
    "# # Use the full corpus\n",
    "# full_docs = list(corpus.values())\n",
    "# print(f\"📚 Using full dataset: {len(full_docs)} documents\")\n",
    "\n",
    "# # Train and evaluate CTM\n",
    "# ctm_model, ctm_metrics, ctm_history = train_and_evaluate_ctm(\n",
    "#     docs=full_docs,\n",
    "#     seed=42,\n",
    "#     max_iters=100,  # 🔧 You can modify this value to control the max iterations\n",
    "#     K_LEAF=20,      # 🔧 You can modify the number of topics\n",
    "#     detailed_output=True\n",
    "# )\n",
    "\n",
    "# # Topic analysis\n",
    "# if ctm_model:\n",
    "#     ctm_topic_info = analyze_model_topics(ctm_model, model_name=\"CTM\", top_words=5)\n",
    "    \n",
    "#     print(f\"\\n🎯 CTM model evaluation complete!\")\n",
    "#     print(f\"📊 Key Metrics:\")\n",
    "#     if ctm_metrics:\n",
    "#         for key, value in ctm_metrics.items():\n",
    "#             if isinstance(value, float):\n",
    "#                 print(f\"   - {key}: {value:.4f}\")\n",
    "#             else:\n",
    "#                 print(f\"   - {key}: {value}\")\n",
    "    \n",
    "#     print(f\"\\n💾 CTM model is trained, features:\")\n",
    "#     print(f\"   1. Correlated Topic Model\")\n",
    "#     print(f\"   2. Models correlations between topics\")\n",
    "#     print(f\"   3. More realistic topic relationships\")\n",
    "#     print(f\"   4. Suitable for complex document collections\")\n",
    "\n",
    "# else:\n",
    "#     print(\"❌ CTM model training failed\")\n",
    "\n",
    "# # Called after CTM model training and evaluation\n",
    "# if ctm_model:\n",
    "#     renyi_entropy = calculate_weighted_renyi_entropy_full_weighted(ctm_model, alpha=2)\n",
    "#     jsd_distance = calculate_weighted_jsd_full(ctm_model)\n",
    "#     renyi_entropy_unweighted = calculate_renyi_entropy_unweighted(ctm_model, alpha=2)\n",
    "#     print(f\"\\n🔬 Renyi entropy weighted by document count (CTM): {renyi_entropy:.4f}\")\n",
    "#     print(f\"🔬 Average JSD (full distribution) (CTM): {jsd_distance:.4f}\")\n",
    "#     print(f\"🔬 Unweighted Renyi entropy (CTM): {renyi_entropy_unweighted:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e154098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Starting PAM model training - Aligned with hLDA\n",
      "============================================================\n",
      "🔧 PAM parameters aligned with hLDA: K1=59, K2=4, Total K=236\n",
      "\n",
      "🎯 Training PAM model with the full dataset...\n",
      "📚 Using full dataset: 970 documents\n",
      "🚀 Starting to train PAM model (seed=42)\n",
      "📊 Data Overview:\n",
      "   - Number of documents: 970\n",
      "   - Average document length: 85.8 words\n",
      "   - Vocabulary size: 1490\n",
      "\n",
      "🔧 Model Parameters:\n",
      "   - Super-topics K1 = 59\n",
      "   - Sub-topics K2 = 4\n",
      "   - Total topics = 236\n",
      "   - Alpha = 0.1\n",
      "   - Eta = 0.05\n",
      "   - Max iterations = 20000\n",
      "   - Coherence metrics = ['c_v', 'c_npmi']\n",
      "🚀 Creating PAM model (K1=59, K2=4, α=0.1, η=0.05, seed=42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v5/6mdkg5713kxgwg5xs24g8rvr0000gn/T/ipykernel_11319/52552382.py:209: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  mdl.train(interval)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAM] iter= 100 llpw=-9.8150 ppl=18305.57 Calculating multiple coherence metrics...\n",
      "[PAM] iter= 100 llpw=-9.8150 ppl=18305.57 c_v=0.5660, c_npmi=0.0225\n",
      "[PAM] iter= 200 llpw=-9.7506 ppl=17165.08 Calculating multiple coherence metrics...\n",
      "[PAM] iter= 200 llpw=-9.7506 ppl=17165.08 c_v=0.5332, c_npmi=0.0179\n",
      "[PAM] iter= 300 llpw=-9.6222 ppl=15096.52 Calculating multiple coherence metrics...\n",
      "[PAM] iter= 300 llpw=-9.6222 ppl=15096.52 c_v=0.5549, c_npmi=0.0263\n",
      "[PAM] iter= 400 llpw=-9.5659 ppl=14269.76 Calculating multiple coherence metrics...\n",
      "[PAM] iter= 400 llpw=-9.5659 ppl=14269.76 c_v=0.5276, c_npmi=0.0285\n",
      "[PAM] iter= 500 llpw=-9.4510 ppl=12720.30 Calculating multiple coherence metrics...\n",
      "[PAM] iter= 500 llpw=-9.4510 ppl=12720.30 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter= 600 llpw=-9.4075 ppl=12179.32 Calculating multiple coherence metrics...\n",
      "[PAM] iter= 600 llpw=-9.4075 ppl=12179.32 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter= 700 llpw=-9.3116 ppl=11065.65 Calculating multiple coherence metrics...\n",
      "[PAM] iter= 700 llpw=-9.3116 ppl=11065.65 c_v=0.5965, c_npmi=0.0513\n",
      "[PAM] iter= 800 llpw=-9.2712 ppl=10627.99 Calculating multiple coherence metrics...\n",
      "[PAM] iter= 800 llpw=-9.2712 ppl=10627.99 c_v=0.5965, c_npmi=0.0513\n",
      "[PAM] iter= 900 llpw=-9.2337 ppl=10236.17 Calculating multiple coherence metrics...\n",
      "[PAM] iter= 900 llpw=-9.2337 ppl=10236.17 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=1000 llpw=-9.2302 ppl=10200.91 Calculating multiple coherence metrics...\n",
      "[PAM] iter=1000 llpw=-9.2302 ppl=10200.91 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=1100 llpw=-9.1839 ppl=9738.80 Calculating multiple coherence metrics...\n",
      "[PAM] iter=1100 llpw=-9.1839 ppl=9738.80 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=1200 llpw=-9.1445 ppl=9362.98 Calculating multiple coherence metrics...\n",
      "[PAM] iter=1200 llpw=-9.1445 ppl=9362.98 c_v=0.5965, c_npmi=0.0513\n",
      "[PAM] iter=1300 llpw=-9.0766 ppl=8748.50 Calculating multiple coherence metrics...\n",
      "[PAM] iter=1300 llpw=-9.0766 ppl=8748.50 c_v=0.5965, c_npmi=0.0513\n",
      "[PAM] iter=1400 llpw=-9.0763 ppl=8745.17 Calculating multiple coherence metrics...\n",
      "[PAM] iter=1400 llpw=-9.0763 ppl=8745.17 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=1500 llpw=-9.0468 ppl=8490.96 Calculating multiple coherence metrics...\n",
      "[PAM] iter=1500 llpw=-9.0468 ppl=8490.96 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=1600 llpw=-9.0144 ppl=8220.48 Calculating multiple coherence metrics...\n",
      "[PAM] iter=1600 llpw=-9.0144 ppl=8220.48 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=1700 llpw=-9.0393 ppl=8428.12 Calculating multiple coherence metrics...\n",
      "[PAM] iter=1700 llpw=-9.0393 ppl=8428.12 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=1800 llpw=-9.0138 ppl=8215.86 Calculating multiple coherence metrics...\n",
      "[PAM] iter=1800 llpw=-9.0138 ppl=8215.86 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=1900 llpw=-8.9920 ppl=8038.43 Calculating multiple coherence metrics...\n",
      "[PAM] iter=1900 llpw=-8.9920 ppl=8038.43 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=2000 llpw=-8.9620 ppl=7801.08 Calculating multiple coherence metrics...\n",
      "[PAM] iter=2000 llpw=-8.9620 ppl=7801.08 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=2100 llpw=-8.9453 ppl=7671.85 Calculating multiple coherence metrics...\n",
      "[PAM] iter=2100 llpw=-8.9453 ppl=7671.85 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=2200 llpw=-8.9271 ppl=7533.66 Calculating multiple coherence metrics...\n",
      "[PAM] iter=2200 llpw=-8.9271 ppl=7533.66 c_v=0.5676, c_npmi=0.0478\n",
      "[PAM] iter=2300 llpw=-8.9145 ppl=7439.21 Calculating multiple coherence metrics...\n",
      "[PAM] iter=2300 llpw=-8.9145 ppl=7439.21 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=2400 llpw=-8.8881 ppl=7245.13 Calculating multiple coherence metrics...\n",
      "[PAM] iter=2400 llpw=-8.8881 ppl=7245.13 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=2500 llpw=-8.8704 ppl=7118.05 Calculating multiple coherence metrics...\n",
      "[PAM] iter=2500 llpw=-8.8704 ppl=7118.05 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=2600 llpw=-8.8470 ppl=6953.83 Calculating multiple coherence metrics...\n",
      "[PAM] iter=2600 llpw=-8.8470 ppl=6953.83 c_v=0.5824, c_npmi=0.0492\n",
      "[PAM] iter=2700 llpw=-8.8196 ppl=6765.44 Calculating multiple coherence metrics...\n",
      "[PAM] iter=2700 llpw=-8.8196 ppl=6765.44 c_v=0.5965, c_npmi=0.0513\n",
      "[PAM] iter=2800 llpw=-8.8570 ppl=7023.12 Calculating multiple coherence metrics...\n",
      "[PAM] iter=2800 llpw=-8.8570 ppl=7023.12 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=2900 llpw=-8.8796 ppl=7184.03 Calculating multiple coherence metrics...\n",
      "[PAM] iter=2900 llpw=-8.8796 ppl=7184.03 c_v=0.5676, c_npmi=0.0478\n",
      "[PAM] iter=3000 llpw=-8.8664 ppl=7089.74 Calculating multiple coherence metrics...\n",
      "[PAM] iter=3000 llpw=-8.8664 ppl=7089.74 c_v=0.5676, c_npmi=0.0478\n",
      "[PAM] iter=3100 llpw=-8.8672 ppl=7095.49 Calculating multiple coherence metrics...\n",
      "[PAM] iter=3100 llpw=-8.8672 ppl=7095.49 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=3200 llpw=-8.8887 ppl=7249.63 Calculating multiple coherence metrics...\n",
      "[PAM] iter=3200 llpw=-8.8887 ppl=7249.63 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=3300 llpw=-8.8805 ppl=7190.48 Calculating multiple coherence metrics...\n",
      "[PAM] iter=3300 llpw=-8.8805 ppl=7190.48 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=3400 llpw=-8.9083 ppl=7393.24 Calculating multiple coherence metrics...\n",
      "[PAM] iter=3400 llpw=-8.9083 ppl=7393.24 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=3500 llpw=-8.8934 ppl=7283.91 Calculating multiple coherence metrics...\n",
      "[PAM] iter=3500 llpw=-8.8934 ppl=7283.91 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=3600 llpw=-8.8805 ppl=7190.28 Calculating multiple coherence metrics...\n",
      "[PAM] iter=3600 llpw=-8.8805 ppl=7190.28 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=3700 llpw=-8.8588 ppl=7036.14 Calculating multiple coherence metrics...\n",
      "[PAM] iter=3700 llpw=-8.8588 ppl=7036.14 c_v=0.5965, c_npmi=0.0513\n",
      "[PAM] iter=3800 llpw=-8.8707 ppl=7120.30 Calculating multiple coherence metrics...\n",
      "[PAM] iter=3800 llpw=-8.8707 ppl=7120.30 c_v=0.5859, c_npmi=0.0524\n",
      "[PAM] iter=3900 llpw=-8.8474 ppl=6956.26 Calculating multiple coherence metrics...\n",
      "[PAM] iter=3900 llpw=-8.8474 ppl=6956.26 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=4000 llpw=-8.8486 ppl=6964.41 Calculating multiple coherence metrics...\n",
      "[PAM] iter=4000 llpw=-8.8486 ppl=6964.41 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=4100 llpw=-8.8748 ppl=7149.38 Calculating multiple coherence metrics...\n",
      "[PAM] iter=4100 llpw=-8.8748 ppl=7149.38 c_v=0.5676, c_npmi=0.0478\n",
      "[PAM] iter=4200 llpw=-8.8470 ppl=6953.56 Calculating multiple coherence metrics...\n",
      "[PAM] iter=4200 llpw=-8.8470 ppl=6953.56 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=4300 llpw=-8.8243 ppl=6797.41 Calculating multiple coherence metrics...\n",
      "[PAM] iter=4300 llpw=-8.8243 ppl=6797.41 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=4400 llpw=-8.8516 ppl=6985.58 Calculating multiple coherence metrics...\n",
      "[PAM] iter=4400 llpw=-8.8516 ppl=6985.58 c_v=0.5676, c_npmi=0.0478\n",
      "[PAM] iter=4500 llpw=-8.8318 ppl=6848.76 Calculating multiple coherence metrics...\n",
      "[PAM] iter=4500 llpw=-8.8318 ppl=6848.76 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=4600 llpw=-8.8336 ppl=6860.92 Calculating multiple coherence metrics...\n",
      "[PAM] iter=4600 llpw=-8.8336 ppl=6860.92 c_v=0.5649, c_npmi=0.0339\n",
      "[PAM] iter=4700 llpw=-8.8199 ppl=6767.33 Calculating multiple coherence metrics...\n",
      "[PAM] iter=4700 llpw=-8.8199 ppl=6767.33 c_v=0.5965, c_npmi=0.0513\n",
      "[PAM] iter=4800 llpw=-8.8503 ppl=6976.78 Calculating multiple coherence metrics...\n",
      "[PAM] iter=4800 llpw=-8.8503 ppl=6976.78 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=4900 llpw=-8.8244 ppl=6797.93 Calculating multiple coherence metrics...\n",
      "[PAM] iter=4900 llpw=-8.8244 ppl=6797.93 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=5000 llpw=-8.8152 ppl=6735.74 Calculating multiple coherence metrics...\n",
      "[PAM] iter=5000 llpw=-8.8152 ppl=6735.74 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=5100 llpw=-8.7914 ppl=6577.51 Calculating multiple coherence metrics...\n",
      "[PAM] iter=5100 llpw=-8.7914 ppl=6577.51 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=5200 llpw=-8.7829 ppl=6521.89 Calculating multiple coherence metrics...\n",
      "[PAM] iter=5200 llpw=-8.7829 ppl=6521.89 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=5300 llpw=-8.7832 ppl=6523.97 Calculating multiple coherence metrics...\n",
      "[PAM] iter=5300 llpw=-8.7832 ppl=6523.97 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=5400 llpw=-8.7679 ppl=6424.87 Calculating multiple coherence metrics...\n",
      "[PAM] iter=5400 llpw=-8.7679 ppl=6424.87 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=5500 llpw=-8.7696 ppl=6435.39 Calculating multiple coherence metrics...\n",
      "[PAM] iter=5500 llpw=-8.7696 ppl=6435.39 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=5600 llpw=-8.7800 ppl=6502.92 Calculating multiple coherence metrics...\n",
      "[PAM] iter=5600 llpw=-8.7800 ppl=6502.92 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=5700 llpw=-8.7416 ppl=6258.12 Calculating multiple coherence metrics...\n",
      "[PAM] iter=5700 llpw=-8.7416 ppl=6258.12 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=5800 llpw=-8.7659 ppl=6411.63 Calculating multiple coherence metrics...\n",
      "[PAM] iter=5800 llpw=-8.7659 ppl=6411.63 c_v=0.5676, c_npmi=0.0478\n",
      "[PAM] iter=5900 llpw=-8.7465 ppl=6288.60 Calculating multiple coherence metrics...\n",
      "[PAM] iter=5900 llpw=-8.7465 ppl=6288.60 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=6000 llpw=-8.7430 ppl=6266.43 Calculating multiple coherence metrics...\n",
      "[PAM] iter=6000 llpw=-8.7430 ppl=6266.43 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=6100 llpw=-8.7488 ppl=6303.38 Calculating multiple coherence metrics...\n",
      "[PAM] iter=6100 llpw=-8.7488 ppl=6303.38 c_v=0.5783, c_npmi=0.0479\n",
      "[PAM] iter=6200 llpw=-8.7597 ppl=6371.95 Calculating multiple coherence metrics...\n",
      "[PAM] iter=6200 llpw=-8.7597 ppl=6371.95 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=6300 llpw=-8.7322 ppl=6199.49 Calculating multiple coherence metrics...\n",
      "[PAM] iter=6300 llpw=-8.7322 ppl=6199.49 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=6400 llpw=-8.7706 ppl=6442.03 Calculating multiple coherence metrics...\n",
      "[PAM] iter=6400 llpw=-8.7706 ppl=6442.03 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=6500 llpw=-8.7518 ppl=6321.84 Calculating multiple coherence metrics...\n",
      "[PAM] iter=6500 llpw=-8.7518 ppl=6321.84 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=6600 llpw=-8.7660 ppl=6412.52 Calculating multiple coherence metrics...\n",
      "[PAM] iter=6600 llpw=-8.7660 ppl=6412.52 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=6700 llpw=-8.7679 ppl=6424.66 Calculating multiple coherence metrics...\n",
      "[PAM] iter=6700 llpw=-8.7679 ppl=6424.66 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=6800 llpw=-8.7444 ppl=6275.65 Calculating multiple coherence metrics...\n",
      "[PAM] iter=6800 llpw=-8.7444 ppl=6275.65 c_v=0.5676, c_npmi=0.0478\n",
      "[PAM] iter=6900 llpw=-8.7714 ppl=6446.98 Calculating multiple coherence metrics...\n",
      "[PAM] iter=6900 llpw=-8.7714 ppl=6446.98 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=7000 llpw=-8.7344 ppl=6213.03 Calculating multiple coherence metrics...\n",
      "[PAM] iter=7000 llpw=-8.7344 ppl=6213.03 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=7100 llpw=-8.7355 ppl=6219.91 Calculating multiple coherence metrics...\n",
      "[PAM] iter=7100 llpw=-8.7355 ppl=6219.91 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=7200 llpw=-8.7547 ppl=6340.29 Calculating multiple coherence metrics...\n",
      "[PAM] iter=7200 llpw=-8.7547 ppl=6340.29 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=7300 llpw=-8.7389 ppl=6241.03 Calculating multiple coherence metrics...\n",
      "[PAM] iter=7300 llpw=-8.7389 ppl=6241.03 c_v=0.5783, c_npmi=0.0479\n",
      "[PAM] iter=7400 llpw=-8.7649 ppl=6405.65 Calculating multiple coherence metrics...\n",
      "[PAM] iter=7400 llpw=-8.7649 ppl=6405.65 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=7500 llpw=-8.7694 ppl=6434.48 Calculating multiple coherence metrics...\n",
      "[PAM] iter=7500 llpw=-8.7694 ppl=6434.48 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=7600 llpw=-8.7625 ppl=6390.38 Calculating multiple coherence metrics...\n",
      "[PAM] iter=7600 llpw=-8.7625 ppl=6390.38 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=7700 llpw=-8.7349 ppl=6216.08 Calculating multiple coherence metrics...\n",
      "[PAM] iter=7700 llpw=-8.7349 ppl=6216.08 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=7800 llpw=-8.7596 ppl=6371.30 Calculating multiple coherence metrics...\n",
      "[PAM] iter=7800 llpw=-8.7596 ppl=6371.30 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=7900 llpw=-8.7435 ppl=6269.93 Calculating multiple coherence metrics...\n",
      "[PAM] iter=7900 llpw=-8.7435 ppl=6269.93 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=8000 llpw=-8.7490 ppl=6304.32 Calculating multiple coherence metrics...\n",
      "[PAM] iter=8000 llpw=-8.7490 ppl=6304.32 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=8100 llpw=-8.7531 ppl=6330.55 Calculating multiple coherence metrics...\n",
      "[PAM] iter=8100 llpw=-8.7531 ppl=6330.55 c_v=0.5676, c_npmi=0.0478\n",
      "[PAM] iter=8200 llpw=-8.7527 ppl=6327.99 Calculating multiple coherence metrics...\n",
      "[PAM] iter=8200 llpw=-8.7527 ppl=6327.99 c_v=0.5676, c_npmi=0.0478\n",
      "[PAM] iter=8300 llpw=-8.7368 ppl=6227.71 Calculating multiple coherence metrics...\n",
      "[PAM] iter=8300 llpw=-8.7368 ppl=6227.71 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=8400 llpw=-8.7090 ppl=6057.21 Calculating multiple coherence metrics...\n",
      "[PAM] iter=8400 llpw=-8.7090 ppl=6057.21 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=8500 llpw=-8.7234 ppl=6144.85 Calculating multiple coherence metrics...\n",
      "[PAM] iter=8500 llpw=-8.7234 ppl=6144.85 c_v=0.5863, c_npmi=0.0429\n",
      "[PAM] iter=8600 llpw=-8.7257 ppl=6159.05 Calculating multiple coherence metrics...\n",
      "[PAM] iter=8600 llpw=-8.7257 ppl=6159.05 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=8700 llpw=-8.7622 ppl=6388.03 Calculating multiple coherence metrics...\n",
      "[PAM] iter=8700 llpw=-8.7622 ppl=6388.03 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=8800 llpw=-8.7036 ppl=6024.29 Calculating multiple coherence metrics...\n",
      "[PAM] iter=8800 llpw=-8.7036 ppl=6024.29 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=8900 llpw=-8.7196 ppl=6121.81 Calculating multiple coherence metrics...\n",
      "[PAM] iter=8900 llpw=-8.7196 ppl=6121.81 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=9000 llpw=-8.7188 ppl=6116.97 Calculating multiple coherence metrics...\n",
      "[PAM] iter=9000 llpw=-8.7188 ppl=6116.97 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=9100 llpw=-8.7225 ppl=6139.81 Calculating multiple coherence metrics...\n",
      "[PAM] iter=9100 llpw=-8.7225 ppl=6139.81 c_v=0.5676, c_npmi=0.0478\n",
      "[PAM] iter=9200 llpw=-8.7011 ppl=6009.35 Calculating multiple coherence metrics...\n",
      "[PAM] iter=9200 llpw=-8.7011 ppl=6009.35 c_v=0.5676, c_npmi=0.0478\n",
      "[PAM] iter=9300 llpw=-8.7414 ppl=6256.66 Calculating multiple coherence metrics...\n",
      "[PAM] iter=9300 llpw=-8.7414 ppl=6256.66 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=9400 llpw=-8.7151 ppl=6094.38 Calculating multiple coherence metrics...\n",
      "[PAM] iter=9400 llpw=-8.7151 ppl=6094.38 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=9500 llpw=-8.7089 ppl=6056.81 Calculating multiple coherence metrics...\n",
      "[PAM] iter=9500 llpw=-8.7089 ppl=6056.81 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=9600 llpw=-8.7279 ppl=6172.64 Calculating multiple coherence metrics...\n",
      "[PAM] iter=9600 llpw=-8.7279 ppl=6172.64 c_v=0.5965, c_npmi=0.0513\n",
      "[PAM] iter=9700 llpw=-8.7217 ppl=6134.64 Calculating multiple coherence metrics...\n",
      "[PAM] iter=9700 llpw=-8.7217 ppl=6134.64 c_v=0.5965, c_npmi=0.0513\n",
      "[PAM] iter=9800 llpw=-8.7236 ppl=6145.97 Calculating multiple coherence metrics...\n",
      "[PAM] iter=9800 llpw=-8.7236 ppl=6145.97 c_v=0.6129, c_npmi=0.0712\n",
      "[PAM] iter=9900 llpw=-8.6920 ppl=5955.15 Calculating multiple coherence metrics...\n",
      "[PAM] iter=9900 llpw=-8.6920 ppl=5955.15 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=10000 llpw=-8.7267 ppl=6165.32 Calculating multiple coherence metrics...\n",
      "[PAM] iter=10000 llpw=-8.7267 ppl=6165.32 c_v=0.5676, c_npmi=0.0478\n",
      "[PAM] iter=10100 llpw=-8.7163 ppl=6101.34 Calculating multiple coherence metrics...\n",
      "[PAM] iter=10100 llpw=-8.7163 ppl=6101.34 c_v=0.5795, c_npmi=0.0409\n",
      "[PAM] iter=10200 llpw=-8.7102 ppl=6064.53 Calculating multiple coherence metrics...\n",
      "[PAM] iter=10200 llpw=-8.7102 ppl=6064.53 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=10300 llpw=-8.7228 ppl=6141.36 Calculating multiple coherence metrics...\n",
      "[PAM] iter=10300 llpw=-8.7228 ppl=6141.36 c_v=0.5965, c_npmi=0.0513\n",
      "[PAM] iter=10400 llpw=-8.7272 ppl=6168.47 Calculating multiple coherence metrics...\n",
      "[PAM] iter=10400 llpw=-8.7272 ppl=6168.47 c_v=0.5965, c_npmi=0.0513\n",
      "[PAM] iter=10500 llpw=-8.6913 ppl=5950.99 Calculating multiple coherence metrics...\n",
      "[PAM] iter=10500 llpw=-8.6913 ppl=5950.99 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=10600 llpw=-8.7223 ppl=6138.56 Calculating multiple coherence metrics...\n",
      "[PAM] iter=10600 llpw=-8.7223 ppl=6138.56 c_v=0.5811, c_npmi=0.0536\n",
      "[PAM] iter=10700 llpw=-8.7146 ppl=6091.03 Calculating multiple coherence metrics...\n",
      "[PAM] iter=10700 llpw=-8.7146 ppl=6091.03 c_v=0.5806, c_npmi=0.0457\n",
      "[PAM] iter=10800 llpw=-8.7078 ppl=6049.65 Calculating multiple coherence metrics...\n",
      "[PAM] iter=10800 llpw=-8.7078 ppl=6049.65 c_v=0.5676, c_npmi=0.0478\n",
      "[PAM] iter=10900 llpw=-8.7117 ppl=6073.71 Calculating multiple coherence metrics...\n",
      "[PAM] iter=10900 llpw=-8.7117 ppl=6073.71 c_v=0.6035, c_npmi=0.0649\n",
      "[PAM] iter=11000 llpw=-8.6759 ppl=5860.02 Calculating multiple coherence metrics...\n",
      "[PAM] iter=11000 llpw=-8.6759 ppl=5860.02 c_v=0.5676, c_npmi=0.0478\n",
      "[PAM] iter=11100 llpw=-8.6888 ppl=5935.80 Calculating multiple coherence metrics...\n",
      "[PAM] iter=11100 llpw=-8.6888 ppl=5935.80 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=11200 llpw=-8.7277 ppl=6171.68 Calculating multiple coherence metrics...\n",
      "[PAM] iter=11200 llpw=-8.7277 ppl=6171.68 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=11300 llpw=-8.7147 ppl=6091.99 Calculating multiple coherence metrics...\n",
      "[PAM] iter=11300 llpw=-8.7147 ppl=6091.99 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=11400 llpw=-8.6766 ppl=5864.05 Calculating multiple coherence metrics...\n",
      "[PAM] iter=11400 llpw=-8.6766 ppl=5864.05 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=11500 llpw=-8.6988 ppl=5995.54 Calculating multiple coherence metrics...\n",
      "[PAM] iter=11500 llpw=-8.6988 ppl=5995.54 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=11600 llpw=-8.7014 ppl=6011.17 Calculating multiple coherence metrics...\n",
      "[PAM] iter=11600 llpw=-8.7014 ppl=6011.17 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=11700 llpw=-8.7072 ppl=6046.46 Calculating multiple coherence metrics...\n",
      "[PAM] iter=11700 llpw=-8.7072 ppl=6046.46 c_v=0.5783, c_npmi=0.0479\n",
      "[PAM] iter=11800 llpw=-8.7111 ppl=6070.05 Calculating multiple coherence metrics...\n",
      "[PAM] iter=11800 llpw=-8.7111 ppl=6070.05 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=11900 llpw=-8.7236 ppl=6146.17 Calculating multiple coherence metrics...\n",
      "[PAM] iter=11900 llpw=-8.7236 ppl=6146.17 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=12000 llpw=-8.7029 ppl=6020.64 Calculating multiple coherence metrics...\n",
      "[PAM] iter=12000 llpw=-8.7029 ppl=6020.64 c_v=0.5676, c_npmi=0.0478\n",
      "[PAM] iter=12100 llpw=-8.6900 ppl=5942.96 Calculating multiple coherence metrics...\n",
      "[PAM] iter=12100 llpw=-8.6900 ppl=5942.96 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=12200 llpw=-8.7210 ppl=6130.25 Calculating multiple coherence metrics...\n",
      "[PAM] iter=12200 llpw=-8.7210 ppl=6130.25 c_v=0.5676, c_npmi=0.0478\n",
      "[PAM] iter=12300 llpw=-8.6976 ppl=5988.38 Calculating multiple coherence metrics...\n",
      "[PAM] iter=12300 llpw=-8.6976 ppl=5988.38 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=12400 llpw=-8.7000 ppl=6002.75 Calculating multiple coherence metrics...\n",
      "[PAM] iter=12400 llpw=-8.7000 ppl=6002.75 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=12500 llpw=-8.7058 ppl=6037.61 Calculating multiple coherence metrics...\n",
      "[PAM] iter=12500 llpw=-8.7058 ppl=6037.61 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=12600 llpw=-8.7032 ppl=6022.22 Calculating multiple coherence metrics...\n",
      "[PAM] iter=12600 llpw=-8.7032 ppl=6022.22 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=12700 llpw=-8.6771 ppl=5866.85 Calculating multiple coherence metrics...\n",
      "[PAM] iter=12700 llpw=-8.6771 ppl=5866.85 c_v=0.5676, c_npmi=0.0478\n",
      "[PAM] iter=12800 llpw=-8.6948 ppl=5971.76 Calculating multiple coherence metrics...\n",
      "[PAM] iter=12800 llpw=-8.6948 ppl=5971.76 c_v=0.5765, c_npmi=0.0444\n",
      "[PAM] iter=12900 llpw=-8.6966 ppl=5982.80 Calculating multiple coherence metrics...\n",
      "[PAM] iter=12900 llpw=-8.6966 ppl=5982.80 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=13000 llpw=-8.6917 ppl=5953.39 Calculating multiple coherence metrics...\n",
      "[PAM] iter=13000 llpw=-8.6917 ppl=5953.39 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=13100 llpw=-8.6751 ppl=5855.37 Calculating multiple coherence metrics...\n",
      "[PAM] iter=13100 llpw=-8.6751 ppl=5855.37 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=13200 llpw=-8.6615 ppl=5776.33 Calculating multiple coherence metrics...\n",
      "[PAM] iter=13200 llpw=-8.6615 ppl=5776.33 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=13300 llpw=-8.6898 ppl=5941.89 Calculating multiple coherence metrics...\n",
      "[PAM] iter=13300 llpw=-8.6898 ppl=5941.89 c_v=0.5649, c_npmi=0.0339\n",
      "[PAM] iter=13400 llpw=-8.6498 ppl=5708.75 Calculating multiple coherence metrics...\n",
      "[PAM] iter=13400 llpw=-8.6498 ppl=5708.75 c_v=0.5676, c_npmi=0.0478\n",
      "[PAM] iter=13500 llpw=-8.6736 ppl=5846.77 Calculating multiple coherence metrics...\n",
      "[PAM] iter=13500 llpw=-8.6736 ppl=5846.77 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=13600 llpw=-8.6641 ppl=5791.08 Calculating multiple coherence metrics...\n",
      "[PAM] iter=13600 llpw=-8.6641 ppl=5791.08 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=13700 llpw=-8.6628 ppl=5783.58 Calculating multiple coherence metrics...\n",
      "[PAM] iter=13700 llpw=-8.6628 ppl=5783.58 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=13800 llpw=-8.6574 ppl=5752.34 Calculating multiple coherence metrics...\n",
      "[PAM] iter=13800 llpw=-8.6574 ppl=5752.34 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=13900 llpw=-8.6590 ppl=5761.81 Calculating multiple coherence metrics...\n",
      "[PAM] iter=13900 llpw=-8.6590 ppl=5761.81 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=14000 llpw=-8.6606 ppl=5770.85 Calculating multiple coherence metrics...\n",
      "[PAM] iter=14000 llpw=-8.6606 ppl=5770.85 c_v=0.6035, c_npmi=0.0649\n",
      "[PAM] iter=14100 llpw=-8.6356 ppl=5628.65 Calculating multiple coherence metrics...\n",
      "[PAM] iter=14100 llpw=-8.6356 ppl=5628.65 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=14200 llpw=-8.6562 ppl=5745.46 Calculating multiple coherence metrics...\n",
      "[PAM] iter=14200 llpw=-8.6562 ppl=5745.46 c_v=0.5965, c_npmi=0.0513\n",
      "[PAM] iter=14300 llpw=-8.6710 ppl=5831.55 Calculating multiple coherence metrics...\n",
      "[PAM] iter=14300 llpw=-8.6710 ppl=5831.55 c_v=0.6059, c_npmi=0.0575\n",
      "[PAM] iter=14400 llpw=-8.6503 ppl=5711.79 Calculating multiple coherence metrics...\n",
      "[PAM] iter=14400 llpw=-8.6503 ppl=5711.79 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=14500 llpw=-8.6485 ppl=5701.45 Calculating multiple coherence metrics...\n",
      "[PAM] iter=14500 llpw=-8.6485 ppl=5701.45 c_v=0.5863, c_npmi=0.0429\n",
      "[PAM] iter=14600 llpw=-8.6541 ppl=5733.53 Calculating multiple coherence metrics...\n",
      "[PAM] iter=14600 llpw=-8.6541 ppl=5733.53 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=14700 llpw=-8.6477 ppl=5697.04 Calculating multiple coherence metrics...\n",
      "[PAM] iter=14700 llpw=-8.6477 ppl=5697.04 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=14800 llpw=-8.6386 ppl=5645.37 Calculating multiple coherence metrics...\n",
      "[PAM] iter=14800 llpw=-8.6386 ppl=5645.37 c_v=0.5824, c_npmi=0.0492\n",
      "[PAM] iter=14900 llpw=-8.6448 ppl=5680.72 Calculating multiple coherence metrics...\n",
      "[PAM] iter=14900 llpw=-8.6448 ppl=5680.72 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=15000 llpw=-8.6334 ppl=5616.15 Calculating multiple coherence metrics...\n",
      "[PAM] iter=15000 llpw=-8.6334 ppl=5616.15 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=15100 llpw=-8.6263 ppl=5576.54 Calculating multiple coherence metrics...\n",
      "[PAM] iter=15100 llpw=-8.6263 ppl=5576.54 c_v=0.5965, c_npmi=0.0513\n",
      "[PAM] iter=15200 llpw=-8.6408 ppl=5658.03 Calculating multiple coherence metrics...\n",
      "[PAM] iter=15200 llpw=-8.6408 ppl=5658.03 c_v=0.6059, c_npmi=0.0575\n",
      "[PAM] iter=15300 llpw=-8.6352 ppl=5626.35 Calculating multiple coherence metrics...\n",
      "[PAM] iter=15300 llpw=-8.6352 ppl=5626.35 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=15400 llpw=-8.6273 ppl=5582.15 Calculating multiple coherence metrics...\n",
      "[PAM] iter=15400 llpw=-8.6273 ppl=5582.15 c_v=0.5676, c_npmi=0.0478\n",
      "[PAM] iter=15500 llpw=-8.6073 ppl=5471.38 Calculating multiple coherence metrics...\n",
      "[PAM] iter=15500 llpw=-8.6073 ppl=5471.38 c_v=0.5965, c_npmi=0.0513\n",
      "[PAM] iter=15600 llpw=-8.6206 ppl=5544.96 Calculating multiple coherence metrics...\n",
      "[PAM] iter=15600 llpw=-8.6206 ppl=5544.96 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=15700 llpw=-8.5922 ppl=5389.58 Calculating multiple coherence metrics...\n",
      "[PAM] iter=15700 llpw=-8.5922 ppl=5389.58 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=15800 llpw=-8.6114 ppl=5494.10 Calculating multiple coherence metrics...\n",
      "[PAM] iter=15800 llpw=-8.6114 ppl=5494.10 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=15900 llpw=-8.6057 ppl=5462.61 Calculating multiple coherence metrics...\n",
      "[PAM] iter=15900 llpw=-8.6057 ppl=5462.61 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=16000 llpw=-8.6225 ppl=5555.07 Calculating multiple coherence metrics...\n",
      "[PAM] iter=16000 llpw=-8.6225 ppl=5555.07 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=16100 llpw=-8.6166 ppl=5522.43 Calculating multiple coherence metrics...\n",
      "[PAM] iter=16100 llpw=-8.6166 ppl=5522.43 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=16200 llpw=-8.6069 ppl=5469.30 Calculating multiple coherence metrics...\n",
      "[PAM] iter=16200 llpw=-8.6069 ppl=5469.30 c_v=0.6059, c_npmi=0.0575\n",
      "[PAM] iter=16300 llpw=-8.6242 ppl=5564.97 Calculating multiple coherence metrics...\n",
      "[PAM] iter=16300 llpw=-8.6242 ppl=5564.97 c_v=0.5965, c_npmi=0.0513\n",
      "[PAM] iter=16400 llpw=-8.6268 ppl=5579.23 Calculating multiple coherence metrics...\n",
      "[PAM] iter=16400 llpw=-8.6268 ppl=5579.23 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=16500 llpw=-8.6221 ppl=5553.30 Calculating multiple coherence metrics...\n",
      "[PAM] iter=16500 llpw=-8.6221 ppl=5553.30 c_v=0.5965, c_npmi=0.0513\n",
      "[PAM] iter=16600 llpw=-8.6328 ppl=5612.54 Calculating multiple coherence metrics...\n",
      "[PAM] iter=16600 llpw=-8.6328 ppl=5612.54 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=16700 llpw=-8.5847 ppl=5349.32 Calculating multiple coherence metrics...\n",
      "[PAM] iter=16700 llpw=-8.5847 ppl=5349.32 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=16800 llpw=-8.5837 ppl=5343.92 Calculating multiple coherence metrics...\n",
      "[PAM] iter=16800 llpw=-8.5837 ppl=5343.92 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=16900 llpw=-8.6106 ppl=5489.44 Calculating multiple coherence metrics...\n",
      "[PAM] iter=16900 llpw=-8.6106 ppl=5489.44 c_v=0.5783, c_npmi=0.0479\n",
      "[PAM] iter=17000 llpw=-8.6113 ppl=5493.61 Calculating multiple coherence metrics...\n",
      "[PAM] iter=17000 llpw=-8.6113 ppl=5493.61 c_v=0.5676, c_npmi=0.0478\n",
      "[PAM] iter=17100 llpw=-8.6046 ppl=5456.70 Calculating multiple coherence metrics...\n",
      "[PAM] iter=17100 llpw=-8.6046 ppl=5456.70 c_v=0.5676, c_npmi=0.0478\n",
      "[PAM] iter=17200 llpw=-8.5925 ppl=5391.28 Calculating multiple coherence metrics...\n",
      "[PAM] iter=17200 llpw=-8.5925 ppl=5391.28 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=17300 llpw=-8.6217 ppl=5550.91 Calculating multiple coherence metrics...\n",
      "[PAM] iter=17300 llpw=-8.6217 ppl=5550.91 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=17400 llpw=-8.5905 ppl=5380.09 Calculating multiple coherence metrics...\n",
      "[PAM] iter=17400 llpw=-8.5905 ppl=5380.09 c_v=0.5649, c_npmi=0.0339\n",
      "[PAM] iter=17500 llpw=-8.5888 ppl=5371.05 Calculating multiple coherence metrics...\n",
      "[PAM] iter=17500 llpw=-8.5888 ppl=5371.05 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=17600 llpw=-8.6018 ppl=5441.54 Calculating multiple coherence metrics...\n",
      "[PAM] iter=17600 llpw=-8.6018 ppl=5441.54 c_v=0.5824, c_npmi=0.0492\n",
      "[PAM] iter=17700 llpw=-8.5883 ppl=5368.69 Calculating multiple coherence metrics...\n",
      "[PAM] iter=17700 llpw=-8.5883 ppl=5368.69 c_v=0.5965, c_npmi=0.0513\n",
      "[PAM] iter=17800 llpw=-8.5705 ppl=5273.68 Calculating multiple coherence metrics...\n",
      "[PAM] iter=17800 llpw=-8.5705 ppl=5273.68 c_v=0.5965, c_npmi=0.0513\n",
      "[PAM] iter=17900 llpw=-8.5807 ppl=5327.93 Calculating multiple coherence metrics...\n",
      "[PAM] iter=17900 llpw=-8.5807 ppl=5327.93 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=18000 llpw=-8.5760 ppl=5302.95 Calculating multiple coherence metrics...\n",
      "[PAM] iter=18000 llpw=-8.5760 ppl=5302.95 c_v=0.5965, c_npmi=0.0513\n",
      "[PAM] iter=18100 llpw=-8.5826 ppl=5337.83 Calculating multiple coherence metrics...\n",
      "[PAM] iter=18100 llpw=-8.5826 ppl=5337.83 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=18200 llpw=-8.5717 ppl=5280.16 Calculating multiple coherence metrics...\n",
      "[PAM] iter=18200 llpw=-8.5717 ppl=5280.16 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=18300 llpw=-8.5893 ppl=5373.59 Calculating multiple coherence metrics...\n",
      "[PAM] iter=18300 llpw=-8.5893 ppl=5373.59 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=18400 llpw=-8.5836 ppl=5343.31 Calculating multiple coherence metrics...\n",
      "[PAM] iter=18400 llpw=-8.5836 ppl=5343.31 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=18500 llpw=-8.5861 ppl=5356.71 Calculating multiple coherence metrics...\n",
      "[PAM] iter=18500 llpw=-8.5861 ppl=5356.71 c_v=0.5965, c_npmi=0.0513\n",
      "[PAM] iter=18600 llpw=-8.5896 ppl=5375.58 Calculating multiple coherence metrics...\n",
      "[PAM] iter=18600 llpw=-8.5896 ppl=5375.58 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=18700 llpw=-8.5937 ppl=5397.44 Calculating multiple coherence metrics...\n",
      "[PAM] iter=18700 llpw=-8.5937 ppl=5397.44 c_v=0.5783, c_npmi=0.0479\n",
      "[PAM] iter=18800 llpw=-8.6006 ppl=5435.09 Calculating multiple coherence metrics...\n",
      "[PAM] iter=18800 llpw=-8.6006 ppl=5435.09 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=18900 llpw=-8.5840 ppl=5345.42 Calculating multiple coherence metrics...\n",
      "[PAM] iter=18900 llpw=-8.5840 ppl=5345.42 c_v=0.5965, c_npmi=0.0513\n",
      "[PAM] iter=19000 llpw=-8.6039 ppl=5453.03 Calculating multiple coherence metrics...\n",
      "[PAM] iter=19000 llpw=-8.6039 ppl=5453.03 c_v=0.5783, c_npmi=0.0479\n",
      "[PAM] iter=19100 llpw=-8.5880 ppl=5366.78 Calculating multiple coherence metrics...\n",
      "[PAM] iter=19100 llpw=-8.5880 ppl=5366.78 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=19200 llpw=-8.6176 ppl=5528.01 Calculating multiple coherence metrics...\n",
      "[PAM] iter=19200 llpw=-8.6176 ppl=5528.01 c_v=0.5676, c_npmi=0.0478\n",
      "[PAM] iter=19300 llpw=-8.5629 ppl=5233.84 Calculating multiple coherence metrics...\n",
      "[PAM] iter=19300 llpw=-8.5629 ppl=5233.84 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=19400 llpw=-8.6054 ppl=5460.98 Calculating multiple coherence metrics...\n",
      "[PAM] iter=19400 llpw=-8.6054 ppl=5460.98 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=19500 llpw=-8.5888 ppl=5371.24 Calculating multiple coherence metrics...\n",
      "[PAM] iter=19500 llpw=-8.5888 ppl=5371.24 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=19600 llpw=-8.5799 ppl=5323.83 Calculating multiple coherence metrics...\n",
      "[PAM] iter=19600 llpw=-8.5799 ppl=5323.83 c_v=0.5965, c_npmi=0.0513\n",
      "[PAM] iter=19700 llpw=-8.5920 ppl=5388.26 Calculating multiple coherence metrics...\n",
      "[PAM] iter=19700 llpw=-8.5920 ppl=5388.26 c_v=0.5689, c_npmi=0.0416\n",
      "[PAM] iter=19800 llpw=-8.5905 ppl=5380.53 Calculating multiple coherence metrics...\n",
      "[PAM] iter=19800 llpw=-8.5905 ppl=5380.53 c_v=0.5730, c_npmi=0.0430\n",
      "[PAM] iter=19900 llpw=-8.5859 ppl=5355.68 Calculating multiple coherence metrics...\n",
      "[PAM] iter=19900 llpw=-8.5859 ppl=5355.68 c_v=0.5965, c_npmi=0.0513\n",
      "[PAM] iter=20000 llpw=-8.5631 ppl=5234.95 Calculating multiple coherence metrics...\n",
      "[PAM] iter=20000 llpw=-8.5631 ppl=5234.95 c_v=0.5730, c_npmi=0.0430\n",
      "\n",
      "✅ Training complete! Time taken: 1285.3 seconds\n",
      "\n",
      "🔍 Performing full evaluation with multiple coherence metrics...\n",
      "\n",
      "🔍 PAM Multiple Coherence Metrics Evaluation:\n",
      "================================================================================\n",
      "📊 Coherence Metrics Results:\n",
      "   📈 C_v (Vector Space Coherence): 0.573035\n",
      "   📈 NPMI (Normalized Pointwise Mutual Information): 0.042982\n",
      "\n",
      "📊 Basic Metrics:\n",
      "   📈 Effective topics: 4\n",
      "   📈 Total topics: 59\n",
      "   📈 Topic utilization: 6.8%\n",
      "\n",
      "💡 Coherence Metric Explanations:\n",
      "   🎯 C_v: Range [0,1], higher is better, based on word vector similarity\n",
      "   🎯 NPMI: Range [-1,1], higher is better, Normalized Pointwise Mutual Information\n",
      "\n",
      "🏆 Composite Coherence Score: 0.5473 (average of normalized scores)\n",
      "\n",
      "📈 Final Evaluation Results:\n",
      "   - Training iterations: 20000\n",
      "   - Log-likelihood per word: -8.563113\n",
      "   - Perplexity: 5234.95\n",
      "   - Effective topics: 4/236\n",
      "\n",
      "📊 Multiple Coherence Metrics:\n",
      "   - C_v (Vector Space): 0.573035\n",
      "   - NPMI (Pointwise Mutual Information): 0.042982\n",
      "   - Composite Score: 0.547263\n",
      "   - Topic Utilization: 6.8%\n",
      "\n",
      "🔍 PAM Topic Analysis (showing top 5 words):\n",
      "================================================================================\n",
      "Topic   0 (weight:0.038): element(0.038), formulation(0.029), finite(0.019), present(0.015), contact(0.015)\n",
      "Topic   1 (weight:0.037): method(0.037), flow(0.036), mesh(0.030), computation(0.024), analysis(0.020)\n",
      "Topic   2 (weight:0.040): method(0.040), problem(0.020), use(0.018), propose(0.017), numerical(0.015)\n",
      "Topic   3 (weight:0.049): model(0.049), material(0.022), simulation(0.014), crack(0.011), use(0.010)\n",
      "\n",
      "📊 PAM Topic Statistics:\n",
      "   - Active topics: 4/59\n",
      "   - Topic activity rate: 6.8%\n",
      "\n",
      "🎯 PAM model evaluation complete!\n",
      "📊 Key Metrics:\n",
      "   - iterations: 20000\n",
      "   - log_likelihood_per_word: -8.5631\n",
      "   - perplexity: 5234.9548\n",
      "   - coherence_c_v: 0.5730\n",
      "   - coherence_npmi: 0.0430\n",
      "   - effective_topics: 4\n",
      "   - training_time_seconds: 1285.2630\n",
      "   - convergence_iterations: 200\n",
      "   - composite_score: 0.5473\n",
      "   - topic_utilization: 0.0678\n",
      "\n",
      "💾 PAM model is trained, features:\n",
      "   1. Two-level hierarchical model (Pseudo-document-based)\n",
      "   2. Aligned with hLDA structure for fair comparison\n",
      "   3. Models super-topics and sub-topics\n",
      "\n",
      "🔬 Renyi entropy weighted by document count (PAM): 0.3338\n",
      "🔬 Average JSD (full distribution) (PAM): 0.1637\n",
      "🔬 Unweighted Renyi entropy (PAM): 0.3416\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== PAM Model Construction and Training (Aligned with hLDA) =====\n",
    "\n",
    "print(\"\\n🎯 Starting PAM model training - Aligned with hLDA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define PAM-specific parameters aligned with hLDA branching factors\n",
    "PAM_K1 = B1  # 59, from hLDA branching factor\n",
    "PAM_K2 = B2  # 4, from hLDA branching factor\n",
    "PAM_TOTAL_K = PAM_K1 * PAM_K2 # 236 topics, close to hLDA's 252 leaf nodes\n",
    "\n",
    "print(f\"🔧 PAM parameters aligned with hLDA: K1={PAM_K1}, K2={PAM_K2}, Total K={PAM_TOTAL_K}\")\n",
    "\n",
    "def build_pam(docs, seed=0, max_iters=2000, use_multiple_coherence=True):\n",
    "    \"\"\"Build and train a PAM model - supports multiple coherence metrics\"\"\"\n",
    "    print(f\"🚀 Creating PAM model (K1={PAM_K1}, K2={PAM_K2}, α={ALPHA}, η={ETA}, seed={seed})\")\n",
    "    # PAModel uses k1 and k2 for its two-level topic structure\n",
    "    mdl = tp.PAModel(k1=PAM_K1, k2=PAM_K2, alpha=ALPHA, subalpha=ALPHA, eta=ETA, seed=seed)\n",
    "    \n",
    "    if use_multiple_coherence:\n",
    "        # Train with multiple coherence metrics\n",
    "        return auto_train_with_multiple_metrics(\n",
    "            mdl, docs, seed=seed, max_iters=max_iters, \n",
    "            coherence_measures=['c_v', 'c_npmi'], \n",
    "            name='PAM'\n",
    "        )\n",
    "    else:\n",
    "        # Train with a traditional single metric\n",
    "        return auto_train(mdl, docs, seed=seed, max_iters=max_iters, name='PAM')\n",
    "\n",
    "def train_and_evaluate_pam(docs, seed=42, max_iters=2000, detailed_output=True, use_multiple_coherence=True):\n",
    "    \"\"\"\n",
    "    Train a PAM model and perform a full evaluation - supports multiple coherence metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🚀 Starting to train PAM model (seed={seed})\")\n",
    "    print(f\"📊 Data Overview:\")\n",
    "    print(f\"   - Number of documents: {len(docs)}\")\n",
    "    print(f\"   - Average document length: {np.mean([len(doc) for doc in docs]):.1f} words\")\n",
    "    print(f\"   - Vocabulary size: {len(set(word for doc in docs for word in doc))}\")\n",
    "    \n",
    "    print(f\"\\n🔧 Model Parameters:\")\n",
    "    print(f\"   - Super-topics K1 = {PAM_K1}\")\n",
    "    print(f\"   - Sub-topics K2 = {PAM_K2}\")\n",
    "    print(f\"   - Total topics = {PAM_TOTAL_K}\")\n",
    "    print(f\"   - Alpha = {ALPHA}\")\n",
    "    print(f\"   - Eta = {ETA}\")\n",
    "    print(f\"   - Max iterations = {max_iters}\")\n",
    "    if use_multiple_coherence:\n",
    "        print(f\"   - Coherence metrics = ['c_v', 'c_npmi']\")\n",
    "    \n",
    "    # Train the model\n",
    "    start_time = time.time()\n",
    "    pam_model, training_history = build_pam(docs, seed=seed, max_iters=max_iters, \n",
    "                                           use_multiple_coherence=use_multiple_coherence)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n✅ Training complete! Time taken: {training_time:.1f} seconds\")\n",
    "    \n",
    "    # Extract final metrics\n",
    "    if training_history:\n",
    "        final_iter, final_llpw, final_ppl, final_coherence_dict = training_history[-1]\n",
    "        \n",
    "        # Perform a full evaluation with multiple coherence metrics\n",
    "        print(f\"\\n🔍 Performing full evaluation with multiple coherence metrics...\")\n",
    "        full_evaluation = evaluate_model_with_multiple_coherence(\n",
    "            pam_model, docs, \"PAM\", \n",
    "            coherence_metrics=['c_v', 'c_npmi'],\n",
    "            top_words_for_coherence=5\n",
    "        )\n",
    "        \n",
    "        metrics = {\n",
    "            'iterations': final_iter,\n",
    "            'log_likelihood_per_word': final_llpw,\n",
    "            'perplexity': final_ppl,\n",
    "            'coherence_c_v': full_evaluation['coherence_metrics'].get('c_v', 0),\n",
    "            'coherence_npmi': full_evaluation['coherence_metrics'].get('c_npmi', 0),\n",
    "            'effective_topics': full_evaluation.get('effective_topics', 0),\n",
    "            'training_time_seconds': training_time,\n",
    "            'convergence_iterations': len(training_history),\n",
    "            'composite_score': full_evaluation.get('composite_score', 0),\n",
    "            'topic_utilization': full_evaluation.get('topic_utilization', 0)\n",
    "        }\n",
    "        \n",
    "        if detailed_output:\n",
    "            print(f\"\\n📈 Final Evaluation Results:\")\n",
    "            print(f\"   - Training iterations: {final_iter}\")\n",
    "            print(f\"   - Log-likelihood per word: {final_llpw:.6f}\")\n",
    "            print(f\"   - Perplexity: {final_ppl:.2f}\")\n",
    "            print(f\"   - Effective topics: {metrics['effective_topics']}/{PAM_TOTAL_K}\")\n",
    "            print(f\"\\n📊 Multiple Coherence Metrics:\")\n",
    "            print(f\"   - C_v (Vector Space): {metrics['coherence_c_v']:.6f}\")\n",
    "            print(f\"   - NPMI (Pointwise Mutual Information): {metrics['coherence_npmi']:.6f}\")\n",
    "            print(f\"   - Composite Score: {metrics['composite_score']:.6f}\")\n",
    "            print(f\"   - Topic Utilization: {metrics['topic_utilization']*100:.1f}%\")\n",
    "    else:\n",
    "        print(\"❌ No training history recorded\")\n",
    "        metrics = {}\n",
    "    \n",
    "    return pam_model, metrics, training_history\n",
    "\n",
    "# ===== Execute PAM Modeling and Evaluation =====\n",
    "print(\"\\n🎯 Training PAM model with the full dataset...\")\n",
    "\n",
    "# Use the full corpus\n",
    "full_docs = list(corpus.values())\n",
    "print(f\"📚 Using full dataset: {len(full_docs)} documents\")\n",
    "\n",
    "# Train and evaluate PAM\n",
    "pam_model, pam_metrics, pam_history = train_and_evaluate_pam(\n",
    "    docs=full_docs,\n",
    "    seed=42,\n",
    "    max_iters=20000,\n",
    "    detailed_output=True,\n",
    "    use_multiple_coherence=True\n",
    ")\n",
    "\n",
    "# Topic analysis\n",
    "if pam_model:\n",
    "    pam_topic_info = analyze_model_topics(pam_model, model_name=\"PAM\", top_words=5)\n",
    "    \n",
    "    print(f\"\\n🎯 PAM model evaluation complete!\")\n",
    "    print(f\"📊 Key Metrics:\")\n",
    "    if pam_metrics:\n",
    "        for key, value in pam_metrics.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"   - {key}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"   - {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\n💾 PAM model is trained, features:\")\n",
    "    print(f\"   1. Two-level hierarchical model (Pseudo-document-based)\")\n",
    "    print(f\"   2. Aligned with hLDA structure for fair comparison\")\n",
    "    print(f\"   3. Models super-topics and sub-topics\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ PAM model training failed\")\n",
    "\n",
    "# Called after PAM model training and evaluation\n",
    "if pam_model:\n",
    "    renyi_entropy = calculate_weighted_renyi_entropy_full_weighted(pam_model, alpha=2)\n",
    "    jsd_distance = calculate_weighted_jsd_full(pam_model)\n",
    "    renyi_entropy_unweighted = calculate_renyi_entropy_unweighted(pam_model, alpha=2)\n",
    "    print(f\"\\n🔬 Renyi entropy weighted by document count (PAM): {renyi_entropy:.4f}\")\n",
    "    print(f\"🔬 Average JSD (full distribution) (PAM): {jsd_distance:.4f}\")\n",
    "    print(f\"🔬 Unweighted Renyi entropy (PAM): {renyi_entropy_unweighted:.4f}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8072cc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Starting HDP model training - Optimized for hLDA comparison\n",
      "============================================================\n",
      "\n",
      "🎯 Training HDP model with the full dataset (for hLDA comparison)...\n",
      "📚 Using full dataset: 970 documents\n",
      "🚀 Starting to train HDP model for comparison with hLDA (seed=24)\n",
      "📊 Data Overview:\n",
      "   - Number of documents: 970\n",
      "   - Average document length: 85.8 words\n",
      "   - Vocabulary size: 1490\n",
      "   - Target number of topics: 100 (aligned with hLDA leaf nodes)\n",
      "🔄 Iteratively adjusting HDP parameters, target number of topics: 100\n",
      "\n",
      "--- Attempt 1/3: gamma=0.5 ---\n",
      "[HDP-1] iter=  50 llpw=-6.6605 ppl=780.98 Calculating multiple coherence metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v5/6mdkg5713kxgwg5xs24g8rvr0000gn/T/ipykernel_55006/1551040497.py:208: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  mdl.train(interval)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HDP-1] iter=  50 llpw=-6.6605 ppl=780.98 c_v=0.5494, c_npmi=0.0549\n",
      "[HDP-1] iter= 100 llpw=-6.6421 ppl=766.73 Calculating multiple coherence metrics...\n",
      "[HDP-1] iter= 100 llpw=-6.6421 ppl=766.73 c_v=0.5431, c_npmi=0.0530\n",
      "[HDP-1] iter= 150 llpw=-6.6326 ppl=759.47 Calculating multiple coherence metrics...\n",
      "[HDP-1] iter= 150 llpw=-6.6326 ppl=759.47 c_v=0.5409, c_npmi=0.0519\n",
      "[HDP-1] iter= 200 llpw=-6.6250 ppl=753.72 Calculating multiple coherence metrics...\n",
      "[HDP-1] iter= 200 llpw=-6.6250 ppl=753.72 c_v=0.5482, c_npmi=0.0546\n",
      "[HDP-1] iter= 250 llpw=-6.6237 ppl=752.71 Calculating multiple coherence metrics...\n",
      "[HDP-1] iter= 250 llpw=-6.6237 ppl=752.71 c_v=0.5512, c_npmi=0.0590\n",
      "[HDP-1] iter= 300 llpw=-6.6192 ppl=749.33 Calculating multiple coherence metrics...\n",
      "[HDP-1] iter= 300 llpw=-6.6192 ppl=749.33 c_v=0.5546, c_npmi=0.0625\n",
      "[HDP-1] iter= 350 llpw=-6.6186 ppl=748.89 Calculating multiple coherence metrics...\n",
      "[HDP-1] iter= 350 llpw=-6.6186 ppl=748.89 c_v=0.5600, c_npmi=0.0732\n",
      "[HDP-1] iter= 400 llpw=-6.6121 ppl=744.05 Calculating multiple coherence metrics...\n",
      "[HDP-1] iter= 400 llpw=-6.6121 ppl=744.05 c_v=0.5583, c_npmi=0.0633\n",
      "[HDP-1] iter= 450 llpw=-6.6118 ppl=743.84 Calculating multiple coherence metrics...\n",
      "[HDP-1] iter= 450 llpw=-6.6118 ppl=743.84 c_v=0.5505, c_npmi=0.0687\n",
      "[HDP-1] iter= 500 llpw=-6.6093 ppl=741.95 Calculating multiple coherence metrics...\n",
      "[HDP-1] iter= 500 llpw=-6.6093 ppl=741.95 c_v=0.5506, c_npmi=0.0623\n",
      "[HDP-1] iter= 550 llpw=-6.6105 ppl=742.89 Calculating multiple coherence metrics...\n",
      "[HDP-1] iter= 550 llpw=-6.6105 ppl=742.89 c_v=0.5463, c_npmi=0.0646\n",
      "[HDP-1] iter= 600 llpw=-6.6071 ppl=740.33 Calculating multiple coherence metrics...\n",
      "[HDP-1] iter= 600 llpw=-6.6071 ppl=740.33 c_v=0.5515, c_npmi=0.0658\n",
      "[HDP-1] iter= 650 llpw=-6.6054 ppl=739.08 Calculating multiple coherence metrics...\n",
      "[HDP-1] iter= 650 llpw=-6.6054 ppl=739.08 c_v=0.5529, c_npmi=0.0642\n",
      "[HDP-1] iter= 700 llpw=-6.6057 ppl=739.29 Calculating multiple coherence metrics...\n",
      "[HDP-1] iter= 700 llpw=-6.6057 ppl=739.29 c_v=0.5513, c_npmi=0.0631\n",
      "[HDP-1] iter= 750 llpw=-6.6053 ppl=739.01 Calculating multiple coherence metrics...\n",
      "[HDP-1] iter= 750 llpw=-6.6053 ppl=739.01 c_v=0.5426, c_npmi=0.0576\n",
      "[HDP-1] iter= 800 llpw=-6.6032 ppl=737.48 Calculating multiple coherence metrics...\n",
      "[HDP-1] iter= 800 llpw=-6.6032 ppl=737.48 c_v=0.5473, c_npmi=0.0631\n",
      "[HDP-1] iter= 850 llpw=-6.5998 ppl=734.93 Calculating multiple coherence metrics...\n",
      "[HDP-1] iter= 850 llpw=-6.5998 ppl=734.93 c_v=0.5498, c_npmi=0.0625\n",
      "[HDP-1] iter= 900 llpw=-6.6033 ppl=737.51 Calculating multiple coherence metrics...\n",
      "[HDP-1] iter= 900 llpw=-6.6033 ppl=737.51 c_v=0.5592, c_npmi=0.0607\n",
      "[HDP-1] iter= 950 llpw=-6.6021 ppl=736.66 Calculating multiple coherence metrics...\n",
      "[HDP-1] iter= 950 llpw=-6.6021 ppl=736.66 c_v=0.5492, c_npmi=0.0585\n",
      "[HDP-1] iter=1000 llpw=-6.6026 ppl=737.00 Calculating multiple coherence metrics...\n",
      "[HDP-1] iter=1000 llpw=-6.6026 ppl=737.00 c_v=0.5558, c_npmi=0.0621\n",
      "📊 Attempt 1 Results:\n",
      "   - Effective topics: 108\n",
      "   - Total topics: 108\n",
      "   - Difference from target: 8\n",
      "\n",
      "--- Attempt 2/3: gamma=1.0 ---\n",
      "[HDP-2] iter=  50 llpw=-6.6531 ppl=775.18 Calculating multiple coherence metrics...\n",
      "[HDP-2] iter=  50 llpw=-6.6531 ppl=775.18 c_v=0.5379, c_npmi=0.0445\n",
      "[HDP-2] iter= 100 llpw=-6.6362 ppl=762.17 Calculating multiple coherence metrics...\n",
      "[HDP-2] iter= 100 llpw=-6.6362 ppl=762.17 c_v=0.5452, c_npmi=0.0480\n",
      "[HDP-2] iter= 150 llpw=-6.6298 ppl=757.30 Calculating multiple coherence metrics...\n",
      "[HDP-2] iter= 150 llpw=-6.6298 ppl=757.30 c_v=0.5422, c_npmi=0.0573\n",
      "[HDP-2] iter= 200 llpw=-6.6267 ppl=754.98 Calculating multiple coherence metrics...\n",
      "[HDP-2] iter= 200 llpw=-6.6267 ppl=754.98 c_v=0.5389, c_npmi=0.0515\n",
      "[HDP-2] iter= 250 llpw=-6.6250 ppl=753.68 Calculating multiple coherence metrics...\n",
      "[HDP-2] iter= 250 llpw=-6.6250 ppl=753.68 c_v=0.5429, c_npmi=0.0515\n",
      "[HDP-2] iter= 300 llpw=-6.6229 ppl=752.12 Calculating multiple coherence metrics...\n",
      "[HDP-2] iter= 300 llpw=-6.6229 ppl=752.12 c_v=0.5411, c_npmi=0.0480\n",
      "[HDP-2] iter= 350 llpw=-6.6179 ppl=748.39 Calculating multiple coherence metrics...\n",
      "[HDP-2] iter= 350 llpw=-6.6179 ppl=748.39 c_v=0.5503, c_npmi=0.0569\n",
      "[HDP-2] iter= 400 llpw=-6.6199 ppl=749.84 Calculating multiple coherence metrics...\n",
      "[HDP-2] iter= 400 llpw=-6.6199 ppl=749.84 c_v=0.5466, c_npmi=0.0577\n",
      "[HDP-2] iter= 450 llpw=-6.6152 ppl=746.35 Calculating multiple coherence metrics...\n",
      "[HDP-2] iter= 450 llpw=-6.6152 ppl=746.35 c_v=0.5507, c_npmi=0.0536\n",
      "[HDP-2] iter= 500 llpw=-6.6182 ppl=748.63 Calculating multiple coherence metrics...\n",
      "[HDP-2] iter= 500 llpw=-6.6182 ppl=748.63 c_v=0.5461, c_npmi=0.0546\n",
      "[HDP-2] iter= 550 llpw=-6.6153 ppl=746.41 Calculating multiple coherence metrics...\n",
      "[HDP-2] iter= 550 llpw=-6.6153 ppl=746.41 c_v=0.5452, c_npmi=0.0551\n",
      "[HDP-2] iter= 600 llpw=-6.6151 ppl=746.25 Calculating multiple coherence metrics...\n",
      "[HDP-2] iter= 600 llpw=-6.6151 ppl=746.25 c_v=0.5418, c_npmi=0.0491\n",
      "[HDP-2] early stop at iter 600\n",
      "📊 Attempt 2 Results:\n",
      "   - Effective topics: 107\n",
      "   - Total topics: 107\n",
      "   - Difference from target: 7\n",
      "\n",
      "--- Attempt 3/3: gamma=2.0 ---\n",
      "[HDP-3] iter=  50 llpw=-6.6582 ppl=779.14 Calculating multiple coherence metrics...\n",
      "[HDP-3] iter=  50 llpw=-6.6582 ppl=779.14 c_v=0.5266, c_npmi=0.0357\n",
      "[HDP-3] iter= 100 llpw=-6.6400 ppl=765.11 Calculating multiple coherence metrics...\n",
      "[HDP-3] iter= 100 llpw=-6.6400 ppl=765.11 c_v=0.5292, c_npmi=0.0429\n",
      "[HDP-3] iter= 150 llpw=-6.6319 ppl=758.94 Calculating multiple coherence metrics...\n",
      "[HDP-3] iter= 150 llpw=-6.6319 ppl=758.94 c_v=0.5395, c_npmi=0.0519\n",
      "[HDP-3] iter= 200 llpw=-6.6301 ppl=757.58 Calculating multiple coherence metrics...\n",
      "[HDP-3] iter= 200 llpw=-6.6301 ppl=757.58 c_v=0.5331, c_npmi=0.0497\n",
      "[HDP-3] iter= 250 llpw=-6.6258 ppl=754.27 Calculating multiple coherence metrics...\n",
      "[HDP-3] iter= 250 llpw=-6.6258 ppl=754.27 c_v=0.5370, c_npmi=0.0540\n",
      "[HDP-3] iter= 300 llpw=-6.6283 ppl=756.21 Calculating multiple coherence metrics...\n",
      "[HDP-3] iter= 300 llpw=-6.6283 ppl=756.21 c_v=0.5326, c_npmi=0.0515\n",
      "[HDP-3] iter= 350 llpw=-6.6268 ppl=755.05 Calculating multiple coherence metrics...\n",
      "[HDP-3] iter= 350 llpw=-6.6268 ppl=755.05 c_v=0.5292, c_npmi=0.0443\n",
      "[HDP-3] iter= 400 llpw=-6.6233 ppl=752.42 Calculating multiple coherence metrics...\n",
      "[HDP-3] iter= 400 llpw=-6.6233 ppl=752.42 c_v=0.5308, c_npmi=0.0541\n",
      "[HDP-3] iter= 450 llpw=-6.6248 ppl=753.52 Calculating multiple coherence metrics...\n",
      "[HDP-3] iter= 450 llpw=-6.6248 ppl=753.52 c_v=0.5397, c_npmi=0.0490\n",
      "[HDP-3] iter= 500 llpw=-6.6227 ppl=751.94 Calculating multiple coherence metrics...\n",
      "[HDP-3] iter= 500 llpw=-6.6227 ppl=751.94 c_v=0.5363, c_npmi=0.0494\n",
      "[HDP-3] iter= 550 llpw=-6.6230 ppl=752.19 Calculating multiple coherence metrics...\n",
      "[HDP-3] iter= 550 llpw=-6.6230 ppl=752.19 c_v=0.5390, c_npmi=0.0482\n",
      "[HDP-3] iter= 600 llpw=-6.6211 ppl=750.77 Calculating multiple coherence metrics...\n",
      "[HDP-3] iter= 600 llpw=-6.6211 ppl=750.77 c_v=0.5276, c_npmi=0.0433\n",
      "[HDP-3] iter= 650 llpw=-6.6231 ppl=752.24 Calculating multiple coherence metrics...\n",
      "[HDP-3] iter= 650 llpw=-6.6231 ppl=752.24 c_v=0.5386, c_npmi=0.0553\n",
      "[HDP-3] iter= 700 llpw=-6.6194 ppl=749.49 Calculating multiple coherence metrics...\n",
      "[HDP-3] iter= 700 llpw=-6.6194 ppl=749.49 c_v=0.5455, c_npmi=0.0592\n",
      "[HDP-3] iter= 750 llpw=-6.6211 ppl=750.78 Calculating multiple coherence metrics...\n",
      "[HDP-3] iter= 750 llpw=-6.6211 ppl=750.78 c_v=0.5385, c_npmi=0.0513\n",
      "[HDP-3] iter= 800 llpw=-6.6224 ppl=751.78 Calculating multiple coherence metrics...\n",
      "[HDP-3] iter= 800 llpw=-6.6224 ppl=751.78 c_v=0.5374, c_npmi=0.0532\n",
      "[HDP-3] iter= 850 llpw=-6.6214 ppl=751.02 Calculating multiple coherence metrics...\n",
      "[HDP-3] iter= 850 llpw=-6.6214 ppl=751.02 c_v=0.5398, c_npmi=0.0560\n",
      "[HDP-3] iter= 900 llpw=-6.6200 ppl=749.96 Calculating multiple coherence metrics...\n",
      "[HDP-3] iter= 900 llpw=-6.6200 ppl=749.96 c_v=0.5354, c_npmi=0.0547\n",
      "[HDP-3] iter= 950 llpw=-6.6197 ppl=749.73 Calculating multiple coherence metrics...\n",
      "[HDP-3] iter= 950 llpw=-6.6197 ppl=749.73 c_v=0.5422, c_npmi=0.0607\n",
      "[HDP-3] iter=1000 llpw=-6.6181 ppl=748.52 Calculating multiple coherence metrics...\n",
      "[HDP-3] iter=1000 llpw=-6.6181 ppl=748.52 c_v=0.5392, c_npmi=0.0597\n",
      "📊 Attempt 3 Results:\n",
      "   - Effective topics: 110\n",
      "   - Total topics: 110\n",
      "   - Difference from target: 10\n",
      "\n",
      "✅ Training complete! Time taken: 166.0 seconds\n",
      "\n",
      "🔍 HDP Multiple Coherence Metrics Evaluation:\n",
      "================================================================================\n",
      "📊 Coherence Metrics Results:\n",
      "   📈 C_v (Vector Space Coherence): 0.541768\n",
      "   📈 NPMI (Normalized Pointwise Mutual Information): 0.049075\n",
      "\n",
      "📊 Basic Metrics:\n",
      "   📈 Effective topics: 107\n",
      "   📈 Total topics: 107\n",
      "   📈 Topic utilization: 100.0%\n",
      "\n",
      "💡 Coherence Metric Explanations:\n",
      "   🎯 C_v: Range [0,1], higher is better, based on word vector similarity\n",
      "   🎯 NPMI: Range [-1,1], higher is better, Normalized Pointwise Mutual Information\n",
      "\n",
      "🏆 Composite Coherence Score: 0.5332 (average of normalized scores)\n",
      "\n",
      "📈 Final Evaluation Results:\n",
      "   - Training iterations: 600\n",
      "   - Log-likelihood per word: -6.615062\n",
      "   - Perplexity: 746.25\n",
      "   - Coherence (C_v): 0.5418\n",
      "   - Coherence (NPMI): 0.0491\n",
      "   - Effective topics: 107\n",
      "   - Total topics: 107\n",
      "   - Target difference: 7\n",
      "   - Gamma used: 1.000\n",
      "\n",
      "🎯 hLDA Comparison Analysis:\n",
      "   - hLDA leaf nodes: 100\n",
      "   - HDP effective topics: 107\n",
      "   - Topic count difference: 7\n",
      "   ✅ Topic count is close to hLDA (difference ≤ 50)\n",
      "\n",
      "🔍 HDP Topic Analysis (showing top 5 words):\n",
      "================================================================================\n",
      "Topic   0 (weight:0.051): damage(0.051), model(0.051), strain(0.039), rate(0.027), dislocation(0.020)\n",
      "Topic   1 (weight:0.052): damage(0.052), interphase(0.042), within(0.031), gradient(0.021), show(0.016)\n",
      "Topic   2 (weight:0.021): method(0.021), model(0.018), element(0.014), use(0.013), numerical(0.012)\n",
      "Topic   3 (weight:0.052): element(0.052), gap(0.024), interface(0.024), multiplier(0.016), propose(0.016)\n",
      "Topic   4 (weight:0.038): pressure(0.038), network(0.027), composite(0.023), concrete(0.023), diffusion(0.023)\n",
      "Topic   5 (weight:0.045): virtual(0.045), element(0.045), problem(0.029), method(0.027), formulation(0.025)\n",
      "Topic   6 (weight:0.024): method(0.024), macroscopic(0.022), et(0.019), framework(0.017), al(0.017)\n",
      "Topic   7 (weight:0.029): method(0.029), discretization(0.027), z(0.025), use(0.023), computation(0.023)\n",
      "Topic   8 (weight:0.049): stress(0.049), recover(0.031), behaviour(0.024), elastic(0.024), mode(0.018)\n",
      "Topic   9 (weight:0.039): computation(0.039), turbine(0.037), computational(0.028), framework(0.022), mdm(0.020)\n",
      "... (and 75 more active topics)\n",
      "\n",
      "📊 HDP Topic Statistics:\n",
      "   - Active topics: 85/107\n",
      "   - Topic activity rate: 79.4%\n",
      "\n",
      "🎯 HDP model evaluation complete!\n",
      "📊 Key Metrics:\n",
      "   - iterations: 600\n",
      "   - log_likelihood_per_word: -6.6151\n",
      "   - perplexity: 746.2514\n",
      "   - coherence_c_v: 0.5418\n",
      "   - effective_topics: 107\n",
      "   - total_topics: 107\n",
      "   - target_diff: 7\n",
      "   - gamma_used: 1.0000\n",
      "   - training_time_seconds: 166.0021\n",
      "   - convergence_iterations: 12\n",
      "   - coherence_c_npmi: 0.0491\n",
      "   - composite_score: 0.5332\n",
      "   - topic_utilization: 1.0000\n",
      "\n",
      "💾 HDP model is trained, features:\n",
      "   1. Non-parametric model (dynamic topic count, adjusted to be close to 252)\n",
      "   2. Hierarchical structure based on Dirichlet Process\n",
      "   3. Topic count optimized via gamma parameter\n",
      "   4. Suitable for fair comparison with hLDA\n",
      "\n",
      "📋 hLDA Comparison Preparation:\n",
      "   - hLDA leaf nodes: 252 topics\n",
      "   - HDP effective topics: 107 topics\n",
      "   - Topic count difference: 7\n",
      "   - Both models use the same dataset and preprocessing\n",
      "   - Perplexity and coherence can be directly compared\n",
      "\n",
      "📈 Do you want to view the HDP training curves?\n",
      "============================================================\n",
      "\n",
      "🔬 Renyi entropy weighted by document count (HDP): 5.3073\n",
      "🔬 Average JSD (full distribution) (HDP): 0.3187\n",
      "🔬 Unweighted Renyi entropy (HDP): 5.1937\n"
     ]
    }
   ],
   "source": [
    "# ===== HDP Model Construction and Training (Optimized for hLDA Comparison) =====\n",
    "\n",
    "print(\"\\n🎯 Starting HDP model training - Optimized for hLDA comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def build_hdp_for_hlda_comparison(docs, seed=0, max_iters=1500, target_topics=252):\n",
    "    \"\"\"\n",
    "    Build and train an HDP model - specifically optimized for comparison with hLDA\n",
    "    \n",
    "    Adjust hyperparameters to approach the target number of topics (252)\n",
    "    \"\"\"\n",
    "    print(f\"🚀 Creating HDP model (target topics≈{target_topics}, seed={seed})\")\n",
    "    \n",
    "    # Calculate parameters related to data size\n",
    "    num_docs = len(docs)\n",
    "    vocab_size = len(set(word for doc in docs for word in doc))\n",
    "    \n",
    "    # Adjust gamma parameter based on data size and target number of topics\n",
    "    # The smaller the gamma, the more topics\n",
    "    # Empirical formula: gamma ≈ target_topics / (num_docs * adjustment_factor)\n",
    "    adjustment_factor = 0.3  # Adjustment factor, can be fine-tuned\n",
    "    gamma = target_topics / (num_docs * adjustment_factor)\n",
    "    \n",
    "    # Limit the range of gamma to avoid extreme values\n",
    "    gamma = max(0.1, min(gamma, 10.0))\n",
    "    \n",
    "    print(f\"📊 Parameter Adjustment Strategy:\")\n",
    "    print(f\"   - Number of documents: {num_docs}\")\n",
    "    print(f\"   - Vocabulary size: {vocab_size}\")\n",
    "    print(f\"   - Target number of topics: {target_topics}\")\n",
    "    print(f\"   - Calculated gamma: {gamma:.3f}\")\n",
    "    \n",
    "    mdl = tp.HDPModel(\n",
    "        initial_k=target_topics,    # Set a large initial number of topics\n",
    "        alpha=ALPHA,                # Document-topic concentration parameter\n",
    "        eta=ETA,                   # Topic-word parameter\n",
    "        gamma=gamma,               # Topic count control parameter (key!)\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    # Train with multiple coherence metrics (C_v + NPMI) to calculate and print NPMI in real-time during training\n",
    "    return auto_train_with_multiple_metrics(\n",
    "        mdl, docs, seed=seed, max_iters=max_iters,\n",
    "        interval=50, coherence_measures=['c_v', 'c_npmi'], name='HDP', fast_coherence=True\n",
    "    )\n",
    "\n",
    "def iterative_hdp_tuning(docs, target_topics=252, seed=42, max_attempts=3):\n",
    "    \"\"\"\n",
    "    Iteratively adjust HDP parameters to try to approach the target number of topics\n",
    "    \"\"\"\n",
    "    print(f\"🔄 Iteratively adjusting HDP parameters, target number of topics: {target_topics}\")\n",
    "    \n",
    "    best_model = None\n",
    "    best_metrics = None\n",
    "    best_history = None\n",
    "    best_diff = float('inf')\n",
    "    \n",
    "    gamma_values = [0.5, 1.0, 2.0]  # Different gamma values to try\n",
    "    \n",
    "    for attempt, gamma in enumerate(gamma_values, 1):\n",
    "        print(f\"\\n--- Attempt {attempt}/{len(gamma_values)}: gamma={gamma} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Create model\n",
    "            mdl = tp.HDPModel(\n",
    "                initial_k=target_topics,\n",
    "                alpha=ALPHA,\n",
    "                eta=ETA,\n",
    "                gamma=gamma,\n",
    "                seed=seed\n",
    "            )\n",
    "            \n",
    "            # Train model (use multiple coherence metrics to include both C_v and NPMI in each printout)\n",
    "            model, history = auto_train_with_multiple_metrics(\n",
    "                mdl, docs, seed=seed, max_iters=1000,\n",
    "                interval=50, coherence_measures=['c_v', 'c_npmi'], name=f'HDP-{attempt}', fast_coherence=True\n",
    "            )\n",
    "            \n",
    "            # Evaluate results\n",
    "            effective_topics = effective_k(model)\n",
    "            total_topics = getattr(model, 'k', 0)\n",
    "            diff = abs(effective_topics - target_topics)\n",
    "            \n",
    "            print(f\"📊 Attempt {attempt} Results:\")\n",
    "            print(f\"   - Effective topics: {effective_topics}\")\n",
    "            print(f\"   - Total topics: {total_topics}\")\n",
    "            print(f\"   - Difference from target: {diff}\")\n",
    "            \n",
    "            # Update best result\n",
    "            if diff < best_diff:\n",
    "                best_model = model\n",
    "                best_history = history\n",
    "                best_diff = diff\n",
    "                \n",
    "                if history:\n",
    "                    final_iter, final_llpw, final_ppl, final_coh = history[-1]\n",
    "                    best_metrics = {\n",
    "                        'iterations': final_iter,\n",
    "                        'log_likelihood_per_word': final_llpw,\n",
    "                        'perplexity': final_ppl,\n",
    "                        'coherence_c_v': final_coh,\n",
    "                        'effective_topics': effective_topics,\n",
    "                        'total_topics': total_topics,\n",
    "                        'target_diff': diff,\n",
    "                        'gamma_used': gamma\n",
    "                    }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Attempt {attempt} failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return best_model, best_metrics, best_history\n",
    "\n",
    "def train_and_evaluate_hdp_for_comparison(docs, seed=42, target_topics=252, detailed_output=True):\n",
    "    \"\"\"\n",
    "    Train an HDP model and perform a full evaluation - specifically for comparison with hLDA\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🚀 Starting to train HDP model for comparison with hLDA (seed={seed})\")\n",
    "    print(f\"📊 Data Overview:\")\n",
    "    print(f\"   - Number of documents: {len(docs)}\")\n",
    "    print(f\"   - Average document length: {np.mean([len(doc) for doc in docs]):.1f} words\")\n",
    "    print(f\"   - Vocabulary size: {len(set(word for doc in docs for word in doc))}\")\n",
    "    print(f\"   - Target number of topics: {target_topics} (aligned with hLDA leaf nodes)\")\n",
    "    \n",
    "    # Train the model\n",
    "    start_time = time.time()\n",
    "    hdp_model, hdp_metrics, hdp_history = iterative_hdp_tuning(\n",
    "        docs, target_topics=target_topics, seed=seed\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n✅ Training complete! Time taken: {training_time:.1f} seconds\")\n",
    "    \n",
    "    # Update metrics\n",
    "    if hdp_metrics:\n",
    "        hdp_metrics['training_time_seconds'] = training_time\n",
    "        hdp_metrics['convergence_iterations'] = len(hdp_history) if hdp_history else 0\n",
    "\n",
    "        # New: Add NPMI evaluation\n",
    "        full_eval = evaluate_model_with_multiple_coherence(\n",
    "            hdp_model, docs, model_name=\"HDP\", coherence_metrics=['c_v', 'c_npmi'], top_words_for_coherence=5\n",
    "        )\n",
    "        hdp_metrics['coherence_c_v'] = full_eval['coherence_metrics'].get('c_v', 0)\n",
    "        hdp_metrics['coherence_c_npmi'] = full_eval['coherence_metrics'].get('c_npmi', 0)\n",
    "        hdp_metrics['composite_score'] = full_eval.get('composite_score', 0)\n",
    "        hdp_metrics['topic_utilization'] = full_eval.get('topic_utilization', 0)\n",
    "\n",
    "        if detailed_output:\n",
    "            print(f\"\\n📈 Final Evaluation Results:\")\n",
    "            print(f\"   - Training iterations: {hdp_metrics.get('iterations', 'N/A')}\")\n",
    "            print(f\"   - Log-likelihood per word: {hdp_metrics.get('log_likelihood_per_word', 0):.6f}\")\n",
    "            print(f\"   - Perplexity: {hdp_metrics.get('perplexity', 0):.2f}\")\n",
    "            print(f\"   - Coherence (C_v): {hdp_metrics.get('coherence_c_v', 0):.4f}\")\n",
    "            print(f\"   - Coherence (NPMI): {hdp_metrics.get('coherence_c_npmi', 0):.4f}\")  # New output\n",
    "            print(f\"   - Effective topics: {hdp_metrics.get('effective_topics', 0)}\")\n",
    "            print(f\"   - Total topics: {hdp_metrics.get('total_topics', 0)}\")\n",
    "            print(f\"   - Target difference: {hdp_metrics.get('target_diff', 0)}\")\n",
    "            print(f\"   - Gamma used: {hdp_metrics.get('gamma_used', 0):.3f}\")\n",
    "            \n",
    "            # Comparison with target\n",
    "            target_diff = hdp_metrics.get('target_diff', float('inf'))\n",
    "            effective_topics = hdp_metrics.get('effective_topics', 0)\n",
    "            \n",
    "            print(f\"\\n🎯 hLDA Comparison Analysis:\")\n",
    "            print(f\"   - hLDA leaf nodes: {target_topics}\")\n",
    "            print(f\"   - HDP effective topics: {effective_topics}\")\n",
    "            print(f\"   - Topic count difference: {target_diff}\")\n",
    "            \n",
    "            if target_diff <= 50:\n",
    "                print(f\"   ✅ Topic count is close to hLDA (difference ≤ 50)\")\n",
    "            elif target_diff <= 100:\n",
    "                print(f\"   ⚠️  Moderate deviation in topic count (difference ≤ 100)\")\n",
    "            else:\n",
    "                print(f\"   ❌ Large difference in topic count (difference > 100)\")\n",
    "    \n",
    "    return hdp_model, hdp_metrics, hdp_history\n",
    "\n",
    "# ===== Execute HDP Modeling and Evaluation =====\n",
    "print(\"\\n🎯 Training HDP model with the full dataset (for hLDA comparison)...\")\n",
    "\n",
    "# Use the full corpus\n",
    "full_docs = list(corpus.values())\n",
    "print(f\"📚 Using full dataset: {len(full_docs)} documents\")\n",
    "\n",
    "# Train and evaluate HDP - aiming for ~252 topics\n",
    "hdp_model, hdp_metrics, hdp_history = train_and_evaluate_hdp_for_comparison(\n",
    "    docs=full_docs,\n",
    "    seed=24,\n",
    "    target_topics=100,  # 252 topics, aligned with hLDA9\n",
    "    detailed_output=True\n",
    ")\n",
    "\n",
    "# Topic analysis\n",
    "if hdp_model:\n",
    "    hdp_topic_info = analyze_model_topics(hdp_model, model_name=\"HDP\", top_words=5)\n",
    "    \n",
    "    print(f\"\\n🎯 HDP model evaluation complete!\")\n",
    "    print(f\"📊 Key Metrics:\")\n",
    "    if hdp_metrics:\n",
    "        for key, value in hdp_metrics.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"   - {key}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"   - {key}: {value}\")\n",
    "            \n",
    "            \n",
    "    \n",
    "    print(f\"\\n💾 HDP model is trained, features:\")\n",
    "    print(f\"   1. Non-parametric model (dynamic topic count, adjusted to be close to {K_LEAF})\")\n",
    "    print(f\"   2. Hierarchical structure based on Dirichlet Process\")\n",
    "    print(f\"   3. Topic count optimized via gamma parameter\")\n",
    "    print(f\"   4. Suitable for fair comparison with hLDA\")\n",
    "    \n",
    "    # Generate comparison preparation info\n",
    "    print(f\"\\n📋 hLDA Comparison Preparation:\")\n",
    "    print(f\"   - hLDA leaf nodes: {K_LEAF} topics\")\n",
    "    print(f\"   - HDP effective topics: {hdp_metrics.get('effective_topics', 'N/A')} topics\")\n",
    "    print(f\"   - Topic count difference: {hdp_metrics.get('target_diff', 'N/A')}\")\n",
    "    print(f\"   - Both models use the same dataset and preprocessing\")\n",
    "    print(f\"   - Perplexity and coherence can be directly compared\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ HDP model training failed\")\n",
    "\n",
    "# ===== Optional: Plot Training Curves =====\n",
    "if hdp_history:\n",
    "    print(f\"\\n📈 Do you want to view the HDP training curves?\")\n",
    "    # plot_training_curves(hdp_history, model_name=\"HDP\", save_plot=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Called after HDP model training and evaluation\n",
    "if hdp_model:\n",
    "    renyi_entropy = calculate_weighted_renyi_entropy_full_weighted(hdp_model, alpha=2)\n",
    "    jsd_distance = calculate_weighted_jsd_full(hdp_model)\n",
    "    renyi_entropy_unweighted = calculate_renyi_entropy_unweighted(hdp_model, alpha=2)\n",
    "    print(f\"\\n🔬 Renyi entropy weighted by document count (HDP): {renyi_entropy:.4f}\")\n",
    "    print(f\"🔬 Average JSD (full distribution) (HDP): {jsd_distance:.4f}\")\n",
    "    print(f\"🔬 Unweighted Renyi entropy (HDP): {renyi_entropy_unweighted:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8565ef0f-5d8d-41f8-91a8-f1d81827730f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2f72119",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
