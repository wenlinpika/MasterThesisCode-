{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f4b7f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最后5轮迭代次数: [90 89 88 87 86]\n",
      "最后5轮总数据条数: 4850\n",
      "\n",
      "收敛结果:\n",
      "收敛文档数量: 589\n",
      "未收敛文档数量: 381\n",
      "总文档数量: 970\n",
      "收敛的文档ID: [969, 303, 330, 328, 327, 326, 323, 320, 319, 315, 313, 310, 308, 307, 306, 305, 333, 334, 349, 361, 360, 358, 354, 352, 351, 350, 348, 347, 346, 342, 341, 340, 339, 337, 304, 241, 301, 269, 267, 264, 262, 258, 256, 254, 252, 250, 248, 246, 245, 272, 288, 300, 295, 294, 292, 291, 290, 289, 287, 274, 286, 282, 281, 278, 276, 275, 362, 363, 364, 365, 445, 444, 443, 442, 441, 440, 439, 437, 436, 433, 432, 430, 429, 427, 453, 454, 455, 470, 479, 477, 471, 469, 456, 467, 465, 464, 463, 462, 461, 458, 457, 425, 424, 423, 390, 389, 388, 387, 385, 383, 382, 381, 380, 377, 376, 375, 374, 373, 372, 371, 370, 369, 367, 366, 394, 422, 409, 421, 420, 418, 417, 416, 414, 413, 412, 411, 395, 407, 406, 404, 403, 402, 401, 399, 397, 242, 240, 484, 87, 86, 84, 83, 82, 78, 77, 71, 70, 67, 66, 65, 64, 63, 62, 61, 88, 89, 90, 105, 116, 113, 112, 111, 107, 106, 104, 101, 98, 97, 95, 92, 60, 239, 57, 26, 24, 23, 21, 19, 15, 14, 12, 7, 6, 5, 3, 1, 0, 28, 29, 44, 56, 55, 52, 51, 50, 49, 48, 47, 46, 43, 30, 42, 41, 39, 37, 36, 34, 33, 32, 31, 120, 208, 207, 205, 203, 202, 201, 199, 197, 196, 194, 191, 190, 189, 188, 186, 184, 183, 209, 211, 226, 238, 237, 233, 229, 228, 227, 225, 224, 222, 221, 219, 218, 217, 215, 181, 180, 147, 146, 145, 144, 143, 142, 141, 139, 138, 149, 133, 132, 130, 127, 126, 124, 123, 150, 178, 165, 174, 172, 171, 170, 167, 166, 151, 161, 160, 159, 158, 157, 156, 153, 483, 243, 816, 813, 812, 809, 808, 806, 800, 797, 796, 794, 792, 791, 790, 818, 846, 845, 844, 843, 842, 840, 839, 837, 835, 833, 831, 830, 829, 828, 825, 822, 821, 848, 786, 754, 752, 751, 750, 749, 742, 741, 740, 739, 738, 735, 733, 732, 729, 757, 785, 784, 783, 782, 781, 780, 779, 778, 777, 775, 774, 772, 770, 769, 767, 766, 764, 763, 761, 760, 847, 727, 910, 937, 935, 934, 932, 931, 928, 922, 919, 918, 916, 915, 912, 940, 956, 486, 964, 963, 962, 961, 960, 959, 958, 957, 955, 954, 953, 951, 950, 949, 948, 947, 944, 943, 911, 909, 850, 908, 876, 875, 874, 873, 872, 871, 870, 868, 866, 864, 863, 861, 860, 859, 857, 856, 855, 854, 853, 852, 851, 880, 895, 907, 905, 904, 903, 902, 901, 900, 899, 897, 896, 894, 881, 892, 891, 890, 889, 888, 887, 885, 884, 883, 728, 726, 606, 573, 572, 571, 569, 567, 563, 562, 561, 560, 557, 555, 554, 552, 548, 575, 576, 592, 603, 602, 601, 598, 596, 595, 594, 593, 591, 578, 589, 588, 587, 586, 584, 579, 545, 544, 500, 511, 508, 505, 504, 503, 502, 501, 495, 494, 493, 492, 491, 490, 487, 513, 515, 543, 530, 542, 541, 535, 534, 532, 531, 527, 526, 525, 524, 523, 521, 520, 519, 605, 547, 607, 667, 694, 693, 691, 690, 689, 687, 685, 684, 683, 681, 680, 679, 678, 675, 673, 671, 670, 669, 724, 723, 722, 720, 719, 718, 699, 711, 709, 707, 701, 700, 668, 725, 666, 635, 633, 632, 631, 628, 626, 625, 624, 622, 621, 620, 619, 615, 613, 612, 611, 636, 609, 638, 652, 665, 664, 663, 662, 661, 660, 657, 656, 655, 654, 658, 651, 644, 640, 643, 639]\n",
      "未收敛的文档ID: [331, 329, 325, 324, 322, 321, 318, 317, 316, 314, 312, 311, 309, 332, 359, 357, 356, 355, 353, 335, 345, 344, 343, 338, 336, 302, 270, 268, 266, 265, 263, 261, 260, 259, 257, 255, 253, 251, 249, 247, 244, 271, 273, 299, 298, 297, 296, 293, 285, 284, 283, 280, 279, 277, 452, 451, 450, 449, 448, 447, 446, 438, 435, 434, 431, 428, 426, 482, 481, 480, 478, 476, 475, 474, 473, 472, 468, 466, 460, 459, 379, 391, 386, 384, 378, 393, 368, 392, 419, 415, 410, 408, 405, 400, 398, 396, 59, 85, 81, 80, 79, 76, 75, 74, 73, 72, 69, 68, 117, 115, 114, 110, 109, 108, 91, 103, 102, 100, 99, 96, 94, 93, 58, 25, 22, 20, 18, 17, 16, 13, 11, 10, 9, 8, 4, 2, 27, 54, 53, 45, 40, 38, 35, 118, 119, 121, 206, 204, 200, 198, 195, 193, 192, 187, 185, 182, 210, 236, 235, 234, 232, 231, 230, 212, 223, 220, 216, 214, 213, 179, 135, 140, 137, 136, 134, 131, 129, 128, 125, 122, 148, 177, 176, 175, 173, 169, 168, 164, 163, 162, 155, 154, 152, 485, 788, 815, 814, 811, 810, 807, 805, 804, 803, 802, 801, 799, 798, 795, 793, 817, 819, 834, 841, 838, 836, 820, 832, 827, 826, 824, 823, 789, 787, 755, 753, 748, 747, 746, 745, 744, 743, 737, 736, 734, 731, 730, 756, 758, 773, 776, 759, 771, 768, 765, 762, 849, 938, 936, 933, 930, 929, 927, 926, 925, 924, 923, 921, 920, 917, 914, 913, 939, 941, 967, 966, 965, 942, 952, 946, 945, 877, 869, 867, 865, 862, 858, 878, 879, 906, 898, 893, 886, 882, 968, 574, 570, 568, 566, 565, 564, 559, 558, 556, 553, 551, 550, 549, 577, 604, 600, 599, 597, 590, 585, 583, 582, 581, 580, 546, 512, 510, 509, 507, 506, 499, 514, 498, 497, 496, 489, 488, 540, 539, 538, 537, 536, 533, 529, 516, 528, 522, 518, 517, 695, 692, 688, 686, 682, 677, 676, 674, 672, 696, 697, 698, 713, 608, 721, 717, 716, 715, 714, 712, 710, 708, 706, 705, 704, 703, 702, 623, 634, 630, 629, 627, 637, 618, 617, 616, 614, 610, 659, 653, 650, 641, 642, 645, 647, 648, 649, 646]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "csv_path = Path('step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1/iteration_document_paths.csv')\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# 校验必要列\n",
    "required = {'iteration', 'leaf_node_id', 'document_id'}\n",
    "missing = required - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f'缺少必要列: {missing}')\n",
    "\n",
    "# 按 iteration 倒序取最后5轮\n",
    "df['iteration'] = pd.to_numeric(df['iteration'], errors='coerce')\n",
    "df = df.dropna(subset=['iteration']).sort_values('iteration', ascending=False)\n",
    "\n",
    "# 获取最后5轮的迭代次数\n",
    "last5_iterations = df['iteration'].unique()[:5]\n",
    "print(f\"最后5轮迭代次数: {last5_iterations}\")\n",
    "\n",
    "# 筛选最后5轮的数据\n",
    "last5_data = df[df['iteration'].isin(last5_iterations)]\n",
    "print(f\"最后5轮总数据条数: {len(last5_data)}\")\n",
    "\n",
    "# 检查每个文档在最后5轮中的路径选择\n",
    "convergent_docs = []\n",
    "non_convergent_docs = []\n",
    "\n",
    "for doc_id in last5_data['document_id'].unique():\n",
    "    doc_data = last5_data[last5_data['document_id'] == doc_id]\n",
    "    unique_paths = doc_data['leaf_node_id'].nunique()\n",
    "    \n",
    "    if unique_paths == 1:\n",
    "        convergent_docs.append(doc_id)\n",
    "        path = doc_data['leaf_node_id'].iloc[0]\n",
    "        # print(f\"文档 {doc_id}: 收敛到路径 {path}\")\n",
    "    else:\n",
    "        non_convergent_docs.append(doc_id)\n",
    "        # print(f\"文档 {doc_id}: 未收敛，选择了 {unique_paths} 个不同路径: {doc_data['leaf_node_id'].unique()}\")\n",
    "\n",
    "print(f\"\\n收敛结果:\")\n",
    "print(f\"收敛文档数量: {len(convergent_docs)}\")\n",
    "print(f\"未收敛文档数量: {len(non_convergent_docs)}\")\n",
    "print(f\"总文档数量: {len(last5_data['document_id'].unique())}\")\n",
    "\n",
    "if len(convergent_docs) > 0:\n",
    "    print(f\"收敛的文档ID: {convergent_docs}\")\n",
    "if len(non_convergent_docs) > 0:\n",
    "    print(f\"未收敛的文档ID: {non_convergent_docs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85b6ea6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最后一轮迭代: 90\n",
      "最后一轮路径结构数据量: 252\n",
      "收敛文档数量: 589\n",
      "非收敛文档数量: 381\n",
      "\n",
      "=== Layer 0 (根节点) 统计 ===\n",
      "   layer_0_node_id  stable_count  unstable_count  total_docs  stability_ratio\n",
      "0                0           589             381         970         0.607216\n",
      "\n",
      "=== Layer 1 节点统计 ===\n",
      "    layer_0_node_id  layer_1_node_id  stable_count  unstable_count  \\\n",
      "0                 0                1           231              65   \n",
      "1                 0                6             6               1   \n",
      "2                 0               36             9               3   \n",
      "3                 0               53            11               3   \n",
      "4                 0               81            20              17   \n",
      "..              ...              ...           ...             ...   \n",
      "57                0            18543             2               3   \n",
      "58                0            18586             0               5   \n",
      "59                0            18709             5               0   \n",
      "60                0            18926             0               7   \n",
      "61                0            18983             0               5   \n",
      "\n",
      "    total_docs  stability_ratio  \n",
      "0          296         0.780405  \n",
      "1            7         0.857143  \n",
      "2           12         0.750000  \n",
      "3           14         0.785714  \n",
      "4           37         0.540541  \n",
      "..         ...              ...  \n",
      "57           5         0.400000  \n",
      "58           5         0.000000  \n",
      "59           5         1.000000  \n",
      "60           7         0.000000  \n",
      "61           5         0.000000  \n",
      "\n",
      "[62 rows x 6 columns]\n",
      "\n",
      "=== 叶子节点 (Layer 2) 统计 ===\n",
      "   layer_0_node_id  layer_1_node_id  layer_2_node_id  stable_count  \\\n",
      "0                0                1                2             8   \n",
      "1                0                1               63             5   \n",
      "2                0                1               65             5   \n",
      "3                0                1              119             5   \n",
      "4                0                1              189             6   \n",
      "5                0                1              231            12   \n",
      "6                0                1              295             6   \n",
      "7                0                1              447             7   \n",
      "8                0                1              501             6   \n",
      "9                0                1             1038             5   \n",
      "\n",
      "   unstable_count  total_docs  stability_ratio  \n",
      "0               0           8         1.000000  \n",
      "1               0           5         1.000000  \n",
      "2               0           5         1.000000  \n",
      "3               4           9         0.555556  \n",
      "4               1           7         0.857143  \n",
      "5               1          13         0.923077  \n",
      "6               0           6         1.000000  \n",
      "7               0           7         1.000000  \n",
      "8               1           7         0.857143  \n",
      "9               0           5         1.000000  \n",
      "\n",
      "=== 总体统计 ===\n",
      "总稳定文档数: 589\n",
      "总不稳定文档数: 381\n",
      "总体稳定率: 60.72%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import ast\n",
    "\n",
    "# 读取路径结构数据\n",
    "structure_csv_path = Path('step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1/iteration_path_structures.csv')\n",
    "structure_df = pd.read_csv(structure_csv_path)\n",
    "\n",
    "# 取最后一轮数据\n",
    "structure_df['iteration'] = pd.to_numeric(structure_df['iteration'], errors='coerce')\n",
    "last_iteration = structure_df['iteration'].max()\n",
    "last_round_data = structure_df[structure_df['iteration'] == last_iteration].copy()\n",
    "\n",
    "print(f\"最后一轮迭代: {last_iteration}\")\n",
    "print(f\"最后一轮路径结构数据量: {len(last_round_data)}\")\n",
    "\n",
    "# 解析documents_in_path列\n",
    "def parse_document_list(doc_str):\n",
    "    try:\n",
    "        if pd.isna(doc_str):\n",
    "            return []\n",
    "        if isinstance(doc_str, str):\n",
    "            return ast.literal_eval(doc_str)\n",
    "        return doc_str\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "last_round_data['documents_in_path'] = last_round_data['documents_in_path'].apply(parse_document_list)\n",
    "\n",
    "# 转换收敛和非收敛文档ID为集合\n",
    "convergent_set = set(convergent_docs)\n",
    "non_convergent_set = set(non_convergent_docs)\n",
    "\n",
    "print(f\"收敛文档数量: {len(convergent_set)}\")\n",
    "print(f\"非收敛文档数量: {len(non_convergent_set)}\")\n",
    "\n",
    "# 分析每个节点的稳定和不稳定文档\n",
    "def analyze_node_stability(row):\n",
    "    docs_in_node = set(row['documents_in_path'])\n",
    "    stable_docs = docs_in_node.intersection(convergent_set)\n",
    "    unstable_docs = docs_in_node.intersection(non_convergent_set)\n",
    "    \n",
    "    return {\n",
    "        'stable_count': len(stable_docs),\n",
    "        'unstable_count': len(unstable_docs),\n",
    "        'total_docs': len(docs_in_node),\n",
    "        'stable_docs': list(stable_docs),\n",
    "        'unstable_docs': list(unstable_docs)\n",
    "    }\n",
    "\n",
    "# 应用分析函数\n",
    "analysis_results = last_round_data.apply(analyze_node_stability, axis=1, result_type='expand')\n",
    "result_df = pd.concat([last_round_data[['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', 'leaf_node_id']], \n",
    "                       analysis_results], axis=1)\n",
    "\n",
    "# 汇聚到父节点 - Layer 0 (根节点)\n",
    "print(\"\\n=== Layer 0 (根节点) 统计 ===\")\n",
    "layer0_stats = result_df.groupby('layer_0_node_id').agg({\n",
    "    'stable_count': 'sum',\n",
    "    'unstable_count': 'sum',\n",
    "    'total_docs': 'sum'\n",
    "}).reset_index()\n",
    "layer0_stats['stability_ratio'] = layer0_stats['stable_count'] / (layer0_stats['stable_count'] + layer0_stats['unstable_count'])\n",
    "print(layer0_stats)\n",
    "\n",
    "# 汇聚到父节点 - Layer 1\n",
    "print(\"\\n=== Layer 1 节点统计 ===\")\n",
    "layer1_stats = result_df.groupby(['layer_0_node_id', 'layer_1_node_id']).agg({\n",
    "    'stable_count': 'sum',\n",
    "    'unstable_count': 'sum',\n",
    "    'total_docs': 'sum'\n",
    "}).reset_index()\n",
    "layer1_stats['stability_ratio'] = layer1_stats['stable_count'] / (layer1_stats['stable_count'] + layer1_stats['unstable_count'])\n",
    "print(layer1_stats)\n",
    "\n",
    "# 叶子节点统计\n",
    "print(\"\\n=== 叶子节点 (Layer 2) 统计 ===\")\n",
    "leaf_stats = result_df.groupby(['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']).agg({\n",
    "    'stable_count': 'sum',\n",
    "    'unstable_count': 'sum',\n",
    "    'total_docs': 'sum'\n",
    "}).reset_index()\n",
    "leaf_stats['stability_ratio'] = leaf_stats['stable_count'] / (leaf_stats['stable_count'] + leaf_stats['unstable_count'])\n",
    "print(leaf_stats.head(10))\n",
    "\n",
    "# 总体统计\n",
    "total_stable = result_df['stable_count'].sum()\n",
    "total_unstable = result_df['unstable_count'].sum()\n",
    "print(f\"\\n=== 总体统计 ===\")\n",
    "print(f\"总稳定文档数: {total_stable}\")\n",
    "print(f\"总不稳定文档数: {total_unstable}\")\n",
    "print(f\"总体稳定率: {total_stable/(total_stable+total_unstable):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ed73e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "\n",
    "# # --- Part 1: 识别持续存在的节点 (基于树结构) ---\n",
    "\n",
    "# # 1. 使用已读取的路径结构数据 `structure_df`\n",
    "# # 筛选出最后5轮的结构数据\n",
    "# last5_structure_df = structure_df[structure_df['iteration'].isin(last5_iterations)]\n",
    "\n",
    "# # 2. 找出每一轮中存在的所有节点ID\n",
    "# node_sets_per_iteration = []\n",
    "# for it in last5_iterations:\n",
    "#     it_df = last5_structure_df[last5_structure_df['iteration'] == it]\n",
    "#     # 从所有层级收集唯一的节点ID\n",
    "#     l0_nodes = set(it_df['layer_0_node_id'].unique())\n",
    "#     l1_nodes = set(it_df['layer_1_node_id'].unique())\n",
    "#     l2_nodes = set(it_df['layer_2_node_id'].unique())\n",
    "#     all_nodes_in_iter = l0_nodes.union(l1_nodes).union(l2_nodes)\n",
    "#     node_sets_per_iteration.append(all_nodes_in_iter)\n",
    "\n",
    "# # 3. 计算在所有5轮中都存在的节点的交集\n",
    "# if node_sets_per_iteration:\n",
    "#     persistent_node_ids = set.intersection(*node_sets_per_iteration)\n",
    "# else:\n",
    "#     persistent_node_ids = set()\n",
    "\n",
    "# print(f\"在最后{len(last5_iterations)}轮的树结构中持续存在的节点数量: {len(persistent_node_ids)}\")\n",
    "\n",
    "\n",
    "# # --- Part 2: 为持续存在的节点计算词汇稳定性 ---\n",
    "\n",
    "# # 1. 读取词汇分布数据\n",
    "# word_dist_path = Path('step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1/iteration_node_word_distributions.csv')\n",
    "# word_df = pd.read_csv(word_dist_path)\n",
    "\n",
    "# # 2. 数据预处理\n",
    "# word_df['iteration'] = pd.to_numeric(word_df['iteration'], errors='coerce')\n",
    "# word_df['count'] = pd.to_numeric(word_df['count'], errors='coerce')\n",
    "# word_df.dropna(subset=['iteration', 'count', 'node_id', 'word'], inplace=True)\n",
    "# last5_words_df = word_df[word_df['iteration'].isin(last5_iterations)]\n",
    "\n",
    "# # 3. 获取Top 10词汇并重塑数据\n",
    "# top10_words_df = last5_words_df.sort_values('count', ascending=False).groupby(['iteration', 'node_id']).head(10)\n",
    "# top_words_sets = top10_words_df.groupby(['iteration', 'node_id'])['word'].apply(set).reset_index()\n",
    "# pivoted_sets = top_words_sets.pivot(index='node_id', columns='iteration', values='word')\n",
    "\n",
    "# # 4. 仅筛选出持续存在的节点进行分析\n",
    "# persistent_nodes_pivoted = pivoted_sets[pivoted_sets.index.isin(persistent_node_ids)].dropna()\n",
    "# print(f\"在持续存在的节点中，拥有完整Top-10词汇历史的节点数量: {len(persistent_nodes_pivoted)}\")\n",
    "\n",
    "# # 5. 定义函数并计算重叠率\n",
    "# def calculate_overlap_rate(row):\n",
    "#     intersection_set = set.intersection(*row)\n",
    "#     return len(intersection_set) / 10.0\n",
    "\n",
    "# node_stability = pd.DataFrame(index=persistent_nodes_pivoted.index)\n",
    "# node_stability['word_overlap_rate'] = persistent_nodes_pivoted.apply(calculate_overlap_rate, axis=1)\n",
    "# node_stability.reset_index(inplace=True)\n",
    "# print(\"持续存在节点的Top-10词汇稳定性计算完成。\")\n",
    "\n",
    "\n",
    "# # --- Part 3: 合并所有指标并保存 ---\n",
    "\n",
    "# # 1. 合并所有层级的文档统计\n",
    "# layer0_combined = layer0_stats.copy(); layer0_combined['layer'] = 'Layer_0'\n",
    "# layer1_combined = layer1_stats.copy(); layer1_combined['layer'] = 'Layer_1'\n",
    "# layer2_combined = leaf_stats.copy(); layer2_combined['layer'] = 'Layer_2'\n",
    "# all_layers_stats = pd.concat([layer0_combined, layer1_combined, layer2_combined], ignore_index=True)\n",
    "\n",
    "# # 2. 添加统一的 node_id 列\n",
    "# all_layers_stats['node_id'] = all_layers_stats['layer_2_node_id'].fillna(\n",
    "#                                  all_layers_stats['layer_1_node_id']).fillna(\n",
    "#                                  all_layers_stats['layer_0_node_id'])\n",
    "\n",
    "# # 3. 将词汇重叠率合并到主表中\n",
    "# all_layers_combined_stats = pd.merge(\n",
    "#     all_layers_stats,\n",
    "#     node_stability[['node_id', 'word_overlap_rate']],\n",
    "#     on='node_id',\n",
    "#     how='left'\n",
    "# )\n",
    "\n",
    "# # 4. *** 新增：添加 is_persistent 标志列 ***\n",
    "# all_layers_combined_stats['is_persistent'] = all_layers_combined_stats['node_id'].isin(persistent_node_ids)\n",
    "\n",
    "# # 5. 整理最终列顺序\n",
    "# final_column_order = [\n",
    "#     'layer', 'node_id', 'is_persistent', 'layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', \n",
    "#     'stable_count', 'unstable_count', 'total_docs', 'stability_ratio', 'word_overlap_rate'\n",
    "# ]\n",
    "# all_layers_combined_stats = all_layers_combined_stats[final_column_order]\n",
    "\n",
    "# # 6. 保存所有结果\n",
    "# output_dir = Path('convergence_analysis_results')\n",
    "# output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# output_file_path = output_dir / 'all_layers_stability_stats.csv'\n",
    "# all_layers_combined_stats.to_csv(output_file_path, index=False, encoding='utf-8')\n",
    "# print(f\"\\n所有层级的组合统计已保存到: {output_file_path}\")\n",
    "\n",
    "# summary_stats = pd.DataFrame({\n",
    "#     'metric': ['总稳定文档数', '总不稳定文档数', '总文档数', '总体稳定率'],\n",
    "#     'value': [total_stable, total_unstable, total_stable + total_unstable, \n",
    "#               f\"{total_stable/(total_stable+total_unstable):.2%}\"]\n",
    "# })\n",
    "# summary_stats.to_csv(output_dir / 'overall_summary.csv', index=False, encoding='utf-8')\n",
    "# print(f\"总体统计摘要已保存到: {output_dir / 'overall_summary.csv'}\")\n",
    "\n",
    "# convergence_docs_df = pd.DataFrame({\n",
    "#     'document_id': convergent_docs + non_convergent_docs,\n",
    "#     'convergence_status': ['convergent'] * len(convergent_docs) + ['non_convergent'] * len(non_convergent_docs)\n",
    "# })\n",
    "# convergence_docs_df.to_csv(output_dir / 'document_convergence_status.csv', index=False, encoding='utf-8')\n",
    "# print(f\"文档收敛状态已保存到: {output_dir / 'document_convergence_status.csv'}\")\n",
    "\n",
    "# print(f\"\\n分析完成，已生成3个核心CSV文件。\")\n",
    "# print(f\"1. {output_dir / 'all_layers_stability_stats.csv'}\")\n",
    "# print(f\"2. {output_dir / 'overall_summary.csv'}\")\n",
    "# print(f\"3. {output_dir / 'document_convergence_status.csv'}\")\n",
    "\n",
    "# # 显示合并后表格的预览\n",
    "# print(\"\\n=== 最终合并表格预览 ===\")\n",
    "# print(all_layers_combined_stats.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3abb2355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在最后5轮的树结构中持续存在的节点数量: 226\n",
      "在持续存在的节点中，拥有完整Top-10词汇历史的节点数量: 226\n",
      "持续存在节点的Top-10词汇稳定性计算完成。\n",
      "已提取最后一轮各节点的稳定与不稳定Top词汇。\n",
      "\n",
      "所有层级的组合统计已保存到: convergence_analysis_results/all_layers_stability_stats.csv\n",
      "总体统计摘要已保存到: convergence_analysis_results/overall_summary.csv\n",
      "文档收敛状态已保存到: convergence_analysis_results/document_convergence_status.csv\n",
      "\n",
      "分析完成，已生成3个核心CSV文件。\n",
      "1. convergence_analysis_results/all_layers_stability_stats.csv\n",
      "2. convergence_analysis_results/overall_summary.csv\n",
      "3. convergence_analysis_results/document_convergence_status.csv\n",
      "\n",
      "=== 最终合并表格预览 ===\n",
      "     layer  node_id  is_persistent  layer_0_node_id  layer_1_node_id  layer_2_node_id  stable_count  unstable_count  total_docs  stability_ratio  word_overlap_rate                                   stable_top_words                                 unstable_top_words\n",
      "0  Layer_0      0.0           True                0              NaN              NaN           589             381         970         0.607216                0.8  [(model, 1370), (method, 1354), (use, 883), (p...                  [(present, 541), (approach, 522)]\n",
      "1  Layer_1      1.0           True                0              1.0              NaN           231              65         296         0.780405                0.8  [(element, 321), (method, 147), (formulation, ...                 [(approximation, 79), (shell, 67)]\n",
      "2  Layer_1      6.0           True                0              6.0              NaN             6               1           7         0.857143                0.8  [(method, 15), (mesh, 13), (computation, 12), ...                           [(turbine, 6), (fsi, 6)]\n",
      "3  Layer_1     36.0           True                0             36.0              NaN             9               3          12         0.750000                0.1                                  [(particular, 3)]  [(complex, 3), (herein, 3), (evolution, 3), (s...\n",
      "4  Layer_1     53.0           True                0             53.0              NaN            11               3          14         0.785714                0.5  [(learning, 5), (task, 5), (deep, 4), (detecti...  [(design, 5), (data, 4), (process, 3), (treatm...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Part 1: 识别持续存在的节点 (基于树结构) ---\n",
    "\n",
    "# 1. 使用已读取的路径结构数据 `structure_df`\n",
    "# 筛选出最后5轮的结构数据\n",
    "last5_structure_df = structure_df[structure_df['iteration'].isin(last5_iterations)]\n",
    "\n",
    "# 2. 找出每一轮中存在的所有节点ID\n",
    "node_sets_per_iteration = []\n",
    "for it in last5_iterations:\n",
    "    it_df = last5_structure_df[last5_structure_df['iteration'] == it]\n",
    "    # 从所有层级收集唯一的节点ID\n",
    "    l0_nodes = set(it_df['layer_0_node_id'].unique())\n",
    "    l1_nodes = set(it_df['layer_1_node_id'].unique())\n",
    "    l2_nodes = set(it_df['layer_2_node_id'].unique())\n",
    "    all_nodes_in_iter = l0_nodes.union(l1_nodes).union(l2_nodes)\n",
    "    node_sets_per_iteration.append(all_nodes_in_iter)\n",
    "\n",
    "# 3. 计算在所有5轮中都存在的节点的交集\n",
    "if node_sets_per_iteration:\n",
    "    persistent_node_ids = set.intersection(*node_sets_per_iteration)\n",
    "else:\n",
    "    persistent_node_ids = set()\n",
    "\n",
    "print(f\"在最后{len(last5_iterations)}轮的树结构中持续存在的节点数量: {len(persistent_node_ids)}\")\n",
    "\n",
    "\n",
    "# --- Part 2: 为持续存在的节点计算词汇稳定性 ---\n",
    "\n",
    "# 1. 读取词汇分布数据\n",
    "word_dist_path = Path('step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1/iteration_node_word_distributions.csv')\n",
    "word_df = pd.read_csv(word_dist_path)\n",
    "\n",
    "# 2. 数据预处理\n",
    "word_df['iteration'] = pd.to_numeric(word_df['iteration'], errors='coerce')\n",
    "word_df['count'] = pd.to_numeric(word_df['count'], errors='coerce')\n",
    "word_df.dropna(subset=['iteration', 'count', 'node_id', 'word'], inplace=True)\n",
    "last5_words_df = word_df[word_df['iteration'].isin(last5_iterations)]\n",
    "\n",
    "# 3. 获取Top 10词汇并重塑数据\n",
    "top10_words_df = last5_words_df.sort_values('count', ascending=False).groupby(['iteration', 'node_id']).head(10)\n",
    "top_words_sets = top10_words_df.groupby(['iteration', 'node_id'])['word'].apply(set).reset_index()\n",
    "pivoted_sets = top_words_sets.pivot(index='node_id', columns='iteration', values='word')\n",
    "\n",
    "# 4. 仅筛选出持续存在的节点进行分析\n",
    "persistent_nodes_pivoted = pivoted_sets[pivoted_sets.index.isin(persistent_node_ids)].dropna()\n",
    "print(f\"在持续存在的节点中，拥有完整Top-10词汇历史的节点数量: {len(persistent_nodes_pivoted)}\")\n",
    "\n",
    "# 5. 定义函数并计算重叠率\n",
    "def calculate_overlap_rate(row):\n",
    "    intersection_set = set.intersection(*row)\n",
    "    return len(intersection_set) / 10.0\n",
    "\n",
    "node_stability = pd.DataFrame(index=persistent_nodes_pivoted.index)\n",
    "node_stability['word_overlap_rate'] = persistent_nodes_pivoted.apply(calculate_overlap_rate, axis=1)\n",
    "node_stability.reset_index(inplace=True)\n",
    "print(\"持续存在节点的Top-10词汇稳定性计算完成。\")\n",
    "\n",
    "\n",
    "# --- Part 3: *** 新增 *** 提取稳定与不稳定词汇列表 ---\n",
    "\n",
    "# 1. 获取最后一轮的Top-10词汇和计数值\n",
    "last_iter_top10_df = top10_words_df[top10_words_df['iteration'] == last_iteration]\n",
    "last_iter_top10_sets = last_iter_top10_df.groupby('node_id')['word'].apply(set).to_dict()\n",
    "last_iter_word_counts = last_iter_top10_df.set_index(['node_id', 'word'])['count'].to_dict()\n",
    "\n",
    "# 2. 计算每个持续存在节点的稳定词汇（交集）\n",
    "stable_words_map = {}\n",
    "for node_id, row in persistent_nodes_pivoted.iterrows():\n",
    "    stable_words_map[node_id] = set.intersection(*row)\n",
    "\n",
    "# 3. 构建包含词汇列表的DataFrame\n",
    "word_details_list = []\n",
    "all_final_nodes = set(last_iter_top10_sets.keys())\n",
    "\n",
    "for node_id in all_final_nodes:\n",
    "    final_words = last_iter_top10_sets.get(node_id, set())\n",
    "    stable_words = stable_words_map.get(node_id, set())\n",
    "    unstable_words = final_words - stable_words\n",
    "    \n",
    "    stable_list = sorted(\n",
    "        [(word, last_iter_word_counts.get((node_id, word), 0)) for word in stable_words],\n",
    "        key=lambda x: x[1], reverse=True\n",
    "    )\n",
    "    unstable_list = sorted(\n",
    "        [(word, last_iter_word_counts.get((node_id, word), 0)) for word in unstable_words],\n",
    "        key=lambda x: x[1], reverse=True\n",
    "    )\n",
    "    \n",
    "    word_details_list.append({\n",
    "        'node_id': node_id,\n",
    "        'stable_top_words': stable_list,\n",
    "        'unstable_top_words': unstable_list\n",
    "    })\n",
    "\n",
    "word_details_df = pd.DataFrame(word_details_list)\n",
    "print(\"已提取最后一轮各节点的稳定与不稳定Top词汇。\")\n",
    "\n",
    "\n",
    "# --- Part 4: 合并所有指标并保存 ---\n",
    "\n",
    "# 1. 合并所有层级的文档统计\n",
    "layer0_combined = layer0_stats.copy(); layer0_combined['layer'] = 'Layer_0'\n",
    "layer1_combined = layer1_stats.copy(); layer1_combined['layer'] = 'Layer_1'\n",
    "layer2_combined = leaf_stats.copy(); layer2_combined['layer'] = 'Layer_2'\n",
    "all_layers_stats = pd.concat([layer0_combined, layer1_combined, layer2_combined], ignore_index=True)\n",
    "\n",
    "# 2. 添加统一的 node_id 列\n",
    "all_layers_stats['node_id'] = all_layers_stats['layer_2_node_id'].fillna(\n",
    "                                 all_layers_stats['layer_1_node_id']).fillna(\n",
    "                                 all_layers_stats['layer_0_node_id'])\n",
    "\n",
    "# 3. 将词汇重叠率和词汇列表合并到主表中\n",
    "all_layers_combined_stats = pd.merge(all_layers_stats, node_stability, on='node_id', how='left')\n",
    "all_layers_combined_stats = pd.merge(all_layers_combined_stats, word_details_df, on='node_id', how='left')\n",
    "\n",
    "\n",
    "# 4. 添加 is_persistent 标志列\n",
    "all_layers_combined_stats['is_persistent'] = all_layers_combined_stats['node_id'].isin(persistent_node_ids)\n",
    "\n",
    "# 5. 整理最终列顺序\n",
    "final_column_order = [\n",
    "    'layer', 'node_id', 'is_persistent', \n",
    "    'layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id', \n",
    "    'stable_count', 'unstable_count', 'total_docs', 'stability_ratio', \n",
    "    'word_overlap_rate', 'stable_top_words', 'unstable_top_words'\n",
    "]\n",
    "# 确保所有列都存在，即使在某些情况下某些列可能为空\n",
    "for col in final_column_order:\n",
    "    if col not in all_layers_combined_stats.columns:\n",
    "        all_layers_combined_stats[col] = None\n",
    "        \n",
    "all_layers_combined_stats = all_layers_combined_stats[final_column_order]\n",
    "\n",
    "# 6. 保存所有结果\n",
    "output_dir = Path('convergence_analysis_results')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "output_file_path = output_dir / 'all_layers_stability_stats.csv'\n",
    "all_layers_combined_stats.to_csv(output_file_path, index=False, encoding='utf-8')\n",
    "print(f\"\\n所有层级的组合统计已保存到: {output_file_path}\")\n",
    "\n",
    "summary_stats = pd.DataFrame({\n",
    "    'metric': ['总稳定文档数', '总不稳定文档数', '总文档数', '总体稳定率'],\n",
    "    'value': [total_stable, total_unstable, total_stable + total_unstable, \n",
    "              f\"{total_stable/(total_stable+total_unstable):.2%}\"]\n",
    "})\n",
    "summary_stats.to_csv(output_dir / 'overall_summary.csv', index=False, encoding='utf-8')\n",
    "print(f\"总体统计摘要已保存到: {output_dir / 'overall_summary.csv'}\")\n",
    "\n",
    "convergence_docs_df = pd.DataFrame({\n",
    "    'document_id': convergent_docs + non_convergent_docs,\n",
    "    'convergence_status': ['convergent'] * len(convergent_docs) + ['non_convergent'] * len(non_convergent_docs)\n",
    "})\n",
    "convergence_docs_df.to_csv(output_dir / 'document_convergence_status.csv', index=False, encoding='utf-8')\n",
    "print(f\"文档收敛状态已保存到: {output_dir / 'document_convergence_status.csv'}\")\n",
    "\n",
    "print(f\"\\n分析完成，已生成3个核心CSV文件。\")\n",
    "print(f\"1. {output_dir / 'all_layers_stability_stats.csv'}\")\n",
    "print(f\"2. {output_dir / 'overall_summary.csv'}\")\n",
    "print(f\"3. {output_dir / 'document_convergence_status.csv'}\")\n",
    "\n",
    "# 显示合并后表格的预览\n",
    "print(\"\\n=== 最终合并表格预览 ===\")\n",
    "pd.set_option('display.max_columns', None) # 显示所有列\n",
    "pd.set_option('display.width', 1000) # 加宽显示\n",
    "print(all_layers_combined_stats.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f9d5b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先，我们研究倒数5轮都存在的文档&节点\n",
    "# 判断主题覆盖度>50%的节点，同时判断其文档的收敛性\n",
    "# 根据top词语绘制节点语义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a294b63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最后一轮共有 315 个节点。\n",
      "其中有 89 个节点不是“持续存在”的，我们将对它们进行分析。\n",
      "\n",
      "分析完成！已计算 89 个非持续存在节点的词汇连续性。\n",
      "\n",
      "=== 非持续存在节点词汇连续性分析 (示例) ===\n",
      "\n",
      "--- Node ID: 19179 ---\n",
      "词汇, 连续出现次数\n",
      "integrator, 4\n",
      "time, 4\n",
      "five, 4\n",
      "continuum, 4\n",
      "balance, 4\n",
      "law, 4\n",
      "discrete, 4\n",
      "integration, 4\n",
      "momentum, 4\n",
      "equation, 2\n",
      "dynamical, 2\n",
      "system, 2\n",
      "irregular, 1\n",
      "function, 1\n",
      "\n",
      "--- Node ID: 19185 ---\n",
      "词汇, 连续出现次数\n",
      "microstructures, 4\n",
      "initiation, 4\n",
      "contribution, 4\n",
      "interface, 4\n",
      "across, 4\n",
      "damage, 4\n",
      "influence, 4\n",
      "general, 4\n",
      "cohesive, 4\n",
      "zone, 4\n",
      "electrical, 4\n",
      "jump, 4\n",
      "effective, 4\n",
      "resistance, 4\n",
      "development, 4\n",
      "study, 4\n",
      "bulk, 4\n",
      "property, 2\n",
      "\n",
      "--- Node ID: 19186 ---\n",
      "词汇, 连续出现次数\n",
      "one, 2\n",
      "achieve, 1\n",
      "cyclic, 1\n",
      "compression, 1\n",
      "modeling, 1\n",
      "metric, 1\n",
      "find, 1\n",
      "manufacture, 1\n",
      "require, 1\n",
      "preserve, 1\n",
      "ratio, 1\n",
      "analyze, 1\n",
      "storage, 1\n",
      "development, 1\n",
      "select, 1\n",
      "additive, 1\n",
      "wavelet, 1\n",
      "result, 1\n",
      "error, 1\n",
      "address, 1\n",
      "hyperreduction, 1\n",
      "reconstruct, 1\n",
      "reconstruction, 1\n",
      "measurement, 1\n",
      "data, 1\n",
      "\n",
      "--- Node ID: 19204 ---\n",
      "词汇, 连续出现次数\n",
      "wave, 1\n",
      "st, 1\n",
      "mesh, 1\n",
      "joint, 1\n",
      "cell, 1\n",
      "representation, 1\n",
      "code, 1\n",
      "propagation, 1\n",
      "discontinuity, 1\n",
      "flow, 1\n",
      "spectral, 1\n",
      "\n",
      "--- Node ID: 19205 ---\n",
      "词汇, 连续出现次数\n",
      "correlation, 4\n",
      "component, 4\n",
      "mesoscale, 4\n",
      "microstructure, 4\n",
      "tensile, 4\n",
      "information, 4\n",
      "level, 4\n",
      "probabilistic, 4\n",
      "overall, 3\n",
      "finiteelement, 2\n",
      "composite, 2\n",
      "function, 1\n",
      "\n",
      "完整结果已保存到 non_persistent_node_word_stability.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v5/6mdkg5713kxgwg5xs24g8rvr0000gn/T/ipykernel_99531/753819475.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  last5_words_df['rank'] = last5_words_df.groupby(['iteration', 'node_id'])['count'].rank(method='min', ascending=False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Part 1: 识别目标节点 ---\n",
    "# 目标节点：在最后一轮存在，但不是在过去5轮都持续存在的节点。\n",
    "\n",
    "# 1. 获取最后一轮的所有节点ID\n",
    "last_round_structure_df = structure_df[structure_df['iteration'] == last_iteration]\n",
    "l0_final = set(last_round_structure_df['layer_0_node_id'].unique())\n",
    "l1_final = set(last_round_structure_df['layer_1_node_id'].unique())\n",
    "l2_final = set(last_round_structure_df['layer_2_node_id'].unique())\n",
    "final_round_node_ids = l0_final.union(l1_final).union(l2_final)\n",
    "\n",
    "# 2. 从最后一轮节点中，排除掉那些持续存在的节点\n",
    "# np.nan 会被识别为节点，需要排除\n",
    "final_round_node_ids.discard(np.nan)\n",
    "non_persistent_final_nodes = final_round_node_ids - persistent_node_ids\n",
    "\n",
    "print(f\"最后一轮共有 {len(final_round_node_ids)} 个节点。\")\n",
    "print(f\"其中有 {len(non_persistent_final_nodes)} 个节点不是“持续存在”的，我们将对它们进行分析。\")\n",
    "\n",
    "\n",
    "# --- Part 2: 计算非持续存在节点的词汇连续性 ---\n",
    "\n",
    "# 1. 预处理词汇数据，筛选出最后5轮\n",
    "# 我们将使用之前已经加载和筛选过的 `last5_words_df`\n",
    "\n",
    "# 2. 获取每个节点在每一轮的Top-10词汇（处理并列情况）\n",
    "# 使用rank函数，如果第10和第11个词数量相同，则都包含进来\n",
    "last5_words_df['rank'] = last5_words_df.groupby(['iteration', 'node_id'])['count'].rank(method='min', ascending=False)\n",
    "top_words_df = last5_words_df[last5_words_df['rank'] <= 10].copy()\n",
    "\n",
    "# 3. 构建一个方便查询的Top词汇字典: {(iteration, node_id): {word1, word2, ...}}\n",
    "top_words_sets_by_iter_node = top_words_df.groupby(['iteration', 'node_id'])['word'].apply(set).to_dict()\n",
    "\n",
    "# 4. 遍历每个目标节点，计算其词汇的连续出现次数\n",
    "non_persistent_stability_results = {}\n",
    "\n",
    "for node_id in non_persistent_final_nodes:\n",
    "    # a. 获取该节点在最后一轮的Top词汇\n",
    "    final_top_words = top_words_sets_by_iter_node.get((last_iteration, node_id), set())\n",
    "    \n",
    "    if not final_top_words:\n",
    "        continue # 如果节点在最后一轮没有词汇，则跳过\n",
    "\n",
    "    node_word_stability = []\n",
    "    # b. 对每一个Top词汇，向前追溯\n",
    "    for word in final_top_words:\n",
    "        consecutive_count = 0\n",
    "        # 从最后一轮开始，倒序遍历\n",
    "        for it in last5_iterations: \n",
    "            # 获取当前迭代轮次的Top词汇集合\n",
    "            current_iter_top_words = top_words_sets_by_iter_node.get((it, node_id), set())\n",
    "            \n",
    "            if word in current_iter_top_words:\n",
    "                consecutive_count += 1\n",
    "            else:\n",
    "                # 一旦中断，就停止计数\n",
    "                break\n",
    "        \n",
    "        node_word_stability.append((word, consecutive_count))\n",
    "    \n",
    "    # c. 按连续出现次数降序排列结果\n",
    "    node_word_stability.sort(key=lambda x: x[1], reverse=True)\n",
    "    non_persistent_stability_results[node_id] = node_word_stability\n",
    "\n",
    "print(f\"\\n分析完成！已计算 {len(non_persistent_stability_results)} 个非持续存在节点的词汇连续性。\")\n",
    "\n",
    "\n",
    "# --- Part 3: 展示结果 ---\n",
    "print(\"\\n=== 非持续存在节点词汇连续性分析 (示例) ===\")\n",
    "# 打印前5个节点的结果作为示例\n",
    "count = 0\n",
    "for node_id, stability_list in non_persistent_stability_results.items():\n",
    "    if count < 5:\n",
    "        print(f\"\\n--- Node ID: {node_id} ---\")\n",
    "        print(\"词汇, 连续出现次数\")\n",
    "        for word, num in stability_list:\n",
    "            print(f\"{word}, {num}\")\n",
    "        count += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 您也可以将结果保存到文件中\n",
    "non_persistent_df = pd.DataFrame.from_dict(non_persistent_stability_results, orient='index')\n",
    "non_persistent_df.to_csv(output_dir / 'non_persistent_node_word_stability.csv')\n",
    "print(\"\\n完整结果已保存到 non_persistent_node_word_stability.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dfda29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
