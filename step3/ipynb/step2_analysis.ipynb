{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18ef8259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from scipy.special import gammaln\n",
    "\n",
    "def calculate_renyi_entropy_vectorized(node_data, all_words, eta_prior=1.0, renyi_alpha=2.0):\n",
    "    \"\"\"\n",
    "    向量化版本的Renyi熵计算\n",
    "    \n",
    "    Parameters:\n",
    "    node_data: DataFrame, 包含word和count列的节点数据\n",
    "    all_words: list, 全量词汇表\n",
    "    eta_prior: float, Dirichlet先验平滑参数（从eta值获取）\n",
    "    renyi_alpha: float, Renyi熵的阶数参数\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (entropy, nonzero_word_count) Renyi熵值和非零词汇数量\n",
    "    \"\"\"\n",
    "    if len(all_words) == 0:\n",
    "        return 0.0, 0\n",
    "    \n",
    "    # 创建词汇到索引的映射\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "    \n",
    "    # 初始化计数向量\n",
    "    counts = np.zeros(len(all_words))\n",
    "    \n",
    "    # 填充实际计数\n",
    "    for _, row in node_data.iterrows():\n",
    "        word = row['word']\n",
    "        if pd.notna(word) and word in word_to_idx:\n",
    "            counts[word_to_idx[word]] = row['count']\n",
    "    \n",
    "    # 统计非零词汇数量（平滑前）\n",
    "    nonzero_word_count = np.sum(counts > 0)\n",
    "    \n",
    "    # 添加eta平滑\n",
    "    smoothed_counts = counts + eta_prior\n",
    "    \n",
    "    # 计算概率分布\n",
    "    probabilities = smoothed_counts / np.sum(smoothed_counts)\n",
    "    \n",
    "    # 计算Renyi熵（使用自然对数）\n",
    "    if renyi_alpha == 1.0:\n",
    "        # Shannon熵（由于alpha平滑，所有概率都>0，无需添加小常数）\n",
    "        entropy = -np.sum(probabilities * np.log(probabilities))\n",
    "    else:\n",
    "        # 一般Renyi熵\n",
    "        entropy = (1 / (1 - renyi_alpha)) * np.log(np.sum(probabilities ** renyi_alpha))\n",
    "    \n",
    "    return entropy, int(nonzero_word_count)\n",
    "\n",
    "def process_all_iteration_files_by_alpha(base_path=\".\", renyi_alpha=2.0):\n",
    "    \"\"\"\n",
    "    针对每个iteration_node_word_distributions.csv单独处理并保存结果\n",
    "    修正版：使用固定的eta=0.05作为Dirichlet平滑参数（适配step3）\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_node_word_distributions.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    # 去重，确保每个文件只处理一次\n",
    "    files = list(set(files))\n",
    "    files.sort()  # 排序以便有序处理\n",
    "    \n",
    "    print(f\"总共找到 {len(files)} 个文件待处理\")\n",
    "    \n",
    "    for idx, file_path in enumerate(files, 1):\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        \n",
    "        # step3中eta值固定为0.05（用于Dirichlet平滑）\n",
    "        eta_prior = 0.05  # 修正：固定使用0.05作为平滑参数\n",
    "        \n",
    "        # 从文件夹名称提取alpha值（仅用于记录文件夹信息）\n",
    "        alpha = 0.1  # 默认值\n",
    "        if 'alpha_' in folder_name:\n",
    "            try:\n",
    "                alpha_part = folder_name.split('alpha_')[1].split('_')[0]\n",
    "                alpha = float(alpha_part)\n",
    "            except (IndexError, ValueError) as e:\n",
    "                # 通过文件夹名称模式匹配\n",
    "                if 'a001' in folder_name:\n",
    "                    alpha = 0.01\n",
    "                elif 'a005' in folder_name:\n",
    "                    alpha = 0.05\n",
    "                elif 'a02' in folder_name:\n",
    "                    alpha = 0.2\n",
    "                elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                    alpha = 0.5\n",
    "                elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                    alpha = 1.0\n",
    "                elif 'a01' in folder_name:\n",
    "                    alpha = 0.1\n",
    "        else:\n",
    "            # 通过文件夹名称模式匹配\n",
    "            if 'a001' in folder_name:\n",
    "                alpha = 0.01\n",
    "            elif 'a005' in folder_name:\n",
    "                alpha = 0.05\n",
    "            elif 'a02' in folder_name:\n",
    "                alpha = 0.2\n",
    "            elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                alpha = 0.5\n",
    "            elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                alpha = 1.0\n",
    "            elif 'a01' in folder_name:\n",
    "                alpha = 0.1\n",
    "        \n",
    "        print(f\"\\n[{idx}/{len(files)}] 处理文件: {file_path}\")\n",
    "        print(f\"文件夹: {folder_name}\")\n",
    "        print(f\"提取的alpha值: {alpha} (仅用于记录)\")\n",
    "        print(f\"使用的eta平滑值: {eta_prior}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # 清理列名，去除单引号、双引号和空格\n",
    "            df.columns = [col.strip(\"'\\\" \") for col in df.columns]\n",
    "            \n",
    "            if 'node_id' not in df.columns:\n",
    "                print(f\"警告：{file_path} 缺少 node_id 列，跳过该文件\")\n",
    "                continue\n",
    "                \n",
    "            max_iteration = df['iteration'].max()\n",
    "            last_iteration_data = df[df['iteration'] == max_iteration]\n",
    "            all_words = list(last_iteration_data['word'].dropna().unique())\n",
    "            \n",
    "            print(f\"最后一轮iteration: {max_iteration}, 词汇表大小: {len(all_words)}, 节点数: {last_iteration_data['node_id'].nunique()}\")\n",
    "            \n",
    "            results = []\n",
    "            for node_id in last_iteration_data['node_id'].unique():\n",
    "                node_data = last_iteration_data[last_iteration_data['node_id'] == node_id]\n",
    "                \n",
    "                # 使用固定的eta_prior=0.05进行Dirichlet平滑\n",
    "                entropy, nonzero_words = calculate_renyi_entropy_vectorized(\n",
    "                    node_data, all_words, eta_prior, renyi_alpha  # 使用eta_prior\n",
    "                )\n",
    "                \n",
    "                # 计算稀疏度（非零词汇占比）\n",
    "                sparsity_ratio = nonzero_words / len(all_words) if len(all_words) > 0 else 0\n",
    "                \n",
    "                results.append({\n",
    "                    'node_id': node_id,\n",
    "                    'renyi_entropy_corrected': entropy,\n",
    "                    'nonzero_word_count': nonzero_words,\n",
    "                    'total_vocabulary_size': len(all_words),\n",
    "                    'sparsity_ratio': sparsity_ratio,\n",
    "                    'eta_used': eta_prior,  # 修正：记录实际使用的eta值\n",
    "                    'alpha_folder': alpha,  # 修正：记录文件夹的alpha值\n",
    "                    'renyi_alpha': renyi_alpha,\n",
    "                    'iteration': max_iteration\n",
    "                })\n",
    "            \n",
    "            # 保存新的corrected_renyi_entropy.csv文件\n",
    "            results_df = pd.DataFrame(results)\n",
    "            output_path = os.path.join(folder_path, 'corrected_renyi_entropy.csv')\n",
    "            results_df.to_csv(output_path, index=False)\n",
    "            print(f\"✓ 保存修正的Renyi熵结果到: {output_path}\")\n",
    "            \n",
    "            # 输出一些统计信息\n",
    "            print(f\"节点词汇稀疏性统计:\")\n",
    "            print(f\"  - 平均非零词汇数: {results_df['nonzero_word_count'].mean():.1f}\")\n",
    "            print(f\"  - 非零词汇数范围: {results_df['nonzero_word_count'].min()}-{results_df['nonzero_word_count'].max()}\")\n",
    "            print(f\"  - 平均稀疏度: {results_df['sparsity_ratio'].mean():.3f}\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"❌ 处理文件 {file_path} 时出错: {str(e)}\")\n",
    "            print(\"详细错误信息:\")\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b21f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Step3: 开始分析Alpha参数对模型的影响\n",
      "================================================================================\n",
      "开始计算修正的Renyi熵...\n",
      "总共找到 18 个文件待处理\n",
      "\n",
      "[1/18] 处理文件: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_1/iteration_node_word_distributions.csv\n",
      "文件夹: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_1\n",
      "提取的alpha值: 0.01 (仅用于记录)\n",
      "使用的eta平滑值: 0.05\n",
      "最后一轮iteration: 285, 词汇表大小: 1490, 节点数: 312\n",
      "✓ 保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_1/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 60.7\n",
      "  - 非零词汇数范围: 0-829\n",
      "  - 平均稀疏度: 0.041\n",
      "==================================================\n",
      "\n",
      "[2/18] 处理文件: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_2/iteration_node_word_distributions.csv\n",
      "文件夹: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_2\n",
      "提取的alpha值: 0.01 (仅用于记录)\n",
      "使用的eta平滑值: 0.05\n",
      "最后一轮iteration: 285, 词汇表大小: 1490, 节点数: 313\n",
      "✓ 保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_2/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 63.2\n",
      "  - 非零词汇数范围: 0-764\n",
      "  - 平均稀疏度: 0.042\n",
      "==================================================\n",
      "\n",
      "[3/18] 处理文件: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_3/iteration_node_word_distributions.csv\n",
      "文件夹: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_3\n",
      "提取的alpha值: 0.01 (仅用于记录)\n",
      "使用的eta平滑值: 0.05\n",
      "最后一轮iteration: 285, 词汇表大小: 1490, 节点数: 296\n",
      "✓ 保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_3/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 63.9\n",
      "  - 非零词汇数范围: 0-796\n",
      "  - 平均稀疏度: 0.043\n",
      "==================================================\n",
      "\n",
      "[4/18] 处理文件: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_1/iteration_node_word_distributions.csv\n",
      "文件夹: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_1\n",
      "提取的alpha值: 0.05 (仅用于记录)\n",
      "使用的eta平滑值: 0.05\n",
      "最后一轮iteration: 175, 词汇表大小: 1490, 节点数: 316\n",
      "✓ 保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_1/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 60.8\n",
      "  - 非零词汇数范围: 0-787\n",
      "  - 平均稀疏度: 0.041\n",
      "==================================================\n",
      "\n",
      "[5/18] 处理文件: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_2/iteration_node_word_distributions.csv\n",
      "文件夹: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_2\n",
      "提取的alpha值: 0.05 (仅用于记录)\n",
      "使用的eta平滑值: 0.05\n",
      "最后一轮iteration: 175, 词汇表大小: 1490, 节点数: 305\n",
      "✓ 保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_2/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 61.3\n",
      "  - 非零词汇数范围: 0-789\n",
      "  - 平均稀疏度: 0.041\n",
      "==================================================\n",
      "\n",
      "[6/18] 处理文件: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_3/iteration_node_word_distributions.csv\n",
      "文件夹: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_3\n",
      "提取的alpha值: 0.05 (仅用于记录)\n",
      "使用的eta平滑值: 0.05\n",
      "最后一轮iteration: 175, 词汇表大小: 1490, 节点数: 275\n",
      "✓ 保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_3/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 59.9\n",
      "  - 非零词汇数范围: 0-941\n",
      "  - 平均稀疏度: 0.040\n",
      "==================================================\n",
      "\n",
      "[7/18] 处理文件: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1/iteration_node_word_distributions.csv\n",
      "文件夹: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1\n",
      "提取的alpha值: 0.1 (仅用于记录)\n",
      "使用的eta平滑值: 0.05\n",
      "最后一轮iteration: 90, 词汇表大小: 1490, 节点数: 315\n",
      "✓ 保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 63.0\n",
      "  - 非零词汇数范围: 0-747\n",
      "  - 平均稀疏度: 0.042\n",
      "==================================================\n",
      "\n",
      "[8/18] 处理文件: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_2/iteration_node_word_distributions.csv\n",
      "文件夹: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_2\n",
      "提取的alpha值: 0.1 (仅用于记录)\n",
      "使用的eta平滑值: 0.05\n",
      "最后一轮iteration: 90, 词汇表大小: 1490, 节点数: 299\n",
      "✓ 保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_2/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 64.3\n",
      "  - 非零词汇数范围: 0-792\n",
      "  - 平均稀疏度: 0.043\n",
      "==================================================\n",
      "\n",
      "[9/18] 处理文件: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_3/iteration_node_word_distributions.csv\n",
      "文件夹: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_3\n",
      "提取的alpha值: 0.1 (仅用于记录)\n",
      "使用的eta平滑值: 0.05\n",
      "最后一轮iteration: 90, 词汇表大小: 1490, 节点数: 322\n",
      "✓ 保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_3/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 61.6\n",
      "  - 非零词汇数范围: 0-737\n",
      "  - 平均稀疏度: 0.041\n",
      "==================================================\n",
      "\n",
      "[10/18] 处理文件: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_1/iteration_node_word_distributions.csv\n",
      "文件夹: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_1\n",
      "提取的alpha值: 0.2 (仅用于记录)\n",
      "使用的eta平滑值: 0.05\n",
      "最后一轮iteration: 170, 词汇表大小: 1490, 节点数: 325\n",
      "✓ 保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_1/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 61.0\n",
      "  - 非零词汇数范围: 0-753\n",
      "  - 平均稀疏度: 0.041\n",
      "==================================================\n",
      "\n",
      "[11/18] 处理文件: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_2/iteration_node_word_distributions.csv\n",
      "文件夹: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_2\n",
      "提取的alpha值: 0.2 (仅用于记录)\n",
      "使用的eta平滑值: 0.05\n",
      "最后一轮iteration: 170, 词汇表大小: 1490, 节点数: 333\n",
      "✓ 保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_2/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 60.7\n",
      "  - 非零词汇数范围: 0-727\n",
      "  - 平均稀疏度: 0.041\n",
      "==================================================\n",
      "\n",
      "[12/18] 处理文件: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_3/iteration_node_word_distributions.csv\n",
      "文件夹: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_3\n",
      "提取的alpha值: 0.2 (仅用于记录)\n",
      "使用的eta平滑值: 0.05\n",
      "最后一轮iteration: 170, 词汇表大小: 1490, 节点数: 325\n",
      "✓ 保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_3/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 62.7\n",
      "  - 非零词汇数范围: 3-735\n",
      "  - 平均稀疏度: 0.042\n",
      "==================================================\n",
      "\n",
      "[13/18] 处理文件: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_1/iteration_node_word_distributions.csv\n",
      "文件夹: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_1\n",
      "提取的alpha值: 0.5 (仅用于记录)\n",
      "使用的eta平滑值: 0.05\n",
      "最后一轮iteration: 275, 词汇表大小: 1490, 节点数: 282\n",
      "✓ 保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_1/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 65.7\n",
      "  - 非零词汇数范围: 0-731\n",
      "  - 平均稀疏度: 0.044\n",
      "==================================================\n",
      "\n",
      "[14/18] 处理文件: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_2/iteration_node_word_distributions.csv\n",
      "文件夹: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_2\n",
      "提取的alpha值: 0.5 (仅用于记录)\n",
      "使用的eta平滑值: 0.05\n",
      "最后一轮iteration: 275, 词汇表大小: 1490, 节点数: 292\n",
      "✓ 保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_2/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 65.1\n",
      "  - 非零词汇数范围: 0-706\n",
      "  - 平均稀疏度: 0.044\n",
      "==================================================\n",
      "\n",
      "[15/18] 处理文件: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_3/iteration_node_word_distributions.csv\n",
      "文件夹: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_3\n",
      "提取的alpha值: 0.5 (仅用于记录)\n",
      "使用的eta平滑值: 0.05\n",
      "最后一轮iteration: 275, 词汇表大小: 1490, 节点数: 292\n",
      "✓ 保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_3/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 61.7\n",
      "  - 非零词汇数范围: 0-767\n",
      "  - 平均稀疏度: 0.041\n",
      "==================================================\n",
      "\n",
      "[16/18] 处理文件: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_1/iteration_node_word_distributions.csv\n",
      "文件夹: depth_3_gamma_0.05_eta_0.05_alpha_1_run_1\n",
      "提取的alpha值: 1.0 (仅用于记录)\n",
      "使用的eta平滑值: 0.05\n",
      "最后一轮iteration: 245, 词汇表大小: 1490, 节点数: 329\n",
      "✓ 保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_1/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 60.7\n",
      "  - 非零词汇数范围: 0-695\n",
      "  - 平均稀疏度: 0.041\n",
      "==================================================\n",
      "\n",
      "[17/18] 处理文件: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_2/iteration_node_word_distributions.csv\n",
      "文件夹: depth_3_gamma_0.05_eta_0.05_alpha_1_run_2\n",
      "提取的alpha值: 1.0 (仅用于记录)\n",
      "使用的eta平滑值: 0.05\n",
      "最后一轮iteration: 245, 词汇表大小: 1490, 节点数: 321\n",
      "✓ 保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_2/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 62.2\n",
      "  - 非零词汇数范围: 1-683\n",
      "  - 平均稀疏度: 0.042\n",
      "==================================================\n",
      "\n",
      "[18/18] 处理文件: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_3/iteration_node_word_distributions.csv\n",
      "文件夹: depth_3_gamma_0.05_eta_0.05_alpha_1_run_3\n",
      "提取的alpha值: 1.0 (仅用于记录)\n",
      "使用的eta平滑值: 0.05\n",
      "最后一轮iteration: 265, 词汇表大小: 1490, 节点数: 307\n",
      "✓ 保存修正的Renyi熵结果到: /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_3/corrected_renyi_entropy.csv\n",
      "节点词汇稀疏性统计:\n",
      "  - 平均非零词汇数: 61.5\n",
      "  - 非零词汇数范围: 0-728\n",
      "  - 平均稀疏度: 0.041\n",
      "==================================================\n",
      "==================================================\n",
      "✅ Step3 Renyi熵计算完成！\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 设置参数 - 适配step3\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step3\"  # step3路径\n",
    "renyi_alpha = 2.0  # Renyi熵阶数参数\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Step3: 开始分析Alpha参数对模型的影响\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. 计算修正的Renyi熵（按alpha值自动调整先验）\n",
    "print(\"开始计算修正的Renyi熵...\")\n",
    "process_all_iteration_files_by_alpha(base_path, renyi_alpha)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"✅ Step3 Renyi熵计算完成！\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83fa2dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_node_document_counts(path_structures_df):\n",
    "    \"\"\"\n",
    "    从叶子节点向上聚合，计算每个节点的文档数和层级关系\n",
    "    \n",
    "    Parameters:\n",
    "    path_structures_df: DataFrame, iteration_path_structures.csv的数据（已经过滤为最后一轮）\n",
    "    \n",
    "    Returns:\n",
    "    dict: {node_id: {'document_count': int, 'layer': int, 'parent_id': int, 'child_ids': list}} 映射\n",
    "    \"\"\"\n",
    "    # 获取所有layer列 - 修正正则表达式\n",
    "    layer_columns = [col for col in path_structures_df.columns if col.startswith('layer_') and col.endswith('_node_id')]\n",
    "    layer_columns.sort()  # 确保按顺序排列\n",
    "    max_layer_idx = len(layer_columns) - 1\n",
    "    \n",
    "    print(f\"[DEBUG] 发现层级列: {layer_columns}\")\n",
    "    print(f\"[DEBUG] 最大层级索引: {max_layer_idx}\")\n",
    "    \n",
    "    # 初始化节点信息字典\n",
    "    node_info = {}\n",
    "    \n",
    "    # 首先建立所有节点的层级和父子关系\n",
    "    for _, row in path_structures_df.iterrows():\n",
    "        path_nodes = []\n",
    "        for layer_idx in range(max_layer_idx + 1):\n",
    "            layer_col = f'layer_{layer_idx}_node_id'\n",
    "            if layer_col in path_structures_df.columns and pd.notna(row[layer_col]):\n",
    "                path_nodes.append(row[layer_col])\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # 为路径中的每个节点建立层级和父子关系\n",
    "        for i, node in enumerate(path_nodes):\n",
    "            if node not in node_info:\n",
    "                node_info[node] = {\n",
    "                    'document_count': 0,\n",
    "                    'layer': i,\n",
    "                    'parent_id': None,\n",
    "                    'child_ids': [],\n",
    "                    'child_count': 0\n",
    "                }\n",
    "            else:\n",
    "                # 更新层级信息（确保一致性）\n",
    "                node_info[node]['layer'] = i\n",
    "            \n",
    "            # 设置父节点关系\n",
    "            if i > 0:  # 不是根节点\n",
    "                parent_node = path_nodes[i-1]\n",
    "                node_info[node]['parent_id'] = parent_node\n",
    "                \n",
    "                # 在父节点的子节点列表中添加当前节点\n",
    "                if parent_node not in node_info:\n",
    "                    node_info[parent_node] = {\n",
    "                        'document_count': 0,\n",
    "                        'layer': i-1,\n",
    "                        'parent_id': None,\n",
    "                        'child_ids': [],\n",
    "                        'child_count': 0\n",
    "                    }\n",
    "                \n",
    "                if node not in node_info[parent_node]['child_ids']:\n",
    "                    node_info[parent_node]['child_ids'].append(node)\n",
    "    \n",
    "    # 然后处理叶子节点的文档数 - 在层级关系建立后进行\n",
    "    for _, row in path_structures_df.iterrows():\n",
    "        leaf_node = row['leaf_node_id']\n",
    "        if pd.notna(leaf_node) and leaf_node in node_info:\n",
    "            node_info[leaf_node]['document_count'] += row['document_count']\n",
    "    \n",
    "    # 从倒数第二层开始向上聚合文档数\n",
    "    for layer_idx in range(max_layer_idx - 1, -1, -1):  # 从倒数第二层到第0层\n",
    "        layer_col = f'layer_{layer_idx}_node_id'\n",
    "        \n",
    "        if layer_col not in path_structures_df.columns:\n",
    "            continue\n",
    "            \n",
    "        # 获取这一层的所有唯一节点\n",
    "        layer_nodes = path_structures_df[layer_col].dropna().unique()\n",
    "        \n",
    "        for node in layer_nodes:\n",
    "            if node in node_info and node_info[node]['document_count'] == 0:\n",
    "                # 计算文档数：汇总所有子节点的文档数\n",
    "                child_doc_count = 0\n",
    "                for child_id in node_info[node]['child_ids']:\n",
    "                    if child_id in node_info:\n",
    "                        child_doc_count += node_info[child_id]['document_count']\n",
    "                \n",
    "                # 如果没有子节点文档数，则直接从路径结构中计算\n",
    "                if child_doc_count == 0:\n",
    "                    total_docs = path_structures_df[path_structures_df[layer_col] == node]['document_count'].sum()\n",
    "                    node_info[node]['document_count'] = total_docs\n",
    "                else:\n",
    "                    node_info[node]['document_count'] = child_doc_count\n",
    "\n",
    "    # 计算每个节点的子节点数量\n",
    "    for node_id, info in node_info.items():\n",
    "        info['child_count'] = len(info['child_ids'])\n",
    "    \n",
    "    return node_info\n",
    "\n",
    "def add_document_counts_to_entropy_files(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    将文档数和层级信息添加到corrected_renyi_entropy.csv文件中（适配step3）\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_path_structures.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        \n",
    "        print(f\"\\n处理路径结构文件: {folder_name}\")\n",
    "        \n",
    "        try:\n",
    "            # 读取path_structures文件\n",
    "            df = pd.read_csv(file_path)\n",
    "            df.columns = [col.strip(\"'\\\" \") for col in df.columns]\n",
    "            \n",
    "            # 获取最后一轮数据\n",
    "            max_iteration = df['iteration'].max()\n",
    "            last_iteration_data = df[df['iteration'] == max_iteration]\n",
    "            \n",
    "            print(f\"最后一轮iteration: {max_iteration}, 路径数: {len(last_iteration_data)}\")\n",
    "            \n",
    "            # 计算每个节点的文档数和层级关系\n",
    "            node_info = calculate_node_document_counts(last_iteration_data)\n",
    "            \n",
    "            print(f\"计算得到 {len(node_info)} 个节点的信息\")\n",
    "            \n",
    "            # 读取对应的corrected_renyi_entropy.csv\n",
    "            entropy_file = os.path.join(folder_path, 'corrected_renyi_entropy.csv')\n",
    "            if os.path.exists(entropy_file):\n",
    "                entropy_df = pd.read_csv(entropy_file)\n",
    "                \n",
    "                # 添加新列 - 修正child_ids格式和child_count计算\n",
    "                entropy_df['document_count'] = entropy_df['node_id'].map(lambda x: node_info.get(x, {}).get('document_count', 0))\n",
    "                entropy_df['layer'] = entropy_df['node_id'].map(lambda x: node_info.get(x, {}).get('layer', -1))\n",
    "                entropy_df['parent_id'] = entropy_df['node_id'].map(lambda x: node_info.get(x, {}).get('parent_id', None))\n",
    "                \n",
    "                # 修正child_ids格式：使用方括号而不是逗号\n",
    "                entropy_df['child_ids'] = entropy_df['node_id'].map(\n",
    "                    lambda x: '[' + ','.join(map(str, node_info.get(x, {}).get('child_ids', []))) + ']' \n",
    "                    if node_info.get(x, {}).get('child_ids') else ''\n",
    "                )\n",
    "                \n",
    "                # 修正child_count：直接使用列表长度\n",
    "                entropy_df['child_count'] = entropy_df['node_id'].map(lambda x: len(node_info.get(x, {}).get('child_ids', [])))\n",
    "\n",
    "                # 保存更新后的文件\n",
    "                entropy_df.to_csv(entropy_file, index=False)\n",
    "                print(f\"已更新 {entropy_file}，添加了document_count, layer, parent_id, child_ids, child_count列\")\n",
    "                \n",
    "                # 显示一些统计信息\n",
    "                print(f\"节点层级统计:\")\n",
    "                print(f\"  - 层级分布: {entropy_df['layer'].value_counts().sort_index().to_dict()}\")\n",
    "                print(f\"  - 文档数范围: {entropy_df['document_count'].min()}-{entropy_df['document_count'].max()}\")\n",
    "                print(f\"  - 根节点数: {entropy_df[entropy_df['parent_id'].isna()].shape[0]}\")\n",
    "                print(f\"  - 叶子节点数: {entropy_df[entropy_df['child_ids'] == ''].shape[0]}\")\n",
    "                print(f\"  - 子节点数分布: {entropy_df['child_count'].value_counts().sort_index().to_dict()}\")\n",
    "            else:\n",
    "                print(f\"警告：未找到对应的entropy文件 {entropy_file}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"处理文件 {file_path} 时出错: {str(e)}\")\n",
    "            print(\"详细错误信息:\")\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4039076d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Step3: 开始添加文档数和层级信息到entropy文件...\n",
      "==================================================\n",
      "\n",
      "处理路径结构文件: depth_3_gamma_0.05_eta_0.05_alpha_1_run_3\n",
      "最后一轮iteration: 265, 路径数: 242\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] 最大层级索引: 2\n",
      "计算得到 307 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_3/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 64, 2: 242}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 242\n",
      "  - 子节点数分布: {0: 242, 1: 14, 2: 15, 3: 11, 4: 7, 5: 7, 6: 5, 7: 3, 9: 1, 42: 1, 64: 1}\n",
      "\n",
      "处理路径结构文件: depth_3_gamma_0.05_eta_0.05_alpha_1_run_1\n",
      "最后一轮iteration: 245, 路径数: 267\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] 最大层级索引: 2\n",
      "计算得到 329 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_1/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 61, 2: 267}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 267\n",
      "  - 子节点数分布: {0: 267, 1: 10, 2: 18, 3: 8, 4: 13, 5: 4, 6: 3, 7: 3, 9: 1, 61: 1, 77: 1}\n",
      "\n",
      "处理路径结构文件: depth_3_gamma_0.05_eta_0.05_alpha_1_run_2\n",
      "最后一轮iteration: 245, 路径数: 263\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] 最大层级索引: 2\n",
      "计算得到 321 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a1/depth_3_gamma_0.05_eta_0.05_alpha_1_run_2/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 57, 2: 263}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 263\n",
      "  - 子节点数分布: {0: 263, 1: 5, 2: 24, 3: 10, 4: 7, 5: 5, 6: 1, 7: 3, 11: 1, 57: 1, 89: 1}\n",
      "\n",
      "处理路径结构文件: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_3\n",
      "最后一轮iteration: 170, 路径数: 246\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] 最大层级索引: 2\n",
      "计算得到 325 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_3/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 78, 2: 246}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 246\n",
      "  - 子节点数分布: {0: 246, 1: 21, 2: 21, 3: 17, 4: 9, 5: 4, 6: 2, 7: 1, 8: 1, 10: 1, 39: 1, 78: 1}\n",
      "\n",
      "处理路径结构文件: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_1\n",
      "最后一轮iteration: 170, 路径数: 257\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] 最大层级索引: 2\n",
      "计算得到 325 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_1/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 67, 2: 257}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 257\n",
      "  - 子节点数分布: {0: 257, 1: 14, 2: 15, 3: 19, 4: 5, 5: 8, 6: 1, 7: 2, 8: 1, 9: 1, 59: 1, 67: 1}\n",
      "\n",
      "处理路径结构文件: depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_2\n",
      "最后一轮iteration: 170, 路径数: 261\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] 最大层级索引: 2\n",
      "计算得到 333 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a02/depth_3_gamma_0.05_eta_0.05_alpha_0.2_run_2/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 71, 2: 261}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 261\n",
      "  - 子节点数分布: {0: 261, 1: 12, 2: 23, 3: 14, 4: 14, 5: 4, 6: 3, 67: 1, 71: 1}\n",
      "\n",
      "处理路径结构文件: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_3\n",
      "最后一轮iteration: 275, 路径数: 236\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] 最大层级索引: 2\n",
      "计算得到 292 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_3/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 55, 2: 236}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 236\n",
      "  - 子节点数分布: {0: 236, 1: 14, 2: 13, 3: 12, 4: 8, 5: 5, 6: 1, 11: 1, 55: 1, 86: 1}\n",
      "\n",
      "处理路径结构文件: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_1\n",
      "最后一轮iteration: 275, 路径数: 236\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] 最大层级索引: 2\n",
      "计算得到 282 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_1/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 45, 2: 236}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 236\n",
      "  - 子节点数分布: {0: 236, 1: 13, 2: 12, 3: 4, 4: 7, 5: 2, 6: 2, 7: 2, 8: 1, 9: 1, 45: 1, 106: 1}\n",
      "\n",
      "处理路径结构文件: depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_2\n",
      "最后一轮iteration: 275, 路径数: 255\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] 最大层级索引: 2\n",
      "计算得到 292 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a05/depth_3_gamma_0.05_eta_0.05_alpha_0.5_run_2/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 36, 2: 255}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 255\n",
      "  - 子节点数分布: {0: 255, 1: 4, 2: 6, 3: 9, 4: 7, 5: 1, 6: 3, 7: 2, 9: 2, 10: 1, 36: 1, 119: 1}\n",
      "\n",
      "处理路径结构文件: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_3\n",
      "最后一轮iteration: 175, 路径数: 212\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] 最大层级索引: 2\n",
      "计算得到 275 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_3/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 62, 2: 212}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 212\n",
      "  - 子节点数分布: {0: 212, 1: 8, 2: 14, 3: 22, 4: 10, 5: 4, 6: 2, 7: 1, 31: 1, 62: 1}\n",
      "\n",
      "处理路径结构文件: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_1\n",
      "最后一轮iteration: 175, 路径数: 255\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] 最大层级索引: 2\n",
      "计算得到 316 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_1/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 60, 2: 255}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 255\n",
      "  - 子节点数分布: {0: 255, 1: 9, 2: 11, 3: 15, 4: 12, 5: 6, 6: 4, 8: 2, 60: 1, 61: 1}\n",
      "\n",
      "处理路径结构文件: depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_2\n",
      "最后一轮iteration: 175, 路径数: 244\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] 最大层级索引: 2\n",
      "计算得到 305 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a005/depth_3_gamma_0.05_eta_0.05_alpha_0.05_run_2/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 60, 2: 244}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 244\n",
      "  - 子节点数分布: {0: 244, 1: 8, 2: 18, 3: 15, 4: 6, 5: 7, 6: 5, 60: 1, 66: 1}\n",
      "\n",
      "处理路径结构文件: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_2\n",
      "最后一轮iteration: 90, 路径数: 243\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] 最大层级索引: 2\n",
      "计算得到 299 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_2/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 55, 2: 243}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 243\n",
      "  - 子节点数分布: {0: 243, 1: 5, 2: 16, 3: 11, 4: 7, 5: 6, 6: 5, 7: 2, 8: 1, 10: 1, 53: 1, 55: 1}\n",
      "\n",
      "处理路径结构文件: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_3\n",
      "最后一轮iteration: 90, 路径数: 262\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] 最大层级索引: 2\n",
      "计算得到 322 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_3/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 59, 2: 262}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 262\n",
      "  - 子节点数分布: {0: 262, 1: 7, 2: 15, 3: 12, 4: 11, 5: 5, 6: 3, 7: 4, 8: 1, 59: 1, 66: 1}\n",
      "\n",
      "处理路径结构文件: depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1\n",
      "最后一轮iteration: 90, 路径数: 252\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] 最大层级索引: 2\n",
      "计算得到 315 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a01/depth_3_gamma_0.05_eta_0.05_alpha_0.1_run_1/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 62, 2: 252}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 252\n",
      "  - 子节点数分布: {0: 252, 1: 6, 2: 21, 3: 14, 4: 7, 5: 6, 6: 3, 7: 1, 8: 2, 9: 1, 54: 1, 62: 1}\n",
      "\n",
      "处理路径结构文件: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_3\n",
      "最后一轮iteration: 285, 路径数: 229\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] 最大层级索引: 2\n",
      "计算得到 296 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_3/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 66, 2: 229}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 229\n",
      "  - 子节点数分布: {0: 229, 1: 11, 2: 22, 3: 19, 4: 7, 5: 5, 6: 1, 58: 1, 66: 1}\n",
      "\n",
      "处理路径结构文件: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_2\n",
      "最后一轮iteration: 285, 路径数: 253\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] 最大层级索引: 2\n",
      "计算得到 313 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_2/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 59, 2: 253}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 253\n",
      "  - 子节点数分布: {0: 253, 1: 6, 2: 16, 3: 22, 4: 8, 5: 4, 6: 2, 59: 1, 85: 1}\n",
      "\n",
      "处理路径结构文件: depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_1\n",
      "最后一轮iteration: 285, 路径数: 246\n",
      "[DEBUG] 发现层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] 最大层级索引: 2\n",
      "计算得到 312 个节点的信息\n",
      "已更新 /Volumes/My Passport/收敛结果/step3/step3_d3_g005_e005_a001/depth_3_gamma_0.05_eta_0.05_alpha_0.01_run_1/corrected_renyi_entropy.csv，添加了document_count, layer, parent_id, child_ids, child_count列\n",
      "节点层级统计:\n",
      "  - 层级分布: {0: 1, 1: 65, 2: 246}\n",
      "  - 文档数范围: 1-970\n",
      "  - 根节点数: 1\n",
      "  - 叶子节点数: 246\n",
      "  - 子节点数分布: {0: 246, 1: 7, 2: 17, 3: 17, 4: 13, 5: 6, 6: 4, 48: 1, 65: 1}\n",
      "==================================================\n",
      "Step3: 文档数和层级信息添加完成！\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 主函数：添加文档数和层级信息到entropy文件（适配step3）\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd \n",
    "\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step3\"  # 修改为step3路径\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Step3: 开始添加文档数和层级信息到entropy文件...\")\n",
    "print(\"=\" * 50)\n",
    "add_document_counts_to_entropy_files(base_path)\n",
    "print(\"=\" * 50)\n",
    "print(\"Step3: 文档数和层级信息添加完成！\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bd33143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jensen_shannon_distance(p, q):\n",
    "    \"\"\"\n",
    "    计算两个概率分布之间的Jensen-Shannon距离\n",
    "    \n",
    "    Parameters:\n",
    "    p, q: array-like, 概率分布（应该已经归一化）\n",
    "    \n",
    "    Returns:\n",
    "    float: Jensen-Shannon距离\n",
    "    \"\"\"\n",
    "    # 确保输入是numpy数组\n",
    "    p = np.array(p)\n",
    "    q = np.array(q)\n",
    "    \n",
    "    # 计算中点分布\n",
    "    m = 0.5 * (p + q)\n",
    "    \n",
    "    # 计算KL散度，添加小常数避免log(0)\n",
    "    eps = 1e-10\n",
    "    kl_pm = np.sum(p * np.log((p + eps) / (m + eps)))\n",
    "    kl_qm = np.sum(q * np.log((q + eps) / (m + eps)))\n",
    "    \n",
    "    # Jensen-Shannon散度\n",
    "    js_divergence = 0.5 * kl_pm + 0.5 * kl_qm\n",
    "    \n",
    "    # Jensen-Shannon距离（散度的平方根）\n",
    "    js_distance = np.sqrt(js_divergence)\n",
    "    \n",
    "    return js_distance\n",
    "\n",
    "def calculate_jensen_shannon_distances_with_weighted_entropy_by_alpha(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    修正版：计算每层节点之间的Jensen-Shannon距离和文档数加权平均Renyi熵\n",
    "    使用固定的eta=0.05作为Dirichlet平滑参数，alpha值仅用于识别文件夹\n",
    "    \n",
    "    注意：Renyi熵计算使用自然对数(loge)，单位为nats\n",
    "    \"\"\"\n",
    "    # 查找所有iteration_node_word_distributions.csv文件\n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_node_word_distributions.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    print(f\"找到 {len(files)} 个词分布文件待处理\")\n",
    "    \n",
    "    # step3中eta值固定为0.05（用于Dirichlet平滑）\n",
    "    eta = 0.05  # 修正：固定使用0.05作为平滑参数\n",
    "    \n",
    "    # 按alpha值分组显示文件分布\n",
    "    files_by_alpha = {}\n",
    "    for file_path in files:\n",
    "        folder_name = os.path.basename(os.path.dirname(file_path))\n",
    "        alpha = 0.1  # 默认值\n",
    "        if 'alpha_' in folder_name:\n",
    "            try:\n",
    "                alpha_part = folder_name.split('alpha_')[1].split('_')[0]\n",
    "                alpha = float(alpha_part)\n",
    "            except:\n",
    "                # 通过文件夹名称模式匹配\n",
    "                if 'a001' in folder_name:\n",
    "                    alpha = 0.01\n",
    "                elif 'a005' in folder_name:\n",
    "                    alpha = 0.05\n",
    "                elif 'a02' in folder_name:\n",
    "                    alpha = 0.2\n",
    "                elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                    alpha = 0.5\n",
    "                elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                    alpha = 1.0\n",
    "                elif 'a01' in folder_name:\n",
    "                    alpha = 0.1\n",
    "        else:\n",
    "            # 通过文件夹名称模式匹配\n",
    "            if 'a001' in folder_name:\n",
    "                alpha = 0.01\n",
    "            elif 'a005' in folder_name:\n",
    "                alpha = 0.05\n",
    "            elif 'a02' in folder_name:\n",
    "                alpha = 0.2\n",
    "            elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                alpha = 0.5\n",
    "            elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                alpha = 1.0\n",
    "            elif 'a01' in folder_name:\n",
    "                alpha = 0.1\n",
    "        \n",
    "        if alpha not in files_by_alpha:\n",
    "            files_by_alpha[alpha] = []\n",
    "        files_by_alpha[alpha].append(file_path)\n",
    "    \n",
    "    print(\"文件分布：\")\n",
    "    for alpha in sorted(files_by_alpha.keys()):\n",
    "        print(f\"  Alpha {alpha}: {len(files_by_alpha[alpha])} 个文件\")\n",
    "    print(f\"使用固定的Eta值: {eta}\")\n",
    "    print()\n",
    "    \n",
    "    # 处理每个文件\n",
    "    for idx, file_path in enumerate(files, 1):\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        \n",
    "        # 从文件夹名称提取alpha值和run信息（仅用于记录）\n",
    "        alpha = 0.1  # 默认值\n",
    "        run_id = \"未知\"\n",
    "        \n",
    "        if 'alpha_' in folder_name:\n",
    "            try:\n",
    "                alpha_part = folder_name.split('alpha_')[1].split('_')[0]\n",
    "                alpha = float(alpha_part)\n",
    "            except:\n",
    "                # 通过文件夹名称模式匹配\n",
    "                if 'a001' in folder_name:\n",
    "                    alpha = 0.01\n",
    "                elif 'a005' in folder_name:\n",
    "                    alpha = 0.05\n",
    "                elif 'a02' in folder_name:\n",
    "                    alpha = 0.2\n",
    "                elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                    alpha = 0.5\n",
    "                elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                    alpha = 1.0\n",
    "                elif 'a01' in folder_name:\n",
    "                    alpha = 0.1\n",
    "        else:\n",
    "            # 通过文件夹名称模式匹配\n",
    "            if 'a001' in folder_name:\n",
    "                alpha = 0.01\n",
    "            elif 'a005' in folder_name:\n",
    "                alpha = 0.05\n",
    "            elif 'a02' in folder_name:\n",
    "                alpha = 0.2\n",
    "            elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                alpha = 0.5\n",
    "            elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                alpha = 1.0\n",
    "            elif 'a01' in folder_name:\n",
    "                alpha = 0.1\n",
    "        \n",
    "        if '_run_' in folder_name:\n",
    "            try:\n",
    "                run_id = folder_name.split('_run_')[1]\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(f\"[{idx}/{len(files)}] 处理 Alpha={alpha}, Run={run_id}\")\n",
    "        print(f\"使用固定Eta={eta}进行Dirichlet平滑\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        try:\n",
    "            # 读取词分布数据\n",
    "            word_df = pd.read_csv(file_path)\n",
    "            word_df.columns = [col.strip(\"'\\\" \") for col in word_df.columns]\n",
    "            \n",
    "            # 获取最后一轮数据\n",
    "            max_iteration = word_df['iteration'].max()\n",
    "            last_iteration_data = word_df[word_df['iteration'] == max_iteration]\n",
    "            \n",
    "            # 获取全量词汇表\n",
    "            all_words = sorted(list(last_iteration_data['word'].dropna().unique()))\n",
    "            \n",
    "            # 读取entropy文件获取层级信息\n",
    "            entropy_file = os.path.join(folder_path, 'corrected_renyi_entropy.csv')\n",
    "            if not os.path.exists(entropy_file):\n",
    "                print(f\"⚠️  未找到entropy文件，跳过此文件\")\n",
    "                continue\n",
    "                \n",
    "            entropy_df = pd.read_csv(entropy_file)\n",
    "            \n",
    "            # 基本信息\n",
    "            print(f\"📊 基本信息:\")\n",
    "            print(f\"   词汇表大小: {len(all_words)}\")\n",
    "            print(f\"   最后iteration: {max_iteration}\")\n",
    "            \n",
    "            # 按层级分组节点\n",
    "            layers = entropy_df.groupby('layer')['node_id'].apply(list).to_dict()\n",
    "            print(f\"   层级分布: {[(layer, len(nodes)) for layer, nodes in layers.items()]}\")\n",
    "            \n",
    "            # 为每个节点构建概率分布\n",
    "            print(f\"🔄 构建概率分布...\")\n",
    "            node_distributions = {}\n",
    "            \n",
    "            for node_id in entropy_df['node_id'].unique():\n",
    "                # 获取该节点的词分布\n",
    "                node_words = last_iteration_data[last_iteration_data['node_id'] == node_id]\n",
    "                \n",
    "                # 初始化计数向量\n",
    "                counts = np.zeros(len(all_words))\n",
    "                word_to_idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "                \n",
    "                # 填充实际计数\n",
    "                for _, row in node_words.iterrows():\n",
    "                    word = row['word']\n",
    "                    if pd.notna(word) and word in word_to_idx:\n",
    "                        counts[word_to_idx[word]] = row['count']\n",
    "                \n",
    "                # 修正：使用固定的eta值进行Dirichlet平滑\n",
    "                smoothed_counts = counts + eta  # 使用eta=0.05而不是alpha\n",
    "                \n",
    "                # 计算概率分布\n",
    "                probabilities = smoothed_counts / np.sum(smoothed_counts)\n",
    "                node_distributions[node_id] = probabilities\n",
    "            \n",
    "            print(f\"   ✓ 完成 {len(node_distributions)} 个节点的概率分布\")\n",
    "            \n",
    "            # 计算每层内节点的JS距离和加权平均熵\n",
    "            all_js_distances = []\n",
    "            layer_avg_distances = []\n",
    "            \n",
    "            print(f\"📐 计算JS距离...\")\n",
    "            for layer, layer_nodes in layers.items():\n",
    "                layer_js_distances = []\n",
    "                n = len(layer_nodes)\n",
    "                \n",
    "                # 计算该层内所有节点对的JS距离\n",
    "                for i, node1 in enumerate(layer_nodes):\n",
    "                    for j, node2 in enumerate(layer_nodes):\n",
    "                        if i < j:  # 只计算上三角矩阵，避免重复和自己与自己\n",
    "                            if node1 in node_distributions and node2 in node_distributions:\n",
    "                                p = node_distributions[node1]\n",
    "                                q = node_distributions[node2]\n",
    "                                \n",
    "                                # 计算Jensen-Shannon距离\n",
    "                                js_distance = jensen_shannon_distance(p, q)\n",
    "                                \n",
    "                                layer_js_distances.append({\n",
    "                                    'layer': layer,\n",
    "                                    'node1_id': node1,\n",
    "                                    'node2_id': node2,\n",
    "                                    'js_distance': js_distance,\n",
    "                                    'node1_doc_count': entropy_df[entropy_df['node_id'] == node1]['document_count'].iloc[0] if len(entropy_df[entropy_df['node_id'] == node1]) > 0 else 0,\n",
    "                                    'node2_doc_count': entropy_df[entropy_df['node_id'] == node2]['document_count'].iloc[0] if len(entropy_df[entropy_df['node_id'] == node2]) > 0 else 0\n",
    "                                })\n",
    "                \n",
    "                all_js_distances.extend(layer_js_distances)\n",
    "                \n",
    "                # 计算该层的平均JS距离\n",
    "                avg_js_distance = 0.0\n",
    "                if layer_js_distances and n > 1:\n",
    "                    total_js_distance = sum(d['js_distance'] for d in layer_js_distances)\n",
    "                    max_pairs = n * (n - 1) // 2\n",
    "                    avg_js_distance = total_js_distance / max_pairs\n",
    "                \n",
    "                # 计算该层的文档数加权平均Renyi熵\n",
    "                layer_entropy_data = entropy_df[entropy_df['layer'] == layer]\n",
    "                total_docs = layer_entropy_data['document_count'].sum()\n",
    "                \n",
    "                if total_docs > 0:\n",
    "                    weighted_entropy = (layer_entropy_data['document_count'] * layer_entropy_data['renyi_entropy_corrected']).sum() / total_docs\n",
    "                else:\n",
    "                    weighted_entropy = 0.0\n",
    "                \n",
    "                layer_avg_distances.append({\n",
    "                    'layer': layer,\n",
    "                    'node_count': n,\n",
    "                    'total_pairs': len(layer_js_distances),\n",
    "                    'max_pairs': n * (n - 1) // 2 if n > 1 else 0,\n",
    "                    'sum_js_distance': sum(d['js_distance'] for d in layer_js_distances),\n",
    "                    'avg_js_distance': avg_js_distance,\n",
    "                    'total_documents': total_docs,\n",
    "                    'weighted_avg_renyi_entropy': weighted_entropy,\n",
    "                    'eta_used': eta,  # 修正：记录实际使用的eta值\n",
    "                    'alpha_folder': alpha  # 修正：记录文件夹的alpha值\n",
    "                })\n",
    "                \n",
    "                # 简洁的层级统计输出\n",
    "                print(f\"   Layer {layer}: {n}节点, JS={avg_js_distance:.4f}, 熵={weighted_entropy:.4f}\")\n",
    "            \n",
    "            # 保存结果文件\n",
    "            if all_js_distances:\n",
    "                js_df = pd.DataFrame(all_js_distances)\n",
    "                output_path = os.path.join(folder_path, 'jensen_shannon_distances.csv')\n",
    "                js_df.to_csv(output_path, index=False)\n",
    "            \n",
    "            if layer_avg_distances:\n",
    "                avg_df = pd.DataFrame(layer_avg_distances)\n",
    "                avg_output_path = os.path.join(folder_path, 'layer_average_js_distances.csv')\n",
    "                avg_df.to_csv(avg_output_path, index=False)\n",
    "            \n",
    "            print(f\"💾 结果已保存\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 处理失败: {str(e)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"✅ 全部文件处理完成！\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cea591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Step3: 开始计算Jensen-Shannon距离和加权平均Renyi熵（按alpha值自动调整）...\n",
      "==================================================\n",
      "找到 18 个词分布文件待处理\n",
      "文件分布：\n",
      "  Alpha 0.01: 3 个文件\n",
      "  Alpha 0.05: 3 个文件\n",
      "  Alpha 0.1: 3 个文件\n",
      "  Alpha 0.2: 3 个文件\n",
      "  Alpha 0.5: 3 个文件\n",
      "  Alpha 1.0: 3 个文件\n",
      "使用固定的Eta值: 0.05\n",
      "\n",
      "================================================================================\n",
      "[1/18] 处理 Alpha=1.0, Run=3\n",
      "使用固定Eta=0.05进行Dirichlet平滑\n",
      "================================================================================\n",
      "📊 基本信息:\n",
      "   词汇表大小: 1490\n",
      "   最后iteration: 265\n",
      "   层级分布: [(0, 1), (1, 64), (2, 242)]\n",
      "🔄 构建概率分布...\n",
      "   ✓ 完成 307 个节点的概率分布\n",
      "📐 计算JS距离...\n",
      "   Layer 0: 1节点, JS=0.0000, 熵=4.9219\n",
      "   Layer 1: 64节点, JS=0.6081, 熵=4.6239\n",
      "   Layer 2: 242节点, JS=0.5566, 熵=4.5810\n",
      "💾 结果已保存\n",
      "================================================================================\n",
      "[2/18] 处理 Alpha=1.0, Run=1\n",
      "使用固定Eta=0.05进行Dirichlet平滑\n",
      "================================================================================\n",
      "📊 基本信息:\n",
      "   词汇表大小: 1490\n",
      "   最后iteration: 245\n",
      "   层级分布: [(0, 1), (1, 61), (2, 267)]\n",
      "🔄 构建概率分布...\n",
      "   ✓ 完成 329 个节点的概率分布\n",
      "📐 计算JS距离...\n",
      "   Layer 0: 1节点, JS=0.0000, 熵=4.8951\n",
      "   Layer 1: 61节点, JS=0.5939, 熵=4.4591\n",
      "   Layer 2: 267节点, JS=0.5639, 熵=4.4854\n",
      "💾 结果已保存\n",
      "================================================================================\n",
      "[3/18] 处理 Alpha=1.0, Run=2\n",
      "使用固定Eta=0.05进行Dirichlet平滑\n",
      "================================================================================\n",
      "📊 基本信息:\n",
      "   词汇表大小: 1490\n",
      "   最后iteration: 245\n",
      "   层级分布: [(0, 1), (1, 57), (2, 263)]\n",
      "🔄 构建概率分布...\n",
      "   ✓ 完成 321 个节点的概率分布\n",
      "📐 计算JS距离...\n",
      "   Layer 0: 1节点, JS=0.0000, 熵=4.9374\n",
      "   Layer 1: 57节点, JS=0.5959, 熵=4.4893\n",
      "   Layer 2: 263节点, JS=0.5677, 熵=4.4976\n",
      "💾 结果已保存\n",
      "================================================================================\n",
      "[4/18] 处理 Alpha=0.2, Run=3\n",
      "使用固定Eta=0.05进行Dirichlet平滑\n",
      "================================================================================\n",
      "📊 基本信息:\n",
      "   词汇表大小: 1490\n",
      "   最后iteration: 170\n",
      "   层级分布: [(0, 1), (1, 78), (2, 246)]\n",
      "🔄 构建概率分布...\n",
      "   ✓ 完成 325 个节点的概率分布\n",
      "📐 计算JS距离...\n",
      "   Layer 0: 1节点, JS=0.0000, 熵=4.9341\n",
      "   Layer 1: 78节点, JS=0.5843, 熵=4.4484\n",
      "   Layer 2: 246节点, JS=0.5702, 熵=4.5200\n",
      "💾 结果已保存\n",
      "================================================================================\n",
      "[5/18] 处理 Alpha=0.2, Run=1\n",
      "使用固定Eta=0.05进行Dirichlet平滑\n",
      "================================================================================\n",
      "📊 基本信息:\n",
      "   词汇表大小: 1490\n",
      "   最后iteration: 170\n",
      "   层级分布: [(0, 1), (1, 67), (2, 257)]\n",
      "🔄 构建概率分布...\n",
      "   ✓ 完成 325 个节点的概率分布\n",
      "📐 计算JS距离...\n",
      "   Layer 0: 1节点, JS=0.0000, 熵=4.9279\n",
      "   Layer 1: 67节点, JS=0.5784, 熵=4.4602\n",
      "   Layer 2: 257节点, JS=0.5663, 熵=4.5185\n",
      "💾 结果已保存\n",
      "================================================================================\n",
      "[6/18] 处理 Alpha=0.2, Run=2\n",
      "使用固定Eta=0.05进行Dirichlet平滑\n",
      "================================================================================\n",
      "📊 基本信息:\n",
      "   词汇表大小: 1490\n",
      "   最后iteration: 170\n",
      "   层级分布: [(0, 1), (1, 71), (2, 261)]\n",
      "🔄 构建概率分布...\n",
      "   ✓ 完成 333 个节点的概率分布\n",
      "📐 计算JS距离...\n",
      "   Layer 0: 1节点, JS=0.0000, 熵=4.9620\n",
      "   Layer 1: 71节点, JS=0.5820, 熵=4.5827\n",
      "   Layer 2: 261节点, JS=0.5629, 熵=4.5345\n",
      "💾 结果已保存\n",
      "================================================================================\n",
      "[7/18] 处理 Alpha=0.5, Run=3\n",
      "使用固定Eta=0.05进行Dirichlet平滑\n",
      "================================================================================\n",
      "📊 基本信息:\n",
      "   词汇表大小: 1490\n",
      "   最后iteration: 275\n",
      "   层级分布: [(0, 1), (1, 55), (2, 236)]\n",
      "🔄 构建概率分布...\n",
      "   ✓ 完成 292 个节点的概率分布\n",
      "📐 计算JS距离...\n",
      "   Layer 0: 1节点, JS=0.0000, 熵=4.9473\n",
      "   Layer 1: 55节点, JS=0.5663, 熵=4.6287\n",
      "   Layer 2: 236节点, JS=0.5642, 熵=4.5321\n",
      "💾 结果已保存\n",
      "================================================================================\n",
      "[8/18] 处理 Alpha=0.5, Run=1\n",
      "使用固定Eta=0.05进行Dirichlet平滑\n",
      "================================================================================\n",
      "📊 基本信息:\n",
      "   词汇表大小: 1490\n",
      "   最后iteration: 275\n",
      "   层级分布: [(0, 1), (1, 45), (2, 236)]\n",
      "🔄 构建概率分布...\n",
      "   ✓ 完成 282 个节点的概率分布\n",
      "📐 计算JS距离...\n",
      "   Layer 0: 1节点, JS=0.0000, 熵=4.9814\n",
      "   Layer 1: 45节点, JS=0.5566, 熵=4.5808\n",
      "   Layer 2: 236节点, JS=0.5794, 熵=4.4689\n",
      "💾 结果已保存\n",
      "================================================================================\n",
      "[9/18] 处理 Alpha=0.5, Run=2\n",
      "使用固定Eta=0.05进行Dirichlet平滑\n",
      "================================================================================\n",
      "📊 基本信息:\n",
      "   词汇表大小: 1490\n",
      "   最后iteration: 275\n",
      "   层级分布: [(0, 1), (1, 36), (2, 255)]\n",
      "🔄 构建概率分布...\n",
      "   ✓ 完成 292 个节点的概率分布\n",
      "📐 计算JS距离...\n",
      "   Layer 0: 1节点, JS=0.0000, 熵=4.9606\n",
      "   Layer 1: 36节点, JS=0.6094, 熵=4.5044\n",
      "   Layer 2: 255节点, JS=0.5702, 熵=4.5200\n",
      "💾 结果已保存\n",
      "================================================================================\n",
      "[10/18] 处理 Alpha=0.05, Run=3\n",
      "使用固定Eta=0.05进行Dirichlet平滑\n",
      "================================================================================\n",
      "📊 基本信息:\n",
      "   词汇表大小: 1490\n",
      "   最后iteration: 175\n",
      "   层级分布: [(0, 1), (1, 62), (2, 212)]\n",
      "🔄 构建概率分布...\n",
      "   ✓ 完成 275 个节点的概率分布\n",
      "📐 计算JS距离...\n",
      "   Layer 0: 1节点, JS=0.0000, 熵=5.0622\n",
      "   Layer 1: 62节点, JS=0.5404, 熵=4.9330\n",
      "   Layer 2: 212节点, JS=0.5595, 熵=4.8250\n",
      "💾 结果已保存\n",
      "================================================================================\n",
      "[11/18] 处理 Alpha=0.05, Run=1\n",
      "使用固定Eta=0.05进行Dirichlet平滑\n",
      "================================================================================\n",
      "📊 基本信息:\n",
      "   词汇表大小: 1490\n",
      "   最后iteration: 175\n",
      "   层级分布: [(0, 1), (1, 60), (2, 255)]\n",
      "🔄 构建概率分布...\n",
      "   ✓ 完成 316 个节点的概率分布\n",
      "📐 计算JS距离...\n",
      "   Layer 0: 1节点, JS=0.0000, 熵=4.9605\n",
      "   Layer 1: 60节点, JS=0.5274, 熵=4.7746\n",
      "   Layer 2: 255节点, JS=0.5677, 熵=4.5770\n",
      "💾 结果已保存\n",
      "================================================================================\n",
      "[12/18] 处理 Alpha=0.05, Run=2\n",
      "使用固定Eta=0.05进行Dirichlet平滑\n",
      "================================================================================\n",
      "📊 基本信息:\n",
      "   词汇表大小: 1490\n",
      "   最后iteration: 175\n",
      "   层级分布: [(0, 1), (1, 60), (2, 244)]\n",
      "🔄 构建概率分布...\n",
      "   ✓ 完成 305 个节点的概率分布\n",
      "📐 计算JS距离...\n",
      "   Layer 0: 1节点, JS=0.0000, 熵=5.0117\n",
      "   Layer 1: 60节点, JS=0.5456, 熵=4.7415\n",
      "   Layer 2: 244节点, JS=0.5679, 熵=4.5716\n",
      "💾 结果已保存\n",
      "================================================================================\n",
      "[13/18] 处理 Alpha=0.1, Run=2\n",
      "使用固定Eta=0.05进行Dirichlet平滑\n",
      "================================================================================\n",
      "📊 基本信息:\n",
      "   词汇表大小: 1490\n",
      "   最后iteration: 90\n",
      "   层级分布: [(0, 1), (1, 55), (2, 243)]\n",
      "🔄 构建概率分布...\n",
      "   ✓ 完成 299 个节点的概率分布\n",
      "📐 计算JS距离...\n",
      "   Layer 0: 1节点, JS=0.0000, 熵=4.9828\n",
      "   Layer 1: 55节点, JS=0.5651, 熵=4.6981\n",
      "   Layer 2: 243节点, JS=0.5757, 熵=4.4911\n",
      "💾 结果已保存\n",
      "================================================================================\n",
      "[14/18] 处理 Alpha=0.1, Run=3\n",
      "使用固定Eta=0.05进行Dirichlet平滑\n",
      "================================================================================\n",
      "📊 基本信息:\n",
      "   词汇表大小: 1490\n",
      "   最后iteration: 90\n",
      "   层级分布: [(0, 1), (1, 59), (2, 262)]\n",
      "🔄 构建概率分布...\n",
      "   ✓ 完成 322 个节点的概率分布\n",
      "📐 计算JS距离...\n",
      "   Layer 0: 1节点, JS=0.0000, 熵=4.9232\n",
      "   Layer 1: 59节点, JS=0.5652, 熵=4.5501\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd \n",
    "# 主函数：计算Jensen-Shannon距离和加权平均Renyi熵\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step3\"  # 根目录\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Step3: 开始计算Jensen-Shannon距离和加权平均Renyi熵（按alpha值自动调整）...\")  # 更新注释\n",
    "print(\"=\" * 50)\n",
    "calculate_jensen_shannon_distances_with_weighted_entropy_by_alpha(base_path)\n",
    "print(\"=\" * 50)\n",
    "print(\"Step3: Jensen-Shannon距离和加权平均Renyi熵计算完成！\")  # 更新注释\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a10a623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_layer_statistics_by_alpha(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    按alpha值汇总各层的JS距离和加权熵统计，在与run文件夹同级位置生成汇总表\n",
    "    修正版：适配step3中的alpha参数而非eta参数\n",
    "    \"\"\"\n",
    "    # 查找所有layer_average_js_distances.csv文件\n",
    "    pattern = os.path.join(base_path, \"**\", \"layer_average_js_distances.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    # 存储所有数据和分组信息\n",
    "    all_data = []\n",
    "    alpha_groups = {}  # 用于存储每个alpha组合的父目录\n",
    "    \n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        parent_folder = os.path.dirname(folder_path)  # run文件夹的父目录\n",
    "        \n",
    "        # 从文件夹名称提取alpha值（适配step3）\n",
    "        alpha = None\n",
    "        if 'alpha_' in folder_name:\n",
    "            try:\n",
    "                alpha_part = folder_name.split('alpha_')[1].split('_')[0]\n",
    "                alpha = float(alpha_part)\n",
    "            except (IndexError, ValueError):\n",
    "                # 通过文件夹名称模式匹配\n",
    "                if 'a001' in folder_name:\n",
    "                    alpha = 0.01\n",
    "                elif 'a005' in folder_name:\n",
    "                    alpha = 0.05\n",
    "                elif 'a02' in folder_name:\n",
    "                    alpha = 0.2\n",
    "                elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                    alpha = 0.5\n",
    "                elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                    alpha = 1.0\n",
    "                elif 'a01' in folder_name:\n",
    "                    alpha = 0.1\n",
    "        else:\n",
    "            # 通过文件夹名称模式匹配\n",
    "            if 'a001' in folder_name:\n",
    "                alpha = 0.01\n",
    "            elif 'a005' in folder_name:\n",
    "                alpha = 0.05\n",
    "            elif 'a02' in folder_name:\n",
    "                alpha = 0.2\n",
    "            elif 'a05' in folder_name and 'a005' not in folder_name:\n",
    "                alpha = 0.5\n",
    "            elif 'a1_' in folder_name or 'a1' in folder_name.split('_')[-1]:\n",
    "                alpha = 1.0\n",
    "            elif 'a01' in folder_name:\n",
    "                alpha = 0.1\n",
    "        \n",
    "        if alpha is None:\n",
    "            print(f\"警告：无法从文件夹名称 {folder_name} 提取alpha值\")\n",
    "            continue\n",
    "        \n",
    "        # 提取run编号\n",
    "        run_match = folder_name.split('_run_')\n",
    "        if len(run_match) > 1:\n",
    "            run_id = run_match[1]\n",
    "        else:\n",
    "            print(f\"警告：无法从文件夹名称 {folder_name} 提取run编号\")\n",
    "            continue\n",
    "        \n",
    "        # 记录alpha组合的父目录\n",
    "        if alpha not in alpha_groups:\n",
    "            alpha_groups[alpha] = parent_folder\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                all_data.append({\n",
    "                    'alpha': alpha,  # 修正：使用alpha而不是eta\n",
    "                    'run_id': run_id,\n",
    "                    'layer': row['layer'],\n",
    "                    'node_count': row['node_count'],\n",
    "                    'avg_js_distance': row['avg_js_distance'],\n",
    "                    'weighted_avg_renyi_entropy': row['weighted_avg_renyi_entropy'],\n",
    "                    'total_documents': row['total_documents'],\n",
    "                    'eta_used': row.get('eta_used', 0.05),  # 记录实际使用的eta值（固定0.05）\n",
    "                    'alpha_folder': row.get('alpha_folder', alpha),  # 记录文件夹的alpha值\n",
    "                    'parent_folder': parent_folder\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"读取文件 {file_path} 时出错: {e}\")\n",
    "    \n",
    "    # 转换为DataFrame\n",
    "    summary_df = pd.DataFrame(all_data)\n",
    "    \n",
    "    if summary_df.empty:\n",
    "        print(\"未找到有效数据\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"各ALPHA值的层级汇总统计（Step3）\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 按alpha分组，生成汇总文件\n",
    "    for alpha, group_data in summary_df.groupby('alpha'):\n",
    "        parent_folder = group_data['parent_folder'].iloc[0]\n",
    "        \n",
    "        print(f\"\\n处理 Alpha={alpha}\")\n",
    "        print(f\"输出目录: {parent_folder}\")\n",
    "        \n",
    "        # 计算各层的汇总统计\n",
    "        layer_summary = group_data.groupby('layer').agg({\n",
    "            'avg_js_distance': ['mean', 'std', 'count'],\n",
    "            'weighted_avg_renyi_entropy': ['mean', 'std', 'count'],\n",
    "            'node_count': ['mean', 'std'],\n",
    "            'total_documents': 'mean',\n",
    "            'eta_used': 'first',  # 记录使用的eta值\n",
    "            'run_id': lambda x: ', '.join(sorted(x.unique()))\n",
    "        }).round(4)\n",
    "        \n",
    "        # 平铺列名\n",
    "        layer_summary.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in layer_summary.columns]\n",
    "        layer_summary = layer_summary.reset_index()\n",
    "        \n",
    "        # 重命名列，使其更清晰\n",
    "        column_mapping = {\n",
    "            'avg_js_distance_mean': 'avg_js_distance_mean',\n",
    "            'avg_js_distance_std': 'avg_js_distance_std', \n",
    "            'avg_js_distance_count': 'run_count',\n",
    "            'weighted_avg_renyi_entropy_mean': 'weighted_avg_renyi_entropy_mean',\n",
    "            'weighted_avg_renyi_entropy_std': 'weighted_avg_renyi_entropy_std',\n",
    "            'weighted_avg_renyi_entropy_count': 'entropy_run_count',\n",
    "            'node_count_mean': 'avg_node_count',\n",
    "            'node_count_std': 'node_count_std',\n",
    "            'total_documents_mean': 'avg_total_documents',\n",
    "            'eta_used_first': 'eta_used',\n",
    "            'run_id_<lambda>': 'included_runs'\n",
    "        }\n",
    "        \n",
    "        for old_name, new_name in column_mapping.items():\n",
    "            if old_name in layer_summary.columns:\n",
    "                layer_summary = layer_summary.rename(columns={old_name: new_name})\n",
    "        \n",
    "        # 添加alpha信息\n",
    "        layer_summary.insert(0, 'alpha', alpha)\n",
    "        \n",
    "        # 保存汇总结果到与run文件夹同级的位置\n",
    "        output_filename = f'alpha_{alpha}_layer_summary.csv'\n",
    "        output_path = os.path.join(parent_folder, output_filename)\n",
    "        layer_summary.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"  保存汇总文件: {output_path}\")\n",
    "        print(f\"  包含运行: {layer_summary['included_runs'].iloc[0] if 'included_runs' in layer_summary.columns else 'N/A'}\")\n",
    "        print(f\"  层数: {len(layer_summary)}\")\n",
    "        print(f\"  使用的Eta值: {layer_summary.get('eta_used', pd.Series([0.05])).iloc[0]}\")\n",
    "        \n",
    "        # 显示简要统计\n",
    "        for _, row in layer_summary.iterrows():\n",
    "            layer_num = int(row['layer'])\n",
    "            js_mean = row['avg_js_distance_mean']\n",
    "            js_std = row['avg_js_distance_std'] if 'avg_js_distance_std' in row else 0\n",
    "            entropy_mean = row['weighted_avg_renyi_entropy_mean']\n",
    "            entropy_std = row['weighted_avg_renyi_entropy_std'] if 'weighted_avg_renyi_entropy_std' in row else 0\n",
    "            node_count = row['avg_node_count']\n",
    "            run_count = int(row['run_count']) if 'run_count' in row else 0\n",
    "            \n",
    "            print(f\"    Layer {layer_num}: JS={js_mean:.4f}(±{js_std:.4f}), 熵={entropy_mean:.4f}(±{entropy_std:.4f}), 节点={node_count:.1f}, runs={run_count}\")\n",
    "    \n",
    "    # 生成总体对比文件（保存在base_path下）\n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"生成总体对比文件\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    overall_summary = summary_df.groupby(['alpha', 'layer']).agg({\n",
    "        'avg_js_distance': ['mean', 'std'],\n",
    "        'weighted_avg_renyi_entropy': ['mean', 'std'],\n",
    "        'node_count': ['mean', 'std'],\n",
    "        'run_id': 'count'\n",
    "    }).round(4)\n",
    "    \n",
    "    # 平铺列名\n",
    "    overall_summary.columns = ['_'.join(col).strip() for col in overall_summary.columns]\n",
    "    overall_summary = overall_summary.reset_index()\n",
    "    \n",
    "    overall_output_path = os.path.join(base_path, 'alpha_layer_comparison.csv')\n",
    "    overall_summary.to_csv(overall_output_path, index=False)\n",
    "    print(f\"总体对比文件保存到: {overall_output_path}\")\n",
    "    \n",
    "    # 显示跨alpha对比\n",
    "    for layer in sorted(summary_df['layer'].unique()):\n",
    "        print(f\"\\nLayer {int(layer)} 跨Alpha对比:\")\n",
    "        print(\"Alpha值     JS距离(±std)      加权熵(±std)      节点数(±std)   运行数\")\n",
    "        print(\"-\" * 75)\n",
    "        \n",
    "        layer_data = overall_summary[overall_summary['layer'] == layer]\n",
    "        for _, row in layer_data.iterrows():\n",
    "            alpha = row['alpha']\n",
    "            js_mean = row['avg_js_distance_mean']\n",
    "            js_std = row['avg_js_distance_std']\n",
    "            entropy_mean = row['weighted_avg_renyi_entropy_mean']\n",
    "            entropy_std = row['weighted_avg_renyi_entropy_std']\n",
    "            node_mean = row['node_count_mean']\n",
    "            node_std = row['node_count_std']\n",
    "            run_count = int(row['run_id_count'])\n",
    "            \n",
    "            print(f\"{alpha:7.3f}    {js_mean:6.4f}(±{js_std:5.4f})   {entropy_mean:6.4f}(±{entropy_std:5.4f})   {node_mean:6.1f}(±{node_std:4.1f})   {run_count:4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f61f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "开始汇总各Eta值的层级统计...\n",
      "======================================================================\n",
      "======================================================================\n",
      "各ETA值的层级汇总统计\n",
      "======================================================================\n",
      "\n",
      "处理 Eta=0.005\n",
      "输出目录: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛\n",
      "  保存汇总文件: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/eta_0.005_layer_summary.csv\n",
      "  包含运行: 1, 2, 3\n",
      "  层数: 3\n",
      "    Layer 0: JS=0.0000(±0.0000), 熵=5.4077(±0.0307), 节点=1.0, runs=3\n",
      "    Layer 1: JS=0.7370(±0.0077), 熵=3.5804(±0.0238), 节点=85.0, runs=3\n",
      "    Layer 2: JS=0.7486(±0.0018), 熵=3.0128(±0.0564), 节点=346.3, runs=3\n",
      "\n",
      "处理 Eta=0.01\n",
      "输出目录: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛\n",
      "  保存汇总文件: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/eta_0.01_layer_summary.csv\n",
      "  包含运行: 1, 2, 3\n",
      "  层数: 3\n",
      "    Layer 0: JS=0.0000(±0.0000), 熵=5.3045(±0.0209), 节点=1.0, runs=3\n",
      "    Layer 1: JS=0.6998(±0.0031), 熵=3.7481(±0.0531), 节点=79.0, runs=3\n",
      "    Layer 2: JS=0.7070(±0.0031), 熵=3.3435(±0.0365), 节点=340.3, runs=3\n",
      "\n",
      "处理 Eta=0.02\n",
      "输出目录: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛\n",
      "  保存汇总文件: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/eta_0.02_layer_summary.csv\n",
      "  包含运行: 1, 2, 3\n",
      "  层数: 3\n",
      "    Layer 0: JS=0.0000(±0.0000), 熵=5.1693(±0.0377), 节点=1.0, runs=3\n",
      "    Layer 1: JS=0.6402(±0.0077), 熵=4.1862(±0.0911), 节点=73.3, runs=3\n",
      "    Layer 2: JS=0.6540(±0.0029), 熵=3.8342(±0.0188), 节点=310.7, runs=3\n",
      "\n",
      "处理 Eta=0.05\n",
      "输出目录: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛\n",
      "  保存汇总文件: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/eta_0.05_layer_summary.csv\n",
      "  包含运行: 1, 2, 3\n",
      "  层数: 3\n",
      "    Layer 0: JS=0.0000(±0.0000), 熵=4.9562(±0.0303), 节点=1.0, runs=3\n",
      "    Layer 1: JS=0.5663(±0.0020), 熵=4.6121(±0.0769), 节点=58.7, runs=3\n",
      "    Layer 2: JS=0.5739(±0.0017), 熵=4.4826(±0.0247), 节点=252.3, runs=3\n",
      "\n",
      "处理 Eta=0.1\n",
      "输出目录: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛\n",
      "  保存汇总文件: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/eta_0.1_layer_summary.csv\n",
      "  包含运行: 1, 2, 3\n",
      "  层数: 3\n",
      "    Layer 0: JS=0.0000(±0.0000), 熵=4.9197(±0.0219), 节点=1.0, runs=3\n",
      "    Layer 1: JS=0.4579(±0.0178), 熵=5.2068(±0.1233), 节点=41.7, runs=3\n",
      "    Layer 2: JS=0.4982(±0.0058), 熵=5.0438(±0.0160), 节点=174.3, runs=3\n",
      "\n",
      "处理 Eta=0.2\n",
      "输出目录: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛\n",
      "  保存汇总文件: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/eta_0.2_layer_summary.csv\n",
      "  包含运行: 1, 2, 3\n",
      "  层数: 3\n",
      "    Layer 0: JS=0.0000(±0.0000), 熵=4.8739(±0.0166), 节点=1.0, runs=3\n",
      "    Layer 1: JS=0.4058(±0.0258), 熵=5.4354(±0.1712), 节点=29.3, runs=3\n",
      "    Layer 2: JS=0.4039(±0.0011), 熵=5.4501(±0.0260), 节点=124.7, runs=3\n",
      "\n",
      "======================================================================\n",
      "生成总体对比文件\n",
      "======================================================================\n",
      "总体对比文件保存到: /Volumes/My Passport/收敛结果/step2/eta_layer_comparison.csv\n",
      "\n",
      "Layer 0 跨Eta对比:\n",
      "Eta值      JS距离(±std)      加权熵(±std)      节点数(±std)   运行数\n",
      "---------------------------------------------------------------------------\n",
      " 0.005    0.0000(±0.0000)   5.4077(±0.0307)      1.0(± 0.0)      3\n",
      " 0.010    0.0000(±0.0000)   5.3045(±0.0209)      1.0(± 0.0)      3\n",
      " 0.020    0.0000(±0.0000)   5.1693(±0.0377)      1.0(± 0.0)      3\n",
      " 0.050    0.0000(±0.0000)   4.9562(±0.0303)      1.0(± 0.0)      3\n",
      " 0.100    0.0000(±0.0000)   4.9197(±0.0219)      1.0(± 0.0)      3\n",
      " 0.200    0.0000(±0.0000)   4.8739(±0.0166)      1.0(± 0.0)      3\n",
      "\n",
      "Layer 1 跨Eta对比:\n",
      "Eta值      JS距离(±std)      加权熵(±std)      节点数(±std)   运行数\n",
      "---------------------------------------------------------------------------\n",
      " 0.005    0.7370(±0.0077)   3.5804(±0.0238)     85.0(± 5.0)      3\n",
      " 0.010    0.6998(±0.0031)   3.7481(±0.0531)     79.0(± 2.6)      3\n",
      " 0.020    0.6402(±0.0077)   4.1862(±0.0911)     73.3(± 1.2)      3\n",
      " 0.050    0.5663(±0.0020)   4.6121(±0.0769)     58.7(± 3.5)      3\n",
      " 0.100    0.4579(±0.0178)   5.2068(±0.1233)     41.7(± 2.1)      3\n",
      " 0.200    0.4058(±0.0258)   5.4354(±0.1712)     29.3(± 1.5)      3\n",
      "\n",
      "Layer 2 跨Eta对比:\n",
      "Eta值      JS距离(±std)      加权熵(±std)      节点数(±std)   运行数\n",
      "---------------------------------------------------------------------------\n",
      " 0.005    0.7486(±0.0018)   3.0128(±0.0564)    346.3(± 9.0)      3\n",
      " 0.010    0.7070(±0.0031)   3.3435(±0.0365)    340.3(±12.9)      3\n",
      " 0.020    0.6540(±0.0029)   3.8342(±0.0188)    310.7(± 2.5)      3\n",
      " 0.050    0.5739(±0.0017)   4.4826(±0.0247)    252.3(± 9.5)      3\n",
      " 0.100    0.4982(±0.0058)   5.0438(±0.0160)    174.3(±11.1)      3\n",
      " 0.200    0.4039(±0.0011)   5.4501(±0.0260)    124.7(± 2.5)      3\n",
      "======================================================================\n",
      "汇总分析完成！\n",
      "======================================================================\n",
      "  保存汇总文件: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/eta_0.2_layer_summary.csv\n",
      "  包含运行: 1, 2, 3\n",
      "  层数: 3\n",
      "    Layer 0: JS=0.0000(±0.0000), 熵=4.8739(±0.0166), 节点=1.0, runs=3\n",
      "    Layer 1: JS=0.4058(±0.0258), 熵=5.4354(±0.1712), 节点=29.3, runs=3\n",
      "    Layer 2: JS=0.4039(±0.0011), 熵=5.4501(±0.0260), 节点=124.7, runs=3\n",
      "\n",
      "======================================================================\n",
      "生成总体对比文件\n",
      "======================================================================\n",
      "总体对比文件保存到: /Volumes/My Passport/收敛结果/step2/eta_layer_comparison.csv\n",
      "\n",
      "Layer 0 跨Eta对比:\n",
      "Eta值      JS距离(±std)      加权熵(±std)      节点数(±std)   运行数\n",
      "---------------------------------------------------------------------------\n",
      " 0.005    0.0000(±0.0000)   5.4077(±0.0307)      1.0(± 0.0)      3\n",
      " 0.010    0.0000(±0.0000)   5.3045(±0.0209)      1.0(± 0.0)      3\n",
      " 0.020    0.0000(±0.0000)   5.1693(±0.0377)      1.0(± 0.0)      3\n",
      " 0.050    0.0000(±0.0000)   4.9562(±0.0303)      1.0(± 0.0)      3\n",
      " 0.100    0.0000(±0.0000)   4.9197(±0.0219)      1.0(± 0.0)      3\n",
      " 0.200    0.0000(±0.0000)   4.8739(±0.0166)      1.0(± 0.0)      3\n",
      "\n",
      "Layer 1 跨Eta对比:\n",
      "Eta值      JS距离(±std)      加权熵(±std)      节点数(±std)   运行数\n",
      "---------------------------------------------------------------------------\n",
      " 0.005    0.7370(±0.0077)   3.5804(±0.0238)     85.0(± 5.0)      3\n",
      " 0.010    0.6998(±0.0031)   3.7481(±0.0531)     79.0(± 2.6)      3\n",
      " 0.020    0.6402(±0.0077)   4.1862(±0.0911)     73.3(± 1.2)      3\n",
      " 0.050    0.5663(±0.0020)   4.6121(±0.0769)     58.7(± 3.5)      3\n",
      " 0.100    0.4579(±0.0178)   5.2068(±0.1233)     41.7(± 2.1)      3\n",
      " 0.200    0.4058(±0.0258)   5.4354(±0.1712)     29.3(± 1.5)      3\n",
      "\n",
      "Layer 2 跨Eta对比:\n",
      "Eta值      JS距离(±std)      加权熵(±std)      节点数(±std)   运行数\n",
      "---------------------------------------------------------------------------\n",
      " 0.005    0.7486(±0.0018)   3.0128(±0.0564)    346.3(± 9.0)      3\n",
      " 0.010    0.7070(±0.0031)   3.3435(±0.0365)    340.3(±12.9)      3\n",
      " 0.020    0.6540(±0.0029)   3.8342(±0.0188)    310.7(± 2.5)      3\n",
      " 0.050    0.5739(±0.0017)   4.4826(±0.0247)    252.3(± 9.5)      3\n",
      " 0.100    0.4982(±0.0058)   5.0438(±0.0160)    174.3(±11.1)      3\n",
      " 0.200    0.4039(±0.0011)   5.4501(±0.0260)    124.7(± 2.5)      3\n",
      "======================================================================\n",
      "汇总分析完成！\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# 执行汇总分析\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step2\"\n",
    "print(\"=\" * 70)\n",
    "print(\"开始汇总各Eta值的层级统计...\")\n",
    "print(\"=\" * 70)\n",
    "aggregate_layer_statistics_by_alpha(base_path)\n",
    "print(\"=\" * 70)\n",
    "print(\"汇总分析完成！\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fd8317c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已汇总所有run的层级均值到 all_layers_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# 汇总所有result_layers.csv，按layer分组求mean（包括nodes_in_layer）\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step2\"\n",
    "pattern = os.path.join(base_path, \"**\", \"result_layers.csv\")\n",
    "files = glob.glob(pattern, recursive=True)\n",
    "\n",
    "all_rows = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "    run_folder = os.path.dirname(file)\n",
    "    folder_name = os.path.basename(run_folder)\n",
    "    \n",
    "    # 从文件夹名称提取eta值\n",
    "    eta = None\n",
    "    if 'eta_' in folder_name:\n",
    "        try:\n",
    "            eta_part = folder_name.split('eta_')[1].split('_')[0]\n",
    "            eta = float(eta_part)\n",
    "        except (IndexError, ValueError):\n",
    "            eta = None\n",
    "    \n",
    "    # 按 layer 分组求均值\n",
    "    grouped = df.groupby('layer').agg({\n",
    "        'entropy_wavg': 'mean',\n",
    "        'distinctiveness_wavg_jsd': 'mean',\n",
    "        'nodes_in_layer': 'mean'\n",
    "    }).reset_index()\n",
    "    grouped['run_folder'] = run_folder\n",
    "    grouped['eta'] = eta\n",
    "    \n",
    "    # 如果有其他参数信息（如 depth, gamma, alpha），可从原df取第一行补充\n",
    "    for col in ['depth', 'gamma', 'alpha']:\n",
    "        if col in df.columns:\n",
    "            grouped[col] = df[col].iloc[0]\n",
    "        else:\n",
    "            # 从文件夹名称提取\n",
    "            if col == 'depth' and 'depth_' in folder_name:\n",
    "                try:\n",
    "                    grouped[col] = int(folder_name.split('depth_')[1].split('_')[0])\n",
    "                except:\n",
    "                    grouped[col] = 3  # 默认depth=3\n",
    "            elif col == 'gamma' and 'gamma_' in folder_name:\n",
    "                try:\n",
    "                    grouped[col] = float(folder_name.split('gamma_')[1].split('_')[0])\n",
    "                except:\n",
    "                    grouped[col] = 0.05  # 默认gamma=0.05\n",
    "            elif col == 'alpha':\n",
    "                grouped[col] = 0.1  # 默认alpha=0.1\n",
    "            else:\n",
    "                grouped[col] = None\n",
    "    \n",
    "    all_rows.append(grouped)\n",
    "\n",
    "summary_df = pd.concat(all_rows, ignore_index=True)\n",
    "summary_df.to_csv(os.path.join(base_path, \"all_layers_summary.csv\"), index=False)\n",
    "print(\"已汇总所有run的层级均值到 all_layers_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61d8c818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已生成所有run按eta和层整体均值表 layer_eta_group_mean.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# filepath: /Volumes/My Passport/收敛结果/step2/all_layers_summary.csv\n",
    "df = pd.read_csv(\"/Volumes/My Passport/收敛结果/step2/all_layers_summary.csv\")\n",
    "\n",
    "# 按 eta 和 layer 分组，求均值和标准差\n",
    "summary = df.groupby(['eta', 'layer']).agg({\n",
    "    'entropy_wavg': ['mean', 'std'],\n",
    "    'distinctiveness_wavg_jsd': ['mean', 'std'],\n",
    "    'nodes_in_layer': ['mean', 'std'],\n",
    "    'depth': 'first',\n",
    "    'gamma': 'first',\n",
    "    'alpha': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# 展开多级列名\n",
    "summary.columns = ['_'.join(col).strip('_') for col in summary.columns]\n",
    "\n",
    "summary.to_csv(\"/Volumes/My Passport/收敛结果/step2/layer_eta_group_mean.csv\", index=False)\n",
    "print(\"已生成所有run按eta和层整体均值表 layer_eta_group_mean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "087ce664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已汇总所有result_layers.csv到 all_result_layers_merged.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step2\"\n",
    "pattern = os.path.join(base_path, \"**\", \"result_layers.csv\")\n",
    "files = glob.glob(pattern, recursive=True)\n",
    "\n",
    "all_rows = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "    df['run_folder'] = os.path.dirname(file)  # 标记来源\n",
    "    \n",
    "    # 从路径中提取参数\n",
    "    folder = os.path.dirname(file)\n",
    "    folder_name = os.path.basename(folder)\n",
    "    \n",
    "    for col in ['depth', 'gamma', 'eta', 'alpha']:\n",
    "        if col not in df.columns:\n",
    "            if f\"{col}_\" in folder_name:\n",
    "                try:\n",
    "                    value = float(folder_name.split(f\"{col}_\")[1].split(\"_\")[0])\n",
    "                except:\n",
    "                    value = None\n",
    "                df[col] = value\n",
    "            else:\n",
    "                # 设置默认值\n",
    "                if col == 'depth':\n",
    "                    df[col] = 3\n",
    "                elif col == 'gamma':\n",
    "                    df[col] = 0.05\n",
    "                elif col == 'alpha':\n",
    "                    df[col] = 0.1\n",
    "                else:\n",
    "                    df[col] = None\n",
    "    \n",
    "    all_rows.append(df)\n",
    "\n",
    "summary_df = pd.concat(all_rows, ignore_index=True)\n",
    "summary_df.to_csv(os.path.join(base_path, \"all_result_layers_merged.csv\"), index=False)\n",
    "print(\"已汇总所有result_layers.csv到 all_result_layers_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5ffe640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已生成每组参数每层的均值表 all_params_layer_mean.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step2\"\n",
    "pattern = os.path.join(base_path, \"**\", \"result_layers.csv\")\n",
    "files = glob.glob(pattern, recursive=True)\n",
    "\n",
    "all_rows = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "    folder = os.path.dirname(file)\n",
    "    folder_name = os.path.basename(folder)\n",
    "    \n",
    "    # 补充参数信息，从文件夹名称提取\n",
    "    for col in ['depth', 'gamma', 'eta', 'alpha']:\n",
    "        if col not in df.columns:\n",
    "            if f\"{col}_\" in folder_name:\n",
    "                try:\n",
    "                    value = float(folder_name.split(f\"{col}_\")[1].split(\"_\")[0])\n",
    "                except:\n",
    "                    value = None\n",
    "                df[col] = value\n",
    "            else:\n",
    "                # 设置默认值\n",
    "                if col == 'depth':\n",
    "                    df[col] = 3\n",
    "                elif col == 'gamma':\n",
    "                    df[col] = 0.05\n",
    "                elif col == 'alpha':\n",
    "                    df[col] = 0.1\n",
    "                else:\n",
    "                    df[col] = None\n",
    "    all_rows.append(df)\n",
    "\n",
    "merged = pd.concat(all_rows, ignore_index=True)\n",
    "\n",
    "# 按参数组和layer分组，计算均值和标准差\n",
    "group_cols = ['depth', 'gamma', 'eta', 'alpha', 'layer']\n",
    "summary = merged.groupby(group_cols).agg({\n",
    "    'entropy_wavg': ['mean', 'std'],\n",
    "    'distinctiveness_wavg_jsd': ['mean', 'std'],\n",
    "    'nodes_in_layer': ['mean', 'std'],\n",
    "}).reset_index()\n",
    "\n",
    "# 展开多级列名\n",
    "summary.columns = ['_'.join(col).strip('_') for col in summary.columns]\n",
    "\n",
    "summary.to_csv(os.path.join(base_path, \"all_params_layer_mean.csv\"), index=False)\n",
    "print(\"已生成每组参数每层的均值表 all_params_layer_mean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2aaaca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总共找到 18 个文件\n",
      "文件路径列表:\n",
      " 1. eta=0.005 | depth_3_gamma_0.05_eta_0.005_run_1\n",
      " 2. eta=0.005 | depth_3_gamma_0.05_eta_0.005_run_2\n",
      " 3. eta=0.005 | depth_3_gamma_0.05_eta_0.005_run_3\n",
      " 4. eta=0.01 | depth_3_gamma_0.05_eta_0.01_run_1\n",
      " 5. eta=0.01 | depth_3_gamma_0.05_eta_0.01_run_2\n",
      " 6. eta=0.01 | depth_3_gamma_0.05_eta_0.01_run_3\n",
      " 7. eta=0.02 | depth_3_gamma_0.05_eta_0.02_run_1\n",
      " 8. eta=0.02 | depth_3_gamma_0.05_eta_0.02_run_2\n",
      " 9. eta=0.02 | depth_3_gamma_0.05_eta_0.02_run_3\n",
      "10. eta=0.05 | depth_3_gamma_0.05_eta_0.05_run_1\n",
      "11. eta=0.05 | depth_3_gamma_0.05_eta_0.05_run_2\n",
      "12. eta=0.05 | depth_3_gamma_0.05_eta_0.05_run_3\n",
      "13. eta=0.1 | depth_3_gamma_0.05_eta_0.1_run_1\n",
      "14. eta=0.1 | depth_3_gamma_0.05_eta_0.1_run_2\n",
      "15. eta=0.1 | depth_3_gamma_0.05_eta_0.1_run_3\n",
      "16. eta=0.2 | depth_3_gamma_0.05_eta_0.2_run_1\n",
      "17. eta=0.2 | depth_3_gamma_0.05_eta_0.2_run_2\n",
      "18. eta=0.2 | depth_3_gamma_0.05_eta_0.2_run_3\n",
      "\n",
      "✓ 没有重复文件路径\n"
     ]
    }
   ],
   "source": [
    "# 简单检查文件数量和路径\n",
    "import os\n",
    "import glob\n",
    "\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step2\"\n",
    "pattern = os.path.join(base_path, \"**\", \"iteration_node_word_distributions.csv\")\n",
    "files = glob.glob(pattern, recursive=True)\n",
    "\n",
    "print(f\"总共找到 {len(files)} 个文件\")\n",
    "print(\"文件路径列表:\")\n",
    "for i, file_path in enumerate(sorted(files), 1):\n",
    "    folder_name = os.path.basename(os.path.dirname(file_path))\n",
    "    # 提取eta值\n",
    "    eta = \"未知\"\n",
    "    if 'eta_' in folder_name:\n",
    "        try:\n",
    "            eta_part = folder_name.split('eta_')[1].split('_')[0]\n",
    "            eta = float(eta_part)\n",
    "        except:\n",
    "            pass\n",
    "    print(f\"{i:2d}. eta={eta} | {folder_name}\")\n",
    "\n",
    "# 检查是否有重复路径\n",
    "if len(files) != len(set(files)):\n",
    "    print(\"\\n⚠️  发现重复文件路径！\")\n",
    "    from collections import Counter\n",
    "    counter = Counter(files)\n",
    "    for file_path, count in counter.items():\n",
    "        if count > 1:\n",
    "            print(f\"重复 {count} 次: {file_path}\")\n",
    "else:\n",
    "    print(\"\\n✓ 没有重复文件路径\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9953d2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️  开始清理计算有误的扩展指标文件...\n",
      "============================================================\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/d3_g005_收敛/depth_3_gamma_0.05_run_1/extended_metrics.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/d3_g005_收敛/depth_3_gamma_0.05_run_2/extended_metrics.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/d3_g005_收敛/depth_3_gamma_0.05_run_3/extended_metrics.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_2/extended_metrics.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_3/extended_metrics.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_1/extended_metrics.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_3/extended_metrics.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_1/extended_metrics.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_2/extended_metrics.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_3/extended_metrics.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_1/extended_metrics.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_2/extended_metrics.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_3/extended_metrics.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_2/extended_metrics.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_1/extended_metrics.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_1/extended_metrics.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_3/extended_metrics.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_2/extended_metrics.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_2/extended_metrics.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_1/extended_metrics.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_3/extended_metrics.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/d3_g005_收敛/depth_3_gamma_0.05_run_1/extended_metrics_corrected.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/d3_g005_收敛/depth_3_gamma_0.05_run_2/extended_metrics_corrected.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/d3_g005_收敛/depth_3_gamma_0.05_run_3/extended_metrics_corrected.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_2/extended_metrics_corrected.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_3/extended_metrics_corrected.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_1/extended_metrics_corrected.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_3/extended_metrics_corrected.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_1/extended_metrics_corrected.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_2/extended_metrics_corrected.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_3/extended_metrics_corrected.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_1/extended_metrics_corrected.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_2/extended_metrics_corrected.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_3/extended_metrics_corrected.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_2/extended_metrics_corrected.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_1/extended_metrics_corrected.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_1/extended_metrics_corrected.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_3/extended_metrics_corrected.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_2/extended_metrics_corrected.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/eta_0.1_extended_summary.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/eta_0.05_extended_summary.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/eta_0.005_extended_summary.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/eta_0.02_extended_summary.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/eta_0.2_extended_summary.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/eta_0.01_extended_summary.csv\n",
      "✓ 已删除: /Volumes/My Passport/收敛结果/step2/eta_extended_metrics_comparison.csv\n",
      "============================================================\n",
      "🎯 清理完成！共删除 46 个文件\n",
      "============================================================\n",
      "📋 保留的正确文件:\n",
      "  corrected_renyi_entropy.csv: 21 个文件\n",
      "  jensen_shannon_distances.csv: 21 个文件\n",
      "  layer_average_js_distances.csv: 21 个文件\n",
      "  *layer_summary.csv: 9 个文件\n",
      "  result_layers.csv: 21 个文件\n",
      "✅ 共保留 93 个正确的指标文件\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def clean_incorrect_metric_files(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    删除计算有误的扩展指标文件\n",
    "    \"\"\"\n",
    "    files_to_delete = [\n",
    "        \"extended_metrics.csv\",\n",
    "        \"extended_metrics_corrected.csv\", \n",
    "        \"*extended_summary.csv\",\n",
    "        \"eta_extended_metrics_comparison.csv\"\n",
    "    ]\n",
    "    \n",
    "    deleted_count = 0\n",
    "    \n",
    "    print(\"🗑️  开始清理计算有误的扩展指标文件...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for pattern in files_to_delete:\n",
    "        # 查找所有匹配的文件\n",
    "        search_pattern = os.path.join(base_path, \"**\", pattern)\n",
    "        files = glob.glob(search_pattern, recursive=True)\n",
    "        \n",
    "        for file_path in files:\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                print(f\"✓ 已删除: {file_path}\")\n",
    "                deleted_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"❌ 删除失败: {file_path} - {e}\")\n",
    "    \n",
    "    # 也删除base_path下的汇总文件\n",
    "    base_files = [\n",
    "        \"eta_extended_metrics_comparison.csv\",\n",
    "        \"all_extended_metrics.csv\"\n",
    "    ]\n",
    "    \n",
    "    for filename in base_files:\n",
    "        file_path = os.path.join(base_path, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                print(f\"✓ 已删除汇总文件: {file_path}\")\n",
    "                deleted_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"❌ 删除汇总文件失败: {file_path} - {e}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"🎯 清理完成！共删除 {deleted_count} 个文件\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 显示清理后保留的文件\n",
    "    print(\"📋 保留的正确文件:\")\n",
    "    preserved_patterns = [\n",
    "        \"corrected_renyi_entropy.csv\",\n",
    "        \"jensen_shannon_distances.csv\", \n",
    "        \"layer_average_js_distances.csv\",\n",
    "        \"*layer_summary.csv\",\n",
    "        \"result_layers.csv\"\n",
    "    ]\n",
    "    \n",
    "    preserved_count = 0\n",
    "    for pattern in preserved_patterns:\n",
    "        search_pattern = os.path.join(base_path, \"**\", pattern)\n",
    "        files = glob.glob(search_pattern, recursive=True)\n",
    "        preserved_count += len(files)\n",
    "        if files:\n",
    "            print(f\"  {pattern}: {len(files)} 个文件\")\n",
    "    \n",
    "    print(f\"✅ 共保留 {preserved_count} 个正确的指标文件\")\n",
    "\n",
    "# 执行清理\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step2\"\n",
    "clean_incorrect_metric_files(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2ac939d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "def calculate_standard_coherence_from_corpus_corrected(corpus, word_distributions_df, top_k=15):\n",
    "    \"\"\"\n",
    "    修正版：计算完整的节点级和全局级一致性指标\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"📊 准备计算标准一致性指标...\")\n",
    "    print(f\"   语料文档数: {len(corpus)}\")\n",
    "    print(f\"   节点数: {word_distributions_df['node_id'].nunique()}\")\n",
    "    \n",
    "    # 1. 准备texts和dictionary\n",
    "    texts = list(corpus.values())\n",
    "    dictionary = Dictionary(texts)\n",
    "    \n",
    "    print(f\"   文档总数: {len(texts)}\")\n",
    "    print(f\"   词典大小: {len(dictionary)}\")\n",
    "    \n",
    "    # 2. 准备topics和节点映射\n",
    "    topics = []\n",
    "    node_topic_mapping = {}\n",
    "    node_to_topic_idx = {}  # 新增：节点ID到主题索引的直接映射\n",
    "    \n",
    "    topic_idx = 0\n",
    "    for node_id in word_distributions_df['node_id'].unique():\n",
    "        node_data = word_distributions_df[word_distributions_df['node_id'] == node_id]\n",
    "        top_words = node_data.nlargest(top_k, 'count')['word'].tolist()\n",
    "        \n",
    "        valid_words = []\n",
    "        for word in top_words:\n",
    "            if pd.notna(word) and word in dictionary.token2id:\n",
    "                valid_words.append(word)\n",
    "        \n",
    "        if len(valid_words) >= 2:\n",
    "            topics.append(valid_words)\n",
    "            node_topic_mapping[node_id] = valid_words\n",
    "            node_to_topic_idx[node_id] = topic_idx  # 直接映射\n",
    "            topic_idx += 1\n",
    "    \n",
    "    print(f\"   有效主题数: {len(topics)}\")\n",
    "    \n",
    "    if len(topics) == 0:\n",
    "        return {}, {}, {}\n",
    "    \n",
    "    # 3. 计算所有一致性指标（全局+每主题）\n",
    "    coherence_measures = ['c_npmi', 'c_v', 'u_mass']\n",
    "    global_coherence = {}\n",
    "    per_topic_coherence = {}\n",
    "    \n",
    "    for measure in coherence_measures:\n",
    "        try:\n",
    "            print(f\"   正在计算 {measure}...\")\n",
    "            \n",
    "            cm = CoherenceModel(\n",
    "                topics=topics,\n",
    "                texts=texts,\n",
    "                dictionary=dictionary,\n",
    "                coherence=measure,\n",
    "                processes=1\n",
    "            )\n",
    "            \n",
    "            # 全局平均一致性\n",
    "            global_score = cm.get_coherence()\n",
    "            global_coherence[measure] = global_score\n",
    "            \n",
    "            # 每个主题的一致性\n",
    "            per_topic_scores = cm.get_coherence_per_topic()\n",
    "            per_topic_coherence[measure] = per_topic_scores\n",
    "            \n",
    "            print(f\"   ✓ {measure}: 全局={global_score:.4f}, 范围=[{min(per_topic_scores):.4f}, {max(per_topic_scores):.4f}]\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 计算 {measure} 时出错: {e}\")\n",
    "            global_coherence[measure] = 0.0\n",
    "            per_topic_coherence[measure] = [0.0] * len(topics)\n",
    "    \n",
    "    return global_coherence, per_topic_coherence, node_to_topic_idx\n",
    "\n",
    "def process_coherence_with_original_corpus_corrected(base_path=\".\", corpus=None, top_k=15):\n",
    "    \"\"\"\n",
    "    修正版：完整计算节点级和全局级一致性指标\n",
    "    \"\"\"\n",
    "    \n",
    "    if corpus is None:\n",
    "        print(\"❌ 必须提供原始语料corpus\")\n",
    "        return\n",
    "    \n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_node_word_distributions.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    print(f\"🔍 找到 {len(files)} 个词分布文件待处理\")\n",
    "    \n",
    "    for idx, file_path in enumerate(files, 1):\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        \n",
    "        # 参数提取\n",
    "        eta = 0.1\n",
    "        gamma = 0.05\n",
    "        depth = 3\n",
    "        alpha = 0.1\n",
    "        \n",
    "        for param_name in ['eta', 'gamma', 'depth', 'alpha']:\n",
    "            if f'{param_name}_' in folder_name:\n",
    "                try:\n",
    "                    param_part = folder_name.split(f'{param_name}_')[1].split('_')[0]\n",
    "                    if param_name == 'depth':\n",
    "                        locals()[param_name] = int(param_part)\n",
    "                    else:\n",
    "                        locals()[param_name] = float(param_part)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"[{idx}/{len(files)}] 处理文件: {folder_name}\")\n",
    "        print(f\"参数 - Eta: {eta}, Gamma: {gamma}, Depth: {depth}, Alpha: {alpha}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            # 读取数据\n",
    "            df = pd.read_csv(file_path)\n",
    "            df.columns = [col.strip(\"'\\\" \") for col in df.columns]\n",
    "            \n",
    "            max_iteration = df['iteration'].max()\n",
    "            last_iteration_data = df[df['iteration'] == max_iteration]\n",
    "            \n",
    "            print(f\"📈 最后iteration: {max_iteration}\")\n",
    "            print(f\"📈 节点数: {last_iteration_data['node_id'].nunique()}\")\n",
    "            \n",
    "            # 计算一致性（修正版）\n",
    "            global_coherence, per_topic_coherence, node_to_topic_idx = calculate_standard_coherence_from_corpus_corrected(\n",
    "                corpus, last_iteration_data, top_k=top_k\n",
    "            )\n",
    "            \n",
    "            if not global_coherence:\n",
    "                print(\"⚠️ 一致性计算失败，跳过此文件\")\n",
    "                continue\n",
    "            \n",
    "            # 准备保存数据\n",
    "            results_data = []\n",
    "            \n",
    "            for node_id in last_iteration_data['node_id'].unique():\n",
    "                node_words = last_iteration_data[last_iteration_data['node_id'] == node_id]\n",
    "                top_words = node_words.nlargest(top_k, 'count')['word'].tolist()\n",
    "                top_words = [word for word in top_words if pd.notna(word)]\n",
    "                \n",
    "                # 获取该节点的各项一致性指标（修正版）\n",
    "                node_coherence_scores = {}\n",
    "                \n",
    "                if node_id in node_to_topic_idx:\n",
    "                    # 直接通过索引获取各项指标\n",
    "                    topic_idx = node_to_topic_idx[node_id]\n",
    "                    \n",
    "                    for measure in ['c_npmi', 'c_v', 'u_mass']:\n",
    "                        if measure in per_topic_coherence:\n",
    "                            measure_name = measure.replace('c_', '') if measure.startswith('c_') else measure\n",
    "                            node_coherence_scores[f'node_{measure_name}'] = per_topic_coherence[measure][topic_idx]\n",
    "                        else:\n",
    "                            measure_name = measure.replace('c_', '') if measure.startswith('c_') else measure\n",
    "                            node_coherence_scores[f'node_{measure_name}'] = 0.0\n",
    "                else:\n",
    "                    # 如果节点不在映射中，设为0\n",
    "                    for measure in ['npmi', 'v', 'u_mass']:\n",
    "                        node_coherence_scores[f'node_{measure}'] = 0.0\n",
    "                \n",
    "                results_data.append({\n",
    "                    'node_id': node_id,\n",
    "                    'eta': eta,\n",
    "                    'gamma': gamma, \n",
    "                    'depth': depth,\n",
    "                    'alpha': alpha,\n",
    "                    'top_k': top_k,\n",
    "                    'top_words': ', '.join(top_words[:10]),\n",
    "                    'word_count': len(top_words),\n",
    "                    \n",
    "                    # 节点级一致性指标（修正）\n",
    "                    'node_npmi': node_coherence_scores.get('node_npmi', 0.0),\n",
    "                    'node_c_v': node_coherence_scores.get('node_v', 0.0),\n",
    "                    'node_u_mass': node_coherence_scores.get('node_u_mass', 0.0),\n",
    "                    \n",
    "                    # 全局级一致性指标\n",
    "                    'global_npmi': global_coherence.get('c_npmi', 0.0),\n",
    "                    'global_c_v': global_coherence.get('c_v', 0.0),\n",
    "                    'global_u_mass': global_coherence.get('u_mass', 0.0),\n",
    "                    \n",
    "                    'iteration': max_iteration\n",
    "                })\n",
    "            \n",
    "            # 保存结果\n",
    "            results_df = pd.DataFrame(results_data)\n",
    "            output_path = os.path.join(folder_path, 'standard_coherence.csv')\n",
    "            results_df.to_csv(output_path, index=False)\n",
    "            \n",
    "            print(f\"💾 标准一致性结果已保存到: {output_path}\")\n",
    "            print(f\"📊 结果摘要:\")\n",
    "            print(f\"   - 全局NPMI: {global_coherence.get('c_npmi', 0.0):.4f}\")\n",
    "            print(f\"   - 全局C_V: {global_coherence.get('c_v', 0.0):.4f}\")\n",
    "            print(f\"   - 全局U_Mass: {global_coherence.get('u_mass', 0.0):.4f}\")\n",
    "            \n",
    "            # 显示节点级指标范围\n",
    "            if len(results_df) > 0:\n",
    "                print(f\"   - 节点NPMI范围: [{results_df['node_npmi'].min():.4f}, {results_df['node_npmi'].max():.4f}]\")\n",
    "                print(f\"   - 节点C_V范围: [{results_df['node_c_v'].min():.4f}, {results_df['node_c_v'].max():.4f}]\")\n",
    "                print(f\"   - 节点U_Mass范围: [{results_df['node_u_mass'].min():.4f}, {results_df['node_u_mass'].max():.4f}]\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"❌ 处理文件 {file_path} 时出错: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    print(f\"\\n✅ 所有文件的标准一致性计算完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fc225850",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 0. set-up part:  import necessary libraries and set up environment \"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from threading import Thread\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "import json\n",
    "import cProfile\n",
    "\n",
    "# download nltk data once time\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "#  chinese character support in matplotlib\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS' 'SimHei' 'DejaVu Sans']  \n",
    "plt.rcParams['axes.unicode_minus'] = False  \n",
    "\n",
    "\n",
    "\"\"\" 1.1 Data Preprocessing: load data, clean text, lemmatization, remove low-frequency words\"\"\"\n",
    "\n",
    "# Map POS tags to WordNet format， Penn Treebank annotation: fine-grained (45 tags), WordNet annotation: coarse-grained (4 tags: a, v, n, r)\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'  # 形容词\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'  # 动词\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n'  # 名词\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'  # 副词\n",
    "    else:\n",
    "        return 'n'  # 默认名词\n",
    "\n",
    "# Text cleaning and lemmatization preprocessing function\n",
    "def clean_and_lemmatize(text):\n",
    "    if pd.isnull(text):\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove non-alphabetic characters using regex\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmatized = [lemmatizer.lemmatize(w, get_wordnet_pos(pos)) for w, pos in pos_tags]\n",
    "    return lemmatized  \n",
    "\n",
    "#-----------------Load data----------------\n",
    "data = pd.read_excel('/Volumes/My Passport/收敛结果/step2/papers_CM.xlsx', usecols=['PaperID', 'Abstract', 'Keywords', 'Year'])\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# clean and lemmatize the abstracts\n",
    "data['Lemmatized_Tokens'] = data['Abstract'].apply(clean_and_lemmatize)\n",
    "\n",
    "# count word frequencies\n",
    "all_tokens = [word for tokens in data['Lemmatized_Tokens'] for word in tokens]\n",
    "word_counts = Counter(all_tokens)\n",
    "\n",
    "# set a minimum frequency threshold for valid words\n",
    "min_freq = 10\n",
    "valid_words = set([word for word, freq in word_counts.items() if freq >= min_freq])\n",
    "\n",
    "# remove rare words based on frequency threshold\n",
    "def remove_rare_words(tokens):\n",
    "    return [word for word in tokens if word in valid_words]\n",
    "\n",
    "data['Filtered_Tokens'] = data['Lemmatized_Tokens'].apply(remove_rare_words)\n",
    "\n",
    "# join tokens back into cleaned abstracts\n",
    "data['Cleaned_Abstract'] = data['Filtered_Tokens'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "# create a cleaned DataFrame with relevant columns\n",
    "cleaned_data = data[['PaperID', 'Year', 'Cleaned_Abstract']]\n",
    "cleaned_data = cleaned_data[~(cleaned_data['PaperID'] == 57188)] # this paper has no abstract\n",
    "cleaned_data = cleaned_data.reset_index(drop=True) \n",
    "cleaned_data.insert(0, 'Document_ID', range(len(cleaned_data))) \n",
    "abstract_list = cleaned_data['Cleaned_Abstract'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "corpus = {doc_id: abstract_list for doc_id, abstract_list in enumerate(abstract_list)}\n",
    "# cleaned_data.to_csv('./data/processed/cleaned_data.xlsx', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "13c96c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🗑️ 清理旧的不完整文件...\n",
      "================================================================================\n",
      "✓ 删除旧文件: depth_3_gamma_0.05_eta_0.1_run_2\n",
      "✓ 删除旧文件: depth_3_gamma_0.05_eta_0.1_run_3\n",
      "✓ 删除旧文件: depth_3_gamma_0.05_eta_0.1_run_1\n",
      "🗑️ 共删除 3 个旧的一致性文件\n",
      "\n",
      "================================================================================\n",
      "🔄 开始重新计算完整的一致性指标...\n",
      "================================================================================\n",
      "🔍 找到 18 个词分布文件待处理\n",
      "\n",
      "================================================================================\n",
      "[1/18] 处理文件: depth_3_gamma_0.05_eta_0.1_run_2\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 175\n",
      "📈 节点数: 231\n",
      "📊 准备计算标准一致性指标...\n",
      "   语料文档数: 970\n",
      "   节点数: 231\n",
      "   文档总数: 970\n",
      "   词典大小: 1490\n",
      "   有效主题数: 231\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 全局=0.0460, 范围=[-0.4494, 0.7414]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 全局=0.5559, 范围=[0.1778, 0.9900]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 全局=-2.9908, 范围=[-14.9995, -0.3317]\n",
      "💾 标准一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_2/standard_coherence.csv\n",
      "📊 结果摘要:\n",
      "   - 全局NPMI: 0.0460\n",
      "   - 全局C_V: 0.5559\n",
      "   - 全局U_Mass: -2.9908\n",
      "   - 节点NPMI范围: [-0.4494, 0.7414]\n",
      "   - 节点C_V范围: [0.1778, 0.9900]\n",
      "   - 节点U_Mass范围: [-14.9995, -0.3317]\n",
      "\n",
      "================================================================================\n",
      "[2/18] 处理文件: depth_3_gamma_0.05_eta_0.1_run_3\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 175\n",
      "📈 节点数: 215\n",
      "📊 准备计算标准一致性指标...\n",
      "   语料文档数: 970\n",
      "   节点数: 215\n",
      "   文档总数: 970\n",
      "   词典大小: 1490\n",
      "   有效主题数: 215\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 全局=0.0681, 范围=[-0.3200, 0.7823]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 全局=0.5683, 范围=[0.1904, 0.9937]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 全局=-2.9775, 范围=[-11.5219, -0.3196]\n",
      "💾 标准一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_3/standard_coherence.csv\n",
      "📊 结果摘要:\n",
      "   - 全局NPMI: 0.0681\n",
      "   - 全局C_V: 0.5683\n",
      "   - 全局U_Mass: -2.9775\n",
      "   - 节点NPMI范围: [-0.3200, 0.7823]\n",
      "   - 节点C_V范围: [0.1904, 0.9937]\n",
      "   - 节点U_Mass范围: [-11.5219, -0.3196]\n",
      "\n",
      "================================================================================\n",
      "[3/18] 处理文件: depth_3_gamma_0.05_eta_0.1_run_1\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 175\n",
      "📈 节点数: 205\n",
      "📊 准备计算标准一致性指标...\n",
      "   语料文档数: 970\n",
      "   节点数: 205\n",
      "   文档总数: 970\n",
      "   词典大小: 1490\n",
      "   有效主题数: 205\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 全局=0.0657, 范围=[-0.3240, 0.4322]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 全局=0.5714, 范围=[0.2010, 0.9101]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 全局=-2.6619, 范围=[-11.6260, -0.6346]\n",
      "💾 标准一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_1/standard_coherence.csv\n",
      "📊 结果摘要:\n",
      "   - 全局NPMI: 0.0657\n",
      "   - 全局C_V: 0.5714\n",
      "   - 全局U_Mass: -2.6619\n",
      "   - 节点NPMI范围: [-0.3240, 0.4322]\n",
      "   - 节点C_V范围: [0.2010, 0.9101]\n",
      "   - 节点U_Mass范围: [-11.6260, -0.6346]\n",
      "\n",
      "================================================================================\n",
      "[4/18] 处理文件: depth_3_gamma_0.05_eta_0.05_run_3\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 90\n",
      "📈 节点数: 322\n",
      "📊 准备计算标准一致性指标...\n",
      "   语料文档数: 970\n",
      "   节点数: 322\n",
      "   文档总数: 970\n",
      "   词典大小: 1490\n",
      "   有效主题数: 322\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 全局=0.0640, 范围=[-0.4658, 0.7414]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 全局=0.5491, 范围=[0.1757, 0.9900]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 全局=-2.8860, 范围=[-13.1559, -0.4397]\n",
      "💾 标准一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_3/standard_coherence.csv\n",
      "📊 结果摘要:\n",
      "   - 全局NPMI: 0.0640\n",
      "   - 全局C_V: 0.5491\n",
      "   - 全局U_Mass: -2.8860\n",
      "   - 节点NPMI范围: [-0.4658, 0.7414]\n",
      "   - 节点C_V范围: [0.1757, 0.9900]\n",
      "   - 节点U_Mass范围: [-13.1559, -0.4397]\n",
      "\n",
      "================================================================================\n",
      "[5/18] 处理文件: depth_3_gamma_0.05_eta_0.05_run_1\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 90\n",
      "📈 节点数: 315\n",
      "📊 准备计算标准一致性指标...\n",
      "   语料文档数: 970\n",
      "   节点数: 315\n",
      "   文档总数: 970\n",
      "   词典大小: 1490\n",
      "   有效主题数: 315\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 全局=0.0474, 范围=[-0.3846, 0.5347]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 全局=0.5476, 范围=[0.2051, 0.9493]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 全局=-2.8742, 范围=[-13.0123, -0.5701]\n",
      "💾 标准一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_1/standard_coherence.csv\n",
      "📊 结果摘要:\n",
      "   - 全局NPMI: 0.0474\n",
      "   - 全局C_V: 0.5476\n",
      "   - 全局U_Mass: -2.8742\n",
      "   - 节点NPMI范围: [-0.3846, 0.5347]\n",
      "   - 节点C_V范围: [0.2051, 0.9493]\n",
      "   - 节点U_Mass范围: [-13.0123, -0.5701]\n",
      "\n",
      "================================================================================\n",
      "[6/18] 处理文件: depth_3_gamma_0.05_eta_0.05_run_2\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 90\n",
      "📈 节点数: 299\n",
      "📊 准备计算标准一致性指标...\n",
      "   语料文档数: 970\n",
      "   节点数: 299\n",
      "   文档总数: 970\n",
      "   词典大小: 1490\n",
      "   有效主题数: 299\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 全局=0.0626, 范围=[-0.3949, 0.4990]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 全局=0.5506, 范围=[0.1782, 0.9313]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 全局=-2.6775, 范围=[-13.4126, -0.3905]\n",
      "💾 标准一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_2/standard_coherence.csv\n",
      "📊 结果摘要:\n",
      "   - 全局NPMI: 0.0626\n",
      "   - 全局C_V: 0.5506\n",
      "   - 全局U_Mass: -2.6775\n",
      "   - 节点NPMI范围: [-0.3949, 0.4990]\n",
      "   - 节点C_V范围: [0.1782, 0.9313]\n",
      "   - 节点U_Mass范围: [-13.4126, -0.3905]\n",
      "\n",
      "================================================================================\n",
      "[7/18] 处理文件: depth_3_gamma_0.05_eta_0.005_run_3\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 115\n",
      "📈 节点数: 427\n",
      "📊 准备计算标准一致性指标...\n",
      "   语料文档数: 970\n",
      "   节点数: 427\n",
      "   文档总数: 970\n",
      "   词典大小: 1490\n",
      "   有效主题数: 427\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 全局=0.0392, 范围=[-0.5759, 0.7414]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 全局=0.5555, 范围=[0.2086, 0.9900]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 全局=-2.7249, 范围=[-12.4467, -0.3742]\n",
      "💾 标准一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_3/standard_coherence.csv\n",
      "📊 结果摘要:\n",
      "   - 全局NPMI: 0.0392\n",
      "   - 全局C_V: 0.5555\n",
      "   - 全局U_Mass: -2.7249\n",
      "   - 节点NPMI范围: [-0.5759, 0.7414]\n",
      "   - 节点C_V范围: [0.2086, 0.9900]\n",
      "   - 节点U_Mass范围: [-12.4467, -0.3742]\n",
      "\n",
      "================================================================================\n",
      "[8/18] 处理文件: depth_3_gamma_0.05_eta_0.005_run_1\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 115\n",
      "📈 节点数: 438\n",
      "📊 准备计算标准一致性指标...\n",
      "   语料文档数: 970\n",
      "   节点数: 438\n",
      "   文档总数: 970\n",
      "   词典大小: 1490\n",
      "   有效主题数: 438\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 全局=0.0465, 范围=[-0.4431, 0.4935]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 全局=0.5667, 范围=[0.1066, 0.9101]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 全局=-2.7149, 范围=[-14.4683, -0.5701]\n",
      "💾 标准一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_1/standard_coherence.csv\n",
      "📊 结果摘要:\n",
      "   - 全局NPMI: 0.0465\n",
      "   - 全局C_V: 0.5667\n",
      "   - 全局U_Mass: -2.7149\n",
      "   - 节点NPMI范围: [-0.4431, 0.4935]\n",
      "   - 节点C_V范围: [0.1066, 0.9101]\n",
      "   - 节点U_Mass范围: [-14.4683, -0.5701]\n",
      "\n",
      "================================================================================\n",
      "[9/18] 处理文件: depth_3_gamma_0.05_eta_0.005_run_2\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 115\n",
      "📈 节点数: 432\n",
      "📊 准备计算标准一致性指标...\n",
      "   语料文档数: 970\n",
      "   节点数: 432\n",
      "   文档总数: 970\n",
      "   词典大小: 1490\n",
      "   有效主题数: 432\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 全局=0.0577, 范围=[-0.4747, 0.7756]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 全局=0.5530, 范围=[0.1305, 0.9918]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 全局=-2.6511, 范围=[-12.7344, -0.2982]\n",
      "💾 标准一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_2/standard_coherence.csv\n",
      "📊 结果摘要:\n",
      "   - 全局NPMI: 0.0577\n",
      "   - 全局C_V: 0.5530\n",
      "   - 全局U_Mass: -2.6511\n",
      "   - 节点NPMI范围: [-0.4747, 0.7756]\n",
      "   - 节点C_V范围: [0.1305, 0.9918]\n",
      "   - 节点U_Mass范围: [-12.7344, -0.2982]\n",
      "\n",
      "================================================================================\n",
      "[10/18] 处理文件: depth_3_gamma_0.05_eta_0.02_run_3\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 155\n",
      "📈 节点数: 388\n",
      "📊 准备计算标准一致性指标...\n",
      "   语料文档数: 970\n",
      "   节点数: 388\n",
      "   文档总数: 970\n",
      "   词典大小: 1490\n",
      "   有效主题数: 388\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 全局=0.0549, 范围=[-0.5662, 0.7414]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 全局=0.5483, 范围=[0.1983, 0.9900]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 全局=-2.6558, 范围=[-15.5162, -0.4020]\n",
      "💾 标准一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_3/standard_coherence.csv\n",
      "📊 结果摘要:\n",
      "   - 全局NPMI: 0.0549\n",
      "   - 全局C_V: 0.5483\n",
      "   - 全局U_Mass: -2.6558\n",
      "   - 节点NPMI范围: [-0.5662, 0.7414]\n",
      "   - 节点C_V范围: [0.1983, 0.9900]\n",
      "   - 节点U_Mass范围: [-15.5162, -0.4020]\n",
      "\n",
      "================================================================================\n",
      "[11/18] 处理文件: depth_3_gamma_0.05_eta_0.02_run_2\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 155\n",
      "📈 节点数: 384\n",
      "📊 准备计算标准一致性指标...\n",
      "   语料文档数: 970\n",
      "   节点数: 384\n",
      "   文档总数: 970\n",
      "   词典大小: 1490\n",
      "   有效主题数: 383\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 全局=0.0529, 范围=[-0.4267, 0.7756]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 全局=0.5580, 范围=[0.1656, 0.9918]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 全局=-2.7573, 范围=[-15.8418, -0.2664]\n",
      "💾 标准一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_2/standard_coherence.csv\n",
      "📊 结果摘要:\n",
      "   - 全局NPMI: 0.0529\n",
      "   - 全局C_V: 0.5580\n",
      "   - 全局U_Mass: -2.7573\n",
      "   - 节点NPMI范围: [-0.4267, 0.7756]\n",
      "   - 节点C_V范围: [0.0000, 0.9918]\n",
      "   - 节点U_Mass范围: [-15.8418, 0.0000]\n",
      "\n",
      "================================================================================\n",
      "[12/18] 处理文件: depth_3_gamma_0.05_eta_0.02_run_1\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 155\n",
      "📈 节点数: 383\n",
      "📊 准备计算标准一致性指标...\n",
      "   语料文档数: 970\n",
      "   节点数: 383\n",
      "   文档总数: 970\n",
      "   词典大小: 1490\n",
      "   有效主题数: 383\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 全局=0.0539, 范围=[-0.4210, 0.7823]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 全局=0.5535, 范围=[0.2070, 0.9937]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 全局=-2.5675, 范围=[-11.1053, -0.3447]\n",
      "💾 标准一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_1/standard_coherence.csv\n",
      "📊 结果摘要:\n",
      "   - 全局NPMI: 0.0539\n",
      "   - 全局C_V: 0.5535\n",
      "   - 全局U_Mass: -2.5675\n",
      "   - 节点NPMI范围: [-0.4210, 0.7823]\n",
      "   - 节点C_V范围: [0.2070, 0.9937]\n",
      "   - 节点U_Mass范围: [-11.1053, -0.3447]\n",
      "\n",
      "================================================================================\n",
      "[13/18] 处理文件: depth_3_gamma_0.05_eta_0.2_run_1\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 375\n",
      "📈 节点数: 151\n",
      "📊 准备计算标准一致性指标...\n",
      "   语料文档数: 970\n",
      "   节点数: 151\n",
      "   文档总数: 970\n",
      "   词典大小: 1490\n",
      "   有效主题数: 151\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 全局=0.0356, 范围=[-0.3866, 0.4266]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 全局=0.5516, 范围=[0.1761, 0.9002]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 全局=-3.2340, 范围=[-12.0902, -0.5699]\n",
      "💾 标准一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_1/standard_coherence.csv\n",
      "📊 结果摘要:\n",
      "   - 全局NPMI: 0.0356\n",
      "   - 全局C_V: 0.5516\n",
      "   - 全局U_Mass: -3.2340\n",
      "   - 节点NPMI范围: [-0.3866, 0.4266]\n",
      "   - 节点C_V范围: [0.1761, 0.9002]\n",
      "   - 节点U_Mass范围: [-12.0902, -0.5699]\n",
      "\n",
      "================================================================================\n",
      "[14/18] 处理文件: depth_3_gamma_0.05_eta_0.2_run_3\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 375\n",
      "📈 节点数: 157\n",
      "📊 准备计算标准一致性指标...\n",
      "   语料文档数: 970\n",
      "   节点数: 157\n",
      "   文档总数: 970\n",
      "   词典大小: 1490\n",
      "   有效主题数: 157\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 全局=0.0391, 范围=[-0.4202, 0.3874]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 全局=0.5682, 范围=[0.1975, 0.9447]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 全局=-3.3077, 范围=[-13.0551, -0.5628]\n",
      "💾 标准一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_3/standard_coherence.csv\n",
      "📊 结果摘要:\n",
      "   - 全局NPMI: 0.0391\n",
      "   - 全局C_V: 0.5682\n",
      "   - 全局U_Mass: -3.3077\n",
      "   - 节点NPMI范围: [-0.4202, 0.3874]\n",
      "   - 节点C_V范围: [0.1975, 0.9447]\n",
      "   - 节点U_Mass范围: [-13.0551, -0.5628]\n",
      "\n",
      "================================================================================\n",
      "[15/18] 处理文件: depth_3_gamma_0.05_eta_0.2_run_2\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 375\n",
      "📈 节点数: 157\n",
      "📊 准备计算标准一致性指标...\n",
      "   语料文档数: 970\n",
      "   节点数: 157\n",
      "   文档总数: 970\n",
      "   词典大小: 1490\n",
      "   有效主题数: 157\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 全局=0.0382, 范围=[-0.4131, 0.5378]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 全局=0.5656, 范围=[0.1848, 0.9005]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 全局=-3.1619, 范围=[-13.0318, -0.5941]\n",
      "💾 标准一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_2/standard_coherence.csv\n",
      "📊 结果摘要:\n",
      "   - 全局NPMI: 0.0382\n",
      "   - 全局C_V: 0.5656\n",
      "   - 全局U_Mass: -3.1619\n",
      "   - 节点NPMI范围: [-0.4131, 0.5378]\n",
      "   - 节点C_V范围: [0.1848, 0.9005]\n",
      "   - 节点U_Mass范围: [-13.0318, -0.5941]\n",
      "\n",
      "================================================================================\n",
      "[16/18] 处理文件: depth_3_gamma_0.05_eta_0.01_run_2\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 195\n",
      "📈 节点数: 403\n",
      "📊 准备计算标准一致性指标...\n",
      "   语料文档数: 970\n",
      "   节点数: 403\n",
      "   文档总数: 970\n",
      "   词典大小: 1490\n",
      "   有效主题数: 403\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 全局=0.0551, 范围=[-0.5228, 0.7617]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 全局=0.5526, 范围=[0.1941, 0.9917]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 全局=-2.7532, 范围=[-13.5579, -0.3146]\n",
      "💾 标准一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_2/standard_coherence.csv\n",
      "📊 结果摘要:\n",
      "   - 全局NPMI: 0.0551\n",
      "   - 全局C_V: 0.5526\n",
      "   - 全局U_Mass: -2.7532\n",
      "   - 节点NPMI范围: [-0.5228, 0.7617]\n",
      "   - 节点C_V范围: [0.1941, 0.9917]\n",
      "   - 节点U_Mass范围: [-13.5579, -0.3146]\n",
      "\n",
      "================================================================================\n",
      "[17/18] 处理文件: depth_3_gamma_0.05_eta_0.01_run_1\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 195\n",
      "📈 节点数: 425\n",
      "📊 准备计算标准一致性指标...\n",
      "   语料文档数: 970\n",
      "   节点数: 425\n",
      "   文档总数: 970\n",
      "   词典大小: 1490\n",
      "   有效主题数: 425\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 全局=0.0601, 范围=[-0.4959, 0.4643]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 全局=0.5579, 范围=[0.1747, 0.9392]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 全局=-2.6749, 范围=[-13.3602, -0.5701]\n",
      "💾 标准一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_1/standard_coherence.csv\n",
      "📊 结果摘要:\n",
      "   - 全局NPMI: 0.0601\n",
      "   - 全局C_V: 0.5579\n",
      "   - 全局U_Mass: -2.6749\n",
      "   - 节点NPMI范围: [-0.4959, 0.4643]\n",
      "   - 节点C_V范围: [0.1747, 0.9392]\n",
      "   - 节点U_Mass范围: [-13.3602, -0.5701]\n",
      "\n",
      "================================================================================\n",
      "[18/18] 处理文件: depth_3_gamma_0.05_eta_0.01_run_3\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 195\n",
      "📈 节点数: 433\n",
      "📊 准备计算标准一致性指标...\n",
      "   语料文档数: 970\n",
      "   节点数: 433\n",
      "   文档总数: 970\n",
      "   词典大小: 1490\n",
      "   有效主题数: 433\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 全局=0.0554, 范围=[-0.4064, 0.5072]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 全局=0.5510, 范围=[0.1961, 0.9221]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 全局=-2.7079, 范围=[-10.7279, -0.5808]\n",
      "💾 标准一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_3/standard_coherence.csv\n",
      "📊 结果摘要:\n",
      "   - 全局NPMI: 0.0554\n",
      "   - 全局C_V: 0.5510\n",
      "   - 全局U_Mass: -2.7079\n",
      "   - 节点NPMI范围: [-0.4064, 0.5072]\n",
      "   - 节点C_V范围: [0.1961, 0.9221]\n",
      "   - 节点U_Mass范围: [-10.7279, -0.5808]\n",
      "\n",
      "✅ 所有文件的标准一致性计算完成！\n",
      "================================================================================\n",
      "✅ 修正版一致性计算完成！\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 删除旧的不完整文件\n",
    "def clean_old_coherence_files(base_path=\".\"):\n",
    "    \"\"\"删除旧的不完整的标准一致性文件\"\"\"\n",
    "    pattern = os.path.join(base_path, \"**\", \"standard_coherence.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    deleted_count = 0\n",
    "    for file_path in files:\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            print(f\"✓ 删除旧文件: {os.path.basename(os.path.dirname(file_path))}\")\n",
    "            deleted_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 删除失败: {file_path} - {e}\")\n",
    "    \n",
    "    print(f\"🗑️ 共删除 {deleted_count} 个旧的一致性文件\")\n",
    "\n",
    "# 执行修正版计算\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step2\"\n",
    "top_k = 5\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"🗑️ 清理旧的不完整文件...\")\n",
    "print(\"=\" * 80)\n",
    "clean_old_coherence_files(base_path)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🔄 开始重新计算完整的一致性指标...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 使用修正版函数\n",
    "process_coherence_with_original_corpus_corrected(base_path, corpus, top_k)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"✅ 修正版一致性计算完成！\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5329afab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def add_layer_and_document_info_to_coherence(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    将corrected_renyi_entropy.csv中的layer和document_count信息添加到standard_coherence.csv中\n",
    "    \n",
    "    Parameters:\n",
    "    base_path: str, 结果文件的根目录\n",
    "    \"\"\"\n",
    "    \n",
    "    # 查找所有包含standard_coherence.csv的文件夹\n",
    "    pattern = os.path.join(base_path, \"**\", \"standard_coherence.csv\")\n",
    "    coherence_files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    print(f\"🔍 找到 {len(coherence_files)} 个标准一致性文件待处理\")\n",
    "    \n",
    "    for idx, coherence_file_path in enumerate(coherence_files, 1):\n",
    "        folder_path = os.path.dirname(coherence_file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"[{idx}/{len(coherence_files)}] 处理文件夹: {folder_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # 检查对应的corrected_renyi_entropy.csv是否存在\n",
    "        entropy_file_path = os.path.join(folder_path, 'corrected_renyi_entropy.csv')\n",
    "        \n",
    "        if not os.path.exists(entropy_file_path):\n",
    "            print(f\"⚠️  未找到对应的entropy文件: {entropy_file_path}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # 读取两个文件\n",
    "            print(\"📖 读取文件...\")\n",
    "            coherence_df = pd.read_csv(coherence_file_path)\n",
    "            entropy_df = pd.read_csv(entropy_file_path)\n",
    "            \n",
    "            print(f\"   一致性文件: {len(coherence_df)} 行\")\n",
    "            print(f\"   熵文件: {len(entropy_df)} 行\")\n",
    "            \n",
    "            # 检查是否已经有layer和document_count列\n",
    "            existing_cols = coherence_df.columns.tolist()\n",
    "            has_layer = 'layer' in existing_cols\n",
    "            has_doc_count = 'document_count' in existing_cols\n",
    "            \n",
    "            print(f\"   当前列: {existing_cols}\")\n",
    "            print(f\"   已有layer列: {has_layer}\")\n",
    "            print(f\"   已有document_count列: {has_doc_count}\")\n",
    "            \n",
    "            # 创建node_id到layer和document_count的映射\n",
    "            node_layer_map = entropy_df.set_index('node_id')['layer'].to_dict()\n",
    "            node_doc_count_map = entropy_df.set_index('node_id')['document_count'].to_dict()\n",
    "            \n",
    "            print(f\"   可映射的节点数: {len(node_layer_map)}\")\n",
    "            \n",
    "            # 添加或更新layer列\n",
    "            if not has_layer:\n",
    "                coherence_df['layer'] = coherence_df['node_id'].map(node_layer_map)\n",
    "                print(\"   ✓ 添加了layer列\")\n",
    "            else:\n",
    "                coherence_df['layer'] = coherence_df['node_id'].map(node_layer_map)\n",
    "                print(\"   ✓ 更新了layer列\")\n",
    "            \n",
    "            # 添加或更新document_count列\n",
    "            if not has_doc_count:\n",
    "                coherence_df['document_count'] = coherence_df['node_id'].map(node_doc_count_map)\n",
    "                print(\"   ✓ 添加了document_count列\")\n",
    "            else:\n",
    "                coherence_df['document_count'] = coherence_df['node_id'].map(node_doc_count_map)\n",
    "                print(\"   ✓ 更新了document_count列\")\n",
    "            \n",
    "            # 检查映射结果\n",
    "            layer_null_count = coherence_df['layer'].isnull().sum()\n",
    "            doc_count_null_count = coherence_df['document_count'].isnull().sum()\n",
    "            \n",
    "            if layer_null_count > 0:\n",
    "                print(f\"   ⚠️  有 {layer_null_count} 个节点未找到layer信息\")\n",
    "            \n",
    "            if doc_count_null_count > 0:\n",
    "                print(f\"   ⚠️  有 {doc_count_null_count} 个节点未找到document_count信息\")\n",
    "            \n",
    "            # 显示层级分布统计\n",
    "            layer_stats = coherence_df['layer'].value_counts().sort_index()\n",
    "            print(f\"   📊 层级分布: {layer_stats.to_dict()}\")\n",
    "            \n",
    "            # 显示文档数统计\n",
    "            doc_stats = coherence_df['document_count'].describe()\n",
    "            print(f\"   📊 文档数统计:\")\n",
    "            print(f\"      最小值: {doc_stats['min']:.0f}\")\n",
    "            print(f\"      最大值: {doc_stats['max']:.0f}\")\n",
    "            print(f\"      平均值: {doc_stats['mean']:.1f}\")\n",
    "            \n",
    "            # 保存更新后的文件\n",
    "            coherence_df.to_csv(coherence_file_path, index=False)\n",
    "            print(f\"💾 已更新并保存: {coherence_file_path}\")\n",
    "            \n",
    "            # 显示更新后的列结构\n",
    "            updated_cols = coherence_df.columns.tolist()\n",
    "            print(f\"   更新后的列: {updated_cols}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"❌ 处理文件 {coherence_file_path} 时出错: {str(e)}\")\n",
    "            print(\"详细错误信息:\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    print(f\"\\n✅ 所有标准一致性文件的layer和document_count信息更新完成！\")\n",
    "\n",
    "def verify_coherence_files_update(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    验证standard_coherence.csv文件的更新情况\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(base_path, \"**\", \"standard_coherence.csv\")\n",
    "    coherence_files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    print(\"🔍 验证更新结果:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    all_have_layer = True\n",
    "    all_have_doc_count = True\n",
    "    \n",
    "    for file_path in coherence_files:\n",
    "        folder_name = os.path.basename(os.path.dirname(file_path))\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            has_layer = 'layer' in df.columns\n",
    "            has_doc_count = 'document_count' in df.columns\n",
    "            \n",
    "            layer_null = df['layer'].isnull().sum() if has_layer else \"无列\"\n",
    "            doc_null = df['document_count'].isnull().sum() if has_doc_count else \"无列\"\n",
    "            \n",
    "            status = \"✅\" if (has_layer and has_doc_count and layer_null == 0 and doc_null == 0) else \"⚠️\"\n",
    "            \n",
    "            print(f\"{status} {folder_name}\")\n",
    "            print(f\"   Layer列: {'有' if has_layer else '无'} (空值: {layer_null})\")\n",
    "            print(f\"   DocCount列: {'有' if has_doc_count else '无'} (空值: {doc_null})\")\n",
    "            \n",
    "            if not has_layer:\n",
    "                all_have_layer = False\n",
    "            if not has_doc_count:\n",
    "                all_have_doc_count = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ {folder_name}: 读取失败 - {e}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"📋 汇总:\")\n",
    "    print(f\"   总文件数: {len(coherence_files)}\")\n",
    "    print(f\"   都有layer列: {'是' if all_have_layer else '否'}\")\n",
    "    print(f\"   都有document_count列: {'是' if all_have_doc_count else '否'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "71ff855d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "开始为standard_coherence.csv添加layer和document_count信息...\n",
      "================================================================================\n",
      "🔍 找到 18 个标准一致性文件待处理\n",
      "\n",
      "================================================================================\n",
      "[1/18] 处理文件夹: depth_3_gamma_0.05_eta_0.1_run_2\n",
      "================================================================================\n",
      "📖 读取文件...\n",
      "   一致性文件: 231 行\n",
      "   熵文件: 231 行\n",
      "   当前列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   已有layer列: False\n",
      "   已有document_count列: False\n",
      "   可映射的节点数: 231\n",
      "   ✓ 添加了layer列\n",
      "   ✓ 添加了document_count列\n",
      "   📊 层级分布: {0: 1, 1: 44, 2: 186}\n",
      "   📊 文档数统计:\n",
      "      最小值: 1\n",
      "      最大值: 970\n",
      "      平均值: 12.6\n",
      "💾 已更新并保存: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_2/standard_coherence.csv\n",
      "   更新后的列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[2/18] 处理文件夹: depth_3_gamma_0.05_eta_0.1_run_3\n",
      "================================================================================\n",
      "📖 读取文件...\n",
      "   一致性文件: 215 行\n",
      "   熵文件: 215 行\n",
      "   当前列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   已有layer列: False\n",
      "   已有document_count列: False\n",
      "   可映射的节点数: 215\n",
      "   ✓ 添加了layer列\n",
      "   ✓ 添加了document_count列\n",
      "   📊 层级分布: {0: 1, 1: 41, 2: 173}\n",
      "   📊 文档数统计:\n",
      "      最小值: 1\n",
      "      最大值: 970\n",
      "      平均值: 13.5\n",
      "💾 已更新并保存: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_3/standard_coherence.csv\n",
      "   更新后的列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[3/18] 处理文件夹: depth_3_gamma_0.05_eta_0.1_run_1\n",
      "================================================================================\n",
      "📖 读取文件...\n",
      "   一致性文件: 205 行\n",
      "   熵文件: 205 行\n",
      "   当前列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   已有layer列: False\n",
      "   已有document_count列: False\n",
      "   可映射的节点数: 205\n",
      "   ✓ 添加了layer列\n",
      "   ✓ 添加了document_count列\n",
      "   📊 层级分布: {0: 1, 1: 40, 2: 164}\n",
      "   📊 文档数统计:\n",
      "      最小值: 1\n",
      "      最大值: 970\n",
      "      平均值: 14.2\n",
      "💾 已更新并保存: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_1/standard_coherence.csv\n",
      "   更新后的列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[4/18] 处理文件夹: depth_3_gamma_0.05_eta_0.05_run_3\n",
      "================================================================================\n",
      "📖 读取文件...\n",
      "   一致性文件: 322 行\n",
      "   熵文件: 322 行\n",
      "   当前列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   已有layer列: False\n",
      "   已有document_count列: False\n",
      "   可映射的节点数: 322\n",
      "   ✓ 添加了layer列\n",
      "   ✓ 添加了document_count列\n",
      "   📊 层级分布: {0: 1, 1: 59, 2: 262}\n",
      "   📊 文档数统计:\n",
      "      最小值: 1\n",
      "      最大值: 970\n",
      "      平均值: 9.0\n",
      "💾 已更新并保存: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_3/standard_coherence.csv\n",
      "   更新后的列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[5/18] 处理文件夹: depth_3_gamma_0.05_eta_0.05_run_1\n",
      "================================================================================\n",
      "📖 读取文件...\n",
      "   一致性文件: 315 行\n",
      "   熵文件: 315 行\n",
      "   当前列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   已有layer列: False\n",
      "   已有document_count列: False\n",
      "   可映射的节点数: 315\n",
      "   ✓ 添加了layer列\n",
      "   ✓ 添加了document_count列\n",
      "   📊 层级分布: {0: 1, 1: 62, 2: 252}\n",
      "   📊 文档数统计:\n",
      "      最小值: 1\n",
      "      最大值: 970\n",
      "      平均值: 9.2\n",
      "💾 已更新并保存: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_1/standard_coherence.csv\n",
      "   更新后的列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[6/18] 处理文件夹: depth_3_gamma_0.05_eta_0.05_run_2\n",
      "================================================================================\n",
      "📖 读取文件...\n",
      "   一致性文件: 299 行\n",
      "   熵文件: 299 行\n",
      "   当前列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   已有layer列: False\n",
      "   已有document_count列: False\n",
      "   可映射的节点数: 299\n",
      "   ✓ 添加了layer列\n",
      "   ✓ 添加了document_count列\n",
      "   📊 层级分布: {0: 1, 1: 55, 2: 243}\n",
      "   📊 文档数统计:\n",
      "      最小值: 1\n",
      "      最大值: 970\n",
      "      平均值: 9.7\n",
      "💾 已更新并保存: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_2/standard_coherence.csv\n",
      "   更新后的列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[7/18] 处理文件夹: depth_3_gamma_0.05_eta_0.005_run_3\n",
      "================================================================================\n",
      "📖 读取文件...\n",
      "   一致性文件: 427 行\n",
      "   熵文件: 427 行\n",
      "   当前列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   已有layer列: False\n",
      "   已有document_count列: False\n",
      "   可映射的节点数: 427\n",
      "   ✓ 添加了layer列\n",
      "   ✓ 添加了document_count列\n",
      "   📊 层级分布: {0: 1, 1: 90, 2: 336}\n",
      "   📊 文档数统计:\n",
      "      最小值: 1\n",
      "      最大值: 970\n",
      "      平均值: 6.8\n",
      "💾 已更新并保存: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_3/standard_coherence.csv\n",
      "   更新后的列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[8/18] 处理文件夹: depth_3_gamma_0.05_eta_0.005_run_1\n",
      "================================================================================\n",
      "📖 读取文件...\n",
      "   一致性文件: 438 行\n",
      "   熵文件: 438 行\n",
      "   当前列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   已有layer列: False\n",
      "   已有document_count列: False\n",
      "   可映射的节点数: 438\n",
      "   ✓ 添加了layer列\n",
      "   ✓ 添加了document_count列\n",
      "   📊 层级分布: {0: 1, 1: 85, 2: 352}\n",
      "   📊 文档数统计:\n",
      "      最小值: 1\n",
      "      最大值: 970\n",
      "      平均值: 6.6\n",
      "💾 已更新并保存: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_1/standard_coherence.csv\n",
      "   更新后的列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[9/18] 处理文件夹: depth_3_gamma_0.05_eta_0.005_run_2\n",
      "================================================================================\n",
      "📖 读取文件...\n",
      "   一致性文件: 432 行\n",
      "   熵文件: 432 行\n",
      "   当前列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   已有layer列: False\n",
      "   已有document_count列: False\n",
      "   可映射的节点数: 432\n",
      "   ✓ 添加了layer列\n",
      "   ✓ 添加了document_count列\n",
      "   📊 层级分布: {0: 1, 1: 80, 2: 351}\n",
      "   📊 文档数统计:\n",
      "      最小值: 1\n",
      "      最大值: 970\n",
      "      平均值: 6.7\n",
      "💾 已更新并保存: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_2/standard_coherence.csv\n",
      "   更新后的列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[10/18] 处理文件夹: depth_3_gamma_0.05_eta_0.02_run_3\n",
      "================================================================================\n",
      "📖 读取文件...\n",
      "   一致性文件: 388 行\n",
      "   熵文件: 388 行\n",
      "   当前列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   已有layer列: False\n",
      "   已有document_count列: False\n",
      "   可映射的节点数: 388\n",
      "   ✓ 添加了layer列\n",
      "   ✓ 添加了document_count列\n",
      "   📊 层级分布: {0: 1, 1: 74, 2: 313}\n",
      "   📊 文档数统计:\n",
      "      最小值: 1\n",
      "      最大值: 970\n",
      "      平均值: 7.5\n",
      "💾 已更新并保存: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_3/standard_coherence.csv\n",
      "   更新后的列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[11/18] 处理文件夹: depth_3_gamma_0.05_eta_0.02_run_2\n",
      "================================================================================\n",
      "📖 读取文件...\n",
      "   一致性文件: 384 行\n",
      "   熵文件: 384 行\n",
      "   当前列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   已有layer列: False\n",
      "   已有document_count列: False\n",
      "   可映射的节点数: 384\n",
      "   ✓ 添加了layer列\n",
      "   ✓ 添加了document_count列\n",
      "   📊 层级分布: {0: 1, 1: 72, 2: 311}\n",
      "   📊 文档数统计:\n",
      "      最小值: 1\n",
      "      最大值: 970\n",
      "      平均值: 7.6\n",
      "💾 已更新并保存: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_2/standard_coherence.csv\n",
      "   更新后的列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[12/18] 处理文件夹: depth_3_gamma_0.05_eta_0.02_run_1\n",
      "================================================================================\n",
      "📖 读取文件...\n",
      "   一致性文件: 383 行\n",
      "   熵文件: 383 行\n",
      "   当前列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   已有layer列: False\n",
      "   已有document_count列: False\n",
      "   可映射的节点数: 383\n",
      "   ✓ 添加了layer列\n",
      "   ✓ 添加了document_count列\n",
      "   📊 层级分布: {0: 1, 1: 74, 2: 308}\n",
      "   📊 文档数统计:\n",
      "      最小值: 1\n",
      "      最大值: 970\n",
      "      平均值: 7.6\n",
      "💾 已更新并保存: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_1/standard_coherence.csv\n",
      "   更新后的列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[13/18] 处理文件夹: depth_3_gamma_0.05_eta_0.2_run_1\n",
      "================================================================================\n",
      "📖 读取文件...\n",
      "   一致性文件: 151 行\n",
      "   熵文件: 151 行\n",
      "   当前列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   已有layer列: False\n",
      "   已有document_count列: False\n",
      "   可映射的节点数: 151\n",
      "   ✓ 添加了layer列\n",
      "   ✓ 添加了document_count列\n",
      "   📊 层级分布: {0: 1, 1: 28, 2: 122}\n",
      "   📊 文档数统计:\n",
      "      最小值: 1\n",
      "      最大值: 970\n",
      "      平均值: 19.3\n",
      "💾 已更新并保存: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_1/standard_coherence.csv\n",
      "   更新后的列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[14/18] 处理文件夹: depth_3_gamma_0.05_eta_0.2_run_3\n",
      "================================================================================\n",
      "📖 读取文件...\n",
      "   一致性文件: 157 行\n",
      "   熵文件: 157 行\n",
      "   当前列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   已有layer列: False\n",
      "   已有document_count列: False\n",
      "   可映射的节点数: 157\n",
      "   ✓ 添加了layer列\n",
      "   ✓ 添加了document_count列\n",
      "   📊 层级分布: {0: 1, 1: 29, 2: 127}\n",
      "   📊 文档数统计:\n",
      "      最小值: 1\n",
      "      最大值: 970\n",
      "      平均值: 18.5\n",
      "💾 已更新并保存: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_3/standard_coherence.csv\n",
      "   更新后的列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[15/18] 处理文件夹: depth_3_gamma_0.05_eta_0.2_run_2\n",
      "================================================================================\n",
      "📖 读取文件...\n",
      "   一致性文件: 157 行\n",
      "   熵文件: 157 行\n",
      "   当前列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   已有layer列: False\n",
      "   已有document_count列: False\n",
      "   可映射的节点数: 157\n",
      "   ✓ 添加了layer列\n",
      "   ✓ 添加了document_count列\n",
      "   📊 层级分布: {0: 1, 1: 31, 2: 125}\n",
      "   📊 文档数统计:\n",
      "      最小值: 1\n",
      "      最大值: 970\n",
      "      平均值: 18.5\n",
      "💾 已更新并保存: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_2/standard_coherence.csv\n",
      "   更新后的列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[16/18] 处理文件夹: depth_3_gamma_0.05_eta_0.01_run_2\n",
      "================================================================================\n",
      "📖 读取文件...\n",
      "   一致性文件: 403 行\n",
      "   熵文件: 403 行\n",
      "   当前列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   已有layer列: False\n",
      "   已有document_count列: False\n",
      "   可映射的节点数: 403\n",
      "   ✓ 添加了layer列\n",
      "   ✓ 添加了document_count列\n",
      "   📊 层级分布: {0: 1, 1: 76, 2: 326}\n",
      "   📊 文档数统计:\n",
      "      最小值: 1\n",
      "      最大值: 970\n",
      "      平均值: 7.2\n",
      "💾 已更新并保存: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_2/standard_coherence.csv\n",
      "   更新后的列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[17/18] 处理文件夹: depth_3_gamma_0.05_eta_0.01_run_1\n",
      "================================================================================\n",
      "📖 读取文件...\n",
      "   一致性文件: 425 行\n",
      "   熵文件: 425 行\n",
      "   当前列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   已有layer列: False\n",
      "   已有document_count列: False\n",
      "   可映射的节点数: 425\n",
      "   ✓ 添加了layer列\n",
      "   ✓ 添加了document_count列\n",
      "   📊 层级分布: {0: 1, 1: 80, 2: 344}\n",
      "   📊 文档数统计:\n",
      "      最小值: 1\n",
      "      最大值: 970\n",
      "      平均值: 6.8\n",
      "💾 已更新并保存: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_1/standard_coherence.csv\n",
      "   更新后的列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[18/18] 处理文件夹: depth_3_gamma_0.05_eta_0.01_run_3\n",
      "================================================================================\n",
      "📖 读取文件...\n",
      "   一致性文件: 433 行\n",
      "   熵文件: 433 行\n",
      "   当前列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   已有layer列: False\n",
      "   已有document_count列: False\n",
      "   可映射的节点数: 433\n",
      "   ✓ 添加了layer列\n",
      "   ✓ 添加了document_count列\n",
      "   📊 层级分布: {0: 1, 1: 81, 2: 351}\n",
      "   📊 文档数统计:\n",
      "      最小值: 1\n",
      "      最大值: 970\n",
      "      平均值: 6.7\n",
      "💾 已更新并保存: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_3/standard_coherence.csv\n",
      "   更新后的列: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "✅ 所有标准一致性文件的layer和document_count信息更新完成！\n",
      "\n",
      "================================================================================\n",
      "验证更新结果...\n",
      "================================================================================\n",
      "🔍 验证更新结果:\n",
      "================================================================================\n",
      "✅ depth_3_gamma_0.05_eta_0.1_run_2\n",
      "   Layer列: 有 (空值: 0)\n",
      "   DocCount列: 有 (空值: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.1_run_3\n",
      "   Layer列: 有 (空值: 0)\n",
      "   DocCount列: 有 (空值: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.1_run_1\n",
      "   Layer列: 有 (空值: 0)\n",
      "   DocCount列: 有 (空值: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.05_run_3\n",
      "   Layer列: 有 (空值: 0)\n",
      "   DocCount列: 有 (空值: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.05_run_1\n",
      "   Layer列: 有 (空值: 0)\n",
      "   DocCount列: 有 (空值: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.05_run_2\n",
      "   Layer列: 有 (空值: 0)\n",
      "   DocCount列: 有 (空值: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.005_run_3\n",
      "   Layer列: 有 (空值: 0)\n",
      "   DocCount列: 有 (空值: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.005_run_1\n",
      "   Layer列: 有 (空值: 0)\n",
      "   DocCount列: 有 (空值: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.005_run_2\n",
      "   Layer列: 有 (空值: 0)\n",
      "   DocCount列: 有 (空值: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.02_run_3\n",
      "   Layer列: 有 (空值: 0)\n",
      "   DocCount列: 有 (空值: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.02_run_2\n",
      "   Layer列: 有 (空值: 0)\n",
      "   DocCount列: 有 (空值: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.02_run_1\n",
      "   Layer列: 有 (空值: 0)\n",
      "   DocCount列: 有 (空值: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.2_run_1\n",
      "   Layer列: 有 (空值: 0)\n",
      "   DocCount列: 有 (空值: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.2_run_3\n",
      "   Layer列: 有 (空值: 0)\n",
      "   DocCount列: 有 (空值: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.2_run_2\n",
      "   Layer列: 有 (空值: 0)\n",
      "   DocCount列: 有 (空值: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.01_run_2\n",
      "   Layer列: 有 (空值: 0)\n",
      "   DocCount列: 有 (空值: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.01_run_1\n",
      "   Layer列: 有 (空值: 0)\n",
      "   DocCount列: 有 (空值: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.01_run_3\n",
      "   Layer列: 有 (空值: 0)\n",
      "   DocCount列: 有 (空值: 0)\n",
      "================================================================================\n",
      "📋 汇总:\n",
      "   总文件数: 18\n",
      "   都有layer列: 是\n",
      "   都有document_count列: 是\n",
      "\n",
      "================================================================================\n",
      "✅ Layer和document_count信息添加完成！\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 执行更新\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step2\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"开始为standard_coherence.csv添加layer和document_count信息...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 添加layer和document_count信息\n",
    "add_layer_and_document_info_to_coherence(base_path)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"验证更新结果...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 验证更新结果\n",
    "verify_coherence_files_update(base_path)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ Layer和document_count信息添加完成！\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "02540932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coherence_layered_analysis(base_path=\".\", corpus=None, top_k=15):\n",
    "    \"\"\"\n",
    "    计算节点一致性指标并按层级进行加权汇总分析\n",
    "    \"\"\"\n",
    "    \n",
    "    if corpus is None:\n",
    "        print(\"❌ 必须提供原始语料corpus\")\n",
    "        return\n",
    "    \n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_node_word_distributions.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    print(f\"🔍 找到 {len(files)} 个词分布文件待处理 (top_k={top_k})\")\n",
    "    \n",
    "    for idx, file_path in enumerate(files, 1):\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        \n",
    "        # 参数提取\n",
    "        eta = 0.1\n",
    "        gamma = 0.05\n",
    "        depth = 3\n",
    "        alpha = 0.1\n",
    "        \n",
    "        for param_name in ['eta', 'gamma', 'depth', 'alpha']:\n",
    "            if f'{param_name}_' in folder_name:\n",
    "                try:\n",
    "                    param_part = folder_name.split(f'{param_name}_')[1].split('_')[0]\n",
    "                    if param_name == 'depth':\n",
    "                        locals()[param_name] = int(param_part)\n",
    "                    else:\n",
    "                        locals()[param_name] = float(param_part)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"[{idx}/{len(files)}] 处理文件: {folder_name} (k={top_k})\")\n",
    "        print(f\"参数 - Eta: {eta}, Gamma: {gamma}, Depth: {depth}, Alpha: {alpha}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            # 读取数据\n",
    "            df = pd.read_csv(file_path)\n",
    "            df.columns = [col.strip(\"'\\\" \") for col in df.columns]\n",
    "            \n",
    "            max_iteration = df['iteration'].max()\n",
    "            last_iteration_data = df[df['iteration'] == max_iteration]\n",
    "            \n",
    "            # 读取层级和文档数信息\n",
    "            entropy_file = os.path.join(folder_path, 'corrected_renyi_entropy.csv')\n",
    "            if not os.path.exists(entropy_file):\n",
    "                print(\"⚠️ 未找到entropy文件，跳过此文件\")\n",
    "                continue\n",
    "                \n",
    "            entropy_df = pd.read_csv(entropy_file)\n",
    "            \n",
    "            print(f\"📈 最后iteration: {max_iteration}\")\n",
    "            print(f\"📈 节点数: {last_iteration_data['node_id'].nunique()}\")\n",
    "            \n",
    "            # 计算节点级一致性（只保留节点级）\n",
    "            texts = list(corpus.values())\n",
    "            dictionary = Dictionary(texts)\n",
    "            \n",
    "            topics = []\n",
    "            node_to_topic_idx = {}\n",
    "            \n",
    "            topic_idx = 0\n",
    "            for node_id in last_iteration_data['node_id'].unique():\n",
    "                node_data = last_iteration_data[last_iteration_data['node_id'] == node_id]\n",
    "                top_words = node_data.nlargest(top_k, 'count')['word'].tolist()\n",
    "                \n",
    "                valid_words = []\n",
    "                for word in top_words:\n",
    "                    if pd.notna(word) and word in dictionary.token2id:\n",
    "                        valid_words.append(word)\n",
    "                \n",
    "                if len(valid_words) >= 2:\n",
    "                    topics.append(valid_words)\n",
    "                    node_to_topic_idx[node_id] = topic_idx\n",
    "                    topic_idx += 1\n",
    "            \n",
    "            if len(topics) == 0:\n",
    "                print(\"⚠️ 没有有效主题，跳过此文件\")\n",
    "                continue\n",
    "            \n",
    "            # 计算各项一致性指标\n",
    "            coherence_measures = ['c_npmi', 'c_v', 'u_mass']\n",
    "            per_topic_coherence = {}\n",
    "            \n",
    "            for measure in coherence_measures:\n",
    "                try:\n",
    "                    print(f\"   正在计算 {measure}...\")\n",
    "                    \n",
    "                    cm = CoherenceModel(\n",
    "                        topics=topics,\n",
    "                        texts=texts,\n",
    "                        dictionary=dictionary,\n",
    "                        coherence=measure,\n",
    "                        processes=1\n",
    "                    )\n",
    "                    \n",
    "                    per_topic_scores = cm.get_coherence_per_topic()\n",
    "                    per_topic_coherence[measure] = per_topic_scores\n",
    "                    \n",
    "                    print(f\"   ✓ {measure}: 范围=[{min(per_topic_scores):.4f}, {max(per_topic_scores):.4f}]\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   ❌ 计算 {measure} 时出错: {e}\")\n",
    "                    per_topic_coherence[measure] = [0.0] * len(topics)\n",
    "            \n",
    "            # 合并节点级一致性和层级信息\n",
    "            node_coherence_data = []\n",
    "            \n",
    "            for node_id in last_iteration_data['node_id'].unique():\n",
    "                node_words = last_iteration_data[last_iteration_data['node_id'] == node_id]\n",
    "                top_words = node_words.nlargest(top_k, 'count')['word'].tolist()\n",
    "                top_words = [word for word in top_words if pd.notna(word)]\n",
    "                \n",
    "                # 获取层级和文档数信息\n",
    "                node_entropy_info = entropy_df[entropy_df['node_id'] == node_id]\n",
    "                if len(node_entropy_info) > 0:\n",
    "                    layer = node_entropy_info['layer'].iloc[0]\n",
    "                    document_count = node_entropy_info['document_count'].iloc[0]\n",
    "                else:\n",
    "                    layer = -1\n",
    "                    document_count = 0\n",
    "                \n",
    "                # 获取节点一致性得分\n",
    "                node_coherence_scores = {}\n",
    "                if node_id in node_to_topic_idx:\n",
    "                    topic_idx = node_to_topic_idx[node_id]\n",
    "                    for measure in ['c_npmi', 'c_v', 'u_mass']:\n",
    "                        if measure in per_topic_coherence:\n",
    "                            measure_name = measure.replace('c_', '') if measure.startswith('c_') else measure\n",
    "                            node_coherence_scores[f'node_{measure_name}'] = per_topic_coherence[measure][topic_idx]\n",
    "                        else:\n",
    "                            measure_name = measure.replace('c_', '') if measure.startswith('c_') else measure\n",
    "                            node_coherence_scores[f'node_{measure_name}'] = 0.0\n",
    "                else:\n",
    "                    for measure in ['npmi', 'v', 'u_mass']:\n",
    "                        node_coherence_scores[f'node_{measure}'] = 0.0\n",
    "                \n",
    "                node_coherence_data.append({\n",
    "                    'node_id': node_id,\n",
    "                    'eta': eta,\n",
    "                    'gamma': gamma, \n",
    "                    'depth': depth,\n",
    "                    'alpha': alpha,\n",
    "                    'layer': layer,\n",
    "                    'document_count': document_count,\n",
    "                    'top_k': top_k,\n",
    "                    'top_words': ', '.join(top_words[:10]),\n",
    "                    'word_count': len(top_words),\n",
    "                    \n",
    "                    # 只保留节点级一致性指标\n",
    "                    'node_npmi': node_coherence_scores.get('node_npmi', 0.0),\n",
    "                    'node_c_v': node_coherence_scores.get('node_v', 0.0),\n",
    "                    'node_u_mass': node_coherence_scores.get('node_u_mass', 0.0),\n",
    "                    \n",
    "                    'iteration': max_iteration\n",
    "                })\n",
    "            \n",
    "            # 保存节点级一致性结果（加上k值）\n",
    "            coherence_df = pd.DataFrame(node_coherence_data)\n",
    "            node_output_path = os.path.join(folder_path, f'node_coherence_k{top_k}.csv')\n",
    "            coherence_df.to_csv(node_output_path, index=False)\n",
    "            \n",
    "            # 计算层级加权平均一致性\n",
    "            layer_coherence_summary = []\n",
    "            \n",
    "            for layer in coherence_df['layer'].unique():\n",
    "                if layer == -1:  # 跳过无效层级\n",
    "                    continue\n",
    "                    \n",
    "                layer_data = coherence_df[coherence_df['layer'] == layer]\n",
    "                total_docs = layer_data['document_count'].sum()\n",
    "                \n",
    "                if total_docs > 0:\n",
    "                    # 按文档数加权平均\n",
    "                    weighted_npmi = (layer_data['document_count'] * layer_data['node_npmi']).sum() / total_docs\n",
    "                    weighted_c_v = (layer_data['document_count'] * layer_data['node_c_v']).sum() / total_docs\n",
    "                    weighted_u_mass = (layer_data['document_count'] * layer_data['node_u_mass']).sum() / total_docs\n",
    "                    \n",
    "                    # 简单平均（不加权）\n",
    "                    simple_npmi = layer_data['node_npmi'].mean()\n",
    "                    simple_c_v = layer_data['node_c_v'].mean()\n",
    "                    simple_u_mass = layer_data['node_u_mass'].mean()\n",
    "                    \n",
    "                    layer_coherence_summary.append({\n",
    "                        'layer': layer,\n",
    "                        'node_count': len(layer_data),\n",
    "                        'total_documents': total_docs,\n",
    "                        'avg_documents_per_node': total_docs / len(layer_data),\n",
    "                        \n",
    "                        # 文档数加权平均一致性\n",
    "                        'weighted_avg_npmi': weighted_npmi,\n",
    "                        'weighted_avg_c_v': weighted_c_v,\n",
    "                        'weighted_avg_u_mass': weighted_u_mass,\n",
    "                        \n",
    "                        # 简单平均一致性\n",
    "                        'simple_avg_npmi': simple_npmi,\n",
    "                        'simple_avg_c_v': simple_c_v,\n",
    "                        'simple_avg_u_mass': simple_u_mass,\n",
    "                        \n",
    "                        # 标准差\n",
    "                        'std_npmi': layer_data['node_npmi'].std(),\n",
    "                        'std_c_v': layer_data['node_c_v'].std(),\n",
    "                        'std_u_mass': layer_data['node_u_mass'].std(),\n",
    "                        \n",
    "                        'top_k': top_k,  # 添加k值记录\n",
    "                        'eta': eta,\n",
    "                        'gamma': gamma,\n",
    "                        'depth': depth,\n",
    "                        'alpha': alpha\n",
    "                    })\n",
    "            \n",
    "            # 保存层级汇总结果（加上k值）\n",
    "            if layer_coherence_summary:\n",
    "                layer_summary_df = pd.DataFrame(layer_coherence_summary)\n",
    "                layer_output_path = os.path.join(folder_path, f'layer_coherence_summary_k{top_k}.csv')\n",
    "                layer_summary_df.to_csv(layer_output_path, index=False)\n",
    "                \n",
    "                print(f\"💾 节点一致性结果已保存到: {node_output_path}\")\n",
    "                print(f\"💾 层级汇总结果已保存到: {layer_output_path}\")\n",
    "                \n",
    "                print(f\"📊 层级一致性汇总 (k={top_k}):\")\n",
    "                for _, row in layer_summary_df.iterrows():\n",
    "                    layer_num = int(row['layer'])\n",
    "                    node_count = int(row['node_count'])\n",
    "                    w_npmi = row['weighted_avg_npmi']\n",
    "                    w_cv = row['weighted_avg_c_v']\n",
    "                    w_umass = row['weighted_avg_u_mass']\n",
    "                    print(f\"   Layer {layer_num} ({node_count}节点): NPMI={w_npmi:.4f}, C_V={w_cv:.4f}, U_Mass={w_umass:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"❌ 处理文件 {file_path} 时出错: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    print(f\"\\n✅ 所有文件的一致性分层分析完成！(k={top_k})\")\n",
    "\n",
    "def aggregate_coherence_by_eta(base_path=\".\", top_k=15):\n",
    "    \"\"\"\n",
    "    按eta值汇总各层的一致性统计（包含k值）\n",
    "    \"\"\"\n",
    "    # 查找所有layer_coherence_summary_k{top_k}.csv文件\n",
    "    pattern = os.path.join(base_path, \"**\", f\"layer_coherence_summary_k{top_k}.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    print(f\"🔍 查找文件模式: layer_coherence_summary_k{top_k}.csv\")\n",
    "    print(f\"🔍 找到 {len(files)} 个层级汇总文件\")\n",
    "    \n",
    "    all_data = []\n",
    "    eta_groups = {}\n",
    "    \n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        parent_folder = os.path.dirname(folder_path)\n",
    "        \n",
    "        # 提取eta值\n",
    "        eta = None\n",
    "        if 'eta_' in folder_name:\n",
    "            try:\n",
    "                eta_part = folder_name.split('eta_')[1].split('_')[0]\n",
    "                eta = float(eta_part)\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # 提取run编号\n",
    "        run_match = folder_name.split('_run_')\n",
    "        if len(run_match) > 1:\n",
    "            run_id = run_match[1]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if eta not in eta_groups:\n",
    "            eta_groups[eta] = parent_folder\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                all_data.append({\n",
    "                    'eta': eta,\n",
    "                    'run_id': run_id,\n",
    "                    'layer': row['layer'],\n",
    "                    'node_count': row['node_count'],\n",
    "                    'total_documents': row['total_documents'],\n",
    "                    'weighted_avg_npmi': row['weighted_avg_npmi'],\n",
    "                    'weighted_avg_c_v': row['weighted_avg_c_v'],\n",
    "                    'weighted_avg_u_mass': row['weighted_avg_u_mass'],\n",
    "                    'simple_avg_npmi': row['simple_avg_npmi'],\n",
    "                    'simple_avg_c_v': row['simple_avg_c_v'],\n",
    "                    'simple_avg_u_mass': row['simple_avg_u_mass'],\n",
    "                    'top_k': top_k,  # 添加k值记录\n",
    "                    'parent_folder': parent_folder\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"读取文件 {file_path} 时出错: {e}\")\n",
    "    \n",
    "    # 转换为DataFrame并按eta分组汇总\n",
    "    summary_df = pd.DataFrame(all_data)\n",
    "    \n",
    "    if summary_df.empty:\n",
    "        print(\"未找到有效数据\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"各ETA值的一致性层级汇总统计 (k={top_k})\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 按eta分组生成汇总文件\n",
    "    for eta, group_data in summary_df.groupby('eta'):\n",
    "        parent_folder = group_data['parent_folder'].iloc[0]\n",
    "        \n",
    "        print(f\"\\n处理 Eta={eta} (k={top_k})\")\n",
    "        \n",
    "        layer_summary = group_data.groupby('layer').agg({\n",
    "            'weighted_avg_npmi': ['mean', 'std', 'count'],\n",
    "            'weighted_avg_c_v': ['mean', 'std', 'count'],\n",
    "            'weighted_avg_u_mass': ['mean', 'std', 'count'],\n",
    "            'simple_avg_npmi': ['mean', 'std'],\n",
    "            'simple_avg_c_v': ['mean', 'std'],\n",
    "            'simple_avg_u_mass': ['mean', 'std'],\n",
    "            'node_count': 'mean',\n",
    "            'total_documents': 'mean',\n",
    "            'run_id': lambda x: ', '.join(sorted(x.unique()))\n",
    "        }).round(4)\n",
    "        \n",
    "        # 平铺列名\n",
    "        layer_summary.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in layer_summary.columns]\n",
    "        layer_summary = layer_summary.reset_index()\n",
    "        layer_summary.insert(0, 'eta', eta)\n",
    "        layer_summary.insert(1, 'top_k', top_k)  # 添加k值列\n",
    "        \n",
    "        # 保存汇总结果（文件名包含k值）\n",
    "        output_filename = f'eta_{eta}_coherence_layer_summary_k{top_k}.csv'\n",
    "        output_path = os.path.join(parent_folder, output_filename)\n",
    "        layer_summary.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"  保存汇总文件: {output_path}\")\n",
    "        print(f\"  层数: {len(layer_summary)}\")\n",
    "        \n",
    "        # 显示简要统计\n",
    "        for _, row in layer_summary.iterrows():\n",
    "            layer_num = int(row['layer'])\n",
    "            w_npmi = row['weighted_avg_npmi_mean']\n",
    "            w_cv = row['weighted_avg_c_v_mean']\n",
    "            w_umass = row['weighted_avg_u_mass_mean']\n",
    "            run_count = int(row['weighted_avg_npmi_count'])\n",
    "            \n",
    "            print(f\"    Layer {layer_num}: W_NPMI={w_npmi:.4f}, W_C_V={w_cv:.4f}, W_U_Mass={w_umass:.4f}, runs={run_count}\")\n",
    "    \n",
    "    # 生成总体对比文件（文件名包含k值）\n",
    "    overall_summary = summary_df.groupby(['eta', 'layer']).agg({\n",
    "        'weighted_avg_npmi': ['mean', 'std'],\n",
    "        'weighted_avg_c_v': ['mean', 'std'],\n",
    "        'weighted_avg_u_mass': ['mean', 'std'],\n",
    "        'run_id': 'count'\n",
    "    }).round(4)\n",
    "    \n",
    "    overall_summary.columns = ['_'.join(col).strip() for col in overall_summary.columns]\n",
    "    overall_summary = overall_summary.reset_index()\n",
    "    overall_summary.insert(2, 'top_k', top_k)  # 添加k值列\n",
    "    \n",
    "    overall_output_path = os.path.join(base_path, f'eta_coherence_layer_comparison_k{top_k}.csv')\n",
    "    overall_summary.to_csv(overall_output_path, index=False)\n",
    "    print(f\"\\n总体对比文件保存到: {overall_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3803f4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "开始计算节点一致性指标并进行分层分析 (k=10)...\n",
      "================================================================================\n",
      "🔍 找到 18 个词分布文件待处理 (top_k=10)\n",
      "\n",
      "================================================================================\n",
      "[1/18] 处理文件: depth_3_gamma_0.05_eta_0.1_run_2 (k=10)\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 175\n",
      "📈 节点数: 231\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 范围=[-0.3876, 0.2668]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 范围=[0.1804, 0.9180]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 范围=[-9.9187, -0.7397]\n",
      "💾 节点一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_2/node_coherence_k10.csv\n",
      "💾 层级汇总结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_2/layer_coherence_summary_k10.csv\n",
      "📊 层级一致性汇总 (k=10):\n",
      "   Layer 0 (1节点): NPMI=0.0271, C_V=0.4820, U_Mass=-0.7397\n",
      "   Layer 1 (44节点): NPMI=-0.0408, C_V=0.4139, U_Mass=-2.4372\n",
      "   Layer 2 (186节点): NPMI=0.0045, C_V=0.4639, U_Mass=-3.1948\n",
      "\n",
      "================================================================================\n",
      "[2/18] 处理文件: depth_3_gamma_0.05_eta_0.1_run_3 (k=10)\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 175\n",
      "📈 节点数: 215\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 范围=[-0.3944, 0.2994]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 范围=[0.1816, 0.8605]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 范围=[-11.2101, -0.7280]\n",
      "💾 节点一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_3/node_coherence_k10.csv\n",
      "💾 层级汇总结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_3/layer_coherence_summary_k10.csv\n",
      "📊 层级一致性汇总 (k=10):\n",
      "   Layer 0 (1节点): NPMI=0.0271, C_V=0.4820, U_Mass=-0.7280\n",
      "   Layer 1 (41节点): NPMI=-0.0199, C_V=0.4916, U_Mass=-2.2490\n",
      "   Layer 2 (173节点): NPMI=-0.0133, C_V=0.4426, U_Mass=-3.4155\n",
      "\n",
      "================================================================================\n",
      "[3/18] 处理文件: depth_3_gamma_0.05_eta_0.1_run_1 (k=10)\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 175\n",
      "📈 节点数: 205\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 范围=[-0.3308, 0.2615]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 范围=[0.1539, 0.8180]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 范围=[-10.2617, -0.6938]\n",
      "💾 节点一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_1/node_coherence_k10.csv\n",
      "💾 层级汇总结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_1/layer_coherence_summary_k10.csv\n",
      "📊 层级一致性汇总 (k=10):\n",
      "   Layer 0 (1节点): NPMI=0.0104, C_V=0.4626, U_Mass=-0.6938\n",
      "   Layer 1 (40节点): NPMI=0.0105, C_V=0.4704, U_Mass=-1.8857\n",
      "   Layer 2 (164节点): NPMI=-0.0055, C_V=0.4408, U_Mass=-3.0843\n",
      "\n",
      "================================================================================\n",
      "[4/18] 处理文件: depth_3_gamma_0.05_eta_0.05_run_3 (k=10)\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 90\n",
      "📈 节点数: 322\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 范围=[-0.4496, 0.3259]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 范围=[0.1696, 0.8656]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 范围=[-13.0773, -0.7052]\n",
      "💾 节点一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_3/node_coherence_k10.csv\n",
      "💾 层级汇总结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_3/layer_coherence_summary_k10.csv\n",
      "📊 层级一致性汇总 (k=10):\n",
      "   Layer 0 (1节点): NPMI=0.0215, C_V=0.4749, U_Mass=-0.7052\n",
      "   Layer 2 (262节点): NPMI=-0.0038, C_V=0.4522, U_Mass=-3.3708\n",
      "   Layer 1 (59节点): NPMI=-0.0099, C_V=0.4630, U_Mass=-2.7355\n",
      "\n",
      "================================================================================\n",
      "[5/18] 处理文件: depth_3_gamma_0.05_eta_0.05_run_1 (k=10)\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 90\n",
      "📈 节点数: 315\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 范围=[-0.3802, 0.3002]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 范围=[0.1249, 0.8760]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 范围=[-10.3079, -0.7100]\n",
      "💾 节点一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_1/node_coherence_k10.csv\n",
      "💾 层级汇总结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_1/layer_coherence_summary_k10.csv\n",
      "📊 层级一致性汇总 (k=10):\n",
      "   Layer 0 (1节点): NPMI=0.0215, C_V=0.4749, U_Mass=-0.7100\n",
      "   Layer 1 (62节点): NPMI=-0.0033, C_V=0.4487, U_Mass=-2.5689\n",
      "   Layer 2 (252节点): NPMI=-0.0188, C_V=0.4437, U_Mass=-3.5353\n",
      "\n",
      "================================================================================\n",
      "[6/18] 处理文件: depth_3_gamma_0.05_eta_0.05_run_2 (k=10)\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 90\n",
      "📈 节点数: 299\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 范围=[-0.3736, 0.2589]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 范围=[0.1643, 0.8618]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 范围=[-10.2849, -0.6925]\n",
      "💾 节点一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_2/node_coherence_k10.csv\n",
      "💾 层级汇总结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_2/layer_coherence_summary_k10.csv\n",
      "📊 层级一致性汇总 (k=10):\n",
      "   Layer 0 (1节点): NPMI=0.0211, C_V=0.4646, U_Mass=-0.6925\n",
      "   Layer 1 (55节点): NPMI=-0.0171, C_V=0.4164, U_Mass=-2.5943\n",
      "   Layer 2 (243节点): NPMI=0.0038, C_V=0.4601, U_Mass=-3.1346\n",
      "\n",
      "================================================================================\n",
      "[7/18] 处理文件: depth_3_gamma_0.05_eta_0.005_run_3 (k=10)\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 115\n",
      "📈 节点数: 427\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 范围=[-0.4416, 0.2953]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 范围=[0.1113, 0.8906]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 范围=[-11.3923, -0.7116]\n",
      "💾 节点一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_3/node_coherence_k10.csv\n",
      "💾 层级汇总结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_3/layer_coherence_summary_k10.csv\n",
      "📊 层级一致性汇总 (k=10):\n",
      "   Layer 0 (1节点): NPMI=0.0215, C_V=0.4749, U_Mass=-0.7116\n",
      "   Layer 1 (90节点): NPMI=-0.0374, C_V=0.5383, U_Mass=-2.9524\n",
      "   Layer 2 (336节点): NPMI=-0.0296, C_V=0.4495, U_Mass=-3.4672\n",
      "\n",
      "================================================================================\n",
      "[8/18] 处理文件: depth_3_gamma_0.05_eta_0.005_run_1 (k=10)\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 115\n",
      "📈 节点数: 438\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 范围=[-0.3827, 0.2615]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 范围=[0.1670, 0.9024]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 范围=[-11.7789, -0.7566]\n",
      "💾 节点一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_1/node_coherence_k10.csv\n",
      "💾 层级汇总结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_1/layer_coherence_summary_k10.csv\n",
      "📊 层级一致性汇总 (k=10):\n",
      "   Layer 0 (1节点): NPMI=0.0087, C_V=0.4960, U_Mass=-0.7566\n",
      "   Layer 1 (85节点): NPMI=-0.0261, C_V=0.4754, U_Mass=-2.7485\n",
      "   Layer 2 (352节点): NPMI=-0.0289, C_V=0.4587, U_Mass=-3.1962\n",
      "\n",
      "================================================================================\n",
      "[9/18] 处理文件: depth_3_gamma_0.05_eta_0.005_run_2 (k=10)\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 115\n",
      "📈 节点数: 432\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 范围=[-0.3654, 0.2855]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 范围=[0.1140, 0.8964]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 范围=[-14.3285, -0.7119]\n",
      "💾 节点一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_2/node_coherence_k10.csv\n",
      "💾 层级汇总结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_2/layer_coherence_summary_k10.csv\n",
      "📊 层级一致性汇总 (k=10):\n",
      "   Layer 0 (1节点): NPMI=0.0215, C_V=0.4749, U_Mass=-0.7119\n",
      "   Layer 1 (80节点): NPMI=-0.0282, C_V=0.4605, U_Mass=-2.7663\n",
      "   Layer 2 (351节点): NPMI=-0.0146, C_V=0.4638, U_Mass=-3.0805\n",
      "\n",
      "================================================================================\n",
      "[10/18] 处理文件: depth_3_gamma_0.05_eta_0.02_run_3 (k=10)\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 155\n",
      "📈 节点数: 388\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 范围=[-0.5056, 0.3156]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 范围=[0.1628, 0.8956]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 范围=[-12.8341, -0.6879]\n",
      "💾 节点一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_3/node_coherence_k10.csv\n",
      "💾 层级汇总结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_3/layer_coherence_summary_k10.csv\n",
      "📊 层级一致性汇总 (k=10):\n",
      "   Layer 0 (1节点): NPMI=0.0211, C_V=0.4646, U_Mass=-0.6879\n",
      "   Layer 1 (74节点): NPMI=-0.0205, C_V=0.4933, U_Mass=-2.7352\n",
      "   Layer 2 (313节点): NPMI=-0.0227, C_V=0.4351, U_Mass=-3.4574\n",
      "\n",
      "================================================================================\n",
      "[11/18] 处理文件: depth_3_gamma_0.05_eta_0.02_run_2 (k=10)\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 155\n",
      "📈 节点数: 384\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 范围=[-0.4785, 0.2615]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 范围=[0.1629, 0.8642]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 范围=[-11.6049, -0.6925]\n",
      "💾 节点一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_2/node_coherence_k10.csv\n",
      "💾 层级汇总结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_2/layer_coherence_summary_k10.csv\n",
      "📊 层级一致性汇总 (k=10):\n",
      "   Layer 0 (1节点): NPMI=0.0211, C_V=0.4646, U_Mass=-0.6925\n",
      "   Layer 1 (72节点): NPMI=-0.0303, C_V=0.4472, U_Mass=-3.1584\n",
      "   Layer 2 (311节点): NPMI=-0.0069, C_V=0.4684, U_Mass=-3.0500\n",
      "\n",
      "================================================================================\n",
      "[12/18] 处理文件: depth_3_gamma_0.05_eta_0.02_run_1 (k=10)\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 155\n",
      "📈 节点数: 383\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 范围=[-0.3979, 0.2615]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 范围=[0.1412, 0.8835]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 范围=[-9.4086, -0.7242]\n",
      "💾 节点一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_1/node_coherence_k10.csv\n",
      "💾 层级汇总结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_1/layer_coherence_summary_k10.csv\n",
      "📊 层级一致性汇总 (k=10):\n",
      "   Layer 0 (1节点): NPMI=0.0104, C_V=0.4626, U_Mass=-0.7242\n",
      "   Layer 1 (74节点): NPMI=-0.0143, C_V=0.4713, U_Mass=-2.3448\n",
      "   Layer 2 (308节点): NPMI=-0.0094, C_V=0.4384, U_Mass=-3.1192\n",
      "\n",
      "================================================================================\n",
      "[13/18] 处理文件: depth_3_gamma_0.05_eta_0.2_run_1 (k=10)\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 375\n",
      "📈 节点数: 151\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 范围=[-0.4279, 0.2905]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 范围=[0.1472, 0.8432]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 范围=[-10.7501, -0.7272]\n",
      "💾 节点一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_1/node_coherence_k10.csv\n",
      "💾 层级汇总结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_1/layer_coherence_summary_k10.csv\n",
      "📊 层级一致性汇总 (k=10):\n",
      "   Layer 0 (1节点): NPMI=0.0271, C_V=0.4820, U_Mass=-0.7272\n",
      "   Layer 1 (28节点): NPMI=-0.0440, C_V=0.3942, U_Mass=-2.9653\n",
      "   Layer 2 (122节点): NPMI=0.0038, C_V=0.4761, U_Mass=-3.1333\n",
      "\n",
      "================================================================================\n",
      "[14/18] 处理文件: depth_3_gamma_0.05_eta_0.2_run_3 (k=10)\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 375\n",
      "📈 节点数: 157\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 范围=[-0.3513, 0.2104]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 范围=[0.2018, 0.8821]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 范围=[-10.0329, -0.7494]\n",
      "💾 节点一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_3/node_coherence_k10.csv\n",
      "💾 层级汇总结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_3/layer_coherence_summary_k10.csv\n",
      "📊 层级一致性汇总 (k=10):\n",
      "   Layer 0 (1节点): NPMI=0.0087, C_V=0.4960, U_Mass=-0.7494\n",
      "   Layer 1 (29节点): NPMI=-0.0036, C_V=0.4543, U_Mass=-2.3312\n",
      "   Layer 2 (127节点): NPMI=-0.0052, C_V=0.4744, U_Mass=-3.4119\n",
      "\n",
      "================================================================================\n",
      "[15/18] 处理文件: depth_3_gamma_0.05_eta_0.2_run_2 (k=10)\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 375\n",
      "📈 节点数: 157\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 范围=[-0.4262, 0.1927]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 范围=[0.1634, 0.8907]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 范围=[-10.2902, -0.7289]\n",
      "💾 节点一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_2/node_coherence_k10.csv\n",
      "💾 层级汇总结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_2/layer_coherence_summary_k10.csv\n",
      "📊 层级一致性汇总 (k=10):\n",
      "   Layer 0 (1节点): NPMI=0.0271, C_V=0.4820, U_Mass=-0.7289\n",
      "   Layer 1 (31节点): NPMI=-0.0251, C_V=0.4800, U_Mass=-2.5175\n",
      "   Layer 2 (125节点): NPMI=0.0029, C_V=0.4722, U_Mass=-3.1690\n",
      "\n",
      "================================================================================\n",
      "[16/18] 处理文件: depth_3_gamma_0.05_eta_0.01_run_2 (k=10)\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 195\n",
      "📈 节点数: 403\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 范围=[-0.4411, 0.2721]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 范围=[0.1222, 0.8817]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 范围=[-14.0808, -0.7119]\n",
      "💾 节点一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_2/node_coherence_k10.csv\n",
      "💾 层级汇总结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_2/layer_coherence_summary_k10.csv\n",
      "📊 层级一致性汇总 (k=10):\n",
      "   Layer 0 (1节点): NPMI=0.0215, C_V=0.4749, U_Mass=-0.7119\n",
      "   Layer 1 (76节点): NPMI=-0.0405, C_V=0.4369, U_Mass=-3.0167\n",
      "   Layer 2 (326节点): NPMI=-0.0089, C_V=0.4779, U_Mass=-3.1062\n",
      "\n",
      "================================================================================\n",
      "[17/18] 处理文件: depth_3_gamma_0.05_eta_0.01_run_1 (k=10)\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 195\n",
      "📈 节点数: 425\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 范围=[-0.3653, 0.3048]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 范围=[0.1631, 0.9314]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 范围=[-10.5441, -0.7047]\n",
      "💾 节点一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_1/node_coherence_k10.csv\n",
      "💾 层级汇总结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_1/layer_coherence_summary_k10.csv\n",
      "📊 层级一致性汇总 (k=10):\n",
      "   Layer 0 (1节点): NPMI=0.0215, C_V=0.4749, U_Mass=-0.7047\n",
      "   Layer 1 (80节点): NPMI=-0.0067, C_V=0.4753, U_Mass=-2.4557\n",
      "   Layer 2 (344节点): NPMI=-0.0209, C_V=0.4441, U_Mass=-3.1966\n",
      "\n",
      "================================================================================\n",
      "[18/18] 处理文件: depth_3_gamma_0.05_eta_0.01_run_3 (k=10)\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 195\n",
      "📈 节点数: 433\n",
      "   正在计算 c_npmi...\n",
      "   ✓ c_npmi: 范围=[-0.3922, 0.2615]\n",
      "   正在计算 c_v...\n",
      "   ✓ c_v: 范围=[0.1552, 0.8972]\n",
      "   正在计算 u_mass...\n",
      "   ✓ u_mass: 范围=[-11.3348, -0.7130]\n",
      "💾 节点一致性结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_3/node_coherence_k10.csv\n",
      "💾 层级汇总结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_3/layer_coherence_summary_k10.csv\n",
      "📊 层级一致性汇总 (k=10):\n",
      "   Layer 0 (1节点): NPMI=0.0215, C_V=0.4749, U_Mass=-0.7130\n",
      "   Layer 1 (81节点): NPMI=-0.0266, C_V=0.4745, U_Mass=-2.9136\n",
      "   Layer 2 (351节点): NPMI=-0.0240, C_V=0.4542, U_Mass=-3.3668\n",
      "\n",
      "✅ 所有文件的一致性分层分析完成！(k=10)\n",
      "\n",
      "================================================================================\n",
      "开始按eta值汇总层级一致性统计 (k=10)...\n",
      "================================================================================\n",
      "🔍 查找文件模式: layer_coherence_summary_k10.csv\n",
      "🔍 找到 18 个层级汇总文件\n",
      "======================================================================\n",
      "各ETA值的一致性层级汇总统计 (k=10)\n",
      "======================================================================\n",
      "\n",
      "处理 Eta=0.005 (k=10)\n",
      "  保存汇总文件: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/eta_0.005_coherence_layer_summary_k10.csv\n",
      "  层数: 3\n",
      "    Layer 0: W_NPMI=0.0172, W_C_V=0.4819, W_U_Mass=-0.7267, runs=3\n",
      "    Layer 1: W_NPMI=-0.0306, W_C_V=0.4914, W_U_Mass=-2.8224, runs=3\n",
      "    Layer 2: W_NPMI=-0.0244, W_C_V=0.4574, W_U_Mass=-3.2480, runs=3\n",
      "\n",
      "处理 Eta=0.01 (k=10)\n",
      "  保存汇总文件: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/eta_0.01_coherence_layer_summary_k10.csv\n",
      "  层数: 3\n",
      "    Layer 0: W_NPMI=0.0215, W_C_V=0.4749, W_U_Mass=-0.7099, runs=3\n",
      "    Layer 1: W_NPMI=-0.0246, W_C_V=0.4622, W_U_Mass=-2.7953, runs=3\n",
      "    Layer 2: W_NPMI=-0.0179, W_C_V=0.4587, W_U_Mass=-3.2232, runs=3\n",
      "\n",
      "处理 Eta=0.02 (k=10)\n",
      "  保存汇总文件: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/eta_0.02_coherence_layer_summary_k10.csv\n",
      "  层数: 3\n",
      "    Layer 0: W_NPMI=0.0175, W_C_V=0.4639, W_U_Mass=-0.7015, runs=3\n",
      "    Layer 1: W_NPMI=-0.0217, W_C_V=0.4706, W_U_Mass=-2.7461, runs=3\n",
      "    Layer 2: W_NPMI=-0.0130, W_C_V=0.4473, W_U_Mass=-3.2089, runs=3\n",
      "\n",
      "处理 Eta=0.05 (k=10)\n",
      "  保存汇总文件: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/eta_0.05_coherence_layer_summary_k10.csv\n",
      "  层数: 3\n",
      "    Layer 0: W_NPMI=0.0214, W_C_V=0.4715, W_U_Mass=-0.7026, runs=3\n",
      "    Layer 1: W_NPMI=-0.0101, W_C_V=0.4427, W_U_Mass=-2.6329, runs=3\n",
      "    Layer 2: W_NPMI=-0.0063, W_C_V=0.4520, W_U_Mass=-3.3469, runs=3\n",
      "\n",
      "处理 Eta=0.1 (k=10)\n",
      "  保存汇总文件: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/eta_0.1_coherence_layer_summary_k10.csv\n",
      "  层数: 3\n",
      "    Layer 0: W_NPMI=0.0216, W_C_V=0.4755, W_U_Mass=-0.7205, runs=3\n",
      "    Layer 1: W_NPMI=-0.0167, W_C_V=0.4586, W_U_Mass=-2.1906, runs=3\n",
      "    Layer 2: W_NPMI=-0.0048, W_C_V=0.4491, W_U_Mass=-3.2316, runs=3\n",
      "\n",
      "处理 Eta=0.2 (k=10)\n",
      "  保存汇总文件: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/eta_0.2_coherence_layer_summary_k10.csv\n",
      "  层数: 3\n",
      "    Layer 0: W_NPMI=0.0210, W_C_V=0.4866, W_U_Mass=-0.7352, runs=3\n",
      "    Layer 1: W_NPMI=-0.0242, W_C_V=0.4428, W_U_Mass=-2.6047, runs=3\n",
      "    Layer 2: W_NPMI=0.0005, W_C_V=0.4742, W_U_Mass=-3.2381, runs=3\n",
      "\n",
      "总体对比文件保存到: /Volumes/My Passport/收敛结果/step2/eta_coherence_layer_comparison_k10.csv\n",
      "================================================================================\n",
      "✅ 一致性分层分析完成！(k=10)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 执行精简版一致性分层分析（文件名包含k值）\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step2\"\n",
    "top_k = 10\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"开始计算节点一致性指标并进行分层分析 (k={top_k})...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 计算节点一致性和层级汇总\n",
    "calculate_coherence_layered_analysis(base_path, corpus, top_k)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"开始按eta值汇总层级一致性统计 (k={top_k})...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 按eta汇总（传入top_k参数）\n",
    "aggregate_coherence_by_eta(base_path, top_k)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"✅ 一致性分层分析完成！(k={top_k})\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1f1602ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity_with_path_mapping_fixed(word_data, path_mapping_data, corpus, test_doc_ids, eta_smoothing=0.1):\n",
    "    \"\"\"\n",
    "    修正版：基于iteration_path_document_mapping.csv计算困惑度\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🔄 构建模型参数（基于路径映射）...\")\n",
    "    \n",
    "    # 1. 构建词典\n",
    "    all_words = sorted(list(word_data['word'].dropna().unique()))\n",
    "    word_to_id = {word: idx for idx, word in enumerate(all_words)}\n",
    "    vocab_size = len(all_words)\n",
    "    \n",
    "    print(f\"   词典大小: {vocab_size}\")\n",
    "    \n",
    "    # 2. 构建节点的词分布 φ (topic-word distribution)\n",
    "    node_word_probs = {}\n",
    "    for node_id in word_data['node_id'].unique():\n",
    "        node_words = word_data[word_data['node_id'] == node_id]\n",
    "        \n",
    "        # 初始化计数向量\n",
    "        word_counts = np.zeros(vocab_size)\n",
    "        \n",
    "        # 填充词计数\n",
    "        for _, row in node_words.iterrows():\n",
    "            word = row['word']\n",
    "            if pd.notna(word) and word in word_to_id:\n",
    "                word_counts[word_to_id[word]] = row['count']\n",
    "        \n",
    "        # 添加平滑并归一化\n",
    "        smoothed_counts = word_counts + eta_smoothing\n",
    "        word_probs = smoothed_counts / smoothed_counts.sum()\n",
    "        node_word_probs[node_id] = word_probs\n",
    "    \n",
    "    print(f\"   构建了 {len(node_word_probs)} 个节点的词分布\")\n",
    "    \n",
    "    # 3. 构建文档路径映射\n",
    "    print(\"🔄 构建文档路径映射...\")\n",
    "    \n",
    "    doc_paths = {}\n",
    "    path_lengths = []\n",
    "    \n",
    "    print(f\"   路径映射数据列: {path_mapping_data.columns.tolist()}\")\n",
    "    \n",
    "    # 找到文档ID列\n",
    "    doc_id_column = 'document_id'\n",
    "    if doc_id_column not in path_mapping_data.columns:\n",
    "        print(\"❌ 未找到document_id列\")\n",
    "        return None\n",
    "    \n",
    "    # 修正：获取层级列（包含_node_id后缀）\n",
    "    layer_columns = []\n",
    "    for i in range(10):  # 假设最多10层\n",
    "        layer_col = f'layer_{i}_node_id'  # 修正：添加_node_id后缀\n",
    "        if layer_col in path_mapping_data.columns:\n",
    "            layer_columns.append(layer_col)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    print(f\"   找到层级列: {layer_columns}\")\n",
    "    \n",
    "    if not layer_columns:\n",
    "        print(\"❌ 未找到任何层级列\")\n",
    "        return None\n",
    "    \n",
    "    # 统计文档ID范围\n",
    "    test_doc_range = f\"[{min(test_doc_ids)}, {max(test_doc_ids)}]\"\n",
    "    data_doc_range = f\"[{path_mapping_data[doc_id_column].min()}, {path_mapping_data[doc_id_column].max()}]\"\n",
    "    print(f\"   测试文档ID范围: {test_doc_range}\")\n",
    "    print(f\"   数据文档ID范围: {data_doc_range}\")\n",
    "    \n",
    "    # 提取每个文档的完整路径\n",
    "    matched_docs = 0\n",
    "    for _, row in path_mapping_data.iterrows():\n",
    "        doc_id = row[doc_id_column]\n",
    "        \n",
    "        # 检查文档ID是否在测试集中\n",
    "        if doc_id not in test_doc_ids:\n",
    "            continue\n",
    "        \n",
    "        # 构建完整路径：从layer_0_node_id到最后一个非空层级\n",
    "        path = []\n",
    "        for layer_col in layer_columns:\n",
    "            if layer_col in row and pd.notna(row[layer_col]):\n",
    "                path.append(int(row[layer_col]))\n",
    "            else:\n",
    "                break  # 遇到空值就停止\n",
    "        \n",
    "        if path:\n",
    "            doc_paths[doc_id] = path\n",
    "            path_lengths.append(len(path))\n",
    "            matched_docs += 1\n",
    "            \n",
    "            # 调试：显示前几个成功匹配的路径\n",
    "            if matched_docs <= 5:\n",
    "                print(f\"   调试 - 文档{doc_id}的完整路径: {path} (长度: {len(path)})\")\n",
    "    \n",
    "    match_rate = matched_docs / len(test_doc_ids) if test_doc_ids else 0\n",
    "    avg_path_length = np.mean(path_lengths) if path_lengths else 0\n",
    "    \n",
    "    print(f\"   匹配到路径的测试文档: {matched_docs}/{len(test_doc_ids)} ({match_rate:.1%})\")\n",
    "    print(f\"   平均路径长度: {avg_path_length:.1f}\")\n",
    "    \n",
    "    if matched_docs == 0:\n",
    "        print(\"❌ 没有匹配到任何文档路径，无法计算困惑度\")\n",
    "        return None\n",
    "    \n",
    "    # 4. 计算困惑度\n",
    "    print(\"🔄 计算困惑度...\")\n",
    "    \n",
    "    total_log_likelihood = 0.0\n",
    "    total_words = 0\n",
    "    valid_docs = 0\n",
    "    doc_perplexities = []\n",
    "    \n",
    "    for doc_id in test_doc_ids:\n",
    "        if doc_id not in corpus or doc_id not in doc_paths:\n",
    "            continue\n",
    "            \n",
    "        doc_words = corpus[doc_id]\n",
    "        if not doc_words:\n",
    "            continue\n",
    "        \n",
    "        doc_path = doc_paths[doc_id]\n",
    "        if not doc_path:\n",
    "            continue\n",
    "            \n",
    "        valid_docs += 1\n",
    "        \n",
    "        # 计算文档的对数似然\n",
    "        doc_log_likelihood = 0.0\n",
    "        doc_word_count = 0\n",
    "        \n",
    "        for word in doc_words:\n",
    "            if word in word_to_id:\n",
    "                word_id = word_to_id[word]\n",
    "                \n",
    "                # 计算词在文档路径下的概率 (路径平均策略)\n",
    "                word_prob = 0.0\n",
    "                valid_nodes = 0\n",
    "                \n",
    "                for node_id in doc_path:\n",
    "                    if node_id in node_word_probs:\n",
    "                        word_prob += node_word_probs[node_id][word_id]\n",
    "                        valid_nodes += 1\n",
    "                \n",
    "                if valid_nodes > 0:\n",
    "                    word_prob /= valid_nodes\n",
    "                \n",
    "                # 确保概率非零\n",
    "                if word_prob <= 0:\n",
    "                    word_prob = 1e-10\n",
    "                \n",
    "                doc_log_likelihood += math.log(word_prob)\n",
    "                doc_word_count += 1\n",
    "        \n",
    "        if doc_word_count > 0:\n",
    "            # 计算单文档困惑度\n",
    "            doc_perplexity = math.exp(-doc_log_likelihood / doc_word_count)\n",
    "            doc_perplexities.append(doc_perplexity)\n",
    "            \n",
    "            total_log_likelihood += doc_log_likelihood\n",
    "            total_words += doc_word_count\n",
    "    \n",
    "    if total_words == 0 or valid_docs == 0:\n",
    "        print(\"⚠️ 没有有效的测试数据\")\n",
    "        return None\n",
    "    \n",
    "    # 计算总体困惑度\n",
    "    overall_perplexity = math.exp(-total_log_likelihood / total_words)\n",
    "    avg_doc_perplexity = np.mean(doc_perplexities) if doc_perplexities else 0.0\n",
    "    \n",
    "    print(f\"   ✓ 处理了 {valid_docs} 个有效测试文档\")\n",
    "    print(f\"   ✓ 总计 {total_words} 个测试词\")\n",
    "    \n",
    "    return {\n",
    "        'perplexity': overall_perplexity,\n",
    "        'avg_doc_perplexity': avg_doc_perplexity,\n",
    "        'log_likelihood': total_log_likelihood,\n",
    "        'total_words': total_words,\n",
    "        'valid_docs': valid_docs,\n",
    "        'matched_docs': matched_docs,\n",
    "        'match_rate': match_rate,\n",
    "        'avg_path_length': avg_path_length\n",
    "    }\n",
    "\n",
    "def calculate_hlda_perplexity_with_path_mapping(base_path=\".\", corpus=None, test_ratio=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    基于iteration_path_document_mapping.csv的hLDA困惑度计算\n",
    "    \n",
    "    Parameters:\n",
    "    base_path: str, 结果文件的根目录\n",
    "    corpus: dict, 原始语料 {doc_id: [word_list]}\n",
    "    test_ratio: float, 测试集比例\n",
    "    random_state: int, 随机种子\n",
    "    \"\"\"\n",
    "    \n",
    "    if corpus is None:\n",
    "        print(\"❌ 必须提供原始语料corpus\")\n",
    "        return\n",
    "    \n",
    "    # 划分训练集和测试集\n",
    "    doc_ids = list(corpus.keys())\n",
    "    train_ids, test_ids = train_test_split(doc_ids, test_size=test_ratio, random_state=random_state)\n",
    "    \n",
    "    print(f\"📊 数据集划分:\")\n",
    "    print(f\"   总文档数: {len(doc_ids)}\")\n",
    "    print(f\"   训练集: {len(train_ids)} 文档\")\n",
    "    print(f\"   测试集: {len(test_ids)} 文档\")\n",
    "    \n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_node_word_distributions.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    print(f\"🔍 找到 {len(files)} 个模型结果文件待处理\")\n",
    "    \n",
    "    for idx, file_path in enumerate(files, 1):\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        \n",
    "        # 动态参数提取\n",
    "        eta = 0.1\n",
    "        gamma = 0.05\n",
    "        depth = 3\n",
    "        alpha = 0.1\n",
    "        \n",
    "        for param_name in ['eta', 'gamma', 'depth', 'alpha']:\n",
    "            if f'{param_name}_' in folder_name:\n",
    "                try:\n",
    "                    param_part = folder_name.split(f'{param_name}_')[1].split('_')[0]\n",
    "                    if param_name == 'depth':\n",
    "                        locals()[param_name] = int(param_part)\n",
    "                    else:\n",
    "                        locals()[param_name] = float(param_part)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ⚠️ 提取参数 {param_name} 失败: {e}\")\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"[{idx}/{len(files)}] 计算困惑度: {folder_name}\")\n",
    "        print(f\"参数 - Eta: {eta}, Gamma: {gamma}, Depth: {depth}, Alpha: {alpha}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            # 读取词分布数据\n",
    "            word_df = pd.read_csv(file_path)\n",
    "            word_df.columns = [col.strip(\"'\\\" \") for col in word_df.columns]\n",
    "            \n",
    "            # 读取路径映射数据\n",
    "            path_mapping_file = os.path.join(folder_path, 'iteration_path_document_mapping.csv')\n",
    "            if not os.path.exists(path_mapping_file):\n",
    "                print(\"⚠️ 未找到路径映射文件，跳过此文件\")\n",
    "                continue\n",
    "                \n",
    "            path_mapping_df = pd.read_csv(path_mapping_file)\n",
    "            path_mapping_df.columns = [col.strip(\"'\\\" \") for col in path_mapping_df.columns]\n",
    "            \n",
    "            # 获取最后一轮数据\n",
    "            max_iteration = word_df['iteration'].max()\n",
    "            last_word_data = word_df[word_df['iteration'] == max_iteration]\n",
    "            last_path_mapping_data = path_mapping_df[path_mapping_df['iteration'] == max_iteration]\n",
    "            \n",
    "            print(f\"📈 最后iteration: {max_iteration}\")\n",
    "            print(f\"📈 节点数: {last_word_data['node_id'].nunique()}\")\n",
    "            print(f\"📈 路径映射数: {len(last_path_mapping_data)}\")\n",
    "            \n",
    "            # 计算困惑度\n",
    "            perplexity_results = compute_perplexity_with_path_mapping(\n",
    "                last_word_data, \n",
    "                last_path_mapping_data, \n",
    "                corpus, \n",
    "                test_ids, \n",
    "                eta\n",
    "            )\n",
    "            \n",
    "            if perplexity_results is not None:\n",
    "                # 保存困惑度结果\n",
    "                perplexity_data = [{\n",
    "                    'eta': eta,\n",
    "                    'gamma': gamma,\n",
    "                    'depth': depth,\n",
    "                    'alpha': alpha,\n",
    "                    'iteration': max_iteration,\n",
    "                    'test_docs_count': len(test_ids),\n",
    "                    'valid_test_docs': perplexity_results['valid_docs'],\n",
    "                    'matched_docs': perplexity_results['matched_docs'],\n",
    "                    'total_test_words': perplexity_results['total_words'],\n",
    "                    'log_likelihood': perplexity_results['log_likelihood'],\n",
    "                    'perplexity': perplexity_results['perplexity'],\n",
    "                    'avg_doc_perplexity': perplexity_results['avg_doc_perplexity'],\n",
    "                    'doc_match_rate': perplexity_results['match_rate'],\n",
    "                    'avg_path_length': perplexity_results['avg_path_length']\n",
    "                }]\n",
    "                \n",
    "                perplexity_df = pd.DataFrame(perplexity_data)\n",
    "                output_path = os.path.join(folder_path, 'perplexity_results_path_mapping.csv')\n",
    "                perplexity_df.to_csv(output_path, index=False)\n",
    "                \n",
    "                print(f\"💾 困惑度结果已保存到: {output_path}\")\n",
    "                print(f\"📊 困惑度结果:\")\n",
    "                print(f\"   - 总困惑度: {perplexity_results['perplexity']:.4f}\")\n",
    "                print(f\"   - 平均文档困惑度: {perplexity_results['avg_doc_perplexity']:.4f}\")\n",
    "                print(f\"   - 文档匹配率: {perplexity_results['match_rate']:.1%}\")\n",
    "                print(f\"   - 平均路径长度: {perplexity_results['avg_path_length']:.1f}\")\n",
    "                print(f\"   - 有效测试文档: {perplexity_results['valid_docs']}/{len(test_ids)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"❌ 处理文件 {file_path} 时出错: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    print(f\"\\n✅ 所有文件的困惑度计算完成（基于路径映射）！\")\n",
    "\n",
    "def aggregate_path_mapping_perplexity_by_eta(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    按eta值汇总基于路径映射的困惑度统计\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(base_path, \"**\", \"perplexity_results_path_mapping.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    print(f\"🔍 找到 {len(files)} 个路径映射困惑度结果文件\")\n",
    "    \n",
    "    all_data = []\n",
    "    eta_groups = {}\n",
    "    \n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        parent_folder = os.path.dirname(folder_path)\n",
    "        \n",
    "        # 提取eta值\n",
    "        eta = None\n",
    "        if 'eta_' in folder_name:\n",
    "            try:\n",
    "                eta_part = folder_name.split('eta_')[1].split('_')[0]\n",
    "                eta = float(eta_part)\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # 提取run编号\n",
    "        run_match = folder_name.split('_run_')\n",
    "        if len(run_match) > 1:\n",
    "            run_id = run_match[1]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if eta not in eta_groups:\n",
    "            eta_groups[eta] = parent_folder\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                all_data.append({\n",
    "                    'eta': eta,\n",
    "                    'run_id': run_id,\n",
    "                    'gamma': row['gamma'],\n",
    "                    'depth': row['depth'],\n",
    "                    'alpha': row['alpha'],\n",
    "                    'perplexity': row['perplexity'],\n",
    "                    'avg_doc_perplexity': row['avg_doc_perplexity'],\n",
    "                    'valid_test_docs': row['valid_test_docs'],\n",
    "                    'total_test_words': row['total_test_words'],\n",
    "                    'doc_match_rate': row['doc_match_rate'],\n",
    "                    'avg_path_length': row['avg_path_length'],\n",
    "                    'parent_folder': parent_folder\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"读取文件 {file_path} 时出错: {e}\")\n",
    "    \n",
    "    # 转换为DataFrame并按eta分组汇总\n",
    "    summary_df = pd.DataFrame(all_data)\n",
    "    \n",
    "    if summary_df.empty:\n",
    "        print(\"未找到有效数据\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"各ETA值的路径映射困惑度汇总统计\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 按eta分组生成汇总文件\n",
    "    for eta, group_data in summary_df.groupby('eta'):\n",
    "        parent_folder = group_data['parent_folder'].iloc[0]\n",
    "        \n",
    "        print(f\"\\n处理 Eta={eta}\")\n",
    "        \n",
    "        # 计算汇总统计\n",
    "        eta_summary = group_data.agg({\n",
    "            'perplexity': ['mean', 'std', 'min', 'max', 'count'],\n",
    "            'avg_doc_perplexity': ['mean', 'std', 'min', 'max'],\n",
    "            'valid_test_docs': ['mean', 'std'],\n",
    "            'total_test_words': 'mean',\n",
    "            'doc_match_rate': ['mean', 'std'],\n",
    "            'avg_path_length': ['mean', 'std'],\n",
    "            'run_id': lambda x: ', '.join(sorted(x.unique()))\n",
    "        }).round(4)\n",
    "        \n",
    "        # 平铺列名\n",
    "        eta_summary.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in eta_summary.columns]\n",
    "        eta_summary = eta_summary.reset_index()\n",
    "        eta_summary.insert(0, 'eta', eta)\n",
    "        \n",
    "        # 保存汇总结果\n",
    "        output_filename = f'eta_{eta}_perplexity_path_mapping_summary.csv'\n",
    "        output_path = os.path.join(parent_folder, output_filename)\n",
    "        eta_summary.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"  保存汇总文件: {output_path}\")\n",
    "        print(f\"  运行数: {int(eta_summary['perplexity_count'].iloc[0])}\")\n",
    "        print(f\"  平均困惑度: {eta_summary['perplexity_mean'].iloc[0]:.4f} (±{eta_summary['perplexity_std'].iloc[0]:.4f})\")\n",
    "        print(f\"  困惑度范围: [{eta_summary['perplexity_min'].iloc[0]:.4f}, {eta_summary['perplexity_max'].iloc[0]:.4f}]\")\n",
    "        print(f\"  平均匹配率: {eta_summary['doc_match_rate_mean'].iloc[0]:.1%}\")\n",
    "        print(f\"  平均路径长度: {eta_summary['avg_path_length_mean'].iloc[0]:.1f}\")\n",
    "    \n",
    "    # 生成总体对比文件\n",
    "    overall_summary = summary_df.groupby('eta').agg({\n",
    "        'perplexity': ['mean', 'std', 'min', 'max'],\n",
    "        'avg_doc_perplexity': ['mean', 'std'],\n",
    "        'doc_match_rate': ['mean', 'std'],\n",
    "        'avg_path_length': ['mean', 'std'],\n",
    "        'run_id': 'count'\n",
    "    }).round(4)\n",
    "    \n",
    "    overall_summary.columns = ['_'.join(col).strip() for col in overall_summary.columns]\n",
    "    overall_summary = overall_summary.reset_index()\n",
    "    \n",
    "    overall_output_path = os.path.join(base_path, 'eta_perplexity_path_mapping_comparison.csv')\n",
    "    overall_summary.to_csv(overall_output_path, index=False)\n",
    "    print(f\"\\n总体对比文件保存到: {overall_output_path}\")\n",
    "    \n",
    "    # 显示跨eta对比\n",
    "    print(f\"\\n跨Eta困惑度对比（基于路径映射）:\")\n",
    "    print(\"Eta值      平均困惑度(±std)     最小值    最大值    匹配率(±std)    平均路径长度    运行数\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    for _, row in overall_summary.iterrows():\n",
    "        eta = row['eta']\n",
    "        mean_perp = row['perplexity_mean']\n",
    "        std_perp = row['perplexity_std']\n",
    "        min_perp = row['perplexity_min']\n",
    "        max_perp = row['perplexity_max']\n",
    "        match_rate = row['doc_match_rate_mean']\n",
    "        match_std = row['doc_match_rate_std']\n",
    "        path_length = row['avg_path_length_mean']\n",
    "        run_count = int(row['run_id_count'])\n",
    "        \n",
    "        print(f\"{eta:6.3f}    {mean_perp:8.4f}(±{std_perp:6.4f})   {min_perp:7.4f}   {max_perp:7.4f}   {match_rate:.1%}(±{match_std:.1%})   {path_length:8.1f}        {run_count:4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "611fa80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "开始完整的困惑度计算...\n",
      "================================================================================\n",
      "📊 数据集划分:\n",
      "   总文档数: 970\n",
      "   训练集: 776 文档\n",
      "   测试集: 194 文档\n",
      "🔍 找到 18 个模型结果文件待处理\n",
      "\n",
      "================================================================================\n",
      "[1/18] 计算困惑度: depth_3_gamma_0.05_eta_0.1_run_2\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 175\n",
      "📈 节点数: 231\n",
      "📈 路径映射数: 970\n",
      "🔄 构建模型参数（基于路径映射）...\n",
      "   词典大小: 1490\n",
      "   构建了 231 个节点的词分布\n",
      "🔄 构建文档路径映射...\n",
      "   路径映射数据列: ['iteration', 'leaf_node_id', 'document_id', 'layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   找到层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   测试文档ID范围: [23, 968]\n",
      "   数据文档ID范围: [0, 969]\n",
      "   调试 - 文档107的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档420的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档656的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档851的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档31的完整路径: [0, 1, 3] (长度: 3)\n",
      "   匹配到路径的测试文档: 194/194 (100.0%)\n",
      "   平均路径长度: 3.0\n",
      "🔄 计算困惑度...\n",
      "   ✓ 处理了 194 个有效测试文档\n",
      "   ✓ 总计 16508 个测试词\n",
      "💾 困惑度结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_2/perplexity_results_final.csv\n",
      "📊 困惑度结果:\n",
      "   - 总困惑度: 402.1430\n",
      "   - 平均文档困惑度: 418.0372\n",
      "   - 文档匹配率: 100.0%\n",
      "   - 平均路径长度: 3.0\n",
      "   - 有效测试文档: 194/194\n",
      "\n",
      "================================================================================\n",
      "[2/18] 计算困惑度: depth_3_gamma_0.05_eta_0.1_run_3\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 175\n",
      "📈 节点数: 215\n",
      "📈 路径映射数: 970\n",
      "🔄 构建模型参数（基于路径映射）...\n",
      "   词典大小: 1490\n",
      "   构建了 215 个节点的词分布\n",
      "🔄 构建文档路径映射...\n",
      "   路径映射数据列: ['iteration', 'leaf_node_id', 'document_id', 'layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   找到层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   测试文档ID范围: [23, 968]\n",
      "   数据文档ID范围: [0, 969]\n",
      "   调试 - 文档72的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档136的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档137的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档139的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档158的完整路径: [0, 1, 2] (长度: 3)\n",
      "   匹配到路径的测试文档: 194/194 (100.0%)\n",
      "   平均路径长度: 3.0\n",
      "🔄 计算困惑度...\n",
      "   ✓ 处理了 194 个有效测试文档\n",
      "   ✓ 总计 16508 个测试词\n",
      "💾 困惑度结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_3/perplexity_results_final.csv\n",
      "📊 困惑度结果:\n",
      "   - 总困惑度: 422.7682\n",
      "   - 平均文档困惑度: 437.3166\n",
      "   - 文档匹配率: 100.0%\n",
      "   - 平均路径长度: 3.0\n",
      "   - 有效测试文档: 194/194\n",
      "\n",
      "================================================================================\n",
      "[3/18] 计算困惑度: depth_3_gamma_0.05_eta_0.1_run_1\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 175\n",
      "📈 节点数: 205\n",
      "📈 路径映射数: 970\n",
      "🔄 构建模型参数（基于路径映射）...\n",
      "   词典大小: 1490\n",
      "   构建了 205 个节点的词分布\n",
      "🔄 构建文档路径映射...\n",
      "   路径映射数据列: ['iteration', 'leaf_node_id', 'document_id', 'layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   找到层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   测试文档ID范围: [23, 968]\n",
      "   数据文档ID范围: [0, 969]\n",
      "   调试 - 文档23的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档33的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档63的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档70的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档72的完整路径: [0, 1, 2] (长度: 3)\n",
      "   匹配到路径的测试文档: 194/194 (100.0%)\n",
      "   平均路径长度: 3.0\n",
      "🔄 计算困惑度...\n",
      "   ✓ 处理了 194 个有效测试文档\n",
      "   ✓ 总计 16508 个测试词\n",
      "💾 困惑度结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_1/perplexity_results_final.csv\n",
      "📊 困惑度结果:\n",
      "   - 总困惑度: 422.8394\n",
      "   - 平均文档困惑度: 450.8606\n",
      "   - 文档匹配率: 100.0%\n",
      "   - 平均路径长度: 3.0\n",
      "   - 有效测试文档: 194/194\n",
      "\n",
      "================================================================================\n",
      "[4/18] 计算困惑度: depth_3_gamma_0.05_eta_0.05_run_3\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 90\n",
      "📈 节点数: 322\n",
      "📈 路径映射数: 970\n",
      "🔄 构建模型参数（基于路径映射）...\n",
      "   词典大小: 1490\n",
      "   构建了 322 个节点的词分布\n",
      "🔄 构建文档路径映射...\n",
      "   路径映射数据列: ['iteration', 'leaf_node_id', 'document_id', 'layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   找到层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   测试文档ID范围: [23, 968]\n",
      "   数据文档ID范围: [0, 969]\n",
      "   调试 - 文档136的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档321的完整路径: [0, 3, 40] (长度: 3)\n",
      "   调试 - 文档168的完整路径: [0, 1, 53] (长度: 3)\n",
      "   调试 - 文档296的完整路径: [0, 1, 53] (长度: 3)\n",
      "   调试 - 文档784的完整路径: [0, 1, 53] (长度: 3)\n",
      "   匹配到路径的测试文档: 194/194 (100.0%)\n",
      "   平均路径长度: 3.0\n",
      "🔄 计算困惑度...\n",
      "   ✓ 处理了 194 个有效测试文档\n",
      "   ✓ 总计 16508 个测试词\n",
      "💾 困惑度结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_3/perplexity_results_final.csv\n",
      "📊 困惑度结果:\n",
      "   - 总困惑度: 377.2561\n",
      "   - 平均文档困惑度: 387.9987\n",
      "   - 文档匹配率: 100.0%\n",
      "   - 平均路径长度: 3.0\n",
      "   - 有效测试文档: 194/194\n",
      "\n",
      "================================================================================\n",
      "[5/18] 计算困惑度: depth_3_gamma_0.05_eta_0.05_run_1\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 90\n",
      "📈 节点数: 315\n",
      "📈 路径映射数: 970\n",
      "🔄 构建模型参数（基于路径映射）...\n",
      "   词典大小: 1490\n",
      "   构建了 315 个节点的词分布\n",
      "🔄 构建文档路径映射...\n",
      "   路径映射数据列: ['iteration', 'leaf_node_id', 'document_id', 'layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   找到层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   测试文档ID范围: [23, 968]\n",
      "   数据文档ID范围: [0, 969]\n",
      "   调试 - 文档306的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档342的完整路径: [0, 1, 63] (长度: 3)\n",
      "   调试 - 文档657的完整路径: [0, 1, 63] (长度: 3)\n",
      "   调试 - 文档521的完整路径: [0, 1, 65] (长度: 3)\n",
      "   调试 - 文档482的完整路径: [0, 1, 119] (长度: 3)\n",
      "   匹配到路径的测试文档: 194/194 (100.0%)\n",
      "   平均路径长度: 3.0\n",
      "🔄 计算困惑度...\n",
      "   ✓ 处理了 194 个有效测试文档\n",
      "   ✓ 总计 16508 个测试词\n",
      "💾 困惑度结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_1/perplexity_results_final.csv\n",
      "📊 困惑度结果:\n",
      "   - 总困惑度: 381.6380\n",
      "   - 平均文档困惑度: 388.1348\n",
      "   - 文档匹配率: 100.0%\n",
      "   - 平均路径长度: 3.0\n",
      "   - 有效测试文档: 194/194\n",
      "\n",
      "================================================================================\n",
      "[6/18] 计算困惑度: depth_3_gamma_0.05_eta_0.05_run_2\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 90\n",
      "📈 节点数: 299\n",
      "📈 路径映射数: 970\n",
      "🔄 构建模型参数（基于路径映射）...\n",
      "   词典大小: 1490\n",
      "   构建了 299 个节点的词分布\n",
      "🔄 构建文档路径映射...\n",
      "   路径映射数据列: ['iteration', 'leaf_node_id', 'document_id', 'layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   找到层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   测试文档ID范围: [23, 968]\n",
      "   数据文档ID范围: [0, 969]\n",
      "   调试 - 文档107的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档137的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档420的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档851的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档31的完整路径: [0, 1, 3] (长度: 3)\n",
      "   匹配到路径的测试文档: 194/194 (100.0%)\n",
      "   平均路径长度: 3.0\n",
      "🔄 计算困惑度...\n",
      "   ✓ 处理了 194 个有效测试文档\n",
      "   ✓ 总计 16508 个测试词\n",
      "💾 困惑度结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_2/perplexity_results_final.csv\n",
      "📊 困惑度结果:\n",
      "   - 总困惑度: 404.1163\n",
      "   - 平均文档困惑度: 416.8886\n",
      "   - 文档匹配率: 100.0%\n",
      "   - 平均路径长度: 3.0\n",
      "   - 有效测试文档: 194/194\n",
      "\n",
      "================================================================================\n",
      "[7/18] 计算困惑度: depth_3_gamma_0.05_eta_0.005_run_3\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 115\n",
      "📈 节点数: 427\n",
      "📈 路径映射数: 970\n",
      "🔄 构建模型参数（基于路径映射）...\n",
      "   词典大小: 1490\n",
      "   构建了 427 个节点的词分布\n",
      "🔄 构建文档路径映射...\n",
      "   路径映射数据列: ['iteration', 'leaf_node_id', 'document_id', 'layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   找到层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   测试文档ID范围: [23, 968]\n",
      "   数据文档ID范围: [0, 969]\n",
      "   调试 - 文档72的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档136的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档244的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档250的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档306的完整路径: [0, 1, 2] (长度: 3)\n",
      "   匹配到路径的测试文档: 194/194 (100.0%)\n",
      "   平均路径长度: 3.0\n",
      "🔄 计算困惑度...\n",
      "   ✓ 处理了 194 个有效测试文档\n",
      "   ✓ 总计 16508 个测试词\n",
      "💾 困惑度结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_3/perplexity_results_final.csv\n",
      "📊 困惑度结果:\n",
      "   - 总困惑度: 486.4271\n",
      "   - 平均文档困惑度: 508.0147\n",
      "   - 文档匹配率: 100.0%\n",
      "   - 平均路径长度: 3.0\n",
      "   - 有效测试文档: 194/194\n",
      "\n",
      "================================================================================\n",
      "[8/18] 计算困惑度: depth_3_gamma_0.05_eta_0.005_run_1\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 115\n",
      "📈 节点数: 438\n",
      "📈 路径映射数: 970\n",
      "🔄 构建模型参数（基于路径映射）...\n",
      "   词典大小: 1490\n",
      "   构建了 438 个节点的词分布\n",
      "🔄 构建文档路径映射...\n",
      "   路径映射数据列: ['iteration', 'leaf_node_id', 'document_id', 'layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   找到层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   测试文档ID范围: [23, 968]\n",
      "   数据文档ID范围: [0, 969]\n",
      "   调试 - 文档23的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档70的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档158的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档199的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档210的完整路径: [0, 1, 2] (长度: 3)\n",
      "   匹配到路径的测试文档: 194/194 (100.0%)\n",
      "   平均路径长度: 3.0\n",
      "🔄 计算困惑度...\n",
      "   ✓ 处理了 194 个有效测试文档\n",
      "   ✓ 总计 16508 个测试词\n",
      "💾 困惑度结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_1/perplexity_results_final.csv\n",
      "📊 困惑度结果:\n",
      "   - 总困惑度: 467.8590\n",
      "   - 平均文档困惑度: 494.3650\n",
      "   - 文档匹配率: 100.0%\n",
      "   - 平均路径长度: 3.0\n",
      "   - 有效测试文档: 194/194\n",
      "\n",
      "================================================================================\n",
      "[9/18] 计算困惑度: depth_3_gamma_0.05_eta_0.005_run_2\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 115\n",
      "📈 节点数: 432\n",
      "📈 路径映射数: 970\n",
      "🔄 构建模型参数（基于路径映射）...\n",
      "   词典大小: 1490\n",
      "   构建了 432 个节点的词分布\n",
      "🔄 构建文档路径映射...\n",
      "   路径映射数据列: ['iteration', 'leaf_node_id', 'document_id', 'layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   找到层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   测试文档ID范围: [23, 968]\n",
      "   数据文档ID范围: [0, 969]\n",
      "   调试 - 文档107的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档137的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档260的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档656的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档851的完整路径: [0, 1, 2] (长度: 3)\n",
      "   匹配到路径的测试文档: 194/194 (100.0%)\n",
      "   平均路径长度: 3.0\n",
      "🔄 计算困惑度...\n",
      "   ✓ 处理了 194 个有效测试文档\n",
      "   ✓ 总计 16508 个测试词\n",
      "💾 困惑度结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_2/perplexity_results_final.csv\n",
      "📊 困惑度结果:\n",
      "   - 总困惑度: 495.7664\n",
      "   - 平均文档困惑度: 517.1703\n",
      "   - 文档匹配率: 100.0%\n",
      "   - 平均路径长度: 3.0\n",
      "   - 有效测试文档: 194/194\n",
      "\n",
      "================================================================================\n",
      "[10/18] 计算困惑度: depth_3_gamma_0.05_eta_0.02_run_3\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 155\n",
      "📈 节点数: 388\n",
      "📈 路径映射数: 970\n",
      "🔄 构建模型参数（基于路径映射）...\n",
      "   词典大小: 1490\n",
      "   构建了 388 个节点的词分布\n",
      "🔄 构建文档路径映射...\n",
      "   路径映射数据列: ['iteration', 'leaf_node_id', 'document_id', 'layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   找到层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   测试文档ID范围: [23, 968]\n",
      "   数据文档ID范围: [0, 969]\n",
      "   调试 - 文档136的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档394的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档521的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档321的完整路径: [0, 3, 40] (长度: 3)\n",
      "   调试 - 文档745的完整路径: [0, 3, 40] (长度: 3)\n",
      "   匹配到路径的测试文档: 194/194 (100.0%)\n",
      "   平均路径长度: 3.0\n",
      "🔄 计算困惑度...\n",
      "   ✓ 处理了 194 个有效测试文档\n",
      "   ✓ 总计 16508 个测试词\n",
      "💾 困惑度结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_3/perplexity_results_final.csv\n",
      "📊 困惑度结果:\n",
      "   - 总困惑度: 417.7155\n",
      "   - 平均文档困惑度: 432.2468\n",
      "   - 文档匹配率: 100.0%\n",
      "   - 平均路径长度: 3.0\n",
      "   - 有效测试文档: 194/194\n",
      "\n",
      "================================================================================\n",
      "[11/18] 计算困惑度: depth_3_gamma_0.05_eta_0.02_run_2\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 155\n",
      "📈 节点数: 384\n",
      "📈 路径映射数: 970\n",
      "🔄 构建模型参数（基于路径映射）...\n",
      "   词典大小: 1490\n",
      "   构建了 384 个节点的词分布\n",
      "🔄 构建文档路径映射...\n",
      "   路径映射数据列: ['iteration', 'leaf_node_id', 'document_id', 'layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   找到层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   测试文档ID范围: [23, 968]\n",
      "   数据文档ID范围: [0, 969]\n",
      "   调试 - 文档107的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档137的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档420的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档656的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档851的完整路径: [0, 1, 2] (长度: 3)\n",
      "   匹配到路径的测试文档: 194/194 (100.0%)\n",
      "   平均路径长度: 3.0\n",
      "🔄 计算困惑度...\n",
      "   ✓ 处理了 194 个有效测试文档\n",
      "   ✓ 总计 16508 个测试词\n",
      "💾 困惑度结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_2/perplexity_results_final.csv\n",
      "📊 困惑度结果:\n",
      "   - 总困惑度: 431.1607\n",
      "   - 平均文档困惑度: 451.2864\n",
      "   - 文档匹配率: 100.0%\n",
      "   - 平均路径长度: 3.0\n",
      "   - 有效测试文档: 194/194\n",
      "\n",
      "================================================================================\n",
      "[12/18] 计算困惑度: depth_3_gamma_0.05_eta_0.02_run_1\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 155\n",
      "📈 节点数: 383\n",
      "📈 路径映射数: 970\n",
      "🔄 构建模型参数（基于路径映射）...\n",
      "   词典大小: 1490\n",
      "   构建了 383 个节点的词分布\n",
      "🔄 构建文档路径映射...\n",
      "   路径映射数据列: ['iteration', 'leaf_node_id', 'document_id', 'layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   找到层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   测试文档ID范围: [23, 968]\n",
      "   数据文档ID范围: [0, 969]\n",
      "   调试 - 文档23的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档63的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档139的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档158的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档306的完整路径: [0, 1, 2] (长度: 3)\n",
      "   匹配到路径的测试文档: 194/194 (100.0%)\n",
      "   平均路径长度: 3.0\n",
      "🔄 计算困惑度...\n",
      "   ✓ 处理了 194 个有效测试文档\n",
      "   ✓ 总计 16508 个测试词\n",
      "💾 困惑度结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_1/perplexity_results_final.csv\n",
      "📊 困惑度结果:\n",
      "   - 总困惑度: 419.8192\n",
      "   - 平均文档困惑度: 438.2396\n",
      "   - 文档匹配率: 100.0%\n",
      "   - 平均路径长度: 3.0\n",
      "   - 有效测试文档: 194/194\n",
      "\n",
      "================================================================================\n",
      "[13/18] 计算困惑度: depth_3_gamma_0.05_eta_0.2_run_1\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 375\n",
      "📈 节点数: 151\n",
      "📈 路径映射数: 970\n",
      "🔄 构建模型参数（基于路径映射）...\n",
      "   词典大小: 1490\n",
      "   构建了 151 个节点的词分布\n",
      "🔄 构建文档路径映射...\n",
      "   路径映射数据列: ['iteration', 'leaf_node_id', 'document_id', 'layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   找到层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   测试文档ID范围: [23, 968]\n",
      "   数据文档ID范围: [0, 969]\n",
      "   调试 - 文档30的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档33的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档49的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档70的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档96的完整路径: [0, 1, 2] (长度: 3)\n",
      "   匹配到路径的测试文档: 194/194 (100.0%)\n",
      "   平均路径长度: 3.0\n",
      "🔄 计算困惑度...\n",
      "   ✓ 处理了 194 个有效测试文档\n",
      "   ✓ 总计 16508 个测试词\n",
      "💾 困惑度结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_1/perplexity_results_final.csv\n",
      "📊 困惑度结果:\n",
      "   - 总困惑度: 447.4397\n",
      "   - 平均文档困惑度: 460.4085\n",
      "   - 文档匹配率: 100.0%\n",
      "   - 平均路径长度: 3.0\n",
      "   - 有效测试文档: 194/194\n",
      "\n",
      "================================================================================\n",
      "[14/18] 计算困惑度: depth_3_gamma_0.05_eta_0.2_run_3\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 375\n",
      "📈 节点数: 157\n",
      "📈 路径映射数: 970\n",
      "🔄 构建模型参数（基于路径映射）...\n",
      "   词典大小: 1490\n",
      "   构建了 157 个节点的词分布\n",
      "🔄 构建文档路径映射...\n",
      "   路径映射数据列: ['iteration', 'leaf_node_id', 'document_id', 'layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   找到层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   测试文档ID范围: [23, 968]\n",
      "   数据文档ID范围: [0, 969]\n",
      "   调试 - 文档96的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档137的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档158的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档213的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档306的完整路径: [0, 1, 2] (长度: 3)\n",
      "   匹配到路径的测试文档: 194/194 (100.0%)\n",
      "   平均路径长度: 3.0\n",
      "🔄 计算困惑度...\n",
      "   ✓ 处理了 194 个有效测试文档\n",
      "   ✓ 总计 16508 个测试词\n",
      "💾 困惑度结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_3/perplexity_results_final.csv\n",
      "📊 困惑度结果:\n",
      "   - 总困惑度: 427.3209\n",
      "   - 平均文档困惑度: 435.8731\n",
      "   - 文档匹配率: 100.0%\n",
      "   - 平均路径长度: 3.0\n",
      "   - 有效测试文档: 194/194\n",
      "\n",
      "================================================================================\n",
      "[15/18] 计算困惑度: depth_3_gamma_0.05_eta_0.2_run_2\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 375\n",
      "📈 节点数: 157\n",
      "📈 路径映射数: 970\n",
      "🔄 构建模型参数（基于路径映射）...\n",
      "   词典大小: 1490\n",
      "   构建了 157 个节点的词分布\n",
      "🔄 构建文档路径映射...\n",
      "   路径映射数据列: ['iteration', 'leaf_node_id', 'document_id', 'layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   找到层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   测试文档ID范围: [23, 968]\n",
      "   数据文档ID范围: [0, 969]\n",
      "   调试 - 文档72的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档96的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档137的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档158的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档213的完整路径: [0, 1, 2] (长度: 3)\n",
      "   匹配到路径的测试文档: 194/194 (100.0%)\n",
      "   平均路径长度: 3.0\n",
      "🔄 计算困惑度...\n",
      "   ✓ 处理了 194 个有效测试文档\n",
      "   ✓ 总计 16508 个测试词\n",
      "💾 困惑度结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_2/perplexity_results_final.csv\n",
      "📊 困惑度结果:\n",
      "   - 总困惑度: 437.6466\n",
      "   - 平均文档困惑度: 453.7614\n",
      "   - 文档匹配率: 100.0%\n",
      "   - 平均路径长度: 3.0\n",
      "   - 有效测试文档: 194/194\n",
      "\n",
      "================================================================================\n",
      "[16/18] 计算困惑度: depth_3_gamma_0.05_eta_0.01_run_2\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 195\n",
      "📈 节点数: 403\n",
      "📈 路径映射数: 970\n",
      "🔄 构建模型参数（基于路径映射）...\n",
      "   词典大小: 1490\n",
      "   构建了 403 个节点的词分布\n",
      "🔄 构建文档路径映射...\n",
      "   路径映射数据列: ['iteration', 'leaf_node_id', 'document_id', 'layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   找到层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   测试文档ID范围: [23, 968]\n",
      "   数据文档ID范围: [0, 969]\n",
      "   调试 - 文档107的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档851的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档31的完整路径: [0, 1, 3] (长度: 3)\n",
      "   调试 - 文档244的完整路径: [0, 1, 3] (长度: 3)\n",
      "   调试 - 文档344的完整路径: [0, 1, 3] (长度: 3)\n",
      "   匹配到路径的测试文档: 194/194 (100.0%)\n",
      "   平均路径长度: 3.0\n",
      "🔄 计算困惑度...\n",
      "   ✓ 处理了 194 个有效测试文档\n",
      "   ✓ 总计 16508 个测试词\n",
      "💾 困惑度结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_2/perplexity_results_final.csv\n",
      "📊 困惑度结果:\n",
      "   - 总困惑度: 469.2210\n",
      "   - 平均文档困惑度: 488.7826\n",
      "   - 文档匹配率: 100.0%\n",
      "   - 平均路径长度: 3.0\n",
      "   - 有效测试文档: 194/194\n",
      "\n",
      "================================================================================\n",
      "[17/18] 计算困惑度: depth_3_gamma_0.05_eta_0.01_run_1\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 195\n",
      "📈 节点数: 425\n",
      "📈 路径映射数: 970\n",
      "🔄 构建模型参数（基于路径映射）...\n",
      "   词典大小: 1490\n",
      "   构建了 425 个节点的词分布\n",
      "🔄 构建文档路径映射...\n",
      "   路径映射数据列: ['iteration', 'leaf_node_id', 'document_id', 'layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   找到层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   测试文档ID范围: [23, 968]\n",
      "   数据文档ID范围: [0, 969]\n",
      "   调试 - 文档23的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档137的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档139的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档158的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档199的完整路径: [0, 1, 2] (长度: 3)\n",
      "   匹配到路径的测试文档: 194/194 (100.0%)\n",
      "   平均路径长度: 3.0\n",
      "🔄 计算困惑度...\n",
      "   ✓ 处理了 194 个有效测试文档\n",
      "   ✓ 总计 16508 个测试词\n",
      "💾 困惑度结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_1/perplexity_results_final.csv\n",
      "📊 困惑度结果:\n",
      "   - 总困惑度: 437.5022\n",
      "   - 平均文档困惑度: 457.3055\n",
      "   - 文档匹配率: 100.0%\n",
      "   - 平均路径长度: 3.0\n",
      "   - 有效测试文档: 194/194\n",
      "\n",
      "================================================================================\n",
      "[18/18] 计算困惑度: depth_3_gamma_0.05_eta_0.01_run_3\n",
      "参数 - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 最后iteration: 195\n",
      "📈 节点数: 433\n",
      "📈 路径映射数: 970\n",
      "🔄 构建模型参数（基于路径映射）...\n",
      "   词典大小: 1490\n",
      "   构建了 433 个节点的词分布\n",
      "🔄 构建文档路径映射...\n",
      "   路径映射数据列: ['iteration', 'leaf_node_id', 'document_id', 'layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   找到层级列: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "   测试文档ID范围: [23, 968]\n",
      "   数据文档ID范围: [0, 969]\n",
      "   调试 - 文档136的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档306的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档394的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档521的完整路径: [0, 1, 2] (长度: 3)\n",
      "   调试 - 文档321的完整路径: [0, 3, 40] (长度: 3)\n",
      "   匹配到路径的测试文档: 194/194 (100.0%)\n",
      "   平均路径长度: 3.0\n",
      "🔄 计算困惑度...\n",
      "   ✓ 处理了 194 个有效测试文档\n",
      "   ✓ 总计 16508 个测试词\n",
      "💾 困惑度结果已保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_3/perplexity_results_final.csv\n",
      "📊 困惑度结果:\n",
      "   - 总困惑度: 435.5875\n",
      "   - 平均文档困惑度: 456.2074\n",
      "   - 文档匹配率: 100.0%\n",
      "   - 平均路径长度: 3.0\n",
      "   - 有效测试文档: 194/194\n",
      "\n",
      "✅ 所有文件的困惑度计算完成！\n",
      "\n",
      "================================================================================\n",
      "开始按eta值汇总困惑度统计...\n",
      "================================================================================\n",
      "🔍 找到 18 个困惑度结果文件\n",
      "📖 读取文件: depth_3_gamma_0.05_eta_0.1_run_2 - 1 行数据\n",
      "📖 读取文件: depth_3_gamma_0.05_eta_0.1_run_3 - 1 行数据\n",
      "📖 读取文件: depth_3_gamma_0.05_eta_0.1_run_1 - 1 行数据\n",
      "📖 读取文件: depth_3_gamma_0.05_eta_0.05_run_3 - 1 行数据\n",
      "📖 读取文件: depth_3_gamma_0.05_eta_0.05_run_1 - 1 行数据\n",
      "📖 读取文件: depth_3_gamma_0.05_eta_0.05_run_2 - 1 行数据\n",
      "📖 读取文件: depth_3_gamma_0.05_eta_0.005_run_3 - 1 行数据\n",
      "📖 读取文件: depth_3_gamma_0.05_eta_0.005_run_1 - 1 行数据\n",
      "📖 读取文件: depth_3_gamma_0.05_eta_0.005_run_2 - 1 行数据\n",
      "📖 读取文件: depth_3_gamma_0.05_eta_0.02_run_3 - 1 行数据\n",
      "📖 读取文件: depth_3_gamma_0.05_eta_0.02_run_2 - 1 行数据\n",
      "📖 读取文件: depth_3_gamma_0.05_eta_0.02_run_1 - 1 行数据\n",
      "📖 读取文件: depth_3_gamma_0.05_eta_0.2_run_1 - 1 行数据\n",
      "📖 读取文件: depth_3_gamma_0.05_eta_0.2_run_3 - 1 行数据\n",
      "📖 读取文件: depth_3_gamma_0.05_eta_0.2_run_2 - 1 行数据\n",
      "📖 读取文件: depth_3_gamma_0.05_eta_0.01_run_2 - 1 行数据\n",
      "📖 读取文件: depth_3_gamma_0.05_eta_0.01_run_1 - 1 行数据\n",
      "📖 读取文件: depth_3_gamma_0.05_eta_0.01_run_3 - 1 行数据\n",
      "📊 数据摘要:\n",
      "   总数据行数: 18\n",
      "   唯一eta值: [0.005, 0.01, 0.02, 0.05, 0.1, 0.2]\n",
      "   每个eta的数据量: {0.005: 3, 0.01: 3, 0.02: 3, 0.05: 3, 0.1: 3, 0.2: 3}\n",
      "================================================================================\n",
      "各ETA值的困惑度汇总统计\n",
      "================================================================================\n",
      "\n",
      "处理 Eta=0.005\n",
      "输出目录: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛\n",
      "该组数据量: 3\n",
      "   聚合字典: ['perplexity', 'avg_doc_perplexity', 'valid_test_docs', 'total_test_words', 'doc_match_rate', 'avg_path_length', 'log_likelihood', 'run_id', 'gamma', 'depth', 'alpha']\n",
      "❌ 处理Eta=0.005时出错: no results\n",
      "\n",
      "处理 Eta=0.01\n",
      "输出目录: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛\n",
      "该组数据量: 3\n",
      "   聚合字典: ['perplexity', 'avg_doc_perplexity', 'valid_test_docs', 'total_test_words', 'doc_match_rate', 'avg_path_length', 'log_likelihood', 'run_id', 'gamma', 'depth', 'alpha']\n",
      "❌ 处理Eta=0.01时出错: no results\n",
      "\n",
      "处理 Eta=0.02\n",
      "输出目录: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛\n",
      "该组数据量: 3\n",
      "   聚合字典: ['perplexity', 'avg_doc_perplexity', 'valid_test_docs', 'total_test_words', 'doc_match_rate', 'avg_path_length', 'log_likelihood', 'run_id', 'gamma', 'depth', 'alpha']\n",
      "❌ 处理Eta=0.02时出错: no results\n",
      "\n",
      "处理 Eta=0.05\n",
      "输出目录: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛\n",
      "该组数据量: 3\n",
      "   聚合字典: ['perplexity', 'avg_doc_perplexity', 'valid_test_docs', 'total_test_words', 'doc_match_rate', 'avg_path_length', 'log_likelihood', 'run_id', 'gamma', 'depth', 'alpha']\n",
      "❌ 处理Eta=0.05时出错: no results\n",
      "\n",
      "处理 Eta=0.1\n",
      "输出目录: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛\n",
      "该组数据量: 3\n",
      "   聚合字典: ['perplexity', 'avg_doc_perplexity', 'valid_test_docs', 'total_test_words', 'doc_match_rate', 'avg_path_length', 'log_likelihood', 'run_id', 'gamma', 'depth', 'alpha']\n",
      "❌ 处理Eta=0.1时出错: no results\n",
      "\n",
      "处理 Eta=0.2\n",
      "输出目录: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛\n",
      "该组数据量: 3\n",
      "   聚合字典: ['perplexity', 'avg_doc_perplexity', 'valid_test_docs', 'total_test_words', 'doc_match_rate', 'avg_path_length', 'log_likelihood', 'run_id', 'gamma', 'depth', 'alpha']\n",
      "❌ 处理Eta=0.2时出错: no results\n",
      "\n",
      "================================================================================\n",
      "生成总体对比文件\n",
      "================================================================================\n",
      "✓ 总体对比文件保存到: /Volumes/My Passport/收敛结果/step2/eta_perplexity_comparison.csv\n",
      "\n",
      "跨Eta困惑度对比:\n",
      "Eta值      平均困惑度(±std)     运行数\n",
      "--------------------------------------------------\n",
      " 0.005    483.3508(±14.2058)           3\n",
      " 0.010    447.4369(±18.8898)           3\n",
      " 0.020    422.8985(±7.2322)           3\n",
      " 0.050    387.6701(±14.4103)           3\n",
      " 0.100    415.9169(±11.9286)           3\n",
      " 0.200    437.4691(±10.0606)           3\n",
      "\n",
      "================================================================================\n",
      "开始困惑度趋势分析...\n",
      "================================================================================\n",
      "\n",
      "📈 困惑度趋势分析:\n",
      "============================================================\n",
      "Eta与平均困惑度的相关系数: -0.2204\n",
      "Eta与文档匹配率的相关系数: nan\n",
      "Eta与平均路径长度的相关系数: nan\n",
      "\n",
      "🏆 最佳表现:\n",
      "   最低平均困惑度: 387.6701 (Eta=0.05)\n",
      "   对应匹配率: 100.0%\n",
      "   运行次数: 3\n",
      "\n",
      "📊 稳定性分析 (变异系数):\n",
      "   Eta 0.005: CV=0.0294\n",
      "   Eta 0.01: CV=0.0422\n",
      "   Eta 0.02: CV=0.0171\n",
      "   Eta 0.05: CV=0.0372\n",
      "   Eta 0.1: CV=0.0287\n",
      "   Eta 0.2: CV=0.0230\n",
      "================================================================================\n",
      "✅ 困惑度计算和汇总分析完成！\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/v5/6mdkg5713kxgwg5xs24g8rvr0000gn/T/ipykernel_97915/1564661990.py\", line 283, in aggregate_perplexity_by_eta_groups\n",
      "    eta_summary = group_data.agg(agg_dict).round(4)\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/frame.py\", line 9342, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 776, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 172, in agg\n",
      "    return self.agg_dict_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 504, in agg_dict_like\n",
      "    results = {\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 505, in <dictcomp>\n",
      "    key: obj._gotitem(key, ndim=1).agg(how) for key, how in arg.items()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/series.py\", line 4605, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 1126, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 175, in agg\n",
      "    return self.agg_list_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 438, in agg_list_like\n",
      "    raise ValueError(\"no results\")\n",
      "ValueError: no results\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/v5/6mdkg5713kxgwg5xs24g8rvr0000gn/T/ipykernel_97915/1564661990.py\", line 283, in aggregate_perplexity_by_eta_groups\n",
      "    eta_summary = group_data.agg(agg_dict).round(4)\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/frame.py\", line 9342, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 776, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 172, in agg\n",
      "    return self.agg_dict_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 504, in agg_dict_like\n",
      "    results = {\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 505, in <dictcomp>\n",
      "    key: obj._gotitem(key, ndim=1).agg(how) for key, how in arg.items()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/series.py\", line 4605, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 1126, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 175, in agg\n",
      "    return self.agg_list_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 438, in agg_list_like\n",
      "    raise ValueError(\"no results\")\n",
      "ValueError: no results\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/v5/6mdkg5713kxgwg5xs24g8rvr0000gn/T/ipykernel_97915/1564661990.py\", line 283, in aggregate_perplexity_by_eta_groups\n",
      "    eta_summary = group_data.agg(agg_dict).round(4)\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/frame.py\", line 9342, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 776, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 172, in agg\n",
      "    return self.agg_dict_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 504, in agg_dict_like\n",
      "    results = {\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 505, in <dictcomp>\n",
      "    key: obj._gotitem(key, ndim=1).agg(how) for key, how in arg.items()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/series.py\", line 4605, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 1126, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 175, in agg\n",
      "    return self.agg_list_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 438, in agg_list_like\n",
      "    raise ValueError(\"no results\")\n",
      "ValueError: no results\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/v5/6mdkg5713kxgwg5xs24g8rvr0000gn/T/ipykernel_97915/1564661990.py\", line 283, in aggregate_perplexity_by_eta_groups\n",
      "    eta_summary = group_data.agg(agg_dict).round(4)\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/frame.py\", line 9342, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 776, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 172, in agg\n",
      "    return self.agg_dict_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 504, in agg_dict_like\n",
      "    results = {\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 505, in <dictcomp>\n",
      "    key: obj._gotitem(key, ndim=1).agg(how) for key, how in arg.items()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/series.py\", line 4605, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 1126, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 175, in agg\n",
      "    return self.agg_list_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 438, in agg_list_like\n",
      "    raise ValueError(\"no results\")\n",
      "ValueError: no results\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/v5/6mdkg5713kxgwg5xs24g8rvr0000gn/T/ipykernel_97915/1564661990.py\", line 283, in aggregate_perplexity_by_eta_groups\n",
      "    eta_summary = group_data.agg(agg_dict).round(4)\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/frame.py\", line 9342, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 776, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 172, in agg\n",
      "    return self.agg_dict_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 504, in agg_dict_like\n",
      "    results = {\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 505, in <dictcomp>\n",
      "    key: obj._gotitem(key, ndim=1).agg(how) for key, how in arg.items()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/series.py\", line 4605, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 1126, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 175, in agg\n",
      "    return self.agg_list_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 438, in agg_list_like\n",
      "    raise ValueError(\"no results\")\n",
      "ValueError: no results\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/v5/6mdkg5713kxgwg5xs24g8rvr0000gn/T/ipykernel_97915/1564661990.py\", line 283, in aggregate_perplexity_by_eta_groups\n",
      "    eta_summary = group_data.agg(agg_dict).round(4)\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/frame.py\", line 9342, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 776, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 172, in agg\n",
      "    return self.agg_dict_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 504, in agg_dict_like\n",
      "    results = {\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 505, in <dictcomp>\n",
      "    key: obj._gotitem(key, ndim=1).agg(how) for key, how in arg.items()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/series.py\", line 4605, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 1126, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 175, in agg\n",
      "    return self.agg_list_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 438, in agg_list_like\n",
      "    raise ValueError(\"no results\")\n",
      "ValueError: no results\n"
     ]
    }
   ],
   "source": [
    "# 首先运行完整的困惑度计算（如果还没有运行）\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def calculate_hlda_perplexity_with_path_mapping_complete(base_path=\".\", corpus=None, test_ratio=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    完整版：基于iteration_path_document_mapping.csv的hLDA困惑度计算\n",
    "    \"\"\"\n",
    "    \n",
    "    if corpus is None:\n",
    "        print(\"❌ 必须提供原始语料corpus\")\n",
    "        return\n",
    "    \n",
    "    # 划分训练集和测试集\n",
    "    doc_ids = list(corpus.keys())\n",
    "    train_ids, test_ids = train_test_split(doc_ids, test_size=test_ratio, random_state=random_state)\n",
    "    \n",
    "    print(f\"📊 数据集划分:\")\n",
    "    print(f\"   总文档数: {len(doc_ids)}\")\n",
    "    print(f\"   训练集: {len(train_ids)} 文档\")\n",
    "    print(f\"   测试集: {len(test_ids)} 文档\")\n",
    "    \n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_node_word_distributions.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    print(f\"🔍 找到 {len(files)} 个模型结果文件待处理\")\n",
    "    \n",
    "    for idx, file_path in enumerate(files, 1):\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        \n",
    "        # 动态参数提取\n",
    "        eta = 0.1\n",
    "        gamma = 0.05\n",
    "        depth = 3\n",
    "        alpha = 0.1\n",
    "        \n",
    "        for param_name in ['eta', 'gamma', 'depth', 'alpha']:\n",
    "            if f'{param_name}_' in folder_name:\n",
    "                try:\n",
    "                    param_part = folder_name.split(f'{param_name}_')[1].split('_')[0]\n",
    "                    if param_name == 'depth':\n",
    "                        locals()[param_name] = int(param_part)\n",
    "                    else:\n",
    "                        locals()[param_name] = float(param_part)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ⚠️ 提取参数 {param_name} 失败: {e}\")\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"[{idx}/{len(files)}] 计算困惑度: {folder_name}\")\n",
    "        print(f\"参数 - Eta: {eta}, Gamma: {gamma}, Depth: {depth}, Alpha: {alpha}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            # 读取词分布数据\n",
    "            word_df = pd.read_csv(file_path)\n",
    "            word_df.columns = [col.strip(\"'\\\" \") for col in word_df.columns]\n",
    "            \n",
    "            # 读取路径映射数据\n",
    "            path_mapping_file = os.path.join(folder_path, 'iteration_path_document_mapping.csv')\n",
    "            if not os.path.exists(path_mapping_file):\n",
    "                print(\"⚠️ 未找到路径映射文件，跳过此文件\")\n",
    "                continue\n",
    "                \n",
    "            path_mapping_df = pd.read_csv(path_mapping_file)\n",
    "            path_mapping_df.columns = [col.strip(\"'\\\" \") for col in path_mapping_df.columns]\n",
    "            \n",
    "            # 获取最后一轮数据\n",
    "            max_iteration = word_df['iteration'].max()\n",
    "            last_word_data = word_df[word_df['iteration'] == max_iteration]\n",
    "            last_path_mapping_data = path_mapping_df[path_mapping_df['iteration'] == max_iteration]\n",
    "            \n",
    "            print(f\"📈 最后iteration: {max_iteration}\")\n",
    "            print(f\"📈 节点数: {last_word_data['node_id'].nunique()}\")\n",
    "            print(f\"📈 路径映射数: {len(last_path_mapping_data)}\")\n",
    "            \n",
    "            # 使用修正版计算困惑度\n",
    "            perplexity_results = compute_perplexity_with_path_mapping_fixed(\n",
    "                last_word_data, \n",
    "                last_path_mapping_data, \n",
    "                corpus, \n",
    "                test_ids, \n",
    "                eta\n",
    "            )\n",
    "            \n",
    "            if perplexity_results is not None:\n",
    "                # 保存困惑度结果\n",
    "                perplexity_data = [{\n",
    "                    'eta': eta,\n",
    "                    'gamma': gamma,\n",
    "                    'depth': depth,\n",
    "                    'alpha': alpha,\n",
    "                    'iteration': max_iteration,\n",
    "                    'test_docs_count': len(test_ids),\n",
    "                    'valid_test_docs': perplexity_results['valid_docs'],\n",
    "                    'matched_docs': perplexity_results['matched_docs'],\n",
    "                    'total_test_words': perplexity_results['total_words'],\n",
    "                    'log_likelihood': perplexity_results['log_likelihood'],\n",
    "                    'perplexity': perplexity_results['perplexity'],\n",
    "                    'avg_doc_perplexity': perplexity_results['avg_doc_perplexity'],\n",
    "                    'doc_match_rate': perplexity_results['match_rate'],\n",
    "                    'avg_path_length': perplexity_results['avg_path_length']\n",
    "                }]\n",
    "                \n",
    "                perplexity_df = pd.DataFrame(perplexity_data)\n",
    "                output_path = os.path.join(folder_path, 'perplexity_results_final.csv')\n",
    "                perplexity_df.to_csv(output_path, index=False)\n",
    "                \n",
    "                print(f\"💾 困惑度结果已保存到: {output_path}\")\n",
    "                print(f\"📊 困惑度结果:\")\n",
    "                print(f\"   - 总困惑度: {perplexity_results['perplexity']:.4f}\")\n",
    "                print(f\"   - 平均文档困惑度: {perplexity_results['avg_doc_perplexity']:.4f}\")\n",
    "                print(f\"   - 文档匹配率: {perplexity_results['match_rate']:.1%}\")\n",
    "                print(f\"   - 平均路径长度: {perplexity_results['avg_path_length']:.1f}\")\n",
    "                print(f\"   - 有效测试文档: {perplexity_results['valid_docs']}/{len(test_ids)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"❌ 处理文件 {file_path} 时出错: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    print(f\"\\n✅ 所有文件的困惑度计算完成！\")\n",
    "\n",
    "def aggregate_perplexity_by_eta_groups(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    按eta值汇总多个run的平均困惑度等指标（修复版）\n",
    "    \"\"\"\n",
    "    \n",
    "    # 查找所有perplexity_results_final.csv文件\n",
    "    pattern = os.path.join(base_path, \"**\", \"perplexity_results_final.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    # 如果没有final文件，寻找其他困惑度文件\n",
    "    if len(files) == 0:\n",
    "        patterns = [\n",
    "            \"perplexity_results_path_mapping.csv\",\n",
    "            \"perplexity_results_test.csv\",\n",
    "            \"perplexity_results.csv\"\n",
    "        ]\n",
    "        for pattern_name in patterns:\n",
    "            pattern = os.path.join(base_path, \"**\", pattern_name)\n",
    "            files = glob.glob(pattern, recursive=True)\n",
    "            if len(files) > 0:\n",
    "                print(f\"🔍 使用文件模式: {pattern_name}\")\n",
    "                break\n",
    "    \n",
    "    print(f\"🔍 找到 {len(files)} 个困惑度结果文件\")\n",
    "    \n",
    "    if len(files) == 0:\n",
    "        print(\"❌ 未找到任何困惑度结果文件\")\n",
    "        return\n",
    "    \n",
    "    all_data = []\n",
    "    eta_groups = {}\n",
    "    \n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        parent_folder = os.path.dirname(folder_path)\n",
    "        \n",
    "        # 提取eta值\n",
    "        eta = None\n",
    "        if 'eta_' in folder_name:\n",
    "            try:\n",
    "                eta_part = folder_name.split('eta_')[1].split('_')[0]\n",
    "                eta = float(eta_part)\n",
    "            except:\n",
    "                print(f\"警告：无法从文件夹名称 {folder_name} 提取eta值\")\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"警告：文件夹名称 {folder_name} 不包含eta信息\")\n",
    "            continue\n",
    "        \n",
    "        # 提取run编号\n",
    "        run_match = folder_name.split('_run_')\n",
    "        if len(run_match) > 1:\n",
    "            run_id = run_match[1]\n",
    "        else:\n",
    "            print(f\"警告：无法从文件夹名称 {folder_name} 提取run编号\")\n",
    "            run_id = \"unknown\"\n",
    "        \n",
    "        if eta not in eta_groups:\n",
    "            eta_groups[eta] = parent_folder\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"📖 读取文件: {folder_name} - {len(df)} 行数据\")\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                # 检查必需字段是否存在\n",
    "                if 'perplexity' not in row:\n",
    "                    print(f\"警告：{file_path} 缺少 perplexity 列\")\n",
    "                    continue\n",
    "                    \n",
    "                all_data.append({\n",
    "                    'eta': eta,\n",
    "                    'run_id': run_id,\n",
    "                    'gamma': row.get('gamma', 0.05),\n",
    "                    'depth': row.get('depth', 3),\n",
    "                    'alpha': row.get('alpha', 0.1),\n",
    "                    'perplexity': row.get('perplexity', 0),\n",
    "                    'avg_doc_perplexity': row.get('avg_doc_perplexity', row.get('perplexity', 0)),\n",
    "                    'valid_test_docs': row.get('valid_test_docs', 0),\n",
    "                    'total_test_words': row.get('total_test_words', 0),\n",
    "                    'doc_match_rate': row.get('doc_match_rate', 0),\n",
    "                    'avg_path_length': row.get('avg_path_length', 0),\n",
    "                    'log_likelihood': row.get('log_likelihood', 0),\n",
    "                    'parent_folder': parent_folder\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"读取文件 {file_path} 时出错: {e}\")\n",
    "    \n",
    "    # 转换为DataFrame\n",
    "    summary_df = pd.DataFrame(all_data)\n",
    "    \n",
    "    if summary_df.empty:\n",
    "        print(\"未找到有效数据\")\n",
    "        return\n",
    "    \n",
    "    print(f\"📊 数据摘要:\")\n",
    "    print(f\"   总数据行数: {len(summary_df)}\")\n",
    "    print(f\"   唯一eta值: {sorted(summary_df['eta'].unique())}\")\n",
    "    print(f\"   每个eta的数据量: {summary_df['eta'].value_counts().sort_index().to_dict()}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"各ETA值的困惑度汇总统计\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 按eta分组生成汇总文件\n",
    "    for eta, group_data in summary_df.groupby('eta'):\n",
    "        parent_folder = group_data['parent_folder'].iloc[0]\n",
    "        \n",
    "        print(f\"\\n处理 Eta={eta}\")\n",
    "        print(f\"输出目录: {parent_folder}\")\n",
    "        print(f\"该组数据量: {len(group_data)}\")\n",
    "        \n",
    "        # 检查group_data是否为空\n",
    "        if len(group_data) == 0:\n",
    "            print(f\"警告：Eta={eta} 组没有数据，跳过\")\n",
    "            continue\n",
    "        \n",
    "        # 修复：更安全的聚合字典构建\n",
    "        agg_dict = {}\n",
    "        \n",
    "        # 检查每列是否存在有效数据，并构建相应的聚合字典\n",
    "        numeric_cols = ['perplexity', 'avg_doc_perplexity', 'valid_test_docs', \n",
    "                       'total_test_words', 'doc_match_rate', 'avg_path_length', 'log_likelihood']\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if col in group_data.columns:\n",
    "                # 检查列是否有非空且非NaN的数据\n",
    "                valid_data = group_data[col].dropna()\n",
    "                if len(valid_data) > 0:\n",
    "                    if col in ['perplexity', 'avg_doc_perplexity', 'doc_match_rate', 'avg_path_length', 'log_likelihood']:\n",
    "                        agg_dict[col] = ['mean', 'std', 'min', 'max']\n",
    "                    else:\n",
    "                        agg_dict[col] = ['mean', 'std']\n",
    "                else:\n",
    "                    print(f\"   警告：{col} 列没有有效数据\")\n",
    "        \n",
    "        # 添加计数 - 使用不会为空的列\n",
    "        if 'run_id' in group_data.columns:\n",
    "            agg_dict['run_id'] = 'count'\n",
    "        \n",
    "        # 参数列 - 使用first（第一个值）\n",
    "        for col in ['gamma', 'depth', 'alpha']:\n",
    "            if col in group_data.columns:\n",
    "                agg_dict[col] = 'first'\n",
    "        \n",
    "        # 检查聚合字典是否为空\n",
    "        if not agg_dict:\n",
    "            print(f\"警告：Eta={eta} 组没有可聚合的列，跳过\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # 执行聚合操作\n",
    "            print(f\"   聚合字典: {list(agg_dict.keys())}\")\n",
    "            eta_summary = group_data.agg(agg_dict).round(4)\n",
    "            \n",
    "            # 平铺列名\n",
    "            eta_summary.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in eta_summary.columns]\n",
    "            eta_summary = eta_summary.reset_index()\n",
    "            eta_summary.insert(0, 'eta', eta)\n",
    "            \n",
    "            # 添加run_id列表\n",
    "            run_ids = ', '.join(sorted(group_data['run_id'].unique()))\n",
    "            eta_summary['run_ids'] = run_ids\n",
    "            \n",
    "            # 保存汇总结果到与run文件夹同级的位置\n",
    "            output_filename = f'eta_{eta}_perplexity_summary.csv'\n",
    "            output_path = os.path.join(parent_folder, output_filename)\n",
    "            eta_summary.to_csv(output_path, index=False)\n",
    "            \n",
    "            print(f\"  ✓ 保存汇总文件: {output_path}\")\n",
    "            \n",
    "            # 显示统计信息\n",
    "            if 'run_id_count' in eta_summary.columns:\n",
    "                print(f\"  运行数: {int(eta_summary['run_id_count'].iloc[0])}\")\n",
    "            \n",
    "            if 'perplexity_mean' in eta_summary.columns:\n",
    "                mean_perp = eta_summary['perplexity_mean'].iloc[0]\n",
    "                std_perp = eta_summary.get('perplexity_std', pd.Series([0])).iloc[0]\n",
    "                print(f\"  平均困惑度: {mean_perp:.4f} (±{std_perp:.4f})\")\n",
    "            \n",
    "            print(f\"  包含运行: {run_ids}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 处理Eta={eta}时出错: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # 生成总体对比文件（保存在base_path下）\n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(\"生成总体对比文件\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # 构建总体聚合字典\n",
    "        overall_agg_dict = {}\n",
    "        \n",
    "        # 检查每列是否有足够的数据进行聚合\n",
    "        for col in ['perplexity', 'avg_doc_perplexity', 'doc_match_rate', 'avg_path_length', \n",
    "                   'valid_test_docs', 'total_test_words', 'log_likelihood']:\n",
    "            if col in summary_df.columns:\n",
    "                valid_data = summary_df[col].dropna()\n",
    "                if len(valid_data) > 0:\n",
    "                    overall_agg_dict[col] = ['mean', 'std']\n",
    "                    if col in ['perplexity', 'avg_doc_perplexity']:\n",
    "                        overall_agg_dict[col].extend(['min', 'max'])\n",
    "        \n",
    "        # 添加计数\n",
    "        if 'run_id' in summary_df.columns:\n",
    "            overall_agg_dict['run_id'] = 'count'\n",
    "        \n",
    "        if not overall_agg_dict:\n",
    "            print(\"警告：没有可聚合的列用于总体对比\")\n",
    "            return None\n",
    "        \n",
    "        overall_summary = summary_df.groupby('eta').agg(overall_agg_dict).round(4)\n",
    "        \n",
    "        # 平铺列名\n",
    "        overall_summary.columns = ['_'.join(col).strip() for col in overall_summary.columns]\n",
    "        overall_summary = overall_summary.reset_index()\n",
    "        \n",
    "        overall_output_path = os.path.join(base_path, 'eta_perplexity_comparison.csv')\n",
    "        overall_summary.to_csv(overall_output_path, index=False)\n",
    "        print(f\"✓ 总体对比文件保存到: {overall_output_path}\")\n",
    "        \n",
    "        # 显示跨eta对比\n",
    "        print(f\"\\n跨Eta困惑度对比:\")\n",
    "        print(\"Eta值      平均困惑度(±std)     运行数\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for _, row in overall_summary.iterrows():\n",
    "            eta = row['eta']\n",
    "            run_count = int(row.get('run_id_count', 0))\n",
    "            \n",
    "            if 'perplexity_mean' in row:\n",
    "                mean_perp = row['perplexity_mean']\n",
    "                std_perp = row.get('perplexity_std', 0)\n",
    "                print(f\"{eta:6.3f}    {mean_perp:8.4f}(±{std_perp:6.4f})        {run_count:4d}\")\n",
    "            else:\n",
    "                print(f\"{eta:6.3f}    数据缺失                    {run_count:4d}\")\n",
    "        \n",
    "        return overall_summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 生成总体对比时出错: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def analyze_perplexity_trends(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    分析困惑度趋势\n",
    "    \"\"\"\n",
    "    # 读取总体对比文件\n",
    "    comparison_file = os.path.join(base_path, 'eta_perplexity_comparison.csv')\n",
    "    \n",
    "    if os.path.exists(comparison_file):\n",
    "        df = pd.read_csv(comparison_file)\n",
    "        \n",
    "        print(f\"\\n📈 困惑度趋势分析:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # 计算eta与困惑度的相关性\n",
    "        eta_perp_corr = df['eta'].corr(df['perplexity_mean'])\n",
    "        eta_match_corr = df['eta'].corr(df['doc_match_rate_mean'])\n",
    "        eta_path_corr = df['eta'].corr(df['avg_path_length_mean'])\n",
    "        \n",
    "        print(f\"Eta与平均困惑度的相关系数: {eta_perp_corr:.4f}\")\n",
    "        print(f\"Eta与文档匹配率的相关系数: {eta_match_corr:.4f}\")\n",
    "        print(f\"Eta与平均路径长度的相关系数: {eta_path_corr:.4f}\")\n",
    "        \n",
    "        # 找出最佳eta值\n",
    "        best_eta_idx = df['perplexity_mean'].idxmin()\n",
    "        best_eta = df.loc[best_eta_idx, 'eta']\n",
    "        best_perplexity = df.loc[best_eta_idx, 'perplexity_mean']\n",
    "        \n",
    "        print(f\"\\n🏆 最佳表现:\")\n",
    "        print(f\"   最低平均困惑度: {best_perplexity:.4f} (Eta={best_eta})\")\n",
    "        print(f\"   对应匹配率: {df.loc[best_eta_idx, 'doc_match_rate_mean']:.1%}\")\n",
    "        print(f\"   运行次数: {int(df.loc[best_eta_idx, 'run_id_count'])}\")\n",
    "        \n",
    "        # 稳定性分析\n",
    "        print(f\"\\n📊 稳定性分析 (变异系数):\")\n",
    "        for _, row in df.iterrows():\n",
    "            eta = row['eta']\n",
    "            cv = row['perplexity_std'] / row['perplexity_mean'] if row['perplexity_mean'] > 0 else 0\n",
    "            print(f\"   Eta {eta}: CV={cv:.4f}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"⚠️ 未找到总体对比文件，请先运行汇总函数\")\n",
    "\n",
    "# 执行完整的困惑度计算和汇总\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step2\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"开始完整的困惑度计算...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. 计算困惑度（如果还没有完成）\n",
    "calculate_hlda_perplexity_with_path_mapping_complete(base_path, corpus, test_ratio=0.2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"开始按eta值汇总困惑度统计...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 2. 按eta汇总\n",
    "overall_summary = aggregate_perplexity_by_eta_groups(base_path)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"开始困惑度趋势分析...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 3. 趋势分析\n",
    "analyze_perplexity_trends(base_path)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"✅ 困惑度计算和汇总分析完成！\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5ad4e823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "开始计算分枝数和基尼系数指标...\n",
      "================================================================================\n",
      "🔍 找到 18 个entropy文件待处理\n",
      "\n",
      "[1/18] 处理文件夹: depth_3_gamma_0.05_eta_0.1_run_2\n",
      "✓ 层级指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_2/layer_branching_gini_metrics.csv\n",
      "✓ 全局指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_2/global_branching_gini_metrics.csv\n",
      "📊 指标摘要:\n",
      "   总节点数: 231\n",
      "   总层数: 3\n",
      "   全局平均分枝: 5.11\n",
      "   全局文档基尼: 0.7333\n",
      "   全局分枝基尼: 0.4790\n",
      "\n",
      "[2/18] 处理文件夹: depth_3_gamma_0.05_eta_0.1_run_3\n",
      "✓ 层级指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_3/layer_branching_gini_metrics.csv\n",
      "✓ 全局指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_3/global_branching_gini_metrics.csv\n",
      "📊 指标摘要:\n",
      "   总节点数: 215\n",
      "   总层数: 3\n",
      "   全局平均分枝: 5.10\n",
      "   全局文档基尼: 0.7276\n",
      "   全局分枝基尼: 0.5579\n",
      "\n",
      "[3/18] 处理文件夹: depth_3_gamma_0.05_eta_0.1_run_1\n",
      "✓ 层级指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_1/layer_branching_gini_metrics.csv\n",
      "✓ 全局指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_1/global_branching_gini_metrics.csv\n",
      "📊 指标摘要:\n",
      "   总节点数: 205\n",
      "   总层数: 3\n",
      "   全局平均分枝: 4.98\n",
      "   全局文档基尼: 0.7307\n",
      "   全局分枝基尼: 0.4993\n",
      "\n",
      "[4/18] 处理文件夹: depth_3_gamma_0.05_eta_0.05_run_3\n",
      "✓ 层级指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_3/layer_branching_gini_metrics.csv\n",
      "✓ 全局指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_3/global_branching_gini_metrics.csv\n",
      "📊 指标摘要:\n",
      "   总节点数: 322\n",
      "   总层数: 3\n",
      "   全局平均分枝: 5.35\n",
      "   全局文档基尼: 0.6835\n",
      "   全局分枝基尼: 0.5263\n",
      "\n",
      "[5/18] 处理文件夹: depth_3_gamma_0.05_eta_0.05_run_1\n",
      "✓ 层级指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_1/layer_branching_gini_metrics.csv\n",
      "✓ 全局指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_1/global_branching_gini_metrics.csv\n",
      "📊 指标摘要:\n",
      "   总节点数: 315\n",
      "   总层数: 3\n",
      "   全局平均分枝: 4.98\n",
      "   全局文档基尼: 0.6757\n",
      "   全局分枝基尼: 0.5166\n",
      "\n",
      "[6/18] 处理文件夹: depth_3_gamma_0.05_eta_0.05_run_2\n",
      "✓ 层级指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_2/layer_branching_gini_metrics.csv\n",
      "✓ 全局指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_2/global_branching_gini_metrics.csv\n",
      "📊 指标摘要:\n",
      "   总节点数: 299\n",
      "   总层数: 3\n",
      "   全局平均分枝: 5.32\n",
      "   全局文档基尼: 0.6964\n",
      "   全局分枝基尼: 0.5091\n",
      "\n",
      "[7/18] 处理文件夹: depth_3_gamma_0.05_eta_0.005_run_3\n",
      "✓ 层级指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_3/layer_branching_gini_metrics.csv\n",
      "✓ 全局指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_3/global_branching_gini_metrics.csv\n",
      "📊 指标摘要:\n",
      "   总节点数: 427\n",
      "   总层数: 3\n",
      "   全局平均分枝: 4.68\n",
      "   全局文档基尼: 0.7188\n",
      "   全局分枝基尼: 0.5825\n",
      "\n",
      "[8/18] 处理文件夹: depth_3_gamma_0.05_eta_0.005_run_1\n",
      "✓ 层级指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_1/layer_branching_gini_metrics.csv\n",
      "✓ 全局指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_1/global_branching_gini_metrics.csv\n",
      "📊 指标摘要:\n",
      "   总节点数: 438\n",
      "   总层数: 3\n",
      "   全局平均分枝: 5.08\n",
      "   全局文档基尼: 0.7105\n",
      "   全局分枝基尼: 0.5222\n",
      "\n",
      "[9/18] 处理文件夹: depth_3_gamma_0.05_eta_0.005_run_2\n",
      "✓ 层级指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_2/layer_branching_gini_metrics.csv\n",
      "✓ 全局指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_2/global_branching_gini_metrics.csv\n",
      "📊 指标摘要:\n",
      "   总节点数: 432\n",
      "   总层数: 3\n",
      "   全局平均分枝: 5.32\n",
      "   全局文档基尼: 0.7291\n",
      "   全局分枝基尼: 0.4957\n",
      "\n",
      "[10/18] 处理文件夹: depth_3_gamma_0.05_eta_0.02_run_3\n",
      "✓ 层级指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_3/layer_branching_gini_metrics.csv\n",
      "✓ 全局指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_3/global_branching_gini_metrics.csv\n",
      "📊 指标摘要:\n",
      "   总节点数: 388\n",
      "   总层数: 3\n",
      "   全局平均分枝: 5.16\n",
      "   全局文档基尼: 0.6993\n",
      "   全局分枝基尼: 0.5467\n",
      "\n",
      "[11/18] 处理文件夹: depth_3_gamma_0.05_eta_0.02_run_2\n",
      "✓ 层级指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_2/layer_branching_gini_metrics.csv\n",
      "✓ 全局指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_2/global_branching_gini_metrics.csv\n",
      "📊 指标摘要:\n",
      "   总节点数: 384\n",
      "   总层数: 3\n",
      "   全局平均分枝: 5.25\n",
      "   全局文档基尼: 0.7107\n",
      "   全局分枝基尼: 0.4956\n",
      "\n",
      "[12/18] 处理文件夹: depth_3_gamma_0.05_eta_0.02_run_1\n",
      "✓ 层级指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_1/layer_branching_gini_metrics.csv\n",
      "✓ 全局指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_1/global_branching_gini_metrics.csv\n",
      "📊 指标摘要:\n",
      "   总节点数: 383\n",
      "   总层数: 3\n",
      "   全局平均分枝: 5.09\n",
      "   全局文档基尼: 0.7051\n",
      "   全局分枝基尼: 0.5166\n",
      "\n",
      "[13/18] 处理文件夹: depth_3_gamma_0.05_eta_0.2_run_1\n",
      "✓ 层级指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_1/layer_branching_gini_metrics.csv\n",
      "✓ 全局指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_1/global_branching_gini_metrics.csv\n",
      "📊 指标摘要:\n",
      "   总节点数: 151\n",
      "   总层数: 3\n",
      "   全局平均分枝: 5.17\n",
      "   全局文档基尼: 0.7523\n",
      "   全局分枝基尼: 0.4690\n",
      "\n",
      "[14/18] 处理文件夹: depth_3_gamma_0.05_eta_0.2_run_3\n",
      "✓ 层级指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_3/layer_branching_gini_metrics.csv\n",
      "✓ 全局指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_3/global_branching_gini_metrics.csv\n",
      "📊 指标摘要:\n",
      "   总节点数: 157\n",
      "   总层数: 3\n",
      "   全局平均分枝: 5.20\n",
      "   全局文档基尼: 0.7440\n",
      "   全局分枝基尼: 0.4739\n",
      "\n",
      "[15/18] 处理文件夹: depth_3_gamma_0.05_eta_0.2_run_2\n",
      "✓ 层级指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_2/layer_branching_gini_metrics.csv\n",
      "✓ 全局指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_2/global_branching_gini_metrics.csv\n",
      "📊 指标摘要:\n",
      "   总节点数: 157\n",
      "   总层数: 3\n",
      "   全局平均分枝: 4.88\n",
      "   全局文档基尼: 0.7593\n",
      "   全局分枝基尼: 0.4896\n",
      "\n",
      "[16/18] 处理文件夹: depth_3_gamma_0.05_eta_0.01_run_2\n",
      "✓ 层级指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_2/layer_branching_gini_metrics.csv\n",
      "✓ 全局指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_2/global_branching_gini_metrics.csv\n",
      "📊 指标摘要:\n",
      "   总节点数: 403\n",
      "   总层数: 3\n",
      "   全局平均分枝: 5.22\n",
      "   全局文档基尼: 0.7153\n",
      "   全局分枝基尼: 0.4736\n",
      "\n",
      "[17/18] 处理文件夹: depth_3_gamma_0.05_eta_0.01_run_1\n",
      "✓ 层级指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_1/layer_branching_gini_metrics.csv\n",
      "✓ 全局指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_1/global_branching_gini_metrics.csv\n",
      "📊 指标摘要:\n",
      "   总节点数: 425\n",
      "   总层数: 3\n",
      "   全局平均分枝: 5.23\n",
      "   全局文档基尼: 0.7017\n",
      "   全局分枝基尼: 0.5356\n",
      "\n",
      "[18/18] 处理文件夹: depth_3_gamma_0.05_eta_0.01_run_3\n",
      "✓ 层级指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_3/layer_branching_gini_metrics.csv\n",
      "✓ 全局指标保存到: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_3/global_branching_gini_metrics.csv\n",
      "📊 指标摘要:\n",
      "   总节点数: 433\n",
      "   总层数: 3\n",
      "   全局平均分枝: 5.27\n",
      "   全局文档基尼: 0.7037\n",
      "   全局分枝基尼: 0.5245\n",
      "\n",
      "================================================================================\n",
      "开始按eta值汇总分枝数和基尼系数统计...\n",
      "================================================================================\n",
      "================================================================================\n",
      "汇总层级分枝数和基尼系数指标...\n",
      "================================================================================\n",
      "🔍 找到 18 个层级指标文件\n",
      "各ETA值的层级分枝数和基尼系数汇总统计\n",
      "================================================================================\n",
      "\n",
      "处理 Eta=0.005\n",
      "  保存层级汇总文件: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/eta_0.005_layer_branching_gini_summary.csv\n",
      "  层数: 3\n",
      "    Layer 0: 分枝=85.00, 文档基尼=0.0000, 分枝基尼=0.0000, runs=3\n",
      "    Layer 1: 分枝=4.09, 文档基尼=0.6052, 分枝基尼=0.4391, runs=3\n",
      "    Layer 2: 分枝=0.00, 文档基尼=0.4484, 分枝基尼=0.0000, runs=3\n",
      "\n",
      "处理 Eta=0.01\n",
      "  保存层级汇总文件: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/eta_0.01_layer_branching_gini_summary.csv\n",
      "  层数: 3\n",
      "    Layer 0: 分枝=79.00, 文档基尼=0.0000, 分枝基尼=0.0000, runs=3\n",
      "    Layer 1: 分枝=4.31, 文档基尼=0.5664, 分枝基尼=0.4184, runs=3\n",
      "    Layer 2: 分枝=0.00, 文档基尼=0.4233, 分枝基尼=0.0000, runs=3\n",
      "\n",
      "处理 Eta=0.02\n",
      "  保存层级汇总文件: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/eta_0.02_layer_branching_gini_summary.csv\n",
      "  层数: 3\n",
      "    Layer 0: 分枝=73.33, 文档基尼=0.0000, 分枝基尼=0.0000, runs=3\n",
      "    Layer 1: 分枝=4.24, 文档基尼=0.5716, 分枝基尼=0.4285, runs=3\n",
      "    Layer 2: 分枝=0.00, 文档基尼=0.4216, 分枝基尼=0.0000, runs=3\n",
      "\n",
      "处理 Eta=0.05\n",
      "  保存层级汇总文件: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/eta_0.05_layer_branching_gini_summary.csv\n",
      "  层数: 3\n",
      "    Layer 0: 分枝=58.67, 文档基尼=0.0000, 分枝基尼=0.0000, runs=3\n",
      "    Layer 1: 分枝=4.31, 文档基尼=0.5321, 分枝基尼=0.4328, runs=3\n",
      "    Layer 2: 分枝=0.00, 文档基尼=0.3857, 分枝基尼=0.0000, runs=3\n",
      "\n",
      "处理 Eta=0.1\n",
      "  保存层级汇总文件: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/eta_0.1_layer_branching_gini_summary.csv\n",
      "  层数: 3\n",
      "    Layer 0: 分枝=41.67, 文档基尼=0.0000, 分枝基尼=0.0000, runs=3\n",
      "    Layer 1: 分枝=4.18, 文档基尼=0.6376, 分枝基尼=0.4325, runs=3\n",
      "    Layer 2: 分枝=0.00, 文档基尼=0.4807, 分枝基尼=0.0000, runs=3\n",
      "\n",
      "处理 Eta=0.2\n",
      "  保存层级汇总文件: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/eta_0.2_layer_branching_gini_summary.csv\n",
      "  层数: 3\n",
      "    Layer 0: 分枝=29.33, 文档基尼=0.0000, 分枝基尼=0.0000, runs=3\n",
      "    Layer 1: 分枝=4.26, 文档基尼=0.6467, 分枝基尼=0.4084, runs=3\n",
      "    Layer 2: 分枝=0.00, 文档基尼=0.5406, 分枝基尼=0.0000, runs=3\n",
      "\n",
      "总体层级对比文件保存到: /Volumes/My Passport/收敛结果/step2/eta_layer_branching_gini_comparison.csv\n",
      "\n",
      "================================================================================\n",
      "汇总全局分枝数和基尼系数指标...\n",
      "================================================================================\n",
      "🔍 找到 18 个全局指标文件\n",
      "各ETA值的全局分枝数和基尼系数汇总统计\n",
      "================================================================================\n",
      "\n",
      "处理 Eta=0.005 全局指标\n",
      "  保存全局汇总文件: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/eta_0.005_global_branching_gini_summary.csv\n",
      "  运行数: 0\n",
      "  全局平均分枝: 0.00\n",
      "  全局文档基尼: 0.0000\n",
      "  全局分枝基尼: 0.0000\n",
      "\n",
      "处理 Eta=0.01 全局指标\n",
      "  保存全局汇总文件: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/eta_0.01_global_branching_gini_summary.csv\n",
      "  运行数: 0\n",
      "  全局平均分枝: 0.00\n",
      "  全局文档基尼: 0.0000\n",
      "  全局分枝基尼: 0.0000\n",
      "\n",
      "处理 Eta=0.02 全局指标\n",
      "  保存全局汇总文件: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/eta_0.02_global_branching_gini_summary.csv\n",
      "  运行数: 0\n",
      "  全局平均分枝: 0.00\n",
      "  全局文档基尼: 0.0000\n",
      "  全局分枝基尼: 0.0000\n",
      "\n",
      "处理 Eta=0.05 全局指标\n",
      "  保存全局汇总文件: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/eta_0.05_global_branching_gini_summary.csv\n",
      "  运行数: 0\n",
      "  全局平均分枝: 0.00\n",
      "  全局文档基尼: 0.0000\n",
      "  全局分枝基尼: 0.0000\n",
      "\n",
      "处理 Eta=0.1 全局指标\n",
      "  保存全局汇总文件: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/eta_0.1_global_branching_gini_summary.csv\n",
      "  运行数: 0\n",
      "  全局平均分枝: 0.00\n",
      "  全局文档基尼: 0.0000\n",
      "  全局分枝基尼: 0.0000\n",
      "\n",
      "处理 Eta=0.2 全局指标\n",
      "  保存全局汇总文件: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/eta_0.2_global_branching_gini_summary.csv\n",
      "  运行数: 0\n",
      "  全局平均分枝: 0.00\n",
      "  全局文档基尼: 0.0000\n",
      "  全局分枝基尼: 0.0000\n",
      "\n",
      "总体全局对比文件保存到: /Volumes/My Passport/收敛结果/step2/eta_global_branching_gini_comparison.csv\n",
      "\n",
      "跨Eta全局指标对比:\n",
      "Eta值      平均分枝(±std)     文档基尼(±std)     分枝基尼(±std)     运行数\n",
      "--------------------------------------------------------------------------------\n",
      " 0.005      5.03(±0.32)     0.7195(±0.0093)     0.5335(±0.0445)        3\n",
      " 0.010      5.24(±0.02)     0.7069(±0.0073)     0.5112(±0.0331)        3\n",
      " 0.020      5.17(±0.08)     0.7050(±0.0057)     0.5196(±0.0257)        3\n",
      " 0.050      5.22(±0.20)     0.6852(±0.0105)     0.5174(±0.0086)        3\n",
      " 0.100      5.06(±0.07)     0.7305(±0.0028)     0.5121(±0.0409)        3\n",
      " 0.200      5.08(±0.18)     0.7519(±0.0077)     0.4775(±0.0108)        3\n",
      "\n",
      "================================================================================\n",
      "显示分枝数和基尼系数汇总报告...\n",
      "================================================================================\n",
      "====================================================================================================\n",
      "分枝数和基尼系数分析汇总报告\n",
      "====================================================================================================\n",
      "\n",
      "📊 层级分枝数和基尼系数分析:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Layer 0 跨Eta对比:\n",
      "Eta值      平均分枝(±std)     文档基尼(±std)     分枝基尼(±std)     运行数\n",
      "---------------------------------------------------------------------------\n",
      " 0.005     85.00(±5.00)     0.0000(±0.0000)     0.0000(±0.0000)        3\n",
      " 0.010     79.00(±2.65)     0.0000(±0.0000)     0.0000(±0.0000)        3\n",
      " 0.020     73.33(±1.15)     0.0000(±0.0000)     0.0000(±0.0000)        3\n",
      " 0.050     58.67(±3.51)     0.0000(±0.0000)     0.0000(±0.0000)        3\n",
      " 0.100     41.67(±2.08)     0.0000(±0.0000)     0.0000(±0.0000)        3\n",
      " 0.200     29.33(±1.53)     0.0000(±0.0000)     0.0000(±0.0000)        3\n",
      "\n",
      "Layer 1 跨Eta对比:\n",
      "Eta值      平均分枝(±std)     文档基尼(±std)     分枝基尼(±std)     运行数\n",
      "---------------------------------------------------------------------------\n",
      " 0.005      4.09(±0.33)     0.6052(±0.0402)     0.4391(±0.0459)        3\n",
      " 0.010      4.31(±0.02)     0.5664(±0.0113)     0.4184(±0.0409)        3\n",
      " 0.020      4.24(±0.08)     0.5716(±0.0264)     0.4285(±0.0307)        3\n",
      " 0.050      4.31(±0.21)     0.5321(±0.0113)     0.4328(±0.0121)        3\n",
      " 0.100      4.18(±0.07)     0.6376(±0.0306)     0.4325(±0.0507)        3\n",
      " 0.200      4.26(±0.19)     0.6467(±0.0201)     0.4084(±0.0061)        3\n",
      "\n",
      "Layer 2 跨Eta对比:\n",
      "Eta值      平均分枝(±std)     文档基尼(±std)     分枝基尼(±std)     运行数\n",
      "---------------------------------------------------------------------------\n",
      " 0.005      0.00(±0.00)     0.4484(±0.0198)     0.0000(±0.0000)        3\n",
      " 0.010      0.00(±0.00)     0.4233(±0.0182)     0.0000(±0.0000)        3\n",
      " 0.020      0.00(±0.00)     0.4216(±0.0197)     0.0000(±0.0000)        3\n",
      " 0.050      0.00(±0.00)     0.3857(±0.0219)     0.0000(±0.0000)        3\n",
      " 0.100      0.00(±0.00)     0.4807(±0.0178)     0.0000(±0.0000)        3\n",
      " 0.200      0.00(±0.00)     0.5406(±0.0159)     0.0000(±0.0000)        3\n",
      "\n",
      "📊 全局分枝数和基尼系数分析:\n",
      "------------------------------------------------------------\n",
      "Eta值      平均分枝(±std)     文档基尼(±std)     分枝基尼(±std)     运行数\n",
      "--------------------------------------------------------------------------------\n",
      " 0.005      5.03(±0.32)     0.7195(±0.0093)     0.5335(±0.0445)        3\n",
      " 0.010      5.24(±0.02)     0.7069(±0.0073)     0.5112(±0.0331)        3\n",
      " 0.020      5.17(±0.08)     0.7050(±0.0057)     0.5196(±0.0257)        3\n",
      " 0.050      5.22(±0.20)     0.6852(±0.0105)     0.5174(±0.0086)        3\n",
      " 0.100      5.06(±0.07)     0.7305(±0.0028)     0.5121(±0.0409)        3\n",
      " 0.200      5.08(±0.18)     0.7519(±0.0077)     0.4775(±0.0108)        3\n",
      "\n",
      "====================================================================================================\n",
      "✅ 分枝数和基尼系数分析完成！\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def calculate_branching_and_gini_metrics(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    计算每个模型的分枝数和基尼系数指标\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(base_path, \"**\", \"corrected_renyi_entropy.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    print(f\"🔍 找到 {len(files)} 个entropy文件待处理\")\n",
    "    \n",
    "    for idx, file_path in enumerate(files, 1):\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        \n",
    "        print(f\"\\n[{idx}/{len(files)}] 处理文件夹: {folder_name}\")\n",
    "        \n",
    "        try:\n",
    "            # 读取entropy文件\n",
    "            entropy_df = pd.read_csv(file_path)\n",
    "            \n",
    "            # 检查必要的列是否存在\n",
    "            required_cols = ['node_id', 'layer', 'document_count', 'child_count']\n",
    "            missing_cols = [col for col in required_cols if col not in entropy_df.columns]\n",
    "            \n",
    "            if missing_cols:\n",
    "                print(f\"⚠️ 缺少必要列: {missing_cols}，跳过此文件\")\n",
    "                continue\n",
    "            \n",
    "            # 1. 计算层级分枝数和基尼系数指标\n",
    "            layer_metrics = []\n",
    "            \n",
    "            for layer in entropy_df['layer'].unique():\n",
    "                if layer == -1:  # 跳过无效层级\n",
    "                    continue\n",
    "                    \n",
    "                layer_nodes = entropy_df[entropy_df['layer'] == layer]\n",
    "                \n",
    "                # 基本统计\n",
    "                node_count = len(layer_nodes)\n",
    "                total_documents = layer_nodes['document_count'].sum()\n",
    "                \n",
    "                # 分枝数统计\n",
    "                child_counts = layer_nodes['child_count'].values\n",
    "                total_branches = child_counts.sum()\n",
    "                \n",
    "                # 非叶子节点统计\n",
    "                non_leaf_nodes = (child_counts > 0).sum()\n",
    "                non_leaf_counts = child_counts[child_counts > 0]\n",
    "                \n",
    "                # 分枝因子统计\n",
    "                if len(non_leaf_counts) > 0:\n",
    "                    avg_branching_factor = non_leaf_counts.mean()\n",
    "                    std_branching_factor = non_leaf_counts.std()\n",
    "                    non_leaf_avg_branching = non_leaf_counts.mean()\n",
    "                else:\n",
    "                    avg_branching_factor = 0.0\n",
    "                    std_branching_factor = 0.0\n",
    "                    non_leaf_avg_branching = 0.0\n",
    "                \n",
    "                # 基尼系数计算\n",
    "                def gini_coefficient(values):\n",
    "                    \"\"\"计算基尼系数\"\"\"\n",
    "                    if len(values) == 0:\n",
    "                        return 0.0\n",
    "                    values = np.array(values)\n",
    "                    values = values[values > 0]  # 只考虑正值\n",
    "                    if len(values) <= 1:\n",
    "                        return 0.0\n",
    "                    \n",
    "                    values = np.sort(values)\n",
    "                    n = len(values)\n",
    "                    cumsum = np.cumsum(values)\n",
    "                    return (n + 1 - 2 * np.sum(cumsum) / cumsum[-1]) / n\n",
    "                \n",
    "                # 文档分布基尼系数\n",
    "                doc_counts = layer_nodes['document_count'].values\n",
    "                gini_doc_distribution = gini_coefficient(doc_counts)\n",
    "                \n",
    "                # 分枝分布基尼系数\n",
    "                gini_branch_distribution = gini_coefficient(child_counts)\n",
    "                \n",
    "                layer_metrics.append({\n",
    "                    'layer': layer,\n",
    "                    'node_count': node_count,\n",
    "                    'total_branches': total_branches,\n",
    "                    'avg_branching_factor': avg_branching_factor,\n",
    "                    'std_branching_factor': std_branching_factor,\n",
    "                    'non_leaf_nodes': non_leaf_nodes,\n",
    "                    'non_leaf_avg_branching': non_leaf_avg_branching,\n",
    "                    'total_documents': total_documents,\n",
    "                    'gini_doc_distribution': gini_doc_distribution,\n",
    "                    'gini_branch_distribution': gini_branch_distribution\n",
    "                })\n",
    "            \n",
    "            # 保存层级指标\n",
    "            if layer_metrics:\n",
    "                layer_df = pd.DataFrame(layer_metrics)\n",
    "                layer_output_path = os.path.join(folder_path, 'layer_branching_gini_metrics.csv')\n",
    "                layer_df.to_csv(layer_output_path, index=False)\n",
    "                print(f\"✓ 层级指标保存到: {layer_output_path}\")\n",
    "            \n",
    "            # 2. 计算全局分枝数和基尼系数指标\n",
    "            total_nodes = len(entropy_df)\n",
    "            total_layers = len(entropy_df['layer'].unique()) - (1 if -1 in entropy_df['layer'].unique() else 0)\n",
    "            \n",
    "            all_child_counts = entropy_df['child_count'].values\n",
    "            total_branches = all_child_counts.sum()\n",
    "            \n",
    "            # 全局分枝统计\n",
    "            non_zero_branches = all_child_counts[all_child_counts > 0]\n",
    "            if len(non_zero_branches) > 0:\n",
    "                global_avg_branching = non_zero_branches.mean()\n",
    "                global_std_branching = non_zero_branches.std()\n",
    "                global_max_branching = non_zero_branches.max()\n",
    "            else:\n",
    "                global_avg_branching = 0.0\n",
    "                global_std_branching = 0.0\n",
    "                global_max_branching = 0\n",
    "            \n",
    "            global_total_documents = entropy_df['document_count'].sum()\n",
    "            \n",
    "            # 全局基尼系数\n",
    "            def gini_coefficient(values):\n",
    "                if len(values) == 0:\n",
    "                    return 0.0\n",
    "                values = np.array(values)\n",
    "                values = values[values > 0]\n",
    "                if len(values) <= 1:\n",
    "                    return 0.0\n",
    "                \n",
    "                values = np.sort(values)\n",
    "                n = len(values)\n",
    "                cumsum = np.cumsum(values)\n",
    "                return (n + 1 - 2 * np.sum(cumsum) / cumsum[-1]) / n\n",
    "            \n",
    "            global_gini_doc_distribution = gini_coefficient(entropy_df['document_count'].values)\n",
    "            global_gini_branch_distribution = gini_coefficient(all_child_counts)\n",
    "            \n",
    "            global_metrics = [{\n",
    "                'total_nodes': total_nodes,\n",
    "                'total_layers': total_layers,\n",
    "                'total_branches': total_branches,\n",
    "                'global_avg_branching': global_avg_branching,\n",
    "                'global_std_branching': global_std_branching,\n",
    "                'global_max_branching': global_max_branching,\n",
    "                'global_total_documents': global_total_documents,\n",
    "                'global_gini_doc_distribution': global_gini_doc_distribution,\n",
    "                'global_gini_branch_distribution': global_gini_branch_distribution\n",
    "            }]\n",
    "            \n",
    "            # 保存全局指标\n",
    "            global_df = pd.DataFrame(global_metrics)\n",
    "            global_output_path = os.path.join(folder_path, 'global_branching_gini_metrics.csv')\n",
    "            global_df.to_csv(global_output_path, index=False)\n",
    "            print(f\"✓ 全局指标保存到: {global_output_path}\")\n",
    "            \n",
    "            # 显示简要统计\n",
    "            print(f\"📊 指标摘要:\")\n",
    "            print(f\"   总节点数: {total_nodes}\")\n",
    "            print(f\"   总层数: {total_layers}\")\n",
    "            print(f\"   全局平均分枝: {global_avg_branching:.2f}\")\n",
    "            print(f\"   全局文档基尼: {global_gini_doc_distribution:.4f}\")\n",
    "            print(f\"   全局分枝基尼: {global_gini_branch_distribution:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"❌ 处理文件 {file_path} 时出错: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "def aggregate_branching_gini_by_eta(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    按eta值汇总分枝数和基尼系数统计\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 汇总层级指标\n",
    "    print(\"=\" * 80)\n",
    "    print(\"汇总层级分枝数和基尼系数指标...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    pattern = os.path.join(base_path, \"**\", \"layer_branching_gini_metrics.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    print(f\"🔍 找到 {len(files)} 个层级指标文件\")\n",
    "    \n",
    "    all_layer_data = []\n",
    "    eta_groups = {}\n",
    "    \n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        parent_folder = os.path.dirname(folder_path)\n",
    "        \n",
    "        # 提取eta值\n",
    "        eta = None\n",
    "        if 'eta_' in folder_name:\n",
    "            try:\n",
    "                eta_part = folder_name.split('eta_')[1].split('_')[0]\n",
    "                eta = float(eta_part)\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # 提取run编号\n",
    "        run_match = folder_name.split('_run_')\n",
    "        if len(run_match) > 1:\n",
    "            run_id = run_match[1]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if eta not in eta_groups:\n",
    "            eta_groups[eta] = parent_folder\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                all_layer_data.append({\n",
    "                    'eta': eta,\n",
    "                    'run_id': run_id,\n",
    "                    'layer': row['layer'],\n",
    "                    'node_count': row['node_count'],\n",
    "                    'total_branches': row['total_branches'],\n",
    "                    'avg_branching_factor': row['avg_branching_factor'],\n",
    "                    'std_branching_factor': row['std_branching_factor'],\n",
    "                    'non_leaf_nodes': row['non_leaf_nodes'],\n",
    "                    'non_leaf_avg_branching': row['non_leaf_avg_branching'],\n",
    "                    'total_documents': row['total_documents'],\n",
    "                    'gini_doc_distribution': row['gini_doc_distribution'],\n",
    "                    'gini_branch_distribution': row['gini_branch_distribution'],\n",
    "                    'parent_folder': parent_folder\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"读取文件 {file_path} 时出错: {e}\")\n",
    "    \n",
    "    # 转换为DataFrame并按eta分组汇总\n",
    "    if all_layer_data:\n",
    "        layer_summary_df = pd.DataFrame(all_layer_data)\n",
    "        \n",
    "        print(\"各ETA值的层级分枝数和基尼系数汇总统计\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # 按eta分组生成层级汇总文件\n",
    "        for eta, group_data in layer_summary_df.groupby('eta'):\n",
    "            parent_folder = group_data['parent_folder'].iloc[0]\n",
    "            \n",
    "            print(f\"\\n处理 Eta={eta}\")\n",
    "            \n",
    "            layer_summary = group_data.groupby('layer').agg({\n",
    "                'node_count': ['mean', 'std'],\n",
    "                'total_branches': ['mean', 'std'],\n",
    "                'avg_branching_factor': ['mean', 'std'],\n",
    "                'std_branching_factor': ['mean', 'std'],\n",
    "                'non_leaf_nodes': ['mean', 'std'],\n",
    "                'non_leaf_avg_branching': ['mean', 'std'],\n",
    "                'total_documents': ['mean', 'std'],\n",
    "                'gini_doc_distribution': ['mean', 'std'],\n",
    "                'gini_branch_distribution': ['mean', 'std'],\n",
    "                'run_id': 'count'\n",
    "            }).round(4)\n",
    "            \n",
    "            # 平铺列名\n",
    "            layer_summary.columns = ['_'.join(col).strip() for col in layer_summary.columns]\n",
    "            layer_summary = layer_summary.reset_index()\n",
    "            layer_summary.insert(0, 'eta', eta)\n",
    "            \n",
    "            # 保存汇总结果\n",
    "            output_filename = f'eta_{eta}_layer_branching_gini_summary.csv'\n",
    "            output_path = os.path.join(parent_folder, output_filename)\n",
    "            layer_summary.to_csv(output_path, index=False)\n",
    "            \n",
    "            print(f\"  保存层级汇总文件: {output_path}\")\n",
    "            print(f\"  层数: {len(layer_summary)}\")\n",
    "            \n",
    "            # 修复：查找正确的计数列名\n",
    "            count_col = None\n",
    "            for col in layer_summary.columns:\n",
    "                if 'run_id' in col and ('count' in col or col.endswith('_count')):\n",
    "                    count_col = col\n",
    "                    break\n",
    "            \n",
    "            # 显示简要统计\n",
    "            for _, row in layer_summary.iterrows():\n",
    "                layer_num = int(row['layer'])\n",
    "                avg_branch = row['avg_branching_factor_mean']\n",
    "                doc_gini = row['gini_doc_distribution_mean']\n",
    "                branch_gini = row['gini_branch_distribution_mean']\n",
    "                run_count = int(row[count_col]) if count_col else 0\n",
    "                \n",
    "                print(f\"    Layer {layer_num}: 分枝={avg_branch:.2f}, 文档基尼={doc_gini:.4f}, 分枝基尼={branch_gini:.4f}, runs={run_count}\")\n",
    "        \n",
    "        # 生成总体层级对比文件\n",
    "        overall_layer_summary = layer_summary_df.groupby(['eta', 'layer']).agg({\n",
    "            'avg_branching_factor': ['mean', 'std'],\n",
    "            'gini_doc_distribution': ['mean', 'std'],\n",
    "            'gini_branch_distribution': ['mean', 'std'],\n",
    "            'node_count': ['mean', 'std'],\n",
    "            'run_id': 'count'\n",
    "        }).round(4)\n",
    "        \n",
    "        overall_layer_summary.columns = ['_'.join(col).strip() for col in overall_layer_summary.columns]\n",
    "        overall_layer_summary = overall_layer_summary.reset_index()\n",
    "        \n",
    "        overall_layer_output_path = os.path.join(base_path, 'eta_layer_branching_gini_comparison.csv')\n",
    "        overall_layer_summary.to_csv(overall_layer_output_path, index=False)\n",
    "        print(f\"\\n总体层级对比文件保存到: {overall_layer_output_path}\")\n",
    "    \n",
    "    # 2. 汇总全局指标\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"汇总全局分枝数和基尼系数指标...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    pattern = os.path.join(base_path, \"**\", \"global_branching_gini_metrics.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    print(f\"🔍 找到 {len(files)} 个全局指标文件\")\n",
    "    \n",
    "    all_global_data = []\n",
    "    \n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        parent_folder = os.path.dirname(folder_path)\n",
    "        \n",
    "        # 提取eta值\n",
    "        eta = None\n",
    "        if 'eta_' in folder_name:\n",
    "            try:\n",
    "                eta_part = folder_name.split('eta_')[1].split('_')[0]\n",
    "                eta = float(eta_part)\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # 提取run编号\n",
    "        run_match = folder_name.split('_run_')\n",
    "        if len(run_match) > 1:\n",
    "            run_id = run_match[1]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                all_global_data.append({\n",
    "                    'eta': eta,\n",
    "                    'run_id': run_id,\n",
    "                    'total_nodes': row['total_nodes'],\n",
    "                    'total_layers': row['total_layers'],\n",
    "                    'total_branches': row['total_branches'],\n",
    "                    'global_avg_branching': row['global_avg_branching'],\n",
    "                    'global_std_branching': row['global_std_branching'],\n",
    "                    'global_max_branching': row['global_max_branching'],\n",
    "                    'global_total_documents': row['global_total_documents'],\n",
    "                    'global_gini_doc_distribution': row['global_gini_doc_distribution'],\n",
    "                    'global_gini_branch_distribution': row['global_gini_branch_distribution'],\n",
    "                    'parent_folder': parent_folder\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"读取文件 {file_path} 时出错: {e}\")\n",
    "    \n",
    "    # 转换为DataFrame并按eta分组汇总\n",
    "    if all_global_data:\n",
    "        global_summary_df = pd.DataFrame(all_global_data)\n",
    "        \n",
    "        print(\"各ETA值的全局分枝数和基尼系数汇总统计\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # 按eta分组生成全局汇总文件\n",
    "        for eta, group_data in global_summary_df.groupby('eta'):\n",
    "            parent_folder = group_data['parent_folder'].iloc[0]\n",
    "            \n",
    "            print(f\"\\n处理 Eta={eta} 全局指标\")\n",
    "            \n",
    "            global_summary = group_data.agg({\n",
    "                'total_nodes': ['mean', 'std'],\n",
    "                'total_layers': ['mean', 'std'],\n",
    "                'total_branches': ['mean', 'std'],\n",
    "                'global_avg_branching': ['mean', 'std'],\n",
    "                'global_std_branching': ['mean', 'std'],\n",
    "                'global_max_branching': ['mean', 'std'],\n",
    "                'global_total_documents': ['mean', 'std'],\n",
    "                'global_gini_doc_distribution': ['mean', 'std'],\n",
    "                'global_gini_branch_distribution': ['mean', 'std'],\n",
    "                'run_id': 'count'\n",
    "            }).round(4)\n",
    "            \n",
    "            # 平铺列名\n",
    "            global_summary.columns = ['_'.join(col).strip() for col in global_summary.columns]\n",
    "            global_summary = global_summary.reset_index()\n",
    "            global_summary.insert(0, 'eta', eta)\n",
    "            \n",
    "            # 保存汇总结果\n",
    "            output_filename = f'eta_{eta}_global_branching_gini_summary.csv'\n",
    "            output_path = os.path.join(parent_folder, output_filename)\n",
    "            global_summary.to_csv(output_path, index=False)\n",
    "            \n",
    "            print(f\"  保存全局汇总文件: {output_path}\")\n",
    "            \n",
    "            # 修复：查找正确的计数列名\n",
    "            count_col = None\n",
    "            for col in global_summary.columns:\n",
    "                if 'run_id' in col and ('count' in col or col.endswith('_count')):\n",
    "                    count_col = col\n",
    "                    break\n",
    "            \n",
    "            # 安全访问列\n",
    "            if len(global_summary) > 0:\n",
    "                run_count = int(global_summary[count_col].iloc[0]) if count_col else 0\n",
    "                \n",
    "                # 安全访问其他列\n",
    "                cols_to_access = ['global_avg_branching_mean', 'global_gini_doc_distribution_mean', 'global_gini_branch_distribution_mean']\n",
    "                values = {}\n",
    "                \n",
    "                for col_name in cols_to_access:\n",
    "                    if col_name in global_summary.columns:\n",
    "                        values[col_name] = global_summary[col_name].iloc[0]\n",
    "                    else:\n",
    "                        values[col_name] = 0.0\n",
    "                \n",
    "                print(f\"  运行数: {run_count}\")\n",
    "                print(f\"  全局平均分枝: {values['global_avg_branching_mean']:.2f}\")\n",
    "                print(f\"  全局文档基尼: {values['global_gini_doc_distribution_mean']:.4f}\")\n",
    "                print(f\"  全局分枝基尼: {values['global_gini_branch_distribution_mean']:.4f}\")\n",
    "        \n",
    "        # 生成总体全局对比文件\n",
    "        overall_global_summary = global_summary_df.groupby('eta').agg({\n",
    "            'total_nodes': ['mean', 'std'],\n",
    "            'total_branches': ['mean', 'std'],\n",
    "            'global_avg_branching': ['mean', 'std'],\n",
    "            'global_gini_doc_distribution': ['mean', 'std'],\n",
    "            'global_gini_branch_distribution': ['mean', 'std'],\n",
    "            'run_id': 'count'\n",
    "        }).round(4)\n",
    "        \n",
    "        overall_global_summary.columns = ['_'.join(col).strip() for col in overall_global_summary.columns]\n",
    "        overall_global_summary = overall_global_summary.reset_index()\n",
    "        \n",
    "        overall_global_output_path = os.path.join(base_path, 'eta_global_branching_gini_comparison.csv')\n",
    "        overall_global_summary.to_csv(overall_global_output_path, index=False)\n",
    "        print(f\"\\n总体全局对比文件保存到: {overall_global_output_path}\")\n",
    "        \n",
    "        # 修复：查找正确的计数列名用于显示\n",
    "        count_col = None\n",
    "        for col in overall_global_summary.columns:\n",
    "            if 'run_id' in col and ('count' in col or col.endswith('_count')):\n",
    "                count_col = col\n",
    "                break\n",
    "        \n",
    "        # 显示跨eta对比\n",
    "        print(f\"\\n跨Eta全局指标对比:\")\n",
    "        print(\"Eta值      平均分枝(±std)     文档基尼(±std)     分枝基尼(±std)     运行数\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for _, row in overall_global_summary.iterrows():\n",
    "            eta = row['eta']\n",
    "            \n",
    "            # 安全访问列\n",
    "            cols_needed = ['global_avg_branching_mean', 'global_avg_branching_std', \n",
    "                          'global_gini_doc_distribution_mean', 'global_gini_doc_distribution_std',\n",
    "                          'global_gini_branch_distribution_mean', 'global_gini_branch_distribution_std']\n",
    "            \n",
    "            values = {}\n",
    "            for col_name in cols_needed:\n",
    "                if col_name in row:\n",
    "                    values[col_name] = row[col_name]\n",
    "                else:\n",
    "                    values[col_name] = 0.0\n",
    "            \n",
    "            run_count = int(row[count_col]) if count_col and count_col in row else 0\n",
    "            \n",
    "            print(f\"{eta:6.3f}    {values['global_avg_branching_mean']:6.2f}(±{values['global_avg_branching_std']:4.2f})     \"\n",
    "                  f\"{values['global_gini_doc_distribution_mean']:6.4f}(±{values['global_gini_doc_distribution_std']:5.4f})     \"\n",
    "                  f\"{values['global_gini_branch_distribution_mean']:6.4f}(±{values['global_gini_branch_distribution_std']:5.4f})     {run_count:4d}\")\n",
    "\n",
    "def display_branching_gini_summary(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    显示分枝数和基尼系数的汇总报告\n",
    "    \"\"\"\n",
    "    print(\"=\" * 100)\n",
    "    print(\"分枝数和基尼系数分析汇总报告\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # 读取总体对比文件\n",
    "    layer_comparison_file = os.path.join(base_path, 'eta_layer_branching_gini_comparison.csv')\n",
    "    global_comparison_file = os.path.join(base_path, 'eta_global_branching_gini_comparison.csv')\n",
    "    \n",
    "    if os.path.exists(layer_comparison_file):\n",
    "        print(\"\\n📊 层级分枝数和基尼系数分析:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        df = pd.read_csv(layer_comparison_file)\n",
    "        \n",
    "        # 修复：查找正确的计数列名\n",
    "        count_col = None\n",
    "        for col in df.columns:\n",
    "            if 'run_id' in col and ('count' in col or col.endswith('_count')):\n",
    "                count_col = col\n",
    "                break\n",
    "        \n",
    "        for layer in sorted(df['layer'].unique()):\n",
    "            print(f\"\\nLayer {int(layer)} 跨Eta对比:\")\n",
    "            print(\"Eta值      平均分枝(±std)     文档基尼(±std)     分枝基尼(±std)     运行数\")\n",
    "            print(\"-\" * 75)\n",
    "            \n",
    "            layer_data = df[df['layer'] == layer]\n",
    "            for _, row in layer_data.iterrows():\n",
    "                eta = row['eta']\n",
    "                avg_branch = row['avg_branching_factor_mean']\n",
    "                branch_std = row['avg_branching_factor_std']\n",
    "                doc_gini = row['gini_doc_distribution_mean']\n",
    "                doc_gini_std = row['gini_doc_distribution_std']\n",
    "                branch_gini = row['gini_branch_distribution_mean']\n",
    "                branch_gini_std = row['gini_branch_distribution_std']\n",
    "                run_count = int(row[count_col]) if count_col else 0\n",
    "                \n",
    "                print(f\"{eta:6.3f}    {avg_branch:6.2f}(±{branch_std:4.2f})     {doc_gini:6.4f}(±{doc_gini_std:5.4f})     {branch_gini:6.4f}(±{branch_gini_std:5.4f})     {run_count:4d}\")\n",
    "    \n",
    "    if os.path.exists(global_comparison_file):\n",
    "        print(f\"\\n📊 全局分枝数和基尼系数分析:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        df = pd.read_csv(global_comparison_file)\n",
    "        \n",
    "        # 修复：查找正确的计数列名\n",
    "        count_col = None\n",
    "        for col in df.columns:\n",
    "            if 'run_id' in col and ('count' in col or col.endswith('_count')):\n",
    "                count_col = col\n",
    "                break\n",
    "        \n",
    "        print(\"Eta值      平均分枝(±std)     文档基尼(±std)     分枝基尼(±std)     运行数\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            eta = row['eta']\n",
    "            avg_branch = row['global_avg_branching_mean']\n",
    "            branch_std = row['global_avg_branching_std']\n",
    "            doc_gini = row['global_gini_doc_distribution_mean']\n",
    "            doc_gini_std = row['global_gini_doc_distribution_std']\n",
    "            branch_gini = row['global_gini_branch_distribution_mean']\n",
    "            branch_gini_std = row['global_gini_branch_distribution_std']\n",
    "            run_count = int(row[count_col]) if count_col else 0\n",
    "            \n",
    "            print(f\"{eta:6.3f}    {avg_branch:6.2f}(±{branch_std:4.2f})     {doc_gini:6.4f}(±{doc_gini_std:5.4f})     {branch_gini:6.4f}(±{branch_gini_std:5.4f})     {run_count:4d}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"✅ 分枝数和基尼系数分析完成！\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "# 执行分枝数和基尼系数分析\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step2\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"开始计算分枝数和基尼系数指标...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. 计算每个模型的分枝数和基尼系数\n",
    "calculate_branching_and_gini_metrics(base_path)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"开始按eta值汇总分枝数和基尼系数统计...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 2. 按eta汇总\n",
    "aggregate_branching_gini_by_eta(base_path)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"显示分枝数和基尼系数汇总报告...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 3. 显示汇总报告\n",
    "display_branching_gini_summary(base_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
