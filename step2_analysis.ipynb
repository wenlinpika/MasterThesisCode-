{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab123656",
   "metadata": {},
   "source": [
    "## Summary: in the step2, we fix $depth=3$, $\\gamma=0.05$ and $alpha=0.1$, to test $eta$\n",
    "\n",
    "### 1. We do same thing for Renyi Entropy and Jenson-shannon Divergence as we did in step 1 analysis.\n",
    "\n",
    "### 2. Differently, we add two semantic indicators: **coherence** and **perplexity**, two structual indicators: **Gini Coefficient** and **Branching Factor** in this step.\n",
    "\n",
    "### 2.1 For each run, We calculated each node's coherence for all chiains by **calculate_standard_coherence_from_corpus_corrected()**\n",
    "- save to: <u>step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_3/standard_coherence.csv</u>\n",
    "\n",
    "### 2.2 For each run, we averaged the coherence for nodes within each layer for each run by **aggregate_coherence_by_eta()**\n",
    "- save to: <u>step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_3/layer_coherence_summary_k5.csv</u>\n",
    "\n",
    "### 3. For each eta, We calculate the layer level coherence and weighted by docs' count coherence by **We calculate_coherence_layered_analysis()** when top_k = 5 selecting top 5 words\n",
    "- save to: <u>step2/step2_d3_g005_e01_收敛/eta_0.1_coherence_layer_summary_k5.csv</u>\n",
    "\n",
    "### 4. For all eta, we aggreagte them coherence by **aggregate_coherence_by_eta()**\n",
    "- save to <u>/Volumes/My Passport/收敛结果/step2/02_eta_coherence_layer_comparison_k5.csv</u>\n",
    "\n",
    "### 5. For each eta, we calculate the perplexity for each eta's run to see its prediction performance by **calculate_hlda_perplexity_with_path_mapping_complete()** \n",
    "- save to: <u>step2/step2_d3_g005_e01_收敛/eta_0.1_perplexity_summary.csv</u>\n",
    "\n",
    "### 6. For all eta, we aggregate all perplexity by **aggregate_perplexity_by_eta_groups()**\n",
    "- save to: <u> step2/03_eta_perplexity_comparison.csv</u>\n",
    "\n",
    "### 7.1 For each run/chain, we calculate its tree structure's branch factor and gini coefficient by **calculate_branching_and_gini_metrics**\n",
    "- save to <u>step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_2/layer_branching_gini_metrics.csv</u>\n",
    "\n",
    "### 7.2 for each eta, we calculate its mean of each layer's performance by **aggregate_branching_gini_by_eta()**\n",
    "- save to: <u>step2/step2_d3_g005_e01_收敛/eta_0.1_layer_branching_gini_summary.csv <\\u>\n",
    "\n",
    "### 7.3 for all eta, we aggregate all results by **display_branching_gini_summary**\n",
    "- save to: <u>step2/04_eta_layer_branching_gini_comparison.csv</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ef8259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from scipy.special import gammaln\n",
    "\n",
    "def calculate_renyi_entropy_vectorized(node_data, all_words, eta_prior=1.0, renyi_alpha=2.0):\n",
    "    \"\"\"\n",
    "    Vectorized version of Renyi entropy calculation\n",
    "    \n",
    "    Parameters:\n",
    "    node_data: DataFrame, node data containing word and count columns\n",
    "    all_words: list, complete vocabulary\n",
    "    eta_prior: float, Dirichlet prior smoothing parameter (obtained from eta value)\n",
    "    renyi_alpha: float, order parameter for Renyi entropy\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (entropy, nonzero_word_count) Renyi entropy value and number of non-zero words\n",
    "    \"\"\"\n",
    "    if len(all_words) == 0:\n",
    "        return 0.0, 0\n",
    "    \n",
    "    # Create word to index mapping\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "    \n",
    "    # Initialize count vector\n",
    "    counts = np.zeros(len(all_words))\n",
    "    \n",
    "    # Fill actual counts\n",
    "    for _, row in node_data.iterrows():\n",
    "        word = row['word']\n",
    "        if pd.notna(word) and word in word_to_idx:\n",
    "            counts[word_to_idx[word]] = row['count']\n",
    "    \n",
    "    # Count non-zero words (before smoothing)\n",
    "    nonzero_word_count = np.sum(counts > 0)\n",
    "    \n",
    "    # Add eta smoothing\n",
    "    smoothed_counts = counts + eta_prior\n",
    "    \n",
    "    # Calculate probability distribution\n",
    "    probabilities = smoothed_counts / np.sum(smoothed_counts)\n",
    "    \n",
    "    # Calculate Renyi entropy (using natural logarithm)\n",
    "    if renyi_alpha == 1.0:\n",
    "        # Shannon entropy (all probabilities > 0 due to alpha smoothing, no need to add small constant)\n",
    "        entropy = -np.sum(probabilities * np.log(probabilities))\n",
    "    else:\n",
    "        # General Renyi entropy\n",
    "        entropy = (1 / (1 - renyi_alpha)) * np.log(np.sum(probabilities ** renyi_alpha))\n",
    "    \n",
    "    return entropy, int(nonzero_word_count)\n",
    "\n",
    "def process_all_iteration_files_by_eta(base_path=\".\", renyi_alpha=2.0):\n",
    "    \"\"\"\n",
    "    Process each iteration_node_word_distributions.csv file separately and save results\n",
    "    Automatically extract eta value from folder name as prior smoothing parameter\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_node_word_distributions.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    # Remove duplicates to ensure each file is processed only once\n",
    "    files = list(set(files))\n",
    "    files.sort()  # Sort for ordered processing\n",
    "    \n",
    "    print(f\"Found {len(files)} files to process\")\n",
    "    \n",
    "    for idx, file_path in enumerate(files, 1):\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        \n",
    "        # Extract eta value from folder name\n",
    "        eta_prior = 0.1  # Default value\n",
    "        if 'eta_' in folder_name:\n",
    "            try:\n",
    "                # Find the numeric part after eta_\n",
    "                eta_part = folder_name.split('eta_')[1].split('_')[0]\n",
    "                eta_prior = float(eta_part)\n",
    "            except (IndexError, ValueError) as e:\n",
    "                print(f\"Warning: Unable to extract eta value from folder name {folder_name}, using default value {eta_prior}\")\n",
    "        \n",
    "        print(f\"\\n[{idx}/{len(files)}] Processing file: {file_path}\")\n",
    "        print(f\"Folder: {folder_name}\")\n",
    "        print(f\"Extracted eta value: {eta_prior}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Clean column names, remove single quotes, double quotes and spaces\n",
    "            df.columns = [col.strip(\"'\\\" \") for col in df.columns]\n",
    "            \n",
    "            if 'node_id' not in df.columns:\n",
    "                print(f\"Warning: {file_path} missing node_id column, skipping this file\")\n",
    "                continue\n",
    "                \n",
    "            max_iteration = df['iteration'].max()\n",
    "            last_iteration_data = df[df['iteration'] == max_iteration]\n",
    "            all_words = list(last_iteration_data['word'].dropna().unique())\n",
    "            \n",
    "            print(f\"Last iteration: {max_iteration}, vocabulary size: {len(all_words)}, number of nodes: {last_iteration_data['node_id'].nunique()}\")\n",
    "            \n",
    "            results = []\n",
    "            for node_id in last_iteration_data['node_id'].unique():\n",
    "                node_data = last_iteration_data[last_iteration_data['node_id'] == node_id]\n",
    "                \n",
    "                entropy, nonzero_words = calculate_renyi_entropy_vectorized(\n",
    "                    node_data, all_words, eta_prior, renyi_alpha\n",
    "                )\n",
    "                \n",
    "                # Calculate sparsity ratio (proportion of non-zero words)\n",
    "                sparsity_ratio = nonzero_words / len(all_words) if len(all_words) > 0 else 0\n",
    "                \n",
    "                results.append({\n",
    "                    'node_id': node_id,\n",
    "                    'renyi_entropy_corrected': entropy,\n",
    "                    'nonzero_word_count': nonzero_words,\n",
    "                    'total_vocabulary_size': len(all_words),\n",
    "                    'sparsity_ratio': sparsity_ratio,\n",
    "                    'eta_prior': eta_prior,\n",
    "                    'renyi_alpha': renyi_alpha,\n",
    "                    'iteration': max_iteration\n",
    "                })\n",
    "            \n",
    "            # Save new corrected_renyi_entropy.csv file\n",
    "            results_df = pd.DataFrame(results)\n",
    "            output_path = os.path.join(folder_path, 'corrected_renyi_entropy.csv')\n",
    "            results_df.to_csv(output_path, index=False)\n",
    "            print(f\"✓ Saved corrected Renyi entropy results to: {output_path}\")\n",
    "            \n",
    "            # Output some statistics\n",
    "            print(f\"Node vocabulary sparsity statistics:\")\n",
    "            print(f\"  - Average non-zero words: {results_df['nonzero_word_count'].mean():.1f}\")\n",
    "            print(f\"  - Non-zero word count range: {results_df['nonzero_word_count'].min()}-{results_df['nonzero_word_count'].max()}\")\n",
    "            print(f\"  - Average sparsity: {results_df['sparsity_ratio'].mean():.3f}\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "                \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"❌ Error processing file {file_path}: {str(e)}\")\n",
    "            print(\"Detailed error information:\")\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b21f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Starting batch calculation of corrected Renyi entropy (automatically adjusting prior by eta value)...\n",
      "==================================================\n",
      "Found 18 files to process\n",
      "\n",
      "[1/18] Processing file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_1/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.005_run_1\n",
      "Extracted eta value: 0.005\n",
      "Last iteration: 115, vocabulary size: 1490, number of nodes: 438\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_1/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero words: 23.0\n",
      "  - Non-zero word count range: 0-1108\n",
      "  - Average sparsity: 0.015\n",
      "==================================================\n",
      "\n",
      "[2/18] Processing file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_2/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.005_run_2\n",
      "Extracted eta value: 0.005\n",
      "Last iteration: 115, vocabulary size: 1490, number of nodes: 432\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_2/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero words: 22.8\n",
      "  - Non-zero word count range: 0-1139\n",
      "  - Average sparsity: 0.015\n",
      "==================================================\n",
      "\n",
      "[3/18] Processing file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_3/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.005_run_3\n",
      "Extracted eta value: 0.005\n",
      "Last iteration: 115, vocabulary size: 1490, number of nodes: 427\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_3/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero words: 23.1\n",
      "  - Non-zero word count range: 0-1087\n",
      "  - Average sparsity: 0.015\n",
      "==================================================\n",
      "\n",
      "[4/18] Processing file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_1/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.01_run_1\n",
      "Extracted eta value: 0.01\n",
      "Last iteration: 195, vocabulary size: 1490, number of nodes: 425\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_1/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero words: 29.3\n",
      "  - Non-zero word count range: 0-1034\n",
      "  - Average sparsity: 0.020\n",
      "==================================================\n",
      "\n",
      "[5/18] Processing file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_2/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.01_run_2\n",
      "Extracted eta value: 0.01\n",
      "Last iteration: 195, vocabulary size: 1490, number of nodes: 403\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_2/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero words: 30.0\n",
      "  - Non-zero word count range: 0-1057\n",
      "  - Average sparsity: 0.020\n",
      "==================================================\n",
      "\n",
      "[6/18] Processing file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_3/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.01_run_3\n",
      "Extracted eta value: 0.01\n",
      "Last iteration: 195, vocabulary size: 1490, number of nodes: 433\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_3/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero words: 28.6\n",
      "  - Non-zero word count range: 0-1023\n",
      "  - Average sparsity: 0.019\n",
      "==================================================\n",
      "\n",
      "[7/18] Processing file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_1/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.02_run_1\n",
      "Extracted eta value: 0.02\n",
      "Last iteration: 155, vocabulary size: 1490, number of nodes: 383\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_1/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero words: 40.2\n",
      "  - Non-zero word count range: 0-922\n",
      "  - Average sparsity: 0.027\n",
      "==================================================\n",
      "\n",
      "[8/18] Processing file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_2/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.02_run_2\n",
      "Extracted eta value: 0.02\n",
      "Last iteration: 155, vocabulary size: 1490, number of nodes: 384\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_2/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero words: 39.3\n",
      "  - Non-zero word count range: 0-920\n",
      "  - Average sparsity: 0.026\n",
      "==================================================\n",
      "\n",
      "[9/18] Processing file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_3/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.02_run_3\n",
      "Extracted eta value: 0.02\n",
      "Last iteration: 155, vocabulary size: 1490, number of nodes: 388\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_3/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero words: 40.5\n",
      "  - Non-zero word count range: 0-886\n",
      "  - Average sparsity: 0.027\n",
      "==================================================\n",
      "\n",
      "[10/18] Processing file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_1/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.05_run_1\n",
      "Extracted eta value: 0.05\n",
      "Last iteration: 90, vocabulary size: 1490, number of nodes: 315\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_1/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero words: 63.0\n",
      "  - Non-zero word count range: 0-747\n",
      "  - Average sparsity: 0.042\n",
      "==================================================\n",
      "\n",
      "[11/18] Processing file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_2/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.05_run_2\n",
      "Extracted eta value: 0.05\n",
      "Last iteration: 90, vocabulary size: 1490, number of nodes: 299\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_2/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero words: 64.3\n",
      "  - Non-zero word count range: 0-792\n",
      "  - Average sparsity: 0.043\n",
      "==================================================\n",
      "\n",
      "[12/18] Processing file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_3/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.05_run_3\n",
      "Extracted eta value: 0.05\n",
      "Last iteration: 90, vocabulary size: 1490, number of nodes: 322\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_3/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero words: 61.6\n",
      "  - Non-zero word count range: 0-737\n",
      "  - Average sparsity: 0.041\n",
      "==================================================\n",
      "\n",
      "[13/18] Processing file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_1/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.1_run_1\n",
      "Extracted eta value: 0.1\n",
      "Last iteration: 175, vocabulary size: 1490, number of nodes: 205\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_1/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero words: 90.8\n",
      "  - Non-zero word count range: 2-766\n",
      "  - Average sparsity: 0.061\n",
      "==================================================\n",
      "\n",
      "[14/18] Processing file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_2/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.1_run_2\n",
      "Extracted eta value: 0.1\n",
      "Last iteration: 175, vocabulary size: 1490, number of nodes: 231\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_2/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero words: 85.4\n",
      "  - Non-zero word count range: 0-743\n",
      "  - Average sparsity: 0.057\n",
      "==================================================\n",
      "\n",
      "[15/18] Processing file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_3/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.1_run_3\n",
      "Extracted eta value: 0.1\n",
      "Last iteration: 175, vocabulary size: 1490, number of nodes: 215\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_3/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero words: 88.9\n",
      "  - Non-zero word count range: 0-759\n",
      "  - Average sparsity: 0.060\n",
      "==================================================\n",
      "\n",
      "[16/18] Processing file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_1/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.2_run_1\n",
      "Extracted eta value: 0.2\n",
      "Last iteration: 375, vocabulary size: 1490, number of nodes: 151\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_1/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero words: 126.6\n",
      "  - Non-zero word count range: 0-853\n",
      "  - Average sparsity: 0.085\n",
      "==================================================\n",
      "\n",
      "[17/18] Processing file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_2/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.2_run_2\n",
      "Extracted eta value: 0.2\n",
      "Last iteration: 375, vocabulary size: 1490, number of nodes: 157\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_2/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero words: 123.6\n",
      "  - Non-zero word count range: 0-802\n",
      "  - Average sparsity: 0.083\n",
      "==================================================\n",
      "\n",
      "[18/18] Processing file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_3/iteration_node_word_distributions.csv\n",
      "Folder: depth_3_gamma_0.05_eta_0.2_run_3\n",
      "Extracted eta value: 0.2\n",
      "Last iteration: 375, vocabulary size: 1490, number of nodes: 157\n",
      "✓ Saved corrected Renyi entropy results to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_3/corrected_renyi_entropy.csv\n",
      "Node vocabulary sparsity statistics:\n",
      "  - Average non-zero words: 125.5\n",
      "  - Non-zero word count range: 0-801\n",
      "  - Average sparsity: 0.084\n",
      "==================================================\n",
      "==================================================\n",
      "All processing completed!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step2\"  # Root directory\n",
    "renyi_alpha = 2.0  # Renyi entropy order parameter\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Starting batch calculation of corrected Renyi entropy (automatically adjusting prior by eta value)...\")\n",
    "print(\"=\" * 50)\n",
    "process_all_iteration_files_by_eta(base_path, renyi_alpha)\n",
    "print(\"=\" * 50)\n",
    "print(\"All processing completed!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83fa2dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_node_document_counts(path_structures_df):\n",
    "    \"\"\"\n",
    "    Aggregate from leaf nodes upward to calculate document count and hierarchical relationships for each node\n",
    "    \n",
    "    Parameters:\n",
    "    path_structures_df: DataFrame, data from iteration_path_structures.csv (filtered for the last iteration)\n",
    "    \n",
    "    Returns:\n",
    "    dict: {node_id: {'document_count': int, 'layer': int, 'parent_id': int, 'child_ids': list}} mapping\n",
    "    \"\"\"\n",
    "    # Get all layer columns - fix regex pattern\n",
    "    layer_columns = [col for col in path_structures_df.columns if col.startswith('layer_') and col.endswith('_node_id')]\n",
    "    layer_columns.sort()  # Ensure ordered arrangement\n",
    "    max_layer_idx = len(layer_columns) - 1\n",
    "    \n",
    "    print(f\"[DEBUG] Found layer columns: {layer_columns}\")\n",
    "    print(f\"[DEBUG] Maximum layer index: {max_layer_idx}\")\n",
    "    \n",
    "    # Initialize node information dictionary\n",
    "    node_info = {}\n",
    "    \n",
    "    # First establish hierarchical and parent-child relationships for all nodes\n",
    "    for _, row in path_structures_df.iterrows():\n",
    "        path_nodes = []\n",
    "        for layer_idx in range(max_layer_idx + 1):\n",
    "            layer_col = f'layer_{layer_idx}_node_id'\n",
    "            if layer_col in path_structures_df.columns and pd.notna(row[layer_col]):\n",
    "                path_nodes.append(row[layer_col])\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # Establish hierarchical and parent-child relationships for each node in the path\n",
    "        for i, node in enumerate(path_nodes):\n",
    "            if node not in node_info:\n",
    "                node_info[node] = {\n",
    "                    'document_count': 0,\n",
    "                    'layer': i,\n",
    "                    'parent_id': None,\n",
    "                    'child_ids': [],\n",
    "                    'child_count': 0\n",
    "                }\n",
    "            else:\n",
    "                # Update layer information (ensure consistency)\n",
    "                node_info[node]['layer'] = i\n",
    "            \n",
    "            # Set parent node relationships\n",
    "            if i > 0:  # Not root node\n",
    "                parent_node = path_nodes[i-1]\n",
    "                node_info[node]['parent_id'] = parent_node\n",
    "                \n",
    "                # Add current node to parent node's child list\n",
    "                if parent_node not in node_info:\n",
    "                    node_info[parent_node] = {\n",
    "                        'document_count': 0,\n",
    "                        'layer': i-1,\n",
    "                        'parent_id': None,\n",
    "                        'child_ids': [],\n",
    "                        'child_count': 0\n",
    "                    }\n",
    "                \n",
    "                if node not in node_info[parent_node]['child_ids']:\n",
    "                    node_info[parent_node]['child_ids'].append(node)\n",
    "    \n",
    "    # Then process leaf node document counts - after establishing hierarchical relationships\n",
    "    for _, row in path_structures_df.iterrows():\n",
    "        leaf_node = row['leaf_node_id']\n",
    "        if pd.notna(leaf_node) and leaf_node in node_info:\n",
    "            node_info[leaf_node]['document_count'] += row['document_count']\n",
    "    \n",
    "    # Aggregate document counts from second-to-last layer upward\n",
    "    for layer_idx in range(max_layer_idx - 1, -1, -1):  # From second-to-last layer to layer 0\n",
    "        layer_col = f'layer_{layer_idx}_node_id'\n",
    "        \n",
    "        if layer_col not in path_structures_df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Get all unique nodes in this layer\n",
    "        layer_nodes = path_structures_df[layer_col].dropna().unique()\n",
    "        \n",
    "        for node in layer_nodes:\n",
    "            if node in node_info and node_info[node]['document_count'] == 0:\n",
    "                # Calculate document count: sum all child node document counts\n",
    "                child_doc_count = 0\n",
    "                for child_id in node_info[node]['child_ids']:\n",
    "                    if child_id in node_info:\n",
    "                        child_doc_count += node_info[child_id]['document_count']\n",
    "                \n",
    "                # If no child node document count, calculate directly from path structure\n",
    "                if child_doc_count == 0:\n",
    "                    total_docs = path_structures_df[path_structures_df[layer_col] == node]['document_count'].sum()\n",
    "                    node_info[node]['document_count'] = total_docs\n",
    "                else:\n",
    "                    node_info[node]['document_count'] = child_doc_count\n",
    "\n",
    "    # Calculate child node count for each node\n",
    "    for node_id, info in node_info.items():\n",
    "        info['child_count'] = len(info['child_ids'])\n",
    "    \n",
    "    return node_info\n",
    "\n",
    "def add_document_counts_to_entropy_files(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    Add document count and hierarchical information to corrected_renyi_entropy.csv files\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_path_structures.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        print(f\"\\nProcessing path structure file: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Read path_structures file\n",
    "            df = pd.read_csv(file_path)\n",
    "            df.columns = [col.strip(\"'\\\" \") for col in df.columns]\n",
    "            \n",
    "            # Get last iteration data\n",
    "            max_iteration = df['iteration'].max()\n",
    "            last_iteration_data = df[df['iteration'] == max_iteration]\n",
    "            \n",
    "            print(f\"Last iteration: {max_iteration}, path count: {len(last_iteration_data)}\")\n",
    "            \n",
    "            # Calculate document count and hierarchical relationships for each node\n",
    "            node_info = calculate_node_document_counts(last_iteration_data)\n",
    "            \n",
    "            print(f\"Calculated information for {len(node_info)} nodes\")\n",
    "            \n",
    "            # Read corresponding corrected_renyi_entropy.csv\n",
    "            entropy_file = os.path.join(folder_path, 'corrected_renyi_entropy.csv')\n",
    "            if os.path.exists(entropy_file):\n",
    "                entropy_df = pd.read_csv(entropy_file)\n",
    "                \n",
    "                # Add new columns - fix child_ids format and child_count calculation\n",
    "                entropy_df['document_count'] = entropy_df['node_id'].map(lambda x: node_info.get(x, {}).get('document_count', 0))\n",
    "                entropy_df['layer'] = entropy_df['node_id'].map(lambda x: node_info.get(x, {}).get('layer', -1))\n",
    "                entropy_df['parent_id'] = entropy_df['node_id'].map(lambda x: node_info.get(x, {}).get('parent_id', None))\n",
    "                \n",
    "                # Fix child_ids format: use square brackets instead of commas\n",
    "                entropy_df['child_ids'] = entropy_df['node_id'].map(\n",
    "                    lambda x: '[' + ','.join(map(str, node_info.get(x, {}).get('child_ids', []))) + ']' \n",
    "                    if node_info.get(x, {}).get('child_ids') else ''\n",
    "                )\n",
    "                \n",
    "                # Fix child_count: use list length directly\n",
    "                entropy_df['child_count'] = entropy_df['node_id'].map(lambda x: len(node_info.get(x, {}).get('child_ids', [])))\n",
    "\n",
    "                # Save updated file\n",
    "                entropy_df.to_csv(entropy_file, index=False)\n",
    "                print(f\"Updated {entropy_file}, added document_count, layer, parent_id, child_ids, child_count columns\")\n",
    "                \n",
    "                # Display some statistics\n",
    "                print(f\"Node layer statistics:\")\n",
    "                print(f\"  - Layer distribution: {entropy_df['layer'].value_counts().sort_index().to_dict()}\")\n",
    "                print(f\"  - Document count range: {entropy_df['document_count'].min()}-{entropy_df['document_count'].max()}\")\n",
    "                print(f\"  - Root node count: {entropy_df[entropy_df['parent_id'].isna()].shape[0]}\")\n",
    "                print(f\"  - Leaf node count: {entropy_df[entropy_df['child_ids'] == ''].shape[0]}\")\n",
    "                print(f\"  - Child count distribution: {entropy_df['child_count'].value_counts().sort_index().to_dict()}\")\n",
    "            else:\n",
    "                print(f\"Warning: Corresponding entropy file not found {entropy_file}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "            print(\"Detailed error information:\")\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4039076d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Starting to add document count and hierarchical information to entropy files...\n",
      "==================================================\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_2/iteration_path_structures.csv\n",
      "Last iteration: 175, path count: 186\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 231 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_2/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 44, 2: 186}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 186\n",
      "  - Child count distribution: {0: 186, 1: 4, 2: 10, 3: 10, 4: 10, 5: 8, 6: 1, 44: 1, 46: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_3/iteration_path_structures.csv\n",
      "Last iteration: 175, path count: 173\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 215 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_3/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 41, 2: 173}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 173\n",
      "  - Child count distribution: {0: 173, 1: 9, 2: 6, 3: 14, 4: 6, 5: 3, 6: 2, 41: 1, 59: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_1/iteration_path_structures.csv\n",
      "Last iteration: 175, path count: 164\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 205 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_1/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 40, 2: 164}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 164\n",
      "  - Child count distribution: {0: 164, 1: 6, 2: 8, 3: 10, 4: 6, 5: 9, 40: 1, 43: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_3/iteration_path_structures.csv\n",
      "Last iteration: 90, path count: 262\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 322 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_3/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 59, 2: 262}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 262\n",
      "  - Child count distribution: {0: 262, 1: 7, 2: 15, 3: 12, 4: 11, 5: 5, 6: 3, 7: 4, 8: 1, 59: 1, 66: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_1/iteration_path_structures.csv\n",
      "Last iteration: 90, path count: 252\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 315 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_1/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 62, 2: 252}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 252\n",
      "  - Child count distribution: {0: 252, 1: 6, 2: 21, 3: 14, 4: 7, 5: 6, 6: 3, 7: 1, 8: 2, 9: 1, 54: 1, 62: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_2/iteration_path_structures.csv\n",
      "Last iteration: 90, path count: 243\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 299 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_2/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 55, 2: 243}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 243\n",
      "  - Child count distribution: {0: 243, 1: 5, 2: 16, 3: 11, 4: 7, 5: 6, 6: 5, 7: 2, 8: 1, 10: 1, 53: 1, 55: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_3/iteration_path_structures.csv\n",
      "Last iteration: 115, path count: 336\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 427 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_3/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 90, 2: 336}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 336\n",
      "  - Child count distribution: {0: 336, 1: 24, 2: 23, 3: 16, 4: 12, 5: 7, 6: 5, 7: 1, 10: 1, 88: 1, 90: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_1/iteration_path_structures.csv\n",
      "Last iteration: 115, path count: 352\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 438 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_1/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 85, 2: 352}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 352\n",
      "  - Child count distribution: {0: 352, 1: 11, 2: 20, 3: 23, 4: 11, 5: 10, 6: 7, 7: 1, 9: 1, 80: 1, 85: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_2/iteration_path_structures.csv\n",
      "Last iteration: 115, path count: 351\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 432 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_2/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 80, 2: 351}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 351\n",
      "  - Child count distribution: {0: 351, 1: 10, 2: 15, 3: 13, 4: 23, 5: 5, 6: 5, 7: 4, 8: 2, 10: 1, 11: 1, 60: 1, 80: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_3/iteration_path_structures.csv\n",
      "Last iteration: 155, path count: 313\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 388 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_3/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 74, 2: 313}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 313\n",
      "  - Child count distribution: {0: 313, 1: 12, 2: 18, 3: 16, 4: 12, 5: 8, 6: 3, 7: 2, 8: 1, 9: 1, 74: 1, 80: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_2/iteration_path_structures.csv\n",
      "Last iteration: 155, path count: 311\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 384 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_2/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 72, 2: 311}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 311\n",
      "  - Child count distribution: {0: 311, 1: 7, 2: 19, 3: 12, 4: 14, 5: 7, 6: 5, 7: 2, 8: 3, 9: 1, 12: 1, 50: 1, 72: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_1/iteration_path_structures.csv\n",
      "Last iteration: 155, path count: 308\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 383 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_1/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 74, 2: 308}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 308\n",
      "  - Child count distribution: {0: 308, 1: 10, 2: 16, 3: 20, 4: 11, 5: 8, 6: 3, 8: 4, 9: 1, 63: 1, 74: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_1/iteration_path_structures.csv\n",
      "Last iteration: 375, path count: 122\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 151 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_1/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 28, 2: 122}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 122\n",
      "  - Child count distribution: {0: 122, 1: 3, 2: 7, 3: 5, 4: 3, 5: 7, 6: 1, 8: 1, 28: 1, 29: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_3/iteration_path_structures.csv\n",
      "Last iteration: 375, path count: 127\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 157 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_3/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 29, 2: 127}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 127\n",
      "  - Child count distribution: {0: 127, 1: 3, 2: 6, 3: 7, 4: 6, 5: 4, 7: 1, 9: 1, 29: 1, 31: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_2/iteration_path_structures.csv\n",
      "Last iteration: 375, path count: 125\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 157 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_2/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 31, 2: 125}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 125\n",
      "  - Child count distribution: {0: 125, 1: 5, 2: 6, 3: 9, 4: 3, 5: 4, 6: 2, 8: 1, 29: 1, 31: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_2/iteration_path_structures.csv\n",
      "Last iteration: 195, path count: 326\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 403 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_2/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 76, 2: 326}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 326\n",
      "  - Child count distribution: {0: 326, 1: 9, 2: 11, 3: 16, 4: 21, 5: 8, 6: 7, 8: 2, 9: 1, 56: 1, 76: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_1/iteration_path_structures.csv\n",
      "Last iteration: 195, path count: 344\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 425 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_1/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 80, 2: 344}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 344\n",
      "  - Child count distribution: {0: 344, 1: 15, 2: 12, 3: 20, 4: 13, 5: 9, 6: 5, 7: 1, 8: 3, 11: 1, 76: 1, 80: 1}\n",
      "\n",
      "Processing path structure file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_3/iteration_path_structures.csv\n",
      "Last iteration: 195, path count: 351\n",
      "[DEBUG] Found layer columns: ['layer_0_node_id', 'layer_1_node_id', 'layer_2_node_id']\n",
      "[DEBUG] Maximum layer index: 2\n",
      "Calculated information for 433 nodes\n",
      "Updated /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_3/corrected_renyi_entropy.csv, added document_count, layer, parent_id, child_ids, child_count columns\n",
      "Node layer statistics:\n",
      "  - Layer distribution: {0: 1, 1: 81, 2: 351}\n",
      "  - Document count range: 1-970\n",
      "  - Root node count: 1\n",
      "  - Leaf node count: 351\n",
      "  - Child count distribution: {0: 351, 1: 9, 2: 22, 3: 16, 4: 13, 5: 11, 6: 4, 7: 4, 10: 1, 81: 2}\n",
      "==================================================\n",
      "Document count and hierarchical information addition completed!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Main function: Add document count and hierarchical information to entropy files\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd \n",
    "\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step2\"  # Root directory\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Starting to add document count and hierarchical information to entropy files...\")\n",
    "print(\"=\" * 50)\n",
    "add_document_counts_to_entropy_files(base_path)\n",
    "print(\"=\" * 50)\n",
    "print(\"Document count and hierarchical information addition completed!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bd33143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jensen_shannon_distance(p, q):\n",
    "    \"\"\"\n",
    "    Calculate Jensen-Shannon distance between two probability distributions\n",
    "    \n",
    "    Parameters:\n",
    "    p, q: array-like, probability distributions (should be normalized)\n",
    "    \n",
    "    Returns:\n",
    "    float: Jensen-Shannon distance\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays\n",
    "    p = np.array(p)\n",
    "    q = np.array(q)\n",
    "    \n",
    "    # Calculate midpoint distribution\n",
    "    m = 0.5 * (p + q)\n",
    "    \n",
    "    # Calculate KL divergence, add small constant to avoid log(0)\n",
    "    eps = 1e-10\n",
    "    kl_pm = np.sum(p * np.log((p + eps) / (m + eps)))\n",
    "    kl_qm = np.sum(q * np.log((q + eps) / (m + eps)))\n",
    "    \n",
    "    # Jensen-Shannon divergence\n",
    "    js_divergence = 0.5 * kl_pm + 0.5 * kl_qm\n",
    "    \n",
    "    # Jensen-Shannon distance (square root of divergence)\n",
    "    js_distance = np.sqrt(js_divergence)\n",
    "    \n",
    "    return js_distance\n",
    "\n",
    "def calculate_jensen_shannon_distances_with_weighted_entropy_by_eta(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    Calculate Jensen-Shannon distances between nodes in each layer and document-weighted average Renyi entropy\n",
    "    Automatically extract eta value from folder name as Dirichlet smoothing parameter\n",
    "    \"\"\"\n",
    "    # Find all iteration_node_word_distributions.csv files\n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_node_word_distributions.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    print(f\"Found {len(files)} word distribution files to process\")\n",
    "    \n",
    "    # Group files by eta value for display\n",
    "    files_by_eta = {}\n",
    "    for file_path in files:\n",
    "        folder_name = os.path.basename(os.path.dirname(file_path))\n",
    "        eta = 0.1  # Default value\n",
    "        if 'eta_' in folder_name:\n",
    "            try:\n",
    "                eta_part = folder_name.split('eta_')[1].split('_')[0]\n",
    "                eta = float(eta_part)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if eta not in files_by_eta:\n",
    "            files_by_eta[eta] = []\n",
    "        files_by_eta[eta].append(file_path)\n",
    "    \n",
    "    print(\"File distribution:\")\n",
    "    for eta in sorted(files_by_eta.keys()):\n",
    "        print(f\"  Eta {eta}: {len(files_by_eta[eta])} files\")\n",
    "    print()\n",
    "    \n",
    "    # Process each file\n",
    "    for idx, file_path in enumerate(files, 1):\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        \n",
    "        # Extract eta value and run information from folder name\n",
    "        eta = 0.1  # Default value\n",
    "        run_id = \"unknown\"\n",
    "        if 'eta_' in folder_name:\n",
    "            try:\n",
    "                eta_part = folder_name.split('eta_')[1].split('_')[0]\n",
    "                eta = float(eta_part)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if '_run_' in folder_name:\n",
    "            try:\n",
    "                run_id = folder_name.split('_run_')[1]\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(f\"[{idx}/{len(files)}] Processing Eta={eta}, Run={run_id}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        try:\n",
    "            # Read word distribution data\n",
    "            word_df = pd.read_csv(file_path)\n",
    "            word_df.columns = [col.strip(\"'\\\" \") for col in word_df.columns]\n",
    "            \n",
    "            # Get last iteration data\n",
    "            max_iteration = word_df['iteration'].max()\n",
    "            last_iteration_data = word_df[word_df['iteration'] == max_iteration]\n",
    "            \n",
    "            # Get complete vocabulary\n",
    "            all_words = sorted(list(last_iteration_data['word'].dropna().unique()))\n",
    "            \n",
    "            # Read entropy file to get layer information\n",
    "            entropy_file = os.path.join(folder_path, 'corrected_renyi_entropy.csv')\n",
    "            if not os.path.exists(entropy_file):\n",
    "                print(f\"⚠️  Entropy file not found, skipping this file\")\n",
    "                continue\n",
    "                \n",
    "            entropy_df = pd.read_csv(entropy_file)\n",
    "            \n",
    "            # Basic information\n",
    "            print(f\"📊 Basic information:\")\n",
    "            print(f\"   Vocabulary size: {len(all_words)}\")\n",
    "            print(f\"   Last iteration: {max_iteration}\")\n",
    "            \n",
    "            # Group nodes by layer\n",
    "            layers = entropy_df.groupby('layer')['node_id'].apply(list).to_dict()\n",
    "            print(f\"   Layer distribution: {[(layer, len(nodes)) for layer, nodes in layers.items()]}\")\n",
    "            \n",
    "            # Build probability distributions for each node\n",
    "            print(f\"🔄 Building probability distributions...\")\n",
    "            node_distributions = {}\n",
    "            \n",
    "            for node_id in entropy_df['node_id'].unique():\n",
    "                # Get word distribution for this node\n",
    "                node_words = last_iteration_data[last_iteration_data['node_id'] == node_id]\n",
    "                \n",
    "                # Initialize count vector\n",
    "                counts = np.zeros(len(all_words))\n",
    "                word_to_idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "                \n",
    "                # Fill actual counts\n",
    "                for _, row in node_words.iterrows():\n",
    "                    word = row['word']\n",
    "                    if pd.notna(word) and word in word_to_idx:\n",
    "                        counts[word_to_idx[word]] = row['count']\n",
    "                \n",
    "                # Add Dirichlet smoothing\n",
    "                smoothed_counts = counts + eta\n",
    "                \n",
    "                # Calculate probability distribution\n",
    "                probabilities = smoothed_counts / np.sum(smoothed_counts)\n",
    "                node_distributions[node_id] = probabilities\n",
    "            \n",
    "            print(f\"   ✓ Completed {len(node_distributions)} node probability distributions\")\n",
    "            \n",
    "            # Calculate JS distances within each layer and weighted average entropy\n",
    "            all_js_distances = []\n",
    "            layer_avg_distances = []\n",
    "            \n",
    "            print(f\"📐 Calculating JS distances...\")\n",
    "            for layer, layer_nodes in layers.items():\n",
    "                layer_js_distances = []\n",
    "                n = len(layer_nodes)\n",
    "                \n",
    "                # Calculate JS distances for all node pairs within this layer\n",
    "                for i, node1 in enumerate(layer_nodes):\n",
    "                    for j, node2 in enumerate(layer_nodes):\n",
    "                        if i < j:  # Only calculate upper triangle to avoid duplicates and self-comparisons\n",
    "                            if node1 in node_distributions and node2 in node_distributions:\n",
    "                                p = node_distributions[node1]\n",
    "                                q = node_distributions[node2]\n",
    "                                \n",
    "                                # Calculate Jensen-Shannon distance\n",
    "                                js_distance = jensen_shannon_distance(p, q)\n",
    "                                \n",
    "                                layer_js_distances.append({\n",
    "                                    'layer': layer,\n",
    "                                    'node1_id': node1,\n",
    "                                    'node2_id': node2,\n",
    "                                    'js_distance': js_distance,\n",
    "                                    'node1_doc_count': entropy_df[entropy_df['node_id'] == node1]['document_count'].iloc[0] if len(entropy_df[entropy_df['node_id'] == node1]) > 0 else 0,\n",
    "                                    'node2_doc_count': entropy_df[entropy_df['node_id'] == node2]['document_count'].iloc[0] if len(entropy_df[entropy_df['node_id'] == node2]) > 0 else 0\n",
    "                                })\n",
    "                \n",
    "                all_js_distances.extend(layer_js_distances)\n",
    "                \n",
    "                # Calculate average JS distance for this layer\n",
    "                avg_js_distance = 0.0\n",
    "                if layer_js_distances and n > 1:\n",
    "                    total_js_distance = sum(d['js_distance'] for d in layer_js_distances)\n",
    "                    max_pairs = n * (n - 1) // 2\n",
    "                    avg_js_distance = total_js_distance / max_pairs\n",
    "                \n",
    "                # Calculate document-weighted average Renyi entropy for this layer\n",
    "                layer_entropy_data = entropy_df[entropy_df['layer'] == layer]\n",
    "                total_docs = layer_entropy_data['document_count'].sum()\n",
    "                \n",
    "                if total_docs > 0:\n",
    "                    weighted_entropy = (layer_entropy_data['document_count'] * layer_entropy_data['renyi_entropy_corrected']).sum() / total_docs\n",
    "                else:\n",
    "                    weighted_entropy = 0.0\n",
    "                \n",
    "                layer_avg_distances.append({\n",
    "                    'layer': layer,\n",
    "                    'node_count': n,\n",
    "                    'total_pairs': len(layer_js_distances),\n",
    "                    'max_pairs': n * (n - 1) // 2 if n > 1 else 0,\n",
    "                    'sum_js_distance': sum(d['js_distance'] for d in layer_js_distances),\n",
    "                    'avg_js_distance': avg_js_distance,\n",
    "                    'total_documents': total_docs,\n",
    "                    'weighted_avg_renyi_entropy': weighted_entropy,\n",
    "                    'eta_used': eta\n",
    "                })\n",
    "                \n",
    "                # Concise layer statistics output\n",
    "                print(f\"   Layer {layer}: {n} nodes, JS={avg_js_distance:.4f}, entropy={weighted_entropy:.4f}\")\n",
    "            \n",
    "            # Save result files\n",
    "            if all_js_distances:\n",
    "                js_df = pd.DataFrame(all_js_distances)\n",
    "                output_path = os.path.join(folder_path, 'jensen_shannon_distances.csv')\n",
    "                js_df.to_csv(output_path, index=False)\n",
    "            \n",
    "            if layer_avg_distances:\n",
    "                avg_df = pd.DataFrame(layer_avg_distances)\n",
    "                avg_output_path = os.path.join(folder_path, 'layer_average_js_distances.csv')\n",
    "                avg_df.to_csv(avg_output_path, index=False)\n",
    "            \n",
    "            print(f\"💾 Results saved\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Processing failed: {str(e)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"✅ All files processed!\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3cea591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Starting Jensen-Shannon distance and weighted average Renyi entropy calculation (auto-adjusting by eta value)...\n",
      "==================================================\n",
      "Found 18 word distribution files to process\n",
      "File distribution:\n",
      "  Eta 0.005: 3 files\n",
      "  Eta 0.01: 3 files\n",
      "  Eta 0.02: 3 files\n",
      "  Eta 0.05: 3 files\n",
      "  Eta 0.1: 3 files\n",
      "  Eta 0.2: 3 files\n",
      "\n",
      "================================================================================\n",
      "[1/18] Processing Eta=0.1, Run=2\n",
      "================================================================================\n",
      "📊 Basic information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 175\n",
      "   Layer distribution: [(0, 1), (1, 44), (2, 186)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 231 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1 nodes, JS=0.0000, entropy=4.8965\n",
      "   Layer 1: 44 nodes, JS=0.4414, entropy=5.3491\n",
      "   Layer 2: 186 nodes, JS=0.4917, entropy=5.0623\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[2/18] Processing Eta=0.1, Run=3\n",
      "================================================================================\n",
      "📊 Basic information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 175\n",
      "   Layer distribution: [(0, 1), (1, 41), (2, 173)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 215 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1 nodes, JS=0.0000, entropy=4.9225\n",
      "   Layer 1: 41 nodes, JS=0.4557, entropy=5.1379\n",
      "   Layer 2: 173 nodes, JS=0.5004, entropy=5.0351\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[3/18] Processing Eta=0.1, Run=1\n",
      "================================================================================\n",
      "📊 Basic information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 175\n",
      "   Layer distribution: [(0, 1), (1, 40), (2, 164)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 205 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1 nodes, JS=0.0000, entropy=4.9401\n",
      "   Layer 1: 40 nodes, JS=0.4768, entropy=5.1334\n",
      "   Layer 2: 164 nodes, JS=0.5026, entropy=5.0340\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[4/18] Processing Eta=0.05, Run=3\n",
      "================================================================================\n",
      "📊 Basic information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 90\n",
      "   Layer distribution: [(0, 1), (1, 59), (2, 262)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 322 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1 nodes, JS=0.0000, entropy=4.9232\n",
      "   Layer 1: 59 nodes, JS=0.5652, entropy=4.5501\n",
      "   Layer 2: 262 nodes, JS=0.5722, entropy=4.4548\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[5/18] Processing Eta=0.05, Run=1\n",
      "================================================================================\n",
      "📊 Basic information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 90\n",
      "   Layer distribution: [(0, 1), (1, 62), (2, 252)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 315 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1 nodes, JS=0.0000, entropy=4.9626\n",
      "   Layer 1: 62 nodes, JS=0.5686, entropy=4.5881\n",
      "   Layer 2: 252 nodes, JS=0.5737, entropy=4.5019\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[6/18] Processing Eta=0.05, Run=2\n",
      "================================================================================\n",
      "📊 Basic information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 90\n",
      "   Layer distribution: [(0, 1), (1, 55), (2, 243)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 299 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1 nodes, JS=0.0000, entropy=4.9828\n",
      "   Layer 1: 55 nodes, JS=0.5651, entropy=4.6981\n",
      "   Layer 2: 243 nodes, JS=0.5757, entropy=4.4911\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[7/18] Processing Eta=0.005, Run=3\n",
      "================================================================================\n",
      "📊 Basic information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 115\n",
      "   Layer distribution: [(0, 1), (1, 90), (2, 336)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 427 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1 nodes, JS=0.0000, entropy=5.3812\n",
      "   Layer 1: 90 nodes, JS=0.7281, entropy=3.5937\n",
      "   Layer 2: 336 nodes, JS=0.7501, entropy=2.9589\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[8/18] Processing Eta=0.005, Run=1\n",
      "================================================================================\n",
      "📊 Basic information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 115\n",
      "   Layer distribution: [(0, 1), (1, 85), (2, 352)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 438 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1 nodes, JS=0.0000, entropy=5.4413\n",
      "   Layer 1: 85 nodes, JS=0.7409, entropy=3.5530\n",
      "   Layer 2: 352 nodes, JS=0.7490, entropy=3.0083\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[9/18] Processing Eta=0.005, Run=2\n",
      "================================================================================\n",
      "📊 Basic information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 115\n",
      "   Layer distribution: [(0, 1), (1, 80), (2, 351)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 432 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1 nodes, JS=0.0000, entropy=5.4007\n",
      "   Layer 1: 80 nodes, JS=0.7420, entropy=3.5947\n",
      "   Layer 2: 351 nodes, JS=0.7466, entropy=3.0714\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[10/18] Processing Eta=0.02, Run=3\n",
      "================================================================================\n",
      "📊 Basic information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 155\n",
      "   Layer distribution: [(0, 1), (1, 74), (2, 313)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 388 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1 nodes, JS=0.0000, entropy=5.1409\n",
      "   Layer 1: 74 nodes, JS=0.6404, entropy=4.0838\n",
      "   Layer 2: 313 nodes, JS=0.6566, entropy=3.8189\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[11/18] Processing Eta=0.02, Run=2\n",
      "================================================================================\n",
      "📊 Basic information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 155\n",
      "   Layer distribution: [(0, 1), (1, 72), (2, 311)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 384 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1 nodes, JS=0.0000, entropy=5.1550\n",
      "   Layer 1: 72 nodes, JS=0.6478, entropy=4.2164\n",
      "   Layer 2: 311 nodes, JS=0.6509, entropy=3.8552\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[12/18] Processing Eta=0.02, Run=1\n",
      "================================================================================\n",
      "📊 Basic information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 155\n",
      "   Layer distribution: [(0, 1), (1, 74), (2, 308)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 383 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1 nodes, JS=0.0000, entropy=5.2121\n",
      "   Layer 1: 74 nodes, JS=0.6324, entropy=4.2583\n",
      "   Layer 2: 308 nodes, JS=0.6545, entropy=3.8284\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[13/18] Processing Eta=0.2, Run=1\n",
      "================================================================================\n",
      "📊 Basic information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 375\n",
      "   Layer distribution: [(0, 1), (1, 28), (2, 122)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 151 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1 nodes, JS=0.0000, entropy=4.8911\n",
      "   Layer 1: 28 nodes, JS=0.4110, entropy=5.5784\n",
      "   Layer 2: 122 nodes, JS=0.4047, entropy=5.4567\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[14/18] Processing Eta=0.2, Run=3\n",
      "================================================================================\n",
      "📊 Basic information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 375\n",
      "   Layer distribution: [(0, 1), (1, 29), (2, 127)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 157 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1 nodes, JS=0.0000, entropy=4.8726\n",
      "   Layer 1: 29 nodes, JS=0.4286, entropy=5.2456\n",
      "   Layer 2: 127 nodes, JS=0.4026, entropy=5.4723\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[15/18] Processing Eta=0.2, Run=2\n",
      "================================================================================\n",
      "📊 Basic information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 375\n",
      "   Layer distribution: [(0, 1), (1, 31), (2, 125)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 157 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1 nodes, JS=0.0000, entropy=4.8580\n",
      "   Layer 1: 31 nodes, JS=0.3778, entropy=5.4821\n",
      "   Layer 2: 125 nodes, JS=0.4045, entropy=5.4214\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[16/18] Processing Eta=0.01, Run=2\n",
      "================================================================================\n",
      "📊 Basic information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 195\n",
      "   Layer distribution: [(0, 1), (1, 76), (2, 326)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 403 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1 nodes, JS=0.0000, entropy=5.3126\n",
      "   Layer 1: 76 nodes, JS=0.6967, entropy=3.8060\n",
      "   Layer 2: 326 nodes, JS=0.7102, entropy=3.3851\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[17/18] Processing Eta=0.01, Run=1\n",
      "================================================================================\n",
      "📊 Basic information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 195\n",
      "   Layer distribution: [(0, 1), (1, 80), (2, 344)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 425 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1 nodes, JS=0.0000, entropy=5.3202\n",
      "   Layer 1: 80 nodes, JS=0.7029, entropy=3.7364\n",
      "   Layer 2: 344 nodes, JS=0.7068, entropy=3.3289\n",
      "💾 Results saved\n",
      "================================================================================\n",
      "[18/18] Processing Eta=0.01, Run=3\n",
      "================================================================================\n",
      "📊 Basic information:\n",
      "   Vocabulary size: 1490\n",
      "   Last iteration: 195\n",
      "   Layer distribution: [(0, 1), (1, 81), (2, 351)]\n",
      "🔄 Building probability distributions...\n",
      "   ✓ Completed 433 node probability distributions\n",
      "📐 Calculating JS distances...\n",
      "   Layer 0: 1 nodes, JS=0.0000, entropy=5.2808\n",
      "   Layer 1: 81 nodes, JS=0.6997, entropy=3.7018\n",
      "   Layer 2: 351 nodes, JS=0.7041, entropy=3.3166\n",
      "💾 Results saved\n",
      "\n",
      "================================================================================\n",
      "✅ All files processed!\n",
      "================================================================================\n",
      "==================================================\n",
      "Jensen-Shannon distance and weighted average Renyi entropy calculation completed!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd \n",
    "\n",
    "# Main function: Calculate Jensen-Shannon distances and weighted average Renyi entropy\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step2\"  # Root directory\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Starting Jensen-Shannon distance and weighted average Renyi entropy calculation (auto-adjusting by eta value)...\")\n",
    "print(\"=\" * 50)\n",
    "calculate_jensen_shannon_distances_with_weighted_entropy_by_eta(base_path)\n",
    "print(\"=\" * 50)\n",
    "print(\"Jensen-Shannon distance and weighted average Renyi entropy calculation completed!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a10a623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_layer_statistics_by_eta(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    Aggregate layer-level JS distance and weighted entropy statistics by eta value,\n",
    "    generate summary tables at the same level as run folders\n",
    "    \"\"\"\n",
    "    # Find all layer_average_js_distances.csv files\n",
    "    pattern = os.path.join(base_path, \"**\", \"layer_average_js_distances.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    # Store all data and grouping information\n",
    "    all_data = []\n",
    "    eta_groups = {}  # Store parent directory for each eta combination\n",
    "    \n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        parent_folder = os.path.dirname(folder_path)  # Parent directory of run folder\n",
    "        \n",
    "        # Extract eta value from folder name\n",
    "        eta = None\n",
    "        if 'eta_' in folder_name:\n",
    "            try:\n",
    "                eta_part = folder_name.split('eta_')[1].split('_')[0]\n",
    "                eta = float(eta_part)\n",
    "            except (IndexError, ValueError):\n",
    "                print(f\"Warning: Unable to extract eta value from folder name {folder_name}\")\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"Warning: Folder name {folder_name} does not contain eta information\")\n",
    "            continue\n",
    "        \n",
    "        # Extract run number\n",
    "        run_match = folder_name.split('_run_')\n",
    "        if len(run_match) > 1:\n",
    "            run_id = run_match[1]\n",
    "        else:\n",
    "            print(f\"Warning: Unable to extract run number from folder name {folder_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Record parent directory for eta combination\n",
    "        if eta not in eta_groups:\n",
    "            eta_groups[eta] = parent_folder\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                all_data.append({\n",
    "                    'eta': eta,\n",
    "                    'run_id': run_id,\n",
    "                    'layer': row['layer'],\n",
    "                    'node_count': row['node_count'],\n",
    "                    'avg_js_distance': row['avg_js_distance'],\n",
    "                    'weighted_avg_renyi_entropy': row['weighted_avg_renyi_entropy'],\n",
    "                    'total_documents': row['total_documents'],\n",
    "                    'parent_folder': parent_folder\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_path}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    summary_df = pd.DataFrame(all_data)\n",
    "    \n",
    "    if summary_df.empty:\n",
    "        print(\"No valid data found\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"Layer-level Summary Statistics by ETA Value\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Group by eta and generate summary files\n",
    "    for eta, group_data in summary_df.groupby('eta'):\n",
    "        parent_folder = group_data['parent_folder'].iloc[0]\n",
    "        \n",
    "        print(f\"\\nProcessing Eta={eta}\")\n",
    "        print(f\"Output directory: {parent_folder}\")\n",
    "        \n",
    "        # Calculate summary statistics for each layer\n",
    "        layer_summary = group_data.groupby('layer').agg({\n",
    "            'avg_js_distance': ['mean', 'std', 'count'],\n",
    "            'weighted_avg_renyi_entropy': ['mean', 'std', 'count'],\n",
    "            'node_count': ['mean', 'std'],\n",
    "            'total_documents': 'mean',\n",
    "            'run_id': lambda x: ', '.join(sorted(x.unique()))\n",
    "        }).round(4)\n",
    "        \n",
    "        # Flatten column names\n",
    "        layer_summary.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in layer_summary.columns]\n",
    "        layer_summary = layer_summary.reset_index()\n",
    "        \n",
    "        # Rename columns for clarity\n",
    "        column_mapping = {\n",
    "            'avg_js_distance_mean': 'avg_js_distance_mean',\n",
    "            'avg_js_distance_std': 'avg_js_distance_std', \n",
    "            'avg_js_distance_count': 'run_count',\n",
    "            'weighted_avg_renyi_entropy_mean': 'weighted_avg_renyi_entropy_mean',\n",
    "            'weighted_avg_renyi_entropy_std': 'weighted_avg_renyi_entropy_std',\n",
    "            'weighted_avg_renyi_entropy_count': 'entropy_run_count',\n",
    "            'node_count_mean': 'avg_node_count',\n",
    "            'node_count_std': 'node_count_std',\n",
    "            'total_documents_mean': 'avg_total_documents',\n",
    "            'run_id_<lambda>': 'included_runs'\n",
    "        }\n",
    "        \n",
    "        for old_name, new_name in column_mapping.items():\n",
    "            if old_name in layer_summary.columns:\n",
    "                layer_summary = layer_summary.rename(columns={old_name: new_name})\n",
    "        \n",
    "        # Add eta information\n",
    "        layer_summary.insert(0, 'eta', eta)\n",
    "        \n",
    "        # Save summary results at the same level as run folders\n",
    "        output_filename = f'eta_{eta}_layer_summary.csv'\n",
    "        output_path = os.path.join(parent_folder, output_filename)\n",
    "        layer_summary.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"  Saved summary file: {output_path}\")\n",
    "        print(f\"  Included runs: {layer_summary['included_runs'].iloc[0] if 'included_runs' in layer_summary.columns else 'N/A'}\")\n",
    "        print(f\"  Number of layers: {len(layer_summary)}\")\n",
    "        \n",
    "        # Display brief statistics\n",
    "        for _, row in layer_summary.iterrows():\n",
    "            layer_num = int(row['layer'])\n",
    "            js_mean = row['avg_js_distance_mean']\n",
    "            js_std = row['avg_js_distance_std'] if 'avg_js_distance_std' in row else 0\n",
    "            entropy_mean = row['weighted_avg_renyi_entropy_mean']\n",
    "            entropy_std = row['weighted_avg_renyi_entropy_std'] if 'weighted_avg_renyi_entropy_std' in row else 0\n",
    "            node_count = row['avg_node_count']\n",
    "            run_count = int(row['run_count']) if 'run_count' in row else 0\n",
    "            \n",
    "            print(f\"    Layer {layer_num}: JS={js_mean:.4f}(±{js_std:.4f}), entropy={entropy_mean:.4f}(±{entropy_std:.4f}), nodes={node_count:.1f}, runs={run_count}\")\n",
    "    \n",
    "    # Generate overall comparison file (saved in base_path)\n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"Generating Overall Comparison File\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    overall_summary = summary_df.groupby(['eta', 'layer']).agg({\n",
    "        'avg_js_distance': ['mean', 'std'],\n",
    "        'weighted_avg_renyi_entropy': ['mean', 'std'],\n",
    "        'node_count': ['mean', 'std'],\n",
    "        'run_id': 'count'\n",
    "    }).round(4)\n",
    "    \n",
    "    # Flatten column names\n",
    "    overall_summary.columns = ['_'.join(col).strip() for col in overall_summary.columns]\n",
    "    overall_summary = overall_summary.reset_index()\n",
    "    \n",
    "    overall_output_path = os.path.join(base_path, 'eta_layer_comparison.csv')\n",
    "    overall_summary.to_csv(overall_output_path, index=False)\n",
    "    print(f\"Overall comparison file saved to: {overall_output_path}\")\n",
    "    \n",
    "    # Display cross-eta comparison\n",
    "    for layer in sorted(summary_df['layer'].unique()):\n",
    "        print(f\"\\nLayer {int(layer)} Cross-Eta Comparison:\")\n",
    "        print(\"Eta Value  JS Distance(±std)   Weighted Entropy(±std)  Node Count(±std)  Run Count\")\n",
    "        print(\"-\" * 75)\n",
    "        \n",
    "        layer_data = overall_summary[overall_summary['layer'] == layer]\n",
    "        for _, row in layer_data.iterrows():\n",
    "            eta = row['eta']\n",
    "            js_mean = row['avg_js_distance_mean']\n",
    "            js_std = row['avg_js_distance_std']\n",
    "            entropy_mean = row['weighted_avg_renyi_entropy_mean']\n",
    "            entropy_std = row['weighted_avg_renyi_entropy_std']\n",
    "            node_mean = row['node_count_mean']\n",
    "            node_std = row['node_count_std']\n",
    "            run_count = int(row['run_id_count'])\n",
    "            \n",
    "            print(f\"{eta:6.3f}    {js_mean:6.4f}(±{js_std:5.4f})   {entropy_mean:6.4f}(±{entropy_std:5.4f})   {node_mean:6.1f}(±{node_std:4.1f})   {run_count:4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f61f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Starting aggregation of layer statistics by Eta values...\n",
      "======================================================================\n",
      "======================================================================\n",
      "Layer-level Summary Statistics by ETA Value\n",
      "======================================================================\n",
      "\n",
      "Processing Eta=0.005\n",
      "Output directory: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛\n",
      "  Saved summary file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/eta_0.005_layer_summary.csv\n",
      "  Included runs: 1, 2, 3\n",
      "  Number of layers: 3\n",
      "    Layer 0: JS=0.0000(±0.0000), entropy=5.4077(±0.0307), nodes=1.0, runs=3\n",
      "    Layer 1: JS=0.7370(±0.0077), entropy=3.5804(±0.0238), nodes=85.0, runs=3\n",
      "    Layer 2: JS=0.7486(±0.0018), entropy=3.0128(±0.0564), nodes=346.3, runs=3\n",
      "\n",
      "Processing Eta=0.01\n",
      "Output directory: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛\n",
      "  Saved summary file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/eta_0.01_layer_summary.csv\n",
      "  Included runs: 1, 2, 3\n",
      "  Number of layers: 3\n",
      "    Layer 0: JS=0.0000(±0.0000), entropy=5.3045(±0.0209), nodes=1.0, runs=3\n",
      "    Layer 1: JS=0.6998(±0.0031), entropy=3.7481(±0.0531), nodes=79.0, runs=3\n",
      "    Layer 2: JS=0.7070(±0.0031), entropy=3.3435(±0.0365), nodes=340.3, runs=3\n",
      "\n",
      "Processing Eta=0.02\n",
      "Output directory: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛\n",
      "  Saved summary file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/eta_0.02_layer_summary.csv\n",
      "  Included runs: 1, 2, 3\n",
      "  Number of layers: 3\n",
      "    Layer 0: JS=0.0000(±0.0000), entropy=5.1693(±0.0377), nodes=1.0, runs=3\n",
      "    Layer 1: JS=0.6402(±0.0077), entropy=4.1862(±0.0911), nodes=73.3, runs=3\n",
      "    Layer 2: JS=0.6540(±0.0029), entropy=3.8342(±0.0188), nodes=310.7, runs=3\n",
      "\n",
      "Processing Eta=0.05\n",
      "Output directory: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛\n",
      "  Saved summary file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/eta_0.05_layer_summary.csv\n",
      "  Included runs: 1, 2, 3\n",
      "  Number of layers: 3\n",
      "    Layer 0: JS=0.0000(±0.0000), entropy=4.9562(±0.0303), nodes=1.0, runs=3\n",
      "    Layer 1: JS=0.5663(±0.0020), entropy=4.6121(±0.0769), nodes=58.7, runs=3\n",
      "    Layer 2: JS=0.5739(±0.0017), entropy=4.4826(±0.0247), nodes=252.3, runs=3\n",
      "\n",
      "Processing Eta=0.1\n",
      "Output directory: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛\n",
      "  Saved summary file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/eta_0.1_layer_summary.csv\n",
      "  Included runs: 1, 2, 3\n",
      "  Number of layers: 3\n",
      "    Layer 0: JS=0.0000(±0.0000), entropy=4.9197(±0.0219), nodes=1.0, runs=3\n",
      "    Layer 1: JS=0.4579(±0.0178), entropy=5.2068(±0.1233), nodes=41.7, runs=3\n",
      "    Layer 2: JS=0.4982(±0.0058), entropy=5.0438(±0.0160), nodes=174.3, runs=3\n",
      "\n",
      "Processing Eta=0.2\n",
      "Output directory: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛\n",
      "  Saved summary file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/eta_0.2_layer_summary.csv\n",
      "  Included runs: 1, 2, 3\n",
      "  Number of layers: 3\n",
      "    Layer 0: JS=0.0000(±0.0000), entropy=4.8739(±0.0166), nodes=1.0, runs=3\n",
      "    Layer 1: JS=0.4058(±0.0258), entropy=5.4354(±0.1712), nodes=29.3, runs=3\n",
      "    Layer 2: JS=0.4039(±0.0011), entropy=5.4501(±0.0260), nodes=124.7, runs=3\n",
      "\n",
      "======================================================================\n",
      "Generating Overall Comparison File\n",
      "======================================================================\n",
      "Overall comparison file saved to: /Volumes/My Passport/收敛结果/step2/eta_layer_comparison.csv\n",
      "\n",
      "Layer 0 Cross-Eta Comparison:\n",
      "Eta Value  JS Distance(±std)   Weighted Entropy(±std)  Node Count(±std)  Run Count\n",
      "---------------------------------------------------------------------------\n",
      " 0.005    0.0000(±0.0000)   5.4077(±0.0307)      1.0(± 0.0)      3\n",
      " 0.010    0.0000(±0.0000)   5.3045(±0.0209)      1.0(± 0.0)      3\n",
      " 0.020    0.0000(±0.0000)   5.1693(±0.0377)      1.0(± 0.0)      3\n",
      " 0.050    0.0000(±0.0000)   4.9562(±0.0303)      1.0(± 0.0)      3\n",
      " 0.100    0.0000(±0.0000)   4.9197(±0.0219)      1.0(± 0.0)      3\n",
      " 0.200    0.0000(±0.0000)   4.8739(±0.0166)      1.0(± 0.0)      3\n",
      "\n",
      "Layer 1 Cross-Eta Comparison:\n",
      "Eta Value  JS Distance(±std)   Weighted Entropy(±std)  Node Count(±std)  Run Count\n",
      "---------------------------------------------------------------------------\n",
      " 0.005    0.7370(±0.0077)   3.5804(±0.0238)     85.0(± 5.0)      3\n",
      " 0.010    0.6998(±0.0031)   3.7481(±0.0531)     79.0(± 2.6)      3\n",
      " 0.020    0.6402(±0.0077)   4.1862(±0.0911)     73.3(± 1.2)      3\n",
      " 0.050    0.5663(±0.0020)   4.6121(±0.0769)     58.7(± 3.5)      3\n",
      " 0.100    0.4579(±0.0178)   5.2068(±0.1233)     41.7(± 2.1)      3\n",
      " 0.200    0.4058(±0.0258)   5.4354(±0.1712)     29.3(± 1.5)      3\n",
      "\n",
      "Layer 2 Cross-Eta Comparison:\n",
      "Eta Value  JS Distance(±std)   Weighted Entropy(±std)  Node Count(±std)  Run Count\n",
      "---------------------------------------------------------------------------\n",
      " 0.005    0.7486(±0.0018)   3.0128(±0.0564)    346.3(± 9.0)      3\n",
      " 0.010    0.7070(±0.0031)   3.3435(±0.0365)    340.3(±12.9)      3\n",
      " 0.020    0.6540(±0.0029)   3.8342(±0.0188)    310.7(± 2.5)      3\n",
      " 0.050    0.5739(±0.0017)   4.4826(±0.0247)    252.3(± 9.5)      3\n",
      " 0.100    0.4982(±0.0058)   5.0438(±0.0160)    174.3(±11.1)      3\n",
      " 0.200    0.4039(±0.0011)   5.4501(±0.0260)    124.7(± 2.5)      3\n",
      "======================================================================\n",
      "Aggregation analysis completed!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Execute aggregation analysis\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step2\"\n",
    "print(\"=\" * 70)\n",
    "print(\"Starting aggregation of layer statistics by Eta values...\")\n",
    "print(\"=\" * 70)\n",
    "aggregate_layer_statistics_by_eta(base_path)\n",
    "print(\"=\" * 70)\n",
    "print(\"Aggregation analysis completed!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fd8317c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已汇总所有run的层级均值到 all_layers_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# 汇总所有result_layers.csv，按layer分组求mean（包括nodes_in_layer）\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step2\"\n",
    "pattern = os.path.join(base_path, \"**\", \"result_layers.csv\")\n",
    "files = glob.glob(pattern, recursive=True)\n",
    "\n",
    "all_rows = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "    run_folder = os.path.dirname(file)\n",
    "    folder_name = os.path.basename(run_folder)\n",
    "    \n",
    "    # 从文件夹名称提取eta值\n",
    "    eta = None\n",
    "    if 'eta_' in folder_name:\n",
    "        try:\n",
    "            eta_part = folder_name.split('eta_')[1].split('_')[0]\n",
    "            eta = float(eta_part)\n",
    "        except (IndexError, ValueError):\n",
    "            eta = None\n",
    "    \n",
    "    # 按 layer 分组求均值\n",
    "    grouped = df.groupby('layer').agg({\n",
    "        'entropy_wavg': 'mean',\n",
    "        'distinctiveness_wavg_jsd': 'mean',\n",
    "        'nodes_in_layer': 'mean'\n",
    "    }).reset_index()\n",
    "    grouped['run_folder'] = run_folder\n",
    "    grouped['eta'] = eta\n",
    "    \n",
    "    # 如果有其他参数信息（如 depth, gamma, alpha），可从原df取第一行补充\n",
    "    for col in ['depth', 'gamma', 'alpha']:\n",
    "        if col in df.columns:\n",
    "            grouped[col] = df[col].iloc[0]\n",
    "        else:\n",
    "            # 从文件夹名称提取\n",
    "            if col == 'depth' and 'depth_' in folder_name:\n",
    "                try:\n",
    "                    grouped[col] = int(folder_name.split('depth_')[1].split('_')[0])\n",
    "                except:\n",
    "                    grouped[col] = 3  # 默认depth=3\n",
    "            elif col == 'gamma' and 'gamma_' in folder_name:\n",
    "                try:\n",
    "                    grouped[col] = float(folder_name.split('gamma_')[1].split('_')[0])\n",
    "                except:\n",
    "                    grouped[col] = 0.05  # 默认gamma=0.05\n",
    "            elif col == 'alpha':\n",
    "                grouped[col] = 0.1  # 默认alpha=0.1\n",
    "            else:\n",
    "                grouped[col] = None\n",
    "    \n",
    "    all_rows.append(grouped)\n",
    "\n",
    "summary_df = pd.concat(all_rows, ignore_index=True)\n",
    "summary_df.to_csv(os.path.join(base_path, \"all_layers_summary.csv\"), index=False)\n",
    "print(\"已汇总所有run的层级均值到 all_layers_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61d8c818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已生成所有run按eta和层整体均值表 layer_eta_group_mean.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# filepath: /Volumes/My Passport/收敛结果/step2/all_layers_summary.csv\n",
    "df = pd.read_csv(\"/Volumes/My Passport/收敛结果/step2/all_layers_summary.csv\")\n",
    "\n",
    "# 按 eta 和 layer 分组，求均值和标准差\n",
    "summary = df.groupby(['eta', 'layer']).agg({\n",
    "    'entropy_wavg': ['mean', 'std'],\n",
    "    'distinctiveness_wavg_jsd': ['mean', 'std'],\n",
    "    'nodes_in_layer': ['mean', 'std'],\n",
    "    'depth': 'first',\n",
    "    'gamma': 'first',\n",
    "    'alpha': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# 展开多级列名\n",
    "summary.columns = ['_'.join(col).strip('_') for col in summary.columns]\n",
    "\n",
    "summary.to_csv(\"/Volumes/My Passport/收敛结果/step2/layer_eta_group_mean.csv\", index=False)\n",
    "print(\"已生成所有run按eta和层整体均值表 layer_eta_group_mean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "087ce664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已汇总所有result_layers.csv到 all_result_layers_merged.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step2\"\n",
    "pattern = os.path.join(base_path, \"**\", \"result_layers.csv\")\n",
    "files = glob.glob(pattern, recursive=True)\n",
    "\n",
    "all_rows = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "    df['run_folder'] = os.path.dirname(file)  # 标记来源\n",
    "    \n",
    "    # 从路径中提取参数\n",
    "    folder = os.path.dirname(file)\n",
    "    folder_name = os.path.basename(folder)\n",
    "    \n",
    "    for col in ['depth', 'gamma', 'eta', 'alpha']:\n",
    "        if col not in df.columns:\n",
    "            if f\"{col}_\" in folder_name:\n",
    "                try:\n",
    "                    value = float(folder_name.split(f\"{col}_\")[1].split(\"_\")[0])\n",
    "                except:\n",
    "                    value = None\n",
    "                df[col] = value\n",
    "            else:\n",
    "                # 设置默认值\n",
    "                if col == 'depth':\n",
    "                    df[col] = 3\n",
    "                elif col == 'gamma':\n",
    "                    df[col] = 0.05\n",
    "                elif col == 'alpha':\n",
    "                    df[col] = 0.1\n",
    "                else:\n",
    "                    df[col] = None\n",
    "    \n",
    "    all_rows.append(df)\n",
    "\n",
    "summary_df = pd.concat(all_rows, ignore_index=True)\n",
    "summary_df.to_csv(os.path.join(base_path, \"all_result_layers_merged.csv\"), index=False)\n",
    "print(\"已汇总所有result_layers.csv到 all_result_layers_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5ffe640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已生成每组参数每层的均值表 all_params_layer_mean.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step2\"\n",
    "pattern = os.path.join(base_path, \"**\", \"result_layers.csv\")\n",
    "files = glob.glob(pattern, recursive=True)\n",
    "\n",
    "all_rows = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "    folder = os.path.dirname(file)\n",
    "    folder_name = os.path.basename(folder)\n",
    "    \n",
    "    # 补充参数信息，从文件夹名称提取\n",
    "    for col in ['depth', 'gamma', 'eta', 'alpha']:\n",
    "        if col not in df.columns:\n",
    "            if f\"{col}_\" in folder_name:\n",
    "                try:\n",
    "                    value = float(folder_name.split(f\"{col}_\")[1].split(\"_\")[0])\n",
    "                except:\n",
    "                    value = None\n",
    "                df[col] = value\n",
    "            else:\n",
    "                # 设置默认值\n",
    "                if col == 'depth':\n",
    "                    df[col] = 3\n",
    "                elif col == 'gamma':\n",
    "                    df[col] = 0.05\n",
    "                elif col == 'alpha':\n",
    "                    df[col] = 0.1\n",
    "                else:\n",
    "                    df[col] = None\n",
    "    all_rows.append(df)\n",
    "\n",
    "merged = pd.concat(all_rows, ignore_index=True)\n",
    "\n",
    "# 按参数组和layer分组，计算均值和标准差\n",
    "group_cols = ['depth', 'gamma', 'eta', 'alpha', 'layer']\n",
    "summary = merged.groupby(group_cols).agg({\n",
    "    'entropy_wavg': ['mean', 'std'],\n",
    "    'distinctiveness_wavg_jsd': ['mean', 'std'],\n",
    "    'nodes_in_layer': ['mean', 'std'],\n",
    "}).reset_index()\n",
    "\n",
    "# 展开多级列名\n",
    "summary.columns = ['_'.join(col).strip('_') for col in summary.columns]\n",
    "\n",
    "summary.to_csv(os.path.join(base_path, \"all_params_layer_mean.csv\"), index=False)\n",
    "print(\"已生成每组参数每层的均值表 all_params_layer_mean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ac939d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "def extract_params_from_folder(folder_name):\n",
    "    params = {'eta': 0.1, 'gamma': 0.05, 'depth': 3, 'alpha': 0.1}\n",
    "    for param in params.keys():\n",
    "        if f'{param}_' in folder_name:\n",
    "            try:\n",
    "                value = folder_name.split(f'{param}_')[1].split('_')[0]\n",
    "                if param == 'depth':\n",
    "                    params[param] = int(value)\n",
    "                else:\n",
    "                    params[param] = float(value)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ 提取参数 {param} 失败: {e}\")\n",
    "    return params['eta'], params['gamma'], params['depth'], params['alpha']\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "def calculate_standard_coherence_from_corpus_corrected(corpus, node_word_df, top_k=15):\n",
    "    \"\"\"\n",
    "    计算每个节点的主题一致性指标（NPMI, C_V, UMass），并返回全局平均一致性\n",
    "    参数:\n",
    "        corpus: dict {doc_id: [word list]}\n",
    "        node_word_df: DataFrame, 包含'node_id', 'word', 'count'等列\n",
    "        top_k: int, 每个节点选取前K高频词\n",
    "    返回:\n",
    "        global_coherence: dict, 全局平均一致性\n",
    "        per_topic_coherence: dict, 每个节点的各项一致性分数列表\n",
    "        node_to_topic_idx: dict, node_id到topic索引的映射\n",
    "    \"\"\"\n",
    "    texts = list(corpus.values())\n",
    "    dictionary = Dictionary(texts)\n",
    "\n",
    "    topics = []\n",
    "    node_to_topic_idx = {}\n",
    "    topic_idx = 0\n",
    "    for node_id in node_word_df['node_id'].unique():\n",
    "        node_data = node_word_df[node_word_df['node_id'] == node_id]\n",
    "        top_words = node_data.nlargest(top_k, 'count')['word'].tolist()\n",
    "        valid_words = [w for w in top_words if pd.notna(w) and w in dictionary.token2id]\n",
    "        if len(valid_words) >= 2:\n",
    "            topics.append(valid_words)\n",
    "            node_to_topic_idx[node_id] = topic_idx\n",
    "            topic_idx += 1\n",
    "\n",
    "    if len(topics) == 0:\n",
    "        return {}, {}, {}\n",
    "\n",
    "    coherence_measures = ['c_npmi', 'c_v', 'u_mass']\n",
    "    per_topic_coherence = {}\n",
    "    global_coherence = {}\n",
    "\n",
    "    for measure in coherence_measures:\n",
    "        try:\n",
    "            cm = CoherenceModel(\n",
    "                topics=topics,\n",
    "                texts=texts,\n",
    "                dictionary=dictionary,\n",
    "                coherence=measure,\n",
    "                processes=1\n",
    "            )\n",
    "            per_topic_scores = cm.get_coherence_per_topic()\n",
    "            per_topic_coherence[measure] = per_topic_scores\n",
    "            global_coherence[measure] = cm.get_coherence()\n",
    "        except Exception as e:\n",
    "            per_topic_coherence[measure] = [0.0] * len(topics)\n",
    "            global_coherence[measure] = 0.0\n",
    "\n",
    "    return global_coherence, per_topic_coherence, node_to_topic_idx\n",
    "\n",
    "def calculate_coherence_layered_analysis(base_path=\".\", corpus=None, top_k=15):\n",
    "    \"\"\"\n",
    "    Calculate node coherence metrics and perform weighted aggregation analysis by layer\n",
    "    \"\"\"\n",
    "    if corpus is None:\n",
    "        print(\"❌ Must provide original corpus\")\n",
    "        return\n",
    "\n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_node_word_distributions.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "\n",
    "    print(f\"🔍 Found {len(files)} word distribution files to process (top_k={top_k})\")\n",
    "\n",
    "    for idx, file_path in enumerate(files, 1):\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "\n",
    "        # 参数提取\n",
    "        eta, gamma, depth, alpha = extract_params_from_folder(folder_name)\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"[{idx}/{len(files)}] Processing file: {folder_name} (k={top_k})\")\n",
    "        print(f\"Parameters - Eta: {eta}, Gamma: {gamma}, Depth: {depth}, Alpha: {alpha}\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        try:\n",
    "            # Read data\n",
    "            df = pd.read_csv(file_path)\n",
    "            df.columns = [col.strip(\"'\\\" \") for col in df.columns]\n",
    "\n",
    "            max_iteration = df['iteration'].max()\n",
    "            last_iteration_data = df[df['iteration'] == max_iteration]\n",
    "\n",
    "            # Read layer and document count information\n",
    "            entropy_file = os.path.join(folder_path, 'corrected_renyi_entropy.csv')\n",
    "            if not os.path.exists(entropy_file):\n",
    "                print(\"⚠️ Entropy file not found, skipping this file\")\n",
    "                continue\n",
    "\n",
    "            entropy_df = pd.read_csv(entropy_file)\n",
    "\n",
    "            print(f\"📈 Last iteration: {max_iteration}\")\n",
    "            print(f\"📈 Number of nodes: {last_iteration_data['node_id'].nunique()}\")\n",
    "\n",
    "            # Calculate node-level coherence (node-level only)\n",
    "            texts = list(corpus.values())\n",
    "            dictionary = Dictionary(texts)\n",
    "\n",
    "            topics = []\n",
    "            node_to_topic_idx = {}\n",
    "\n",
    "            topic_idx = 0\n",
    "            for node_id in last_iteration_data['node_id'].unique():\n",
    "                node_data = last_iteration_data[last_iteration_data['node_id'] == node_id]\n",
    "                top_words = node_data.nlargest(top_k, 'count')['word'].tolist()\n",
    "\n",
    "                valid_words = []\n",
    "                for word in top_words:\n",
    "                    if pd.notna(word) and word in dictionary.token2id:\n",
    "                        valid_words.append(word)\n",
    "\n",
    "                if len(valid_words) >= 2:\n",
    "                    topics.append(valid_words)\n",
    "                    node_to_topic_idx[node_id] = topic_idx\n",
    "                    topic_idx += 1\n",
    "\n",
    "            if len(topics) == 0:\n",
    "                print(\"⚠️ No valid topics, skipping this file\")\n",
    "                continue\n",
    "\n",
    "            # Calculate coherence metrics\n",
    "            coherence_measures = ['c_npmi', 'c_v', 'u_mass']\n",
    "            per_topic_coherence = {}\n",
    "\n",
    "            for measure in coherence_measures:\n",
    "                try:\n",
    "                    print(f\"   Calculating {measure}...\")\n",
    "\n",
    "                    cm = CoherenceModel(\n",
    "                        topics=topics,\n",
    "                        texts=texts,\n",
    "                        dictionary=dictionary,\n",
    "                        coherence=measure,\n",
    "                        processes=1\n",
    "                    )\n",
    "\n",
    "                    per_topic_scores = cm.get_coherence_per_topic()\n",
    "                    per_topic_coherence[measure] = per_topic_scores\n",
    "\n",
    "                    print(f\"   ✓ {measure}: Range=[{min(per_topic_scores):.4f}, {max(per_topic_scores):.4f}]\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"   ❌ Error calculating {measure}: {e}\")\n",
    "                    per_topic_coherence[measure] = [0.0] * len(topics)\n",
    "\n",
    "            # Merge node-level coherence with layer information\n",
    "            node_coherence_data = []\n",
    "\n",
    "            for node_id in last_iteration_data['node_id'].unique():\n",
    "                node_words = last_iteration_data[last_iteration_data['node_id'] == node_id]\n",
    "                top_words = node_words.nlargest(top_k, 'count')['word'].tolist()\n",
    "                top_words = [word for word in top_words if pd.notna(word)]\n",
    "\n",
    "                # Get layer and document count information\n",
    "                node_entropy_info = entropy_df[entropy_df['node_id'] == node_id]\n",
    "                if len(node_entropy_info) > 0:\n",
    "                    layer = node_entropy_info['layer'].iloc[0]\n",
    "                    document_count = node_entropy_info['document_count'].iloc[0]\n",
    "                else:\n",
    "                    layer = -1\n",
    "                    document_count = 0\n",
    "\n",
    "                # Get node coherence scores\n",
    "                node_coherence_scores = {}\n",
    "                if node_id in node_to_topic_idx:\n",
    "                    topic_idx = node_to_topic_idx[node_id]\n",
    "                    for measure in ['c_npmi', 'c_v', 'u_mass']:\n",
    "                        if measure in per_topic_coherence:\n",
    "                            measure_name = measure.replace('c_', '') if measure.startswith('c_') else measure\n",
    "                            node_coherence_scores[f'node_{measure_name}'] = per_topic_coherence[measure][topic_idx]\n",
    "                        else:\n",
    "                            measure_name = measure.replace('c_', '') if measure.startswith('c_') else measure\n",
    "                            node_coherence_scores[f'node_{measure_name}'] = 0.0\n",
    "                else:\n",
    "                    for measure in ['npmi', 'v', 'u_mass']:\n",
    "                        node_coherence_scores[f'node_{measure}'] = 0.0\n",
    "\n",
    "                node_coherence_data.append({\n",
    "                    'node_id': node_id,\n",
    "                    'eta': eta,\n",
    "                    'gamma': gamma,\n",
    "                    'depth': depth,\n",
    "                    'alpha': alpha,\n",
    "                    'layer': layer,\n",
    "                    'document_count': document_count,\n",
    "                    'top_k': top_k,\n",
    "                    'top_words': ', '.join(top_words[:10]),\n",
    "                    'word_count': len(top_words),\n",
    "\n",
    "                    # Node-level coherence metrics only\n",
    "                    'node_npmi': node_coherence_scores.get('node_npmi', 0.0),\n",
    "                    'node_c_v': node_coherence_scores.get('node_v', 0.0),\n",
    "                    'node_u_mass': node_coherence_scores.get('node_u_mass', 0.0),\n",
    "\n",
    "                    'iteration': max_iteration\n",
    "                })\n",
    "\n",
    "            # Save node-level coherence results (with k value)\n",
    "            coherence_df = pd.DataFrame(node_coherence_data)\n",
    "            node_output_path = os.path.join(folder_path, f'node_coherence_k{top_k}.csv')\n",
    "            coherence_df.to_csv(node_output_path, index=False)\n",
    "\n",
    "            # Calculate layer-weighted average coherence\n",
    "            layer_coherence_summary = []\n",
    "\n",
    "            for layer in coherence_df['layer'].unique():\n",
    "                if layer == -1:  # Skip invalid layers\n",
    "                    continue\n",
    "\n",
    "                layer_data = coherence_df[coherence_df['layer'] == layer]\n",
    "                total_docs = layer_data['document_count'].sum()\n",
    "\n",
    "                if total_docs > 0:\n",
    "                    # Document count weighted average\n",
    "                    weighted_npmi = (layer_data['document_count'] * layer_data['node_npmi']).sum() / total_docs\n",
    "                    weighted_c_v = (layer_data['document_count'] * layer_data['node_c_v']).sum() / total_docs\n",
    "                    weighted_u_mass = (layer_data['document_count'] * layer_data['node_u_mass']).sum() / total_docs\n",
    "\n",
    "                    # Simple average (unweighted)\n",
    "                    simple_npmi = layer_data['node_npmi'].mean()\n",
    "                    simple_c_v = layer_data['node_c_v'].mean()\n",
    "                    simple_u_mass = layer_data['node_u_mass'].mean()\n",
    "\n",
    "                    layer_coherence_summary.append({\n",
    "                        'layer': layer,\n",
    "                        'node_count': len(layer_data),\n",
    "                        'total_documents': total_docs,\n",
    "                        'avg_documents_per_node': total_docs / len(layer_data),\n",
    "\n",
    "                        # Document count weighted average coherence\n",
    "                        'weighted_avg_npmi': weighted_npmi,\n",
    "                        'weighted_avg_c_v': weighted_c_v,\n",
    "                        'weighted_avg_u_mass': weighted_u_mass,\n",
    "\n",
    "                        # Simple average coherence\n",
    "                        'simple_avg_npmi': simple_npmi,\n",
    "                        'simple_avg_c_v': simple_c_v,\n",
    "                        'simple_avg_u_mass': simple_u_mass,\n",
    "\n",
    "                        # Standard deviations\n",
    "                        'std_npmi': layer_data['node_npmi'].std(),\n",
    "                        'std_c_v': layer_data['node_c_v'].std(),\n",
    "                        'std_u_mass': layer_data['node_u_mass'].std(),\n",
    "\n",
    "                        'top_k': top_k,  # Add k value record\n",
    "                        'eta': eta,\n",
    "                        'gamma': gamma,\n",
    "                        'depth': depth,\n",
    "                        'alpha': alpha\n",
    "                    })\n",
    "\n",
    "            # Save layer summary results (with k value)\n",
    "            if layer_coherence_summary:\n",
    "                layer_summary_df = pd.DataFrame(layer_coherence_summary)\n",
    "                layer_output_path = os.path.join(folder_path, f'layer_coherence_summary_k{top_k}.csv')\n",
    "                layer_summary_df.to_csv(layer_output_path, index=False)\n",
    "\n",
    "                print(f\"💾 Node coherence results saved to: {node_output_path}\")\n",
    "                print(f\"💾 Layer summary results saved to: {layer_output_path}\")\n",
    "\n",
    "                print(f\"📊 Layer coherence summary (k={top_k}):\")\n",
    "                for _, row in layer_summary_df.iterrows():\n",
    "                    layer_num = int(row['layer'])\n",
    "                    node_count = int(row['node_count'])\n",
    "                    w_npmi = row['weighted_avg_npmi']\n",
    "                    w_cv = row['weighted_avg_c_v']\n",
    "                    w_umass = row['weighted_avg_u_mass']\n",
    "                    print(f\"   Layer {layer_num} ({node_count} nodes): NPMI={w_npmi:.4f}, C_V={w_cv:.4f}, U_Mass={w_umass:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"❌ Error processing file {file_path}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    print(f\"\\n✅ Coherence layered analysis for all files completed! (k={top_k})\")\n",
    "\n",
    "def process_coherence_with_original_corpus_corrected(base_path=\".\", corpus=None, top_k=15):\n",
    "    \"\"\"\n",
    "    Corrected version: Complete calculation of node-level and global-level coherence metrics\n",
    "    \"\"\"\n",
    "    if corpus is None:\n",
    "        print(\"❌ Must provide original corpus\")\n",
    "        return\n",
    "\n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_node_word_distributions.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "\n",
    "    print(f\"🔍 Found {len(files)} word distribution files to process\")\n",
    "\n",
    "    for idx, file_path in enumerate(files, 1):\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "\n",
    "        # 参数提取\n",
    "        eta, gamma, depth, alpha = extract_params_from_folder(folder_name)\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"[{idx}/{len(files)}] Processing file: {folder_name}\")\n",
    "        print(f\"Parameters - Eta: {eta}, Gamma: {gamma}, Depth: {depth}, Alpha: {alpha}\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        try:\n",
    "            # Read data\n",
    "            df = pd.read_csv(file_path)\n",
    "            df.columns = [col.strip(\"'\\\" \") for col in df.columns]\n",
    "\n",
    "            max_iteration = df['iteration'].max()\n",
    "            last_iteration_data = df[df['iteration'] == max_iteration]\n",
    "\n",
    "            print(f\"📈 Last iteration: {max_iteration}\")\n",
    "            print(f\"📈 Number of nodes: {last_iteration_data['node_id'].nunique()}\")\n",
    "\n",
    "            # Calculate coherence (corrected version)\n",
    "            global_coherence, per_topic_coherence, node_to_topic_idx = calculate_standard_coherence_from_corpus_corrected(\n",
    "                corpus, last_iteration_data, top_k=top_k\n",
    "            )\n",
    "\n",
    "            if not global_coherence:\n",
    "                print(\"⚠️ Coherence calculation failed, skipping this file\")\n",
    "                continue\n",
    "\n",
    "            # Prepare data for saving\n",
    "            results_data = []\n",
    "\n",
    "            for node_id in last_iteration_data['node_id'].unique():\n",
    "                node_words = last_iteration_data[last_iteration_data['node_id'] == node_id]\n",
    "                top_words = node_words.nlargest(top_k, 'count')['word'].tolist()\n",
    "                top_words = [word for word in top_words if pd.notna(word)]\n",
    "\n",
    "                # Get various coherence metrics for this node (corrected version)\n",
    "                node_coherence_scores = {}\n",
    "\n",
    "                if node_id in node_to_topic_idx:\n",
    "                    # Get metrics directly through index\n",
    "                    topic_idx = node_to_topic_idx[node_id]\n",
    "\n",
    "                    for measure in ['c_npmi', 'c_v', 'u_mass']:\n",
    "                        if measure in per_topic_coherence:\n",
    "                            measure_name = measure.replace('c_', '') if measure.startswith('c_') else measure\n",
    "                            node_coherence_scores[f'node_{measure_name}'] = per_topic_coherence[measure][topic_idx]\n",
    "                        else:\n",
    "                            measure_name = measure.replace('c_', '') if measure.startswith('c_') else measure\n",
    "                            node_coherence_scores[f'node_{measure_name}'] = 0.0\n",
    "                else:\n",
    "                    # If node not in mapping, set to 0\n",
    "                    for measure in ['npmi', 'v', 'u_mass']:\n",
    "                        node_coherence_scores[f'node_{measure}'] = 0.0\n",
    "\n",
    "                results_data.append({\n",
    "                    'node_id': node_id,\n",
    "                    'eta': eta,\n",
    "                    'gamma': gamma,\n",
    "                    'depth': depth,\n",
    "                    'alpha': alpha,\n",
    "                    'top_k': top_k,\n",
    "                    'top_words': ', '.join(top_words[:10]),\n",
    "                    'word_count': len(top_words),\n",
    "\n",
    "                    # Node-level coherence metrics (corrected)\n",
    "                    'node_npmi': node_coherence_scores.get('node_npmi', 0.0),\n",
    "                    'node_c_v': node_coherence_scores.get('node_v', 0.0),\n",
    "                    'node_u_mass': node_coherence_scores.get('node_u_mass', 0.0),\n",
    "\n",
    "                    # Global-level coherence metrics\n",
    "                    'global_npmi': global_coherence.get('c_npmi', 0.0),\n",
    "                    'global_c_v': global_coherence.get('c_v', 0.0),\n",
    "                    'global_u_mass': global_coherence.get('u_mass', 0.0),\n",
    "\n",
    "                    'iteration': max_iteration\n",
    "                })\n",
    "\n",
    "            # Save results\n",
    "            results_df = pd.DataFrame(results_data)\n",
    "            output_path = os.path.join(folder_path, 'standard_coherence.csv')\n",
    "            results_df.to_csv(output_path, index=False)\n",
    "\n",
    "            print(f\"💾 Standard coherence results saved to: {output_path}\")\n",
    "            print(f\"📊 Results summary:\")\n",
    "            print(f\"   - Global NPMI: {global_coherence.get('c_npmi', 0.0):.4f}\")\n",
    "            print(f\"   - Global C_V: {global_coherence.get('c_v', 0.0):.4f}\")\n",
    "            print(f\"   - Global U_Mass: {global_coherence.get('u_mass', 0.0):.4f}\")\n",
    "\n",
    "            # Display node-level metric ranges\n",
    "            if len(results_df) > 0:\n",
    "                print(f\"   - Node NPMI range: [{results_df['node_npmi'].min():.4f}, {results_df['node_npmi'].max():.4f}]\")\n",
    "                print(f\"   - Node C_V range: [{results_df['node_c_v'].min():.4f}, {results_df['node_c_v'].max():.4f}]\")\n",
    "                print(f\"   - Node U_Mass range: [{results_df['node_u_mass'].min():.4f}, {results_df['node_u_mass'].max():.4f}]\")\n",
    "\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"❌ Error processing file {file_path}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    print(f\"\\n✅ Standard coherence calculation for all files completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc225850",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 0. Set-up part: import necessary libraries and set up environment \"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from threading import Thread\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "import json\n",
    "import cProfile\n",
    "\n",
    "# Download nltk data once\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# Chinese character support in matplotlib\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "\"\"\" 1.1 Data Preprocessing: load data, clean text, lemmatization, remove low-frequency words \"\"\"\n",
    "\n",
    "# Map POS tags to WordNet format: Penn Treebank annotation (fine-grained, 45 tags), WordNet annotation (coarse-grained, 4 tags: a, v, n, r)\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'  # adjective\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'  # verb\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n'  # noun\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'  # adverb\n",
    "    else:\n",
    "        return 'n'  # default noun\n",
    "\n",
    "# Text cleaning and lemmatization preprocessing function\n",
    "def clean_and_lemmatize(text):\n",
    "    if pd.isnull(text):\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove non-alphabetic characters using regex\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmatized = [lemmatizer.lemmatize(w, get_wordnet_pos(pos)) for w, pos in pos_tags]\n",
    "    return lemmatized\n",
    "\n",
    "# -----------------Load data----------------\n",
    "data = pd.read_excel('/Volumes/My Passport/收敛结果/step2/papers_CM.xlsx', usecols=['PaperID', 'Abstract', 'Keywords', 'Year'])\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Clean and lemmatize the abstracts\n",
    "data['Lemmatized_Tokens'] = data['Abstract'].apply(clean_and_lemmatize)\n",
    "\n",
    "# Count word frequencies\n",
    "all_tokens = [word for tokens in data['Lemmatized_Tokens'] for word in tokens]\n",
    "word_counts = Counter(all_tokens)\n",
    "\n",
    "# Set a minimum frequency threshold for valid words\n",
    "min_freq = 10\n",
    "valid_words = set([word for word, freq in word_counts.items() if freq >= min_freq])\n",
    "\n",
    "# Remove rare words based on frequency threshold\n",
    "def remove_rare_words(tokens):\n",
    "    return [word for word in tokens if word in valid_words]\n",
    "\n",
    "data['Filtered_Tokens'] = data['Lemmatized_Tokens'].apply(remove_rare_words)\n",
    "\n",
    "# Join tokens back into cleaned abstracts\n",
    "data['Cleaned_Abstract'] = data['Filtered_Tokens'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "# Create a cleaned DataFrame with relevant columns\n",
    "cleaned_data = data[['PaperID', 'Year', 'Cleaned_Abstract']]\n",
    "cleaned_data = cleaned_data[~(cleaned_data['PaperID'] == 57188)] # this paper has no abstract\n",
    "cleaned_data = cleaned_data.reset_index(drop=True)\n",
    "cleaned_data.insert(0, 'Document_ID', range(len(cleaned_data)))\n",
    "abstract_list = cleaned_data['Cleaned_Abstract'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "corpus = {doc_id: abstract_list for doc_id, abstract_list in enumerate(abstract_list)}\n",
    "# cleaned_data.to_csv('./data/processed/cleaned_data.xlsx', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13c96c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🗑️ Cleaning old incomplete files...\n",
      "================================================================================\n",
      "🗑️ Total deleted 0 old coherence files\n",
      "\n",
      "================================================================================\n",
      "🔄 Starting recalculation of complete coherence metrics...\n",
      "================================================================================\n",
      "🔍 Found 18 word distribution files to process\n",
      "\n",
      "================================================================================\n",
      "[1/18] Processing file: depth_3_gamma_0.05_eta_0.1_run_2\n",
      "Parameters - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 175\n",
      "📈 Number of nodes: 231\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_2/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0460\n",
      "   - Global C_V: 0.5559\n",
      "   - Global U_Mass: -2.9908\n",
      "   - Node NPMI range: [-0.4494, 0.7414]\n",
      "   - Node C_V range: [0.1778, 0.9900]\n",
      "   - Node U_Mass range: [-14.9995, -0.3317]\n",
      "\n",
      "================================================================================\n",
      "[2/18] Processing file: depth_3_gamma_0.05_eta_0.1_run_3\n",
      "Parameters - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 175\n",
      "📈 Number of nodes: 215\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_3/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0681\n",
      "   - Global C_V: 0.5683\n",
      "   - Global U_Mass: -2.9775\n",
      "   - Node NPMI range: [-0.3200, 0.7823]\n",
      "   - Node C_V range: [0.1904, 0.9937]\n",
      "   - Node U_Mass range: [-11.5219, -0.3196]\n",
      "\n",
      "================================================================================\n",
      "[3/18] Processing file: depth_3_gamma_0.05_eta_0.1_run_1\n",
      "Parameters - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 175\n",
      "📈 Number of nodes: 205\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_1/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0657\n",
      "   - Global C_V: 0.5714\n",
      "   - Global U_Mass: -2.6619\n",
      "   - Node NPMI range: [-0.3240, 0.4322]\n",
      "   - Node C_V range: [0.2010, 0.9101]\n",
      "   - Node U_Mass range: [-11.6260, -0.6346]\n",
      "\n",
      "================================================================================\n",
      "[4/18] Processing file: depth_3_gamma_0.05_eta_0.05_run_3\n",
      "Parameters - Eta: 0.05, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 90\n",
      "📈 Number of nodes: 322\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_3/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0640\n",
      "   - Global C_V: 0.5491\n",
      "   - Global U_Mass: -2.8860\n",
      "   - Node NPMI range: [-0.4658, 0.7414]\n",
      "   - Node C_V range: [0.1757, 0.9900]\n",
      "   - Node U_Mass range: [-13.1559, -0.4397]\n",
      "\n",
      "================================================================================\n",
      "[5/18] Processing file: depth_3_gamma_0.05_eta_0.05_run_1\n",
      "Parameters - Eta: 0.05, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 90\n",
      "📈 Number of nodes: 315\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_1/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0474\n",
      "   - Global C_V: 0.5476\n",
      "   - Global U_Mass: -2.8742\n",
      "   - Node NPMI range: [-0.3846, 0.5347]\n",
      "   - Node C_V range: [0.2051, 0.9493]\n",
      "   - Node U_Mass range: [-13.0123, -0.5701]\n",
      "\n",
      "================================================================================\n",
      "[6/18] Processing file: depth_3_gamma_0.05_eta_0.05_run_2\n",
      "Parameters - Eta: 0.05, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 90\n",
      "📈 Number of nodes: 299\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_2/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0626\n",
      "   - Global C_V: 0.5506\n",
      "   - Global U_Mass: -2.6775\n",
      "   - Node NPMI range: [-0.3949, 0.4990]\n",
      "   - Node C_V range: [0.1782, 0.9313]\n",
      "   - Node U_Mass range: [-13.4126, -0.3905]\n",
      "\n",
      "================================================================================\n",
      "[7/18] Processing file: depth_3_gamma_0.05_eta_0.005_run_3\n",
      "Parameters - Eta: 0.005, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 115\n",
      "📈 Number of nodes: 427\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_3/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0392\n",
      "   - Global C_V: 0.5555\n",
      "   - Global U_Mass: -2.7249\n",
      "   - Node NPMI range: [-0.5759, 0.7414]\n",
      "   - Node C_V range: [0.2086, 0.9900]\n",
      "   - Node U_Mass range: [-12.4467, -0.3742]\n",
      "\n",
      "================================================================================\n",
      "[8/18] Processing file: depth_3_gamma_0.05_eta_0.005_run_1\n",
      "Parameters - Eta: 0.005, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 115\n",
      "📈 Number of nodes: 438\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_1/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0465\n",
      "   - Global C_V: 0.5667\n",
      "   - Global U_Mass: -2.7149\n",
      "   - Node NPMI range: [-0.4431, 0.4935]\n",
      "   - Node C_V range: [0.1066, 0.9101]\n",
      "   - Node U_Mass range: [-14.4683, -0.5701]\n",
      "\n",
      "================================================================================\n",
      "[9/18] Processing file: depth_3_gamma_0.05_eta_0.005_run_2\n",
      "Parameters - Eta: 0.005, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 115\n",
      "📈 Number of nodes: 432\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_2/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0577\n",
      "   - Global C_V: 0.5530\n",
      "   - Global U_Mass: -2.6511\n",
      "   - Node NPMI range: [-0.4747, 0.7756]\n",
      "   - Node C_V range: [0.1305, 0.9918]\n",
      "   - Node U_Mass range: [-12.7344, -0.2982]\n",
      "\n",
      "================================================================================\n",
      "[10/18] Processing file: depth_3_gamma_0.05_eta_0.02_run_3\n",
      "Parameters - Eta: 0.02, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 155\n",
      "📈 Number of nodes: 388\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_3/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0549\n",
      "   - Global C_V: 0.5483\n",
      "   - Global U_Mass: -2.6558\n",
      "   - Node NPMI range: [-0.5662, 0.7414]\n",
      "   - Node C_V range: [0.1983, 0.9900]\n",
      "   - Node U_Mass range: [-15.5162, -0.4020]\n",
      "\n",
      "================================================================================\n",
      "[11/18] Processing file: depth_3_gamma_0.05_eta_0.02_run_2\n",
      "Parameters - Eta: 0.02, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 155\n",
      "📈 Number of nodes: 384\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_2/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0529\n",
      "   - Global C_V: 0.5580\n",
      "   - Global U_Mass: -2.7573\n",
      "   - Node NPMI range: [-0.4267, 0.7756]\n",
      "   - Node C_V range: [0.0000, 0.9918]\n",
      "   - Node U_Mass range: [-15.8418, 0.0000]\n",
      "\n",
      "================================================================================\n",
      "[12/18] Processing file: depth_3_gamma_0.05_eta_0.02_run_1\n",
      "Parameters - Eta: 0.02, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 155\n",
      "📈 Number of nodes: 383\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_1/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0539\n",
      "   - Global C_V: 0.5535\n",
      "   - Global U_Mass: -2.5675\n",
      "   - Node NPMI range: [-0.4210, 0.7823]\n",
      "   - Node C_V range: [0.2070, 0.9937]\n",
      "   - Node U_Mass range: [-11.1053, -0.3447]\n",
      "\n",
      "================================================================================\n",
      "[13/18] Processing file: depth_3_gamma_0.05_eta_0.2_run_1\n",
      "Parameters - Eta: 0.2, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 375\n",
      "📈 Number of nodes: 151\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_1/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0356\n",
      "   - Global C_V: 0.5516\n",
      "   - Global U_Mass: -3.2340\n",
      "   - Node NPMI range: [-0.3866, 0.4266]\n",
      "   - Node C_V range: [0.1761, 0.9002]\n",
      "   - Node U_Mass range: [-12.0902, -0.5699]\n",
      "\n",
      "================================================================================\n",
      "[14/18] Processing file: depth_3_gamma_0.05_eta_0.2_run_3\n",
      "Parameters - Eta: 0.2, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 375\n",
      "📈 Number of nodes: 157\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_3/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0391\n",
      "   - Global C_V: 0.5682\n",
      "   - Global U_Mass: -3.3077\n",
      "   - Node NPMI range: [-0.4202, 0.3874]\n",
      "   - Node C_V range: [0.1975, 0.9447]\n",
      "   - Node U_Mass range: [-13.0551, -0.5628]\n",
      "\n",
      "================================================================================\n",
      "[15/18] Processing file: depth_3_gamma_0.05_eta_0.2_run_2\n",
      "Parameters - Eta: 0.2, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 375\n",
      "📈 Number of nodes: 157\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_2/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0382\n",
      "   - Global C_V: 0.5656\n",
      "   - Global U_Mass: -3.1619\n",
      "   - Node NPMI range: [-0.4131, 0.5378]\n",
      "   - Node C_V range: [0.1848, 0.9005]\n",
      "   - Node U_Mass range: [-13.0318, -0.5941]\n",
      "\n",
      "================================================================================\n",
      "[16/18] Processing file: depth_3_gamma_0.05_eta_0.01_run_2\n",
      "Parameters - Eta: 0.01, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 195\n",
      "📈 Number of nodes: 403\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_2/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0551\n",
      "   - Global C_V: 0.5526\n",
      "   - Global U_Mass: -2.7532\n",
      "   - Node NPMI range: [-0.5228, 0.7617]\n",
      "   - Node C_V range: [0.1941, 0.9917]\n",
      "   - Node U_Mass range: [-13.5579, -0.3146]\n",
      "\n",
      "================================================================================\n",
      "[17/18] Processing file: depth_3_gamma_0.05_eta_0.01_run_1\n",
      "Parameters - Eta: 0.01, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 195\n",
      "📈 Number of nodes: 425\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_1/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0601\n",
      "   - Global C_V: 0.5579\n",
      "   - Global U_Mass: -2.6749\n",
      "   - Node NPMI range: [-0.4959, 0.4643]\n",
      "   - Node C_V range: [0.1747, 0.9392]\n",
      "   - Node U_Mass range: [-13.3602, -0.5701]\n",
      "\n",
      "================================================================================\n",
      "[18/18] Processing file: depth_3_gamma_0.05_eta_0.01_run_3\n",
      "Parameters - Eta: 0.01, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 195\n",
      "📈 Number of nodes: 433\n",
      "💾 Standard coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_3/standard_coherence.csv\n",
      "📊 Results summary:\n",
      "   - Global NPMI: 0.0554\n",
      "   - Global C_V: 0.5510\n",
      "   - Global U_Mass: -2.7079\n",
      "   - Node NPMI range: [-0.4064, 0.5072]\n",
      "   - Node C_V range: [0.1961, 0.9221]\n",
      "   - Node U_Mass range: [-10.7279, -0.5808]\n",
      "\n",
      "✅ Standard coherence calculation for all files completed!\n",
      "================================================================================\n",
      "✅ Corrected coherence calculation completed!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Delete old incomplete files\n",
    "def clean_old_coherence_files(base_path=\".\"):\n",
    "    \"\"\"Delete old incomplete standard coherence files\"\"\"\n",
    "    pattern = os.path.join(base_path, \"**\", \"standard_coherence.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    deleted_count = 0\n",
    "    for file_path in files:\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            print(f\"✓ Deleted old file: {os.path.basename(os.path.dirname(file_path))}\")\n",
    "            deleted_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to delete: {file_path} - {e}\")\n",
    "    \n",
    "    print(f\"🗑️ Total deleted {deleted_count} old coherence files\")\n",
    "\n",
    "# Execute corrected version calculation\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step2\"\n",
    "top_k = 5\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"🗑️ Cleaning old incomplete files...\")\n",
    "print(\"=\" * 80)\n",
    "clean_old_coherence_files(base_path)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🔄 Starting recalculation of complete coherence metrics...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use corrected version function\n",
    "process_coherence_with_original_corpus_corrected(base_path, corpus, top_k)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"✅ Corrected coherence calculation completed!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5329afab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_layer_and_document_info_to_coherence(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    Add 'layer' and 'document_count' info from corrected_renyi_entropy.csv to standard_coherence.csv\n",
    "\n",
    "    Parameters:\n",
    "    base_path: str, root directory of result files\n",
    "    \"\"\"\n",
    "    # Find all folders containing standard_coherence.csv\n",
    "    pattern = os.path.join(base_path, \"**\", \"standard_coherence.csv\")\n",
    "    coherence_files = glob.glob(pattern, recursive=True)\n",
    "\n",
    "    print(f\"🔍 Found {len(coherence_files)} standard coherence files to process\")\n",
    "\n",
    "    for idx, coherence_file_path in enumerate(coherence_files, 1):\n",
    "        folder_path = os.path.dirname(coherence_file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"[{idx}/{len(coherence_files)}] Processing folder: {folder_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        # Check if corresponding corrected_renyi_entropy.csv exists\n",
    "        entropy_file_path = os.path.join(folder_path, 'corrected_renyi_entropy.csv')\n",
    "\n",
    "        if not os.path.exists(entropy_file_path):\n",
    "            print(f\"⚠️  Entropy file not found: {entropy_file_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Read both files\n",
    "            print(\"📖 Reading files...\")\n",
    "            coherence_df = pd.read_csv(coherence_file_path)\n",
    "            entropy_df = pd.read_csv(entropy_file_path)\n",
    "\n",
    "            print(f\"   Coherence file: {len(coherence_df)} rows\")\n",
    "            print(f\"   Entropy file: {len(entropy_df)} rows\")\n",
    "\n",
    "            # Check if layer and document_count columns already exist\n",
    "            existing_cols = coherence_df.columns.tolist()\n",
    "            has_layer = 'layer' in existing_cols\n",
    "            has_doc_count = 'document_count' in existing_cols\n",
    "\n",
    "            print(f\"   Current columns: {existing_cols}\")\n",
    "            print(f\"   Has layer column: {has_layer}\")\n",
    "            print(f\"   Has document_count column: {has_doc_count}\")\n",
    "\n",
    "            # Create node_id to layer and document_count mapping\n",
    "            node_layer_map = entropy_df.set_index('node_id')['layer'].to_dict()\n",
    "            node_doc_count_map = entropy_df.set_index('node_id')['document_count'].to_dict()\n",
    "\n",
    "            print(f\"   Mappable nodes: {len(node_layer_map)}\")\n",
    "\n",
    "            # Add or update layer column\n",
    "            coherence_df['layer'] = coherence_df['node_id'].map(node_layer_map)\n",
    "            print(\"   ✓ Layer column added/updated\")\n",
    "\n",
    "            # Add or update document_count column\n",
    "            coherence_df['document_count'] = coherence_df['node_id'].map(node_doc_count_map)\n",
    "            print(\"   ✓ Document_count column added/updated\")\n",
    "\n",
    "            # Check mapping results\n",
    "            layer_null_count = coherence_df['layer'].isnull().sum()\n",
    "            doc_count_null_count = coherence_df['document_count'].isnull().sum()\n",
    "\n",
    "            if layer_null_count > 0:\n",
    "                print(f\"   ⚠️  {layer_null_count} nodes missing layer info\")\n",
    "\n",
    "            if doc_count_null_count > 0:\n",
    "                print(f\"   ⚠️  {doc_count_null_count} nodes missing document_count info\")\n",
    "\n",
    "            # Show layer distribution stats\n",
    "            layer_stats = coherence_df['layer'].value_counts().sort_index()\n",
    "            print(f\"   📊 Layer distribution: {layer_stats.to_dict()}\")\n",
    "\n",
    "            # Show document count stats\n",
    "            doc_stats = coherence_df['document_count'].describe()\n",
    "            print(f\"   📊 Document count stats:\")\n",
    "            print(f\"      Min: {doc_stats['min']:.0f}\")\n",
    "            print(f\"      Max: {doc_stats['max']:.0f}\")\n",
    "            print(f\"      Mean: {doc_stats['mean']:.1f}\")\n",
    "\n",
    "            # Save updated file\n",
    "            coherence_df.to_csv(coherence_file_path, index=False)\n",
    "            print(f\"💾 Updated and saved: {coherence_file_path}\")\n",
    "\n",
    "            # Show updated columns\n",
    "            updated_cols = coherence_df.columns.tolist()\n",
    "            print(f\"   Updated columns: {updated_cols}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"❌ Error processing file {coherence_file_path}: {str(e)}\")\n",
    "            print(\"Detailed error info:\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    print(f\"\\n✅ All standard coherence files updated with layer and document_count info!\")\n",
    "\n",
    "\n",
    "def verify_coherence_files_update(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    Verify update status of standard_coherence.csv files\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(base_path, \"**\", \"standard_coherence.csv\")\n",
    "    coherence_files = glob.glob(pattern, recursive=True)\n",
    "\n",
    "    print(\"🔍 Verifying update results:\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    all_have_layer = True\n",
    "    all_have_doc_count = True\n",
    "\n",
    "    for file_path in coherence_files:\n",
    "        folder_name = os.path.basename(os.path.dirname(file_path))\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            has_layer = 'layer' in df.columns\n",
    "            has_doc_count = 'document_count' in df.columns\n",
    "\n",
    "            layer_null = df['layer'].isnull().sum() if has_layer else \"No column\"\n",
    "            doc_null = df['document_count'].isnull().sum() if has_doc_count else \"No column\"\n",
    "\n",
    "            status = \"✅\" if (has_layer and has_doc_count and layer_null == 0 and doc_null == 0) else \"⚠️\"\n",
    "\n",
    "            print(f\"{status} {folder_name}\")\n",
    "            print(f\"   Layer column: {'Yes' if has_layer else 'No'} (Nulls: {layer_null})\")\n",
    "            print(f\"   DocCount column: {'Yes' if has_doc_count else 'No'} (Nulls: {doc_null})\")\n",
    "\n",
    "            if not has_layer:\n",
    "                all_have_layer = False\n",
    "            if not has_doc_count:\n",
    "                all_have_doc_count = False\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {folder_name}: Read failed - {e}\")\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(f\"📋 Summary:\")\n",
    "    print(f\"   Total files: {len(coherence_files)}\")\n",
    "    print(f\"   All have layer column: {'Yes' if all_have_layer else 'No'}\")\n",
    "    print(f\"   All have document_count column: {'Yes' if all_have_doc_count else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71ff855d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Starting to add layer and document_count information to standard_coherence.csv ...\n",
      "================================================================================\n",
      "🔍 Found 18 standard coherence files to process\n",
      "\n",
      "================================================================================\n",
      "[1/18] Processing folder: depth_3_gamma_0.05_eta_0.1_run_2\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 231 rows\n",
      "   Entropy file: 231 rows\n",
      "   Current columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 231\n",
      "   ✓ Layer column added/updated\n",
      "   ✓ Document_count column added/updated\n",
      "   📊 Layer distribution: {0: 1, 1: 44, 2: 186}\n",
      "   📊 Document count stats:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 12.6\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_2/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[2/18] Processing folder: depth_3_gamma_0.05_eta_0.1_run_3\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 215 rows\n",
      "   Entropy file: 215 rows\n",
      "   Current columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 215\n",
      "   ✓ Layer column added/updated\n",
      "   ✓ Document_count column added/updated\n",
      "   📊 Layer distribution: {0: 1, 1: 41, 2: 173}\n",
      "   📊 Document count stats:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 13.5\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_3/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[3/18] Processing folder: depth_3_gamma_0.05_eta_0.1_run_1\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 205 rows\n",
      "   Entropy file: 205 rows\n",
      "   Current columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 205\n",
      "   ✓ Layer column added/updated\n",
      "   ✓ Document_count column added/updated\n",
      "   📊 Layer distribution: {0: 1, 1: 40, 2: 164}\n",
      "   📊 Document count stats:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 14.2\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_1/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[4/18] Processing folder: depth_3_gamma_0.05_eta_0.05_run_3\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 322 rows\n",
      "   Entropy file: 322 rows\n",
      "   Current columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 322\n",
      "   ✓ Layer column added/updated\n",
      "   ✓ Document_count column added/updated\n",
      "   📊 Layer distribution: {0: 1, 1: 59, 2: 262}\n",
      "   📊 Document count stats:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 9.0\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_3/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[5/18] Processing folder: depth_3_gamma_0.05_eta_0.05_run_1\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 315 rows\n",
      "   Entropy file: 315 rows\n",
      "   Current columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 315\n",
      "   ✓ Layer column added/updated\n",
      "   ✓ Document_count column added/updated\n",
      "   📊 Layer distribution: {0: 1, 1: 62, 2: 252}\n",
      "   📊 Document count stats:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 9.2\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_1/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[6/18] Processing folder: depth_3_gamma_0.05_eta_0.05_run_2\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 299 rows\n",
      "   Entropy file: 299 rows\n",
      "   Current columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 299\n",
      "   ✓ Layer column added/updated\n",
      "   ✓ Document_count column added/updated\n",
      "   📊 Layer distribution: {0: 1, 1: 55, 2: 243}\n",
      "   📊 Document count stats:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 9.7\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_2/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[7/18] Processing folder: depth_3_gamma_0.05_eta_0.005_run_3\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 427 rows\n",
      "   Entropy file: 427 rows\n",
      "   Current columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 427\n",
      "   ✓ Layer column added/updated\n",
      "   ✓ Document_count column added/updated\n",
      "   📊 Layer distribution: {0: 1, 1: 90, 2: 336}\n",
      "   📊 Document count stats:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 6.8\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_3/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[8/18] Processing folder: depth_3_gamma_0.05_eta_0.005_run_1\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 438 rows\n",
      "   Entropy file: 438 rows\n",
      "   Current columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 438\n",
      "   ✓ Layer column added/updated\n",
      "   ✓ Document_count column added/updated\n",
      "   📊 Layer distribution: {0: 1, 1: 85, 2: 352}\n",
      "   📊 Document count stats:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 6.6\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_1/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[9/18] Processing folder: depth_3_gamma_0.05_eta_0.005_run_2\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 432 rows\n",
      "   Entropy file: 432 rows\n",
      "   Current columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 432\n",
      "   ✓ Layer column added/updated\n",
      "   ✓ Document_count column added/updated\n",
      "   📊 Layer distribution: {0: 1, 1: 80, 2: 351}\n",
      "   📊 Document count stats:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 6.7\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_2/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[10/18] Processing folder: depth_3_gamma_0.05_eta_0.02_run_3\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 388 rows\n",
      "   Entropy file: 388 rows\n",
      "   Current columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 388\n",
      "   ✓ Layer column added/updated\n",
      "   ✓ Document_count column added/updated\n",
      "   📊 Layer distribution: {0: 1, 1: 74, 2: 313}\n",
      "   📊 Document count stats:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 7.5\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_3/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[11/18] Processing folder: depth_3_gamma_0.05_eta_0.02_run_2\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 384 rows\n",
      "   Entropy file: 384 rows\n",
      "   Current columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 384\n",
      "   ✓ Layer column added/updated\n",
      "   ✓ Document_count column added/updated\n",
      "   📊 Layer distribution: {0: 1, 1: 72, 2: 311}\n",
      "   📊 Document count stats:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 7.6\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_2/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[12/18] Processing folder: depth_3_gamma_0.05_eta_0.02_run_1\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 383 rows\n",
      "   Entropy file: 383 rows\n",
      "   Current columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 383\n",
      "   ✓ Layer column added/updated\n",
      "   ✓ Document_count column added/updated\n",
      "   📊 Layer distribution: {0: 1, 1: 74, 2: 308}\n",
      "   📊 Document count stats:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 7.6\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_1/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[13/18] Processing folder: depth_3_gamma_0.05_eta_0.2_run_1\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 151 rows\n",
      "   Entropy file: 151 rows\n",
      "   Current columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 151\n",
      "   ✓ Layer column added/updated\n",
      "   ✓ Document_count column added/updated\n",
      "   📊 Layer distribution: {0: 1, 1: 28, 2: 122}\n",
      "   📊 Document count stats:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 19.3\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_1/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[14/18] Processing folder: depth_3_gamma_0.05_eta_0.2_run_3\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 157 rows\n",
      "   Entropy file: 157 rows\n",
      "   Current columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 157\n",
      "   ✓ Layer column added/updated\n",
      "   ✓ Document_count column added/updated\n",
      "   📊 Layer distribution: {0: 1, 1: 29, 2: 127}\n",
      "   📊 Document count stats:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 18.5\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_3/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[15/18] Processing folder: depth_3_gamma_0.05_eta_0.2_run_2\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 157 rows\n",
      "   Entropy file: 157 rows\n",
      "   Current columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 157\n",
      "   ✓ Layer column added/updated\n",
      "   ✓ Document_count column added/updated\n",
      "   📊 Layer distribution: {0: 1, 1: 31, 2: 125}\n",
      "   📊 Document count stats:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 18.5\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_2/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[16/18] Processing folder: depth_3_gamma_0.05_eta_0.01_run_2\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 403 rows\n",
      "   Entropy file: 403 rows\n",
      "   Current columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 403\n",
      "   ✓ Layer column added/updated\n",
      "   ✓ Document_count column added/updated\n",
      "   📊 Layer distribution: {0: 1, 1: 76, 2: 326}\n",
      "   📊 Document count stats:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 7.2\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_2/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[17/18] Processing folder: depth_3_gamma_0.05_eta_0.01_run_1\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 425 rows\n",
      "   Entropy file: 425 rows\n",
      "   Current columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 425\n",
      "   ✓ Layer column added/updated\n",
      "   ✓ Document_count column added/updated\n",
      "   📊 Layer distribution: {0: 1, 1: 80, 2: 344}\n",
      "   📊 Document count stats:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 6.8\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_1/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "================================================================================\n",
      "[18/18] Processing folder: depth_3_gamma_0.05_eta_0.01_run_3\n",
      "================================================================================\n",
      "📖 Reading files...\n",
      "   Coherence file: 433 rows\n",
      "   Entropy file: 433 rows\n",
      "   Current columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration']\n",
      "   Has layer column: False\n",
      "   Has document_count column: False\n",
      "   Mappable nodes: 433\n",
      "   ✓ Layer column added/updated\n",
      "   ✓ Document_count column added/updated\n",
      "   📊 Layer distribution: {0: 1, 1: 81, 2: 351}\n",
      "   📊 Document count stats:\n",
      "      Min: 1\n",
      "      Max: 970\n",
      "      Mean: 6.7\n",
      "💾 Updated and saved: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_3/standard_coherence.csv\n",
      "   Updated columns: ['node_id', 'eta', 'gamma', 'depth', 'alpha', 'top_k', 'top_words', 'word_count', 'node_npmi', 'node_c_v', 'node_u_mass', 'global_npmi', 'global_c_v', 'global_u_mass', 'iteration', 'layer', 'document_count']\n",
      "\n",
      "✅ All standard coherence files updated with layer and document_count info!\n",
      "\n",
      "================================================================================\n",
      "Verifying update results ...\n",
      "================================================================================\n",
      "🔍 Verifying update results:\n",
      "================================================================================\n",
      "✅ depth_3_gamma_0.05_eta_0.1_run_2\n",
      "   Layer column: Yes (Nulls: 0)\n",
      "   DocCount column: Yes (Nulls: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.1_run_3\n",
      "   Layer column: Yes (Nulls: 0)\n",
      "   DocCount column: Yes (Nulls: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.1_run_1\n",
      "   Layer column: Yes (Nulls: 0)\n",
      "   DocCount column: Yes (Nulls: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.05_run_3\n",
      "   Layer column: Yes (Nulls: 0)\n",
      "   DocCount column: Yes (Nulls: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.05_run_1\n",
      "   Layer column: Yes (Nulls: 0)\n",
      "   DocCount column: Yes (Nulls: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.05_run_2\n",
      "   Layer column: Yes (Nulls: 0)\n",
      "   DocCount column: Yes (Nulls: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.005_run_3\n",
      "   Layer column: Yes (Nulls: 0)\n",
      "   DocCount column: Yes (Nulls: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.005_run_1\n",
      "   Layer column: Yes (Nulls: 0)\n",
      "   DocCount column: Yes (Nulls: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.005_run_2\n",
      "   Layer column: Yes (Nulls: 0)\n",
      "   DocCount column: Yes (Nulls: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.02_run_3\n",
      "   Layer column: Yes (Nulls: 0)\n",
      "   DocCount column: Yes (Nulls: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.02_run_2\n",
      "   Layer column: Yes (Nulls: 0)\n",
      "   DocCount column: Yes (Nulls: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.02_run_1\n",
      "   Layer column: Yes (Nulls: 0)\n",
      "   DocCount column: Yes (Nulls: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.2_run_1\n",
      "   Layer column: Yes (Nulls: 0)\n",
      "   DocCount column: Yes (Nulls: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.2_run_3\n",
      "   Layer column: Yes (Nulls: 0)\n",
      "   DocCount column: Yes (Nulls: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.2_run_2\n",
      "   Layer column: Yes (Nulls: 0)\n",
      "   DocCount column: Yes (Nulls: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.01_run_2\n",
      "   Layer column: Yes (Nulls: 0)\n",
      "   DocCount column: Yes (Nulls: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.01_run_1\n",
      "   Layer column: Yes (Nulls: 0)\n",
      "   DocCount column: Yes (Nulls: 0)\n",
      "✅ depth_3_gamma_0.05_eta_0.01_run_3\n",
      "   Layer column: Yes (Nulls: 0)\n",
      "   DocCount column: Yes (Nulls: 0)\n",
      "================================================================================\n",
      "📋 Summary:\n",
      "   Total files: 18\n",
      "   All have layer column: Yes\n",
      "   All have document_count column: Yes\n",
      "\n",
      "================================================================================\n",
      "✅ Layer and document_count information addition completed!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Execute update\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step2\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Starting to add layer and document_count information to standard_coherence.csv ...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Add layer and document_count information\n",
    "add_layer_and_document_info_to_coherence(base_path)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Verifying update results ...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Verify update results\n",
    "verify_coherence_files_update(base_path)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ Layer and document_count information addition completed!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02540932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_coherence_by_eta(base_path=\".\", top_k=15):\n",
    "    \"\"\"\n",
    "    Aggregate layer-level coherence statistics by eta value (including k value),\n",
    "    and generate weighted and unweighted (simple) summary files.\n",
    "    \"\"\"\n",
    "    # Find all layer_coherence_summary_k{top_k}.csv files\n",
    "    pattern = os.path.join(base_path, \"**\", f\"layer_coherence_summary_k{top_k}.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "\n",
    "    print(f\"🔍 Search pattern: layer_coherence_summary_k{top_k}.csv\")\n",
    "    print(f\"🔍 Found {len(files)} layer summary files\")\n",
    "\n",
    "    all_data = []\n",
    "    eta_groups = {}\n",
    "\n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        parent_folder = os.path.dirname(folder_path)\n",
    "\n",
    "        # Extract eta value\n",
    "        eta = None\n",
    "        if 'eta_' in folder_name:\n",
    "            try:\n",
    "                eta_part = folder_name.split('eta_')[1].split('_')[0]\n",
    "                eta = float(eta_part)\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Extract run number\n",
    "        run_match = folder_name.split('_run_')\n",
    "        if len(run_match) > 1:\n",
    "            run_id = run_match[1]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if eta not in eta_groups:\n",
    "            eta_groups[eta] = parent_folder\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                all_data.append({\n",
    "                    'eta': eta,\n",
    "                    'run_id': run_id,\n",
    "                    'layer': row['layer'],\n",
    "                    'node_count': row['node_count'],\n",
    "                    'total_documents': row['total_documents'],\n",
    "                    'weighted_avg_npmi': row['weighted_avg_npmi'],\n",
    "                    'weighted_avg_c_v': row['weighted_avg_c_v'],\n",
    "                    'weighted_avg_u_mass': row['weighted_avg_u_mass'],\n",
    "                    'simple_avg_npmi': row['simple_avg_npmi'],\n",
    "                    'simple_avg_c_v': row['simple_avg_c_v'],\n",
    "                    'simple_avg_u_mass': row['simple_avg_u_mass'],\n",
    "                    'top_k': top_k,\n",
    "                    'parent_folder': parent_folder\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "    # Convert to DataFrame and aggregate by eta\n",
    "    summary_df = pd.DataFrame(all_data)\n",
    "\n",
    "    if summary_df.empty:\n",
    "        print(\"No valid data found\")\n",
    "        return\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Layer Coherence Summary Statistics by ETA Value (k={top_k})\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    for eta, group_data in summary_df.groupby('eta'):\n",
    "        parent_folder = group_data['parent_folder'].iloc[0]\n",
    "\n",
    "        print(f\"\\nProcessing Eta={eta} (k={top_k})\")\n",
    "\n",
    "        # Weighted aggregation\n",
    "        layer_summary = group_data.groupby('layer').agg({\n",
    "            'weighted_avg_npmi': ['mean', 'std', 'count'],\n",
    "            'weighted_avg_c_v': ['mean', 'std', 'count'],\n",
    "            'weighted_avg_u_mass': ['mean', 'std', 'count'],\n",
    "            'simple_avg_npmi': ['mean', 'std'],\n",
    "            'simple_avg_c_v': ['mean', 'std'],\n",
    "            'simple_avg_u_mass': ['mean', 'std'],\n",
    "            'node_count': 'mean',\n",
    "            'total_documents': 'mean',\n",
    "            'run_id': lambda x: ', '.join(sorted(x.unique()))\n",
    "        }).round(4)\n",
    "\n",
    "        # Flatten column names\n",
    "        layer_summary.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in layer_summary.columns]\n",
    "        layer_summary = layer_summary.reset_index()\n",
    "        layer_summary.insert(0, 'eta', eta)\n",
    "        layer_summary.insert(1, 'top_k', top_k)\n",
    "\n",
    "        # Save weighted + simple aggregation results (filename includes k and eta value)\n",
    "        output_filename = f'eta_{eta}_coherence_layer_comparison_k{top_k}.csv'\n",
    "        output_path = os.path.join(parent_folder, output_filename)\n",
    "        layer_summary.to_csv(output_path, index=False)\n",
    "\n",
    "        print(f\"  Saved summary file: {output_path}\")\n",
    "        print(f\"  Number of layers: {len(layer_summary)}\")\n",
    "\n",
    "        # Show brief statistics\n",
    "        for _, row in layer_summary.iterrows():\n",
    "            layer_num = int(row['layer'])\n",
    "            w_npmi = row['weighted_avg_npmi_mean']\n",
    "            w_cv = row['weighted_avg_c_v_mean']\n",
    "            w_umass = row['weighted_avg_u_mass_mean']\n",
    "            run_count = int(row['weighted_avg_npmi_count'])\n",
    "            print(f\"    Layer {layer_num}: W_NPMI={w_npmi:.4f}, W_C_V={w_cv:.4f}, W_U_Mass={w_umass:.4f}, runs={run_count}\")\n",
    "\n",
    "        # Save unweighted simple coherence summary (filename includes k and eta value)\n",
    "        simple_summary = group_data.groupby('layer').agg({\n",
    "            'simple_avg_npmi': ['mean', 'std'],\n",
    "            'simple_avg_c_v': ['mean', 'std'],\n",
    "            'simple_avg_u_mass': ['mean', 'std'],\n",
    "            'node_count': ['mean', 'std'],\n",
    "            'run_id': 'count'\n",
    "        }).round(4)\n",
    "        simple_summary.columns = ['_'.join(col).strip() for col in simple_summary.columns]\n",
    "        simple_summary = simple_summary.reset_index()\n",
    "        simple_summary.insert(0, 'eta', eta)\n",
    "        simple_summary.insert(1, 'top_k', top_k)\n",
    "        simple_output_filename = f'eta_{eta}_coherence_layer_comparison_k{top_k}_simple.csv'\n",
    "        simple_output_path = os.path.join(parent_folder, simple_output_filename)\n",
    "        simple_summary.to_csv(simple_output_path, index=False)\n",
    "        print(f\"  Saved simple (unweighted) coherence file: {simple_output_path}\")\n",
    "\n",
    "    # Generate overall comparison file (filename includes k value)\n",
    "    overall_summary = summary_df.groupby(['eta', 'layer']).agg({\n",
    "        'weighted_avg_npmi': ['mean', 'std'],\n",
    "        'weighted_avg_c_v': ['mean', 'std'],\n",
    "        'weighted_avg_u_mass': ['mean', 'std'],\n",
    "        'simple_avg_npmi': ['mean', 'std'],\n",
    "        'simple_avg_c_v': ['mean', 'std'],\n",
    "        'simple_avg_u_mass': ['mean', 'std'],\n",
    "        'node_count': 'mean',\n",
    "        'total_documents': 'mean',\n",
    "        'run_id': 'count'\n",
    "    }).round(4)\n",
    "\n",
    "    overall_summary.columns = ['_'.join(col).strip() for col in overall_summary.columns]\n",
    "    overall_summary = overall_summary.reset_index()\n",
    "    overall_summary.insert(2, 'top_k', top_k)\n",
    "\n",
    "    overall_output_path = os.path.join(base_path, f'eta_coherence_layer_comparison_k{top_k}.csv')\n",
    "    overall_summary.to_csv(overall_output_path, index=False)\n",
    "    print(f\"\\nOverall comparison file saved to: {overall_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3803f4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Starting node coherence calculation and layered analysis (k=5)...\n",
      "================================================================================\n",
      "🔍 Found 18 word distribution files to process (top_k=5)\n",
      "\n",
      "================================================================================\n",
      "[1/18] Processing file: depth_3_gamma_0.05_eta_0.1_run_2 (k=5)\n",
      "Parameters - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 175\n",
      "📈 Number of nodes: 231\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.4494, 0.7414]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.1778, 0.9900]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-14.9995, -0.3317]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_2/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_2/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=-0.0013, C_V=0.4948, U_Mass=-0.5699\n",
      "   Layer 1 (44 nodes): NPMI=-0.0083, C_V=0.5269, U_Mass=-1.7559\n",
      "   Layer 2 (186 nodes): NPMI=0.0763, C_V=0.5772, U_Mass=-2.8969\n",
      "\n",
      "================================================================================\n",
      "[2/18] Processing file: depth_3_gamma_0.05_eta_0.1_run_3 (k=5)\n",
      "Parameters - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 175\n",
      "📈 Number of nodes: 215\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.3200, 0.7823]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.1904, 0.9937]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-11.5219, -0.3196]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_3/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_3/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=0.0002, C_V=0.4710, U_Mass=-0.6366\n",
      "   Layer 1 (41 nodes): NPMI=0.0221, C_V=0.5810, U_Mass=-2.2671\n",
      "   Layer 2 (173 nodes): NPMI=0.0850, C_V=0.5613, U_Mass=-2.7649\n",
      "\n",
      "================================================================================\n",
      "[3/18] Processing file: depth_3_gamma_0.05_eta_0.1_run_1 (k=5)\n",
      "Parameters - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 175\n",
      "📈 Number of nodes: 205\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.3240, 0.4322]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.2010, 0.9101]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-11.6260, -0.6346]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_1/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_1/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=-0.0100, C_V=0.4994, U_Mass=-0.7534\n",
      "   Layer 1 (40 nodes): NPMI=0.0967, C_V=0.6061, U_Mass=-1.4213\n",
      "   Layer 2 (164 nodes): NPMI=0.0730, C_V=0.5559, U_Mass=-2.5240\n",
      "\n",
      "================================================================================\n",
      "[4/18] Processing file: depth_3_gamma_0.05_eta_0.05_run_3 (k=5)\n",
      "Parameters - Eta: 0.05, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 90\n",
      "📈 Number of nodes: 322\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.4658, 0.7414]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.1757, 0.9900]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-13.1559, -0.4397]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_3/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_3/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=0.0139, C_V=0.4857, U_Mass=-0.5941\n",
      "   Layer 2 (262 nodes): NPMI=0.0886, C_V=0.5748, U_Mass=-2.7801\n",
      "   Layer 1 (59 nodes): NPMI=0.0712, C_V=0.5415, U_Mass=-2.3718\n",
      "\n",
      "================================================================================\n",
      "[5/18] Processing file: depth_3_gamma_0.05_eta_0.05_run_1 (k=5)\n",
      "Parameters - Eta: 0.05, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 90\n",
      "📈 Number of nodes: 315\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.3846, 0.5347]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.2051, 0.9493]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-13.0123, -0.5701]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_1/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_1/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=0.0051, C_V=0.5174, U_Mass=-0.5701\n",
      "   Layer 1 (62 nodes): NPMI=0.0709, C_V=0.5708, U_Mass=-2.1004\n",
      "   Layer 2 (252 nodes): NPMI=0.0534, C_V=0.5571, U_Mass=-3.0668\n",
      "\n",
      "================================================================================\n",
      "[6/18] Processing file: depth_3_gamma_0.05_eta_0.05_run_2 (k=5)\n",
      "Parameters - Eta: 0.05, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 90\n",
      "📈 Number of nodes: 299\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.3949, 0.4990]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.1782, 0.9313]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-13.4126, -0.3905]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_2/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_2/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=0.0139, C_V=0.4857, U_Mass=-0.6050\n",
      "   Layer 1 (55 nodes): NPMI=0.0297, C_V=0.5589, U_Mass=-2.0098\n",
      "   Layer 2 (243 nodes): NPMI=0.0879, C_V=0.5786, U_Mass=-2.5301\n",
      "\n",
      "================================================================================\n",
      "[7/18] Processing file: depth_3_gamma_0.05_eta_0.005_run_3 (k=5)\n",
      "Parameters - Eta: 0.005, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 115\n",
      "📈 Number of nodes: 427\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.5759, 0.7414]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.2086, 0.9900]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-12.4467, -0.3742]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_3/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_3/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=0.0139, C_V=0.4857, U_Mass=-0.6038\n",
      "   Layer 1 (90 nodes): NPMI=0.0351, C_V=0.6237, U_Mass=-2.2270\n",
      "   Layer 2 (336 nodes): NPMI=0.0909, C_V=0.5830, U_Mass=-2.5098\n",
      "\n",
      "================================================================================\n",
      "[8/18] Processing file: depth_3_gamma_0.05_eta_0.005_run_1 (k=5)\n",
      "Parameters - Eta: 0.005, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 115\n",
      "📈 Number of nodes: 438\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.4431, 0.4935]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.1066, 0.9101]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-14.4683, -0.5701]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_1/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_1/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=0.0051, C_V=0.5174, U_Mass=-0.5701\n",
      "   Layer 1 (85 nodes): NPMI=0.0631, C_V=0.5841, U_Mass=-1.9247\n",
      "   Layer 2 (352 nodes): NPMI=0.0654, C_V=0.5720, U_Mass=-2.5299\n",
      "\n",
      "================================================================================\n",
      "[9/18] Processing file: depth_3_gamma_0.05_eta_0.005_run_2 (k=5)\n",
      "Parameters - Eta: 0.005, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 115\n",
      "📈 Number of nodes: 432\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.4747, 0.7756]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.1305, 0.9918]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-12.7344, -0.2982]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_2/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_2/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=0.0139, C_V=0.4857, U_Mass=-0.6050\n",
      "   Layer 1 (80 nodes): NPMI=0.0290, C_V=0.5295, U_Mass=-2.2866\n",
      "   Layer 2 (351 nodes): NPMI=0.0821, C_V=0.5754, U_Mass=-2.5285\n",
      "\n",
      "================================================================================\n",
      "[10/18] Processing file: depth_3_gamma_0.05_eta_0.02_run_3 (k=5)\n",
      "Parameters - Eta: 0.02, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 155\n",
      "📈 Number of nodes: 388\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.5662, 0.7414]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.1983, 0.9900]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-15.5162, -0.4020]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_3/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_3/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=0.0139, C_V=0.4857, U_Mass=-0.6038\n",
      "   Layer 1 (74 nodes): NPMI=0.0376, C_V=0.6059, U_Mass=-2.1362\n",
      "   Layer 2 (313 nodes): NPMI=0.0786, C_V=0.5648, U_Mass=-2.5748\n",
      "\n",
      "================================================================================\n",
      "[11/18] Processing file: depth_3_gamma_0.05_eta_0.02_run_2 (k=5)\n",
      "Parameters - Eta: 0.02, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 155\n",
      "📈 Number of nodes: 384\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.4267, 0.7756]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.1656, 0.9918]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-15.8418, -0.2664]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_2/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_2/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=0.0139, C_V=0.4857, U_Mass=-0.6050\n",
      "   Layer 1 (72 nodes): NPMI=0.0244, C_V=0.5486, U_Mass=-2.1228\n",
      "   Layer 2 (311 nodes): NPMI=0.0721, C_V=0.5758, U_Mass=-2.6395\n",
      "\n",
      "================================================================================\n",
      "[12/18] Processing file: depth_3_gamma_0.05_eta_0.02_run_1 (k=5)\n",
      "Parameters - Eta: 0.02, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 155\n",
      "📈 Number of nodes: 383\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.4210, 0.7823]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.2070, 0.9937]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-11.1053, -0.3447]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_1/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_1/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=0.0139, C_V=0.4857, U_Mass=-0.5922\n",
      "   Layer 1 (74 nodes): NPMI=0.0346, C_V=0.5527, U_Mass=-2.0060\n",
      "   Layer 2 (308 nodes): NPMI=0.0710, C_V=0.5635, U_Mass=-2.3589\n",
      "\n",
      "================================================================================\n",
      "[13/18] Processing file: depth_3_gamma_0.05_eta_0.2_run_1 (k=5)\n",
      "Parameters - Eta: 0.2, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 375\n",
      "📈 Number of nodes: 151\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.3866, 0.4266]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.1761, 0.9002]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-12.0902, -0.5699]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_1/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_1/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=-0.0013, C_V=0.4948, U_Mass=-0.5699\n",
      "   Layer 1 (28 nodes): NPMI=-0.0191, C_V=0.4915, U_Mass=-2.1928\n",
      "   Layer 2 (122 nodes): NPMI=0.0751, C_V=0.5907, U_Mass=-2.5874\n",
      "\n",
      "================================================================================\n",
      "[14/18] Processing file: depth_3_gamma_0.05_eta_0.2_run_3 (k=5)\n",
      "Parameters - Eta: 0.2, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 375\n",
      "📈 Number of nodes: 157\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.4202, 0.3874]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.1975, 0.9447]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-13.0551, -0.5628]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_3/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_3/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=0.0139, C_V=0.4857, U_Mass=-0.5928\n",
      "   Layer 1 (29 nodes): NPMI=0.0399, C_V=0.5361, U_Mass=-2.0246\n",
      "   Layer 2 (127 nodes): NPMI=0.0628, C_V=0.5805, U_Mass=-2.9911\n",
      "\n",
      "================================================================================\n",
      "[15/18] Processing file: depth_3_gamma_0.05_eta_0.2_run_2 (k=5)\n",
      "Parameters - Eta: 0.2, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 375\n",
      "📈 Number of nodes: 157\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.4131, 0.5378]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.1848, 0.9005]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-13.0318, -0.5941]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_2/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_2/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=0.0139, C_V=0.4857, U_Mass=-0.5941\n",
      "   Layer 1 (31 nodes): NPMI=0.0184, C_V=0.5483, U_Mass=-2.1839\n",
      "   Layer 2 (125 nodes): NPMI=0.0772, C_V=0.5895, U_Mass=-2.6090\n",
      "\n",
      "================================================================================\n",
      "[16/18] Processing file: depth_3_gamma_0.05_eta_0.01_run_2 (k=5)\n",
      "Parameters - Eta: 0.01, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 195\n",
      "📈 Number of nodes: 403\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.5228, 0.7617]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.1941, 0.9917]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-13.5579, -0.3146]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_2/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_2/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=0.0139, C_V=0.4857, U_Mass=-0.6050\n",
      "   Layer 1 (76 nodes): NPMI=0.0334, C_V=0.5401, U_Mass=-2.3932\n",
      "   Layer 2 (326 nodes): NPMI=0.0780, C_V=0.5760, U_Mass=-2.6506\n",
      "\n",
      "================================================================================\n",
      "[17/18] Processing file: depth_3_gamma_0.05_eta_0.01_run_1 (k=5)\n",
      "Parameters - Eta: 0.01, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 195\n",
      "📈 Number of nodes: 425\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.4959, 0.4643]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.1747, 0.9392]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-13.3602, -0.5701]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_1/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_1/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=0.0051, C_V=0.5174, U_Mass=-0.5701\n",
      "   Layer 1 (80 nodes): NPMI=0.0801, C_V=0.5768, U_Mass=-1.9448\n",
      "   Layer 2 (344 nodes): NPMI=0.0743, C_V=0.5654, U_Mass=-2.4811\n",
      "\n",
      "================================================================================\n",
      "[18/18] Processing file: depth_3_gamma_0.05_eta_0.01_run_3 (k=5)\n",
      "Parameters - Eta: 0.01, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 195\n",
      "📈 Number of nodes: 433\n",
      "   Calculating c_npmi...\n",
      "   ✓ c_npmi: Range=[-0.4064, 0.5072]\n",
      "   Calculating c_v...\n",
      "   ✓ c_v: Range=[0.1961, 0.9221]\n",
      "   Calculating u_mass...\n",
      "   ✓ u_mass: Range=[-10.7279, -0.5808]\n",
      "💾 Node coherence results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_3/node_coherence_k5.csv\n",
      "💾 Layer summary results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_3/layer_coherence_summary_k5.csv\n",
      "📊 Layer coherence summary (k=5):\n",
      "   Layer 0 (1 nodes): NPMI=-0.0013, C_V=0.4948, U_Mass=-0.5808\n",
      "   Layer 1 (81 nodes): NPMI=0.0593, C_V=0.6168, U_Mass=-2.2967\n",
      "   Layer 2 (351 nodes): NPMI=0.0815, C_V=0.5745, U_Mass=-2.7566\n",
      "\n",
      "✅ Coherence layered analysis for all files completed! (k=5)\n",
      "\n",
      "================================================================================\n",
      "Starting aggregation of layer coherence statistics by eta value (k=5)...\n",
      "================================================================================\n",
      "🔍 Search pattern: layer_coherence_summary_k5.csv\n",
      "🔍 Found 18 layer summary files\n",
      "======================================================================\n",
      "Layer Coherence Summary Statistics by ETA Value (k=5)\n",
      "======================================================================\n",
      "\n",
      "Processing Eta=0.005 (k=5)\n",
      "  Saved summary file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/eta_0.005_coherence_layer_comparison_k5.csv\n",
      "  Number of layers: 3\n",
      "    Layer 0: W_NPMI=0.0110, W_C_V=0.4963, W_U_Mass=-0.5930, runs=3\n",
      "    Layer 1: W_NPMI=0.0424, W_C_V=0.5791, W_U_Mass=-2.1461, runs=3\n",
      "    Layer 2: W_NPMI=0.0795, W_C_V=0.5768, W_U_Mass=-2.5227, runs=3\n",
      "  Saved simple (unweighted) coherence file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/eta_0.005_coherence_layer_comparison_k5_simple.csv\n",
      "\n",
      "Processing Eta=0.01 (k=5)\n",
      "  Saved summary file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/eta_0.01_coherence_layer_comparison_k5.csv\n",
      "  Number of layers: 3\n",
      "    Layer 0: W_NPMI=0.0059, W_C_V=0.4993, W_U_Mass=-0.5853, runs=3\n",
      "    Layer 1: W_NPMI=0.0576, W_C_V=0.5779, W_U_Mass=-2.2116, runs=3\n",
      "    Layer 2: W_NPMI=0.0779, W_C_V=0.5720, W_U_Mass=-2.6294, runs=3\n",
      "  Saved simple (unweighted) coherence file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/eta_0.01_coherence_layer_comparison_k5_simple.csv\n",
      "\n",
      "Processing Eta=0.02 (k=5)\n",
      "  Saved summary file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/eta_0.02_coherence_layer_comparison_k5.csv\n",
      "  Number of layers: 3\n",
      "    Layer 0: W_NPMI=0.0139, W_C_V=0.4857, W_U_Mass=-0.6004, runs=3\n",
      "    Layer 1: W_NPMI=0.0322, W_C_V=0.5691, W_U_Mass=-2.0884, runs=3\n",
      "    Layer 2: W_NPMI=0.0739, W_C_V=0.5680, W_U_Mass=-2.5244, runs=3\n",
      "  Saved simple (unweighted) coherence file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/eta_0.02_coherence_layer_comparison_k5_simple.csv\n",
      "\n",
      "Processing Eta=0.05 (k=5)\n",
      "  Saved summary file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/eta_0.05_coherence_layer_comparison_k5.csv\n",
      "  Number of layers: 3\n",
      "    Layer 0: W_NPMI=0.0110, W_C_V=0.4963, W_U_Mass=-0.5897, runs=3\n",
      "    Layer 1: W_NPMI=0.0573, W_C_V=0.5571, W_U_Mass=-2.1607, runs=3\n",
      "    Layer 2: W_NPMI=0.0766, W_C_V=0.5701, W_U_Mass=-2.7923, runs=3\n",
      "  Saved simple (unweighted) coherence file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/eta_0.05_coherence_layer_comparison_k5_simple.csv\n",
      "\n",
      "Processing Eta=0.1 (k=5)\n",
      "  Saved summary file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/eta_0.1_coherence_layer_comparison_k5.csv\n",
      "  Number of layers: 3\n",
      "    Layer 0: W_NPMI=-0.0037, W_C_V=0.4884, W_U_Mass=-0.6533, runs=3\n",
      "    Layer 1: W_NPMI=0.0369, W_C_V=0.5714, W_U_Mass=-1.8148, runs=3\n",
      "    Layer 2: W_NPMI=0.0781, W_C_V=0.5648, W_U_Mass=-2.7286, runs=3\n",
      "  Saved simple (unweighted) coherence file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/eta_0.1_coherence_layer_comparison_k5_simple.csv\n",
      "\n",
      "Processing Eta=0.2 (k=5)\n",
      "  Saved summary file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/eta_0.2_coherence_layer_comparison_k5.csv\n",
      "  Number of layers: 3\n",
      "    Layer 0: W_NPMI=0.0088, W_C_V=0.4888, W_U_Mass=-0.5856, runs=3\n",
      "    Layer 1: W_NPMI=0.0131, W_C_V=0.5253, W_U_Mass=-2.1338, runs=3\n",
      "    Layer 2: W_NPMI=0.0717, W_C_V=0.5869, W_U_Mass=-2.7292, runs=3\n",
      "  Saved simple (unweighted) coherence file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/eta_0.2_coherence_layer_comparison_k5_simple.csv\n",
      "\n",
      "Overall comparison file saved to: /Volumes/My Passport/收敛结果/step2/eta_coherence_layer_comparison_k5.csv\n",
      "================================================================================\n",
      "✅ Layered coherence analysis completed! (k=5)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Execute streamlined layered coherence analysis (filename includes k value)\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step2\"\n",
    "top_k = 5\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Starting node coherence calculation and layered analysis (k={top_k})...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate node coherence and layer summary\n",
    "calculate_coherence_layered_analysis(base_path, corpus, top_k)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Starting aggregation of layer coherence statistics by eta value (k={top_k})...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Aggregate by eta (pass top_k parameter)\n",
    "aggregate_coherence_by_eta(base_path, top_k)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"✅ Layered coherence analysis completed! (k={top_k})\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8af242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c493a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity_with_path_mapping_fixed(node_word_df, path_mapping_df, corpus, test_doc_ids, eta_prior=0.1):\n",
    "    \"\"\"\n",
    "    Compute hLDA perplexity for test documents using path-document mapping and node-word distributions.\n",
    "    Args:\n",
    "        node_word_df: DataFrame, node-word distributions for the last iteration\n",
    "        path_mapping_df: DataFrame, path-document mapping for the last iteration\n",
    "        corpus: dict, {doc_id: [word list]}\n",
    "        test_doc_ids: list, document IDs in the test set\n",
    "        eta_prior: float, Dirichlet prior smoothing parameter (from eta value)\n",
    "    Returns:\n",
    "        dict: {\n",
    "            'perplexity': float,\n",
    "            'avg_doc_perplexity': float,\n",
    "            'log_likelihood': float,\n",
    "            'valid_docs': int,\n",
    "            'matched_docs': int,\n",
    "            'total_words': int,\n",
    "            'match_rate': float,\n",
    "            'avg_path_length': float\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Build node-word probability distributions\n",
    "    all_words = sorted(node_word_df['word'].dropna().unique())\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "    node_distributions = {}\n",
    "\n",
    "    for node_id in node_word_df['node_id'].unique():\n",
    "        node_data = node_word_df[node_word_df['node_id'] == node_id]\n",
    "        counts = np.zeros(len(all_words))\n",
    "        for _, row in node_data.iterrows():\n",
    "            word = row['word']\n",
    "            if pd.notna(word) and word in word_to_idx:\n",
    "                counts[word_to_idx[word]] = row['count']\n",
    "        smoothed_counts = counts + eta_prior\n",
    "        probabilities = smoothed_counts / np.sum(smoothed_counts)\n",
    "        node_distributions[node_id] = probabilities\n",
    "\n",
    "    # Map each test document to its path (sequence of node_ids)\n",
    "    doc_path_map = {}\n",
    "    for _, row in path_mapping_df.iterrows():\n",
    "        doc_id = row['doc_id'] if 'doc_id' in row else row['document_id']\n",
    "        if doc_id in test_doc_ids:\n",
    "            # Extract path as a list of node_ids for all layers\n",
    "            path = []\n",
    "            for col in path_mapping_df.columns:\n",
    "                if col.startswith('layer_') and col.endswith('_node_id') and pd.notna(row[col]):\n",
    "                    path.append(row[col])\n",
    "            if path:\n",
    "                doc_path_map[doc_id] = path\n",
    "\n",
    "    log_likelihood = 0.0\n",
    "    total_words = 0\n",
    "    valid_docs = 0\n",
    "    matched_docs = 0\n",
    "    path_lengths = []\n",
    "\n",
    "    for doc_id in test_doc_ids:\n",
    "        words = corpus.get(doc_id, [])\n",
    "        if not words or doc_id not in doc_path_map:\n",
    "            continue\n",
    "        path = doc_path_map[doc_id]\n",
    "        path_lengths.append(len(path))\n",
    "        matched_docs += 1\n",
    "\n",
    "        # For each word, average probability over all nodes in the path\n",
    "        word_probs = []\n",
    "        for word in words:\n",
    "            if word not in word_to_idx:\n",
    "                continue\n",
    "            prob_sum = 0.0\n",
    "            for node_id in path:\n",
    "                if node_id in node_distributions:\n",
    "                    prob_sum += node_distributions[node_id][word_to_idx[word]]\n",
    "            if len(path) > 0:\n",
    "                avg_prob = prob_sum / len(path)\n",
    "                word_probs.append(avg_prob)\n",
    "        if word_probs:\n",
    "            valid_docs += 1\n",
    "            total_words += len(word_probs)\n",
    "            log_likelihood += np.sum(np.log(np.maximum(word_probs, 1e-12)))  # avoid log(0)\n",
    "\n",
    "    if total_words == 0 or valid_docs == 0:\n",
    "        return {\n",
    "            'perplexity': np.nan,\n",
    "            'avg_doc_perplexity': np.nan,\n",
    "            'log_likelihood': 0.0,\n",
    "            'valid_docs': valid_docs,\n",
    "            'matched_docs': matched_docs,\n",
    "            'total_words': total_words,\n",
    "            'match_rate': matched_docs / len(test_doc_ids) if test_doc_ids else 0,\n",
    "            'avg_path_length': np.mean(path_lengths) if path_lengths else 0\n",
    "        }\n",
    "\n",
    "    avg_doc_perplexity = np.exp(-log_likelihood / total_words)\n",
    "    perplexity = avg_doc_perplexity\n",
    "    match_rate = matched_docs / len(test_doc_ids) if test_doc_ids else 0\n",
    "    avg_path_length = np.mean(path_lengths) if path_lengths else 0\n",
    "\n",
    "    return {\n",
    "        'perplexity': perplexity,\n",
    "        'avg_doc_perplexity': avg_doc_perplexity,\n",
    "        'log_likelihood': log_likelihood,\n",
    "        'valid_docs': valid_docs,\n",
    "        'matched_docs': matched_docs,\n",
    "        'total_words': total_words,\n",
    "        'match_rate': match_rate,\n",
    "        'avg_path_length': avg_path_length\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d1337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Starting full perplexity computation...\n",
      "================================================================================\n",
      "📊 Dataset split:\n",
      "   Total documents: 970\n",
      "   Training set: 776 documents\n",
      "   Test set: 194 documents\n",
      "🔍 Found 18 model result folders to process\n",
      "\n",
      "================================================================================\n",
      "[1/18] Computing perplexity for: depth_3_gamma_0.05_eta_0.1_run_2\n",
      "Parameters - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 175\n",
      "📈 Number of nodes: 231\n",
      "📈 Path mappings: 970\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_2/perplexity_results_final.csv\n",
      "📊 Perplexity summary:\n",
      "   - Perplexity: 402.1430\n",
      "   - Average doc perplexity: 402.1430\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test docs: 194/194\n",
      "\n",
      "================================================================================\n",
      "[2/18] Computing perplexity for: depth_3_gamma_0.05_eta_0.1_run_3\n",
      "Parameters - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 175\n",
      "📈 Number of nodes: 215\n",
      "📈 Path mappings: 970\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_3/perplexity_results_final.csv\n",
      "📊 Perplexity summary:\n",
      "   - Perplexity: 422.7682\n",
      "   - Average doc perplexity: 422.7682\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test docs: 194/194\n",
      "\n",
      "================================================================================\n",
      "[3/18] Computing perplexity for: depth_3_gamma_0.05_eta_0.1_run_1\n",
      "Parameters - Eta: 0.1, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 175\n",
      "📈 Number of nodes: 205\n",
      "📈 Path mappings: 970\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_1/perplexity_results_final.csv\n",
      "📊 Perplexity summary:\n",
      "   - Perplexity: 422.8394\n",
      "   - Average doc perplexity: 422.8394\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test docs: 194/194\n",
      "\n",
      "================================================================================\n",
      "[4/18] Computing perplexity for: depth_3_gamma_0.05_eta_0.05_run_3\n",
      "Parameters - Eta: 0.05, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 90\n",
      "📈 Number of nodes: 322\n",
      "📈 Path mappings: 970\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_3/perplexity_results_final.csv\n",
      "📊 Perplexity summary:\n",
      "   - Perplexity: 344.6936\n",
      "   - Average doc perplexity: 344.6936\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test docs: 194/194\n",
      "\n",
      "================================================================================\n",
      "[5/18] Computing perplexity for: depth_3_gamma_0.05_eta_0.05_run_1\n",
      "Parameters - Eta: 0.05, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 90\n",
      "📈 Number of nodes: 315\n",
      "📈 Path mappings: 970\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_1/perplexity_results_final.csv\n",
      "📊 Perplexity summary:\n",
      "   - Perplexity: 350.8027\n",
      "   - Average doc perplexity: 350.8027\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test docs: 194/194\n",
      "\n",
      "================================================================================\n",
      "[6/18] Computing perplexity for: depth_3_gamma_0.05_eta_0.05_run_2\n",
      "Parameters - Eta: 0.05, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 90\n",
      "📈 Number of nodes: 299\n",
      "📈 Path mappings: 970\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_2/perplexity_results_final.csv\n",
      "📊 Perplexity summary:\n",
      "   - Perplexity: 374.0022\n",
      "   - Average doc perplexity: 374.0022\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test docs: 194/194\n",
      "\n",
      "================================================================================\n",
      "[7/18] Computing perplexity for: depth_3_gamma_0.05_eta_0.005_run_3\n",
      "Parameters - Eta: 0.005, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 115\n",
      "📈 Number of nodes: 427\n",
      "📈 Path mappings: 970\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_3/perplexity_results_final.csv\n",
      "📊 Perplexity summary:\n",
      "   - Perplexity: 449.8593\n",
      "   - Average doc perplexity: 449.8593\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test docs: 194/194\n",
      "\n",
      "================================================================================\n",
      "[8/18] Computing perplexity for: depth_3_gamma_0.05_eta_0.005_run_1\n",
      "Parameters - Eta: 0.005, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 115\n",
      "📈 Number of nodes: 438\n",
      "📈 Path mappings: 970\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_1/perplexity_results_final.csv\n",
      "📊 Perplexity summary:\n",
      "   - Perplexity: 429.6800\n",
      "   - Average doc perplexity: 429.6800\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test docs: 194/194\n",
      "\n",
      "================================================================================\n",
      "[9/18] Computing perplexity for: depth_3_gamma_0.05_eta_0.005_run_2\n",
      "Parameters - Eta: 0.005, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 115\n",
      "📈 Number of nodes: 432\n",
      "📈 Path mappings: 970\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_2/perplexity_results_final.csv\n",
      "📊 Perplexity summary:\n",
      "   - Perplexity: 463.9480\n",
      "   - Average doc perplexity: 463.9480\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test docs: 194/194\n",
      "\n",
      "================================================================================\n",
      "[10/18] Computing perplexity for: depth_3_gamma_0.05_eta_0.02_run_3\n",
      "Parameters - Eta: 0.02, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 155\n",
      "📈 Number of nodes: 388\n",
      "📈 Path mappings: 970\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_3/perplexity_results_final.csv\n",
      "📊 Perplexity summary:\n",
      "   - Perplexity: 364.2593\n",
      "   - Average doc perplexity: 364.2593\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test docs: 194/194\n",
      "\n",
      "================================================================================\n",
      "[11/18] Computing perplexity for: depth_3_gamma_0.05_eta_0.02_run_2\n",
      "Parameters - Eta: 0.02, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 155\n",
      "📈 Number of nodes: 384\n",
      "📈 Path mappings: 970\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_2/perplexity_results_final.csv\n",
      "📊 Perplexity summary:\n",
      "   - Perplexity: 378.2930\n",
      "   - Average doc perplexity: 378.2930\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test docs: 194/194\n",
      "\n",
      "================================================================================\n",
      "[12/18] Computing perplexity for: depth_3_gamma_0.05_eta_0.02_run_1\n",
      "Parameters - Eta: 0.02, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 155\n",
      "📈 Number of nodes: 383\n",
      "📈 Path mappings: 970\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_1/perplexity_results_final.csv\n",
      "📊 Perplexity summary:\n",
      "   - Perplexity: 370.4950\n",
      "   - Average doc perplexity: 370.4950\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test docs: 194/194\n",
      "\n",
      "================================================================================\n",
      "[13/18] Computing perplexity for: depth_3_gamma_0.05_eta_0.2_run_1\n",
      "Parameters - Eta: 0.2, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 375\n",
      "📈 Number of nodes: 151\n",
      "📈 Path mappings: 970\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_1/perplexity_results_final.csv\n",
      "📊 Perplexity summary:\n",
      "   - Perplexity: 476.7846\n",
      "   - Average doc perplexity: 476.7846\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test docs: 194/194\n",
      "\n",
      "================================================================================\n",
      "[14/18] Computing perplexity for: depth_3_gamma_0.05_eta_0.2_run_3\n",
      "Parameters - Eta: 0.2, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 375\n",
      "📈 Number of nodes: 157\n",
      "📈 Path mappings: 970\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_3/perplexity_results_final.csv\n",
      "📊 Perplexity summary:\n",
      "   - Perplexity: 456.4301\n",
      "   - Average doc perplexity: 456.4301\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test docs: 194/194\n",
      "\n",
      "================================================================================\n",
      "[15/18] Computing perplexity for: depth_3_gamma_0.05_eta_0.2_run_2\n",
      "Parameters - Eta: 0.2, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 375\n",
      "📈 Number of nodes: 157\n",
      "📈 Path mappings: 970\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_2/perplexity_results_final.csv\n",
      "📊 Perplexity summary:\n",
      "   - Perplexity: 468.0193\n",
      "   - Average doc perplexity: 468.0193\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test docs: 194/194\n",
      "\n",
      "================================================================================\n",
      "[16/18] Computing perplexity for: depth_3_gamma_0.05_eta_0.01_run_2\n",
      "Parameters - Eta: 0.01, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 195\n",
      "📈 Number of nodes: 403\n",
      "📈 Path mappings: 970\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_2/perplexity_results_final.csv\n",
      "📊 Perplexity summary:\n",
      "   - Perplexity: 420.4142\n",
      "   - Average doc perplexity: 420.4142\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test docs: 194/194\n",
      "\n",
      "================================================================================\n",
      "[17/18] Computing perplexity for: depth_3_gamma_0.05_eta_0.01_run_1\n",
      "Parameters - Eta: 0.01, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 195\n",
      "📈 Number of nodes: 425\n",
      "📈 Path mappings: 970\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_1/perplexity_results_final.csv\n",
      "📊 Perplexity summary:\n",
      "   - Perplexity: 389.7291\n",
      "   - Average doc perplexity: 389.7291\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test docs: 194/194\n",
      "\n",
      "================================================================================\n",
      "[18/18] Computing perplexity for: depth_3_gamma_0.05_eta_0.01_run_3\n",
      "Parameters - Eta: 0.01, Gamma: 0.05, Depth: 3, Alpha: 0.1\n",
      "================================================================================\n",
      "📈 Last iteration: 195\n",
      "📈 Number of nodes: 433\n",
      "📈 Path mappings: 970\n",
      "💾 Perplexity results saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_3/perplexity_results_final.csv\n",
      "📊 Perplexity summary:\n",
      "   - Perplexity: 379.7033\n",
      "   - Average doc perplexity: 379.7033\n",
      "   - Document match rate: 100.0%\n",
      "   - Average path length: 3.0\n",
      "   - Valid test docs: 194/194\n",
      "\n",
      "✅ Perplexity computation completed for all folders!\n",
      "\n",
      "================================================================================\n",
      "Starting aggregation of perplexity by eta...\n",
      "================================================================================\n",
      "🔍 Found 18 perplexity result files\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.1_run_2 - 1 rows\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.1_run_3 - 1 rows\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.1_run_1 - 1 rows\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.05_run_3 - 1 rows\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.05_run_1 - 1 rows\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.05_run_2 - 1 rows\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.005_run_3 - 1 rows\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.005_run_1 - 1 rows\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.005_run_2 - 1 rows\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.02_run_3 - 1 rows\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.02_run_2 - 1 rows\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.02_run_1 - 1 rows\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.2_run_1 - 1 rows\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.2_run_3 - 1 rows\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.2_run_2 - 1 rows\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.01_run_2 - 1 rows\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.01_run_1 - 1 rows\n",
      "📖 Reading file: depth_3_gamma_0.05_eta_0.01_run_3 - 1 rows\n",
      "📊 Data summary:\n",
      "   Total rows: 18\n",
      "   Unique eta values: [0.005, 0.01, 0.02, 0.05, 0.1, 0.2]\n",
      "   Counts per eta: {0.005: 3, 0.01: 3, 0.02: 3, 0.05: 3, 0.1: 3, 0.2: 3}\n",
      "================================================================================\n",
      "Perplexity aggregation by ETA\n",
      "================================================================================\n",
      "\n",
      "Processing Eta=0.005\n",
      "Output directory: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛\n",
      "Group size: 3\n",
      "   Aggregating using keys: ['perplexity', 'avg_doc_perplexity', 'valid_test_docs', 'total_test_words', 'doc_match_rate', 'avg_path_length', 'log_likelihood', 'run_id', 'gamma', 'depth', 'alpha']\n",
      "❌ Error processing Eta=0.005: no results\n",
      "\n",
      "Processing Eta=0.01\n",
      "Output directory: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛\n",
      "Group size: 3\n",
      "   Aggregating using keys: ['perplexity', 'avg_doc_perplexity', 'valid_test_docs', 'total_test_words', 'doc_match_rate', 'avg_path_length', 'log_likelihood', 'run_id', 'gamma', 'depth', 'alpha']\n",
      "❌ Error processing Eta=0.01: no results\n",
      "\n",
      "Processing Eta=0.02\n",
      "Output directory: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛\n",
      "Group size: 3\n",
      "   Aggregating using keys: ['perplexity', 'avg_doc_perplexity', 'valid_test_docs', 'total_test_words', 'doc_match_rate', 'avg_path_length', 'log_likelihood', 'run_id', 'gamma', 'depth', 'alpha']\n",
      "❌ Error processing Eta=0.02: no results\n",
      "\n",
      "Processing Eta=0.05\n",
      "Output directory: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛\n",
      "Group size: 3\n",
      "   Aggregating using keys: ['perplexity', 'avg_doc_perplexity', 'valid_test_docs', 'total_test_words', 'doc_match_rate', 'avg_path_length', 'log_likelihood', 'run_id', 'gamma', 'depth', 'alpha']\n",
      "❌ Error processing Eta=0.05: no results\n",
      "\n",
      "Processing Eta=0.1\n",
      "Output directory: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛\n",
      "Group size: 3\n",
      "   Aggregating using keys: ['perplexity', 'avg_doc_perplexity', 'valid_test_docs', 'total_test_words', 'doc_match_rate', 'avg_path_length', 'log_likelihood', 'run_id', 'gamma', 'depth', 'alpha']\n",
      "❌ Error processing Eta=0.1: no results\n",
      "\n",
      "Processing Eta=0.2\n",
      "Output directory: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛\n",
      "Group size: 3\n",
      "   Aggregating using keys: ['perplexity', 'avg_doc_perplexity', 'valid_test_docs', 'total_test_words', 'doc_match_rate', 'avg_path_length', 'log_likelihood', 'run_id', 'gamma', 'depth', 'alpha']\n",
      "❌ Error processing Eta=0.2: no results\n",
      "\n",
      "================================================================================\n",
      "Generating overall comparison file\n",
      "================================================================================\n",
      "✓ Overall comparison saved to: /Volumes/My Passport/收敛结果/step2/eta_perplexity_comparison.csv\n",
      "\n",
      "Cross-eta perplexity comparison:\n",
      "Eta      Avg Perplexity(±std)   Runs\n",
      "--------------------------------------------------\n",
      " 0.005    447.8291(±17.2239)           3\n",
      " 0.010    396.6156(±21.2111)           3\n",
      " 0.020    371.0158(±7.0313)           3\n",
      " 0.050    356.4995(±15.4625)           3\n",
      " 0.100    415.9169(±11.9286)           3\n",
      " 0.200    467.0780(±10.2098)           3\n",
      "\n",
      "================================================================================\n",
      "Starting perplexity trend analysis...\n",
      "================================================================================\n",
      "\n",
      "📈 Perplexity trend analysis:\n",
      "============================================================\n",
      "Correlation eta vs avg perplexity: 0.5509\n",
      "Correlation eta vs doc match rate: nan\n",
      "Correlation eta vs avg path length: nan\n",
      "\n",
      "🏆 Best performance:\n",
      "   Lowest average perplexity: 356.4995 (Eta=0.05)\n",
      "   Corresponding match rate: 100.0%\n",
      "   Runs: 3\n",
      "\n",
      "📊 Stability (coefficient of variation):\n",
      "   Eta 0.005: CV=0.0385\n",
      "   Eta 0.01: CV=0.0535\n",
      "   Eta 0.02: CV=0.0190\n",
      "   Eta 0.05: CV=0.0434\n",
      "   Eta 0.1: CV=0.0287\n",
      "   Eta 0.2: CV=0.0219\n",
      "================================================================================\n",
      "✅ Perplexity computation and aggregation completed!\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/v5/6mdkg5713kxgwg5xs24g8rvr0000gn/T/ipykernel_1460/147612263.py\", line 300, in aggregate_perplexity_by_eta_groups\n",
      "    eta_summary = group_data.agg(agg_dict).round(4)\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/frame.py\", line 9342, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 776, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 172, in agg\n",
      "    return self.agg_dict_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 504, in agg_dict_like\n",
      "    results = {\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 505, in <dictcomp>\n",
      "    key: obj._gotitem(key, ndim=1).agg(how) for key, how in arg.items()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/series.py\", line 4605, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 1126, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 175, in agg\n",
      "    return self.agg_list_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 438, in agg_list_like\n",
      "    raise ValueError(\"no results\")\n",
      "ValueError: no results\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/v5/6mdkg5713kxgwg5xs24g8rvr0000gn/T/ipykernel_1460/147612263.py\", line 300, in aggregate_perplexity_by_eta_groups\n",
      "    eta_summary = group_data.agg(agg_dict).round(4)\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/frame.py\", line 9342, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 776, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 172, in agg\n",
      "    return self.agg_dict_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 504, in agg_dict_like\n",
      "    results = {\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 505, in <dictcomp>\n",
      "    key: obj._gotitem(key, ndim=1).agg(how) for key, how in arg.items()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/series.py\", line 4605, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 1126, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 175, in agg\n",
      "    return self.agg_list_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 438, in agg_list_like\n",
      "    raise ValueError(\"no results\")\n",
      "ValueError: no results\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/v5/6mdkg5713kxgwg5xs24g8rvr0000gn/T/ipykernel_1460/147612263.py\", line 300, in aggregate_perplexity_by_eta_groups\n",
      "    eta_summary = group_data.agg(agg_dict).round(4)\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/frame.py\", line 9342, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 776, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 172, in agg\n",
      "    return self.agg_dict_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 504, in agg_dict_like\n",
      "    results = {\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 505, in <dictcomp>\n",
      "    key: obj._gotitem(key, ndim=1).agg(how) for key, how in arg.items()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/series.py\", line 4605, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 1126, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 175, in agg\n",
      "    return self.agg_list_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 438, in agg_list_like\n",
      "    raise ValueError(\"no results\")\n",
      "ValueError: no results\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/v5/6mdkg5713kxgwg5xs24g8rvr0000gn/T/ipykernel_1460/147612263.py\", line 300, in aggregate_perplexity_by_eta_groups\n",
      "    eta_summary = group_data.agg(agg_dict).round(4)\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/frame.py\", line 9342, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 776, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 172, in agg\n",
      "    return self.agg_dict_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 504, in agg_dict_like\n",
      "    results = {\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 505, in <dictcomp>\n",
      "    key: obj._gotitem(key, ndim=1).agg(how) for key, how in arg.items()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/series.py\", line 4605, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 1126, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 175, in agg\n",
      "    return self.agg_list_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 438, in agg_list_like\n",
      "    raise ValueError(\"no results\")\n",
      "ValueError: no results\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/v5/6mdkg5713kxgwg5xs24g8rvr0000gn/T/ipykernel_1460/147612263.py\", line 300, in aggregate_perplexity_by_eta_groups\n",
      "    eta_summary = group_data.agg(agg_dict).round(4)\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/frame.py\", line 9342, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 776, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 172, in agg\n",
      "    return self.agg_dict_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 504, in agg_dict_like\n",
      "    results = {\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 505, in <dictcomp>\n",
      "    key: obj._gotitem(key, ndim=1).agg(how) for key, how in arg.items()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/series.py\", line 4605, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 1126, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 175, in agg\n",
      "    return self.agg_list_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 438, in agg_list_like\n",
      "    raise ValueError(\"no results\")\n",
      "ValueError: no results\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/v5/6mdkg5713kxgwg5xs24g8rvr0000gn/T/ipykernel_1460/147612263.py\", line 300, in aggregate_perplexity_by_eta_groups\n",
      "    eta_summary = group_data.agg(agg_dict).round(4)\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/frame.py\", line 9342, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 776, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 172, in agg\n",
      "    return self.agg_dict_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 504, in agg_dict_like\n",
      "    results = {\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 505, in <dictcomp>\n",
      "    key: obj._gotitem(key, ndim=1).agg(how) for key, how in arg.items()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/series.py\", line 4605, in aggregate\n",
      "    result = op.agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 1126, in agg\n",
      "    result = super().agg()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 175, in agg\n",
      "    return self.agg_list_like()\n",
      "  File \"/Users/wenlinsuniverse/opt/anaconda3/envs/huggingface/lib/python3.8/site-packages/pandas/core/apply.py\", line 438, in agg_list_like\n",
      "    raise ValueError(\"no results\")\n",
      "ValueError: no results\n"
     ]
    }
   ],
   "source": [
    "# First run full perplexity computation (if not already run)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "def extract_eta_from_folder(folder_name, default=0.1):\n",
    "    \"\"\"\n",
    "    Robust eta extraction from folder name.\n",
    "    Supports patterns like: eta_0.1, eta0.1, eta-0.1, eta_1, etc.\n",
    "    Returns float or default on failure.\n",
    "    \"\"\"\n",
    "    if not isinstance(folder_name, str):\n",
    "        return default\n",
    "    \n",
    "    # Primary pattern: eta followed by separator and number\n",
    "    m = re.search(r'eta[_-]?([0-9]+(?:\\.[0-9]+)?)', folder_name, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        try:\n",
    "            return float(m.group(1))\n",
    "        except:\n",
    "            return default\n",
    "    \n",
    "    # Fallback pattern: looser match\n",
    "    m2 = re.search(r'eta\\s*[:=]?\\s*([0-9]+(?:\\.[0-9]+)?)', folder_name, flags=re.IGNORECASE)\n",
    "    if m2:\n",
    "        try:\n",
    "            return float(m2.group(1))\n",
    "        except:\n",
    "            return default\n",
    "    \n",
    "    return default\n",
    "\n",
    "def extract_params_from_folder(folder_name):\n",
    "    \"\"\"Extract all parameters from folder name using robust regex\"\"\"\n",
    "    params = {'eta': 0.1, 'gamma': 0.05, 'depth': 3, 'alpha': 0.1}\n",
    "    \n",
    "    for param in params.keys():\n",
    "        pattern = rf'{param}[_-]?([0-9]+(?:\\.[0-9]+)?)'\n",
    "        m = re.search(pattern, folder_name, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            try:\n",
    "                value = m.group(1)\n",
    "                if param == 'depth':\n",
    "                    params[param] = int(value)\n",
    "                else:\n",
    "                    params[param] = float(value)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Failed to extract parameter {param}: {e}\")\n",
    "    \n",
    "    return params['eta'], params['gamma'], params['depth'], params['alpha']\n",
    "\n",
    "def calculate_hlda_perplexity_with_path_mapping_complete(base_path=\".\", corpus=None, test_ratio=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Full version: hLDA perplexity calculation based on iteration_path_document_mapping.csv\n",
    "    \"\"\"\n",
    "    if corpus is None:\n",
    "        print(\"❌ corpus (original text data) must be provided\")\n",
    "        return\n",
    "\n",
    "    # Split train/test\n",
    "    doc_ids = list(corpus.keys())\n",
    "    train_ids, test_ids = train_test_split(doc_ids, test_size=test_ratio, random_state=random_state)\n",
    "\n",
    "    print(f\"📊 Dataset split:\")\n",
    "    print(f\"   Total documents: {len(doc_ids)}\")\n",
    "    print(f\"   Training set: {len(train_ids)} documents\")\n",
    "    print(f\"   Test set: {len(test_ids)} documents\")\n",
    "\n",
    "    pattern = os.path.join(base_path, \"**\", \"iteration_node_word_distributions.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "\n",
    "    print(f\"🔍 Found {len(files)} model result folders to process\")\n",
    "\n",
    "    for idx, file_path in enumerate(files, 1):\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "\n",
    "        # Use robust parameter extraction\n",
    "        eta, gamma, depth, alpha = extract_params_from_folder(folder_name)\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"[{idx}/{len(files)}] Computing perplexity for: {folder_name}\")\n",
    "        print(f\"Parameters - Eta: {eta}, Gamma: {gamma}, Depth: {depth}, Alpha: {alpha}\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        try:\n",
    "            # read node-word distributions\n",
    "            word_df = pd.read_csv(file_path)\n",
    "            word_df.columns = [col.strip(\"'\\\" \") for col in word_df.columns]\n",
    "\n",
    "            # read path-document mapping\n",
    "            path_mapping_file = os.path.join(folder_path, 'iteration_path_document_mapping.csv')\n",
    "            if not os.path.exists(path_mapping_file):\n",
    "                print(\"⚠️ Path-document mapping file not found, skipping this folder\")\n",
    "                continue\n",
    "\n",
    "            path_mapping_df = pd.read_csv(path_mapping_file)\n",
    "            path_mapping_df.columns = [col.strip(\"'\\\" \") for col in path_mapping_df.columns]\n",
    "\n",
    "            # select last iteration\n",
    "            max_iteration = word_df['iteration'].max()\n",
    "            last_word_data = word_df[word_df['iteration'] == max_iteration]\n",
    "            last_path_mapping_data = path_mapping_df[path_mapping_df['iteration'] == max_iteration]\n",
    "\n",
    "            print(f\"📈 Last iteration: {max_iteration}\")\n",
    "            print(f\"📈 Number of nodes: {last_word_data['node_id'].nunique()}\")\n",
    "            print(f\"📈 Path mappings: {len(last_path_mapping_data)}\")\n",
    "\n",
    "            # compute perplexity using the fixed function\n",
    "            perplexity_results = compute_perplexity_with_path_mapping_fixed(\n",
    "                last_word_data,\n",
    "                last_path_mapping_data,\n",
    "                corpus,\n",
    "                test_ids,\n",
    "                eta\n",
    "            )\n",
    "\n",
    "            if perplexity_results is not None:\n",
    "                # save perplexity results\n",
    "                perplexity_data = [{\n",
    "                    'eta': eta,\n",
    "                    'gamma': gamma,\n",
    "                    'depth': depth,\n",
    "                    'alpha': alpha,\n",
    "                    'iteration': max_iteration,\n",
    "                    'test_docs_count': len(test_ids),\n",
    "                    'valid_test_docs': perplexity_results['valid_docs'],\n",
    "                    'matched_docs': perplexity_results['matched_docs'],\n",
    "                    'total_test_words': perplexity_results['total_words'],\n",
    "                    'log_likelihood': perplexity_results['log_likelihood'],\n",
    "                    'perplexity': perplexity_results['perplexity'],\n",
    "                    'avg_doc_perplexity': perplexity_results['avg_doc_perplexity'],\n",
    "                    'doc_match_rate': perplexity_results['match_rate'],\n",
    "                    'avg_path_length': perplexity_results['avg_path_length']\n",
    "                }]\n",
    "\n",
    "                perplexity_df = pd.DataFrame(perplexity_data)\n",
    "                output_path = os.path.join(folder_path, 'perplexity_results_final.csv')\n",
    "                perplexity_df.to_csv(output_path, index=False)\n",
    "\n",
    "                print(f\"💾 Perplexity results saved to: {output_path}\")\n",
    "                print(f\"📊 Perplexity summary:\")\n",
    "                print(f\"   - Perplexity: {perplexity_results['perplexity']:.4f}\")\n",
    "                print(f\"   - Average doc perplexity: {perplexity_results['avg_doc_perplexity']:.4f}\")\n",
    "                print(f\"   - Document match rate: {perplexity_results['match_rate']:.1%}\")\n",
    "                print(f\"   - Average path length: {perplexity_results['avg_path_length']:.1f}\")\n",
    "                print(f\"   - Valid test docs: {perplexity_results['valid_docs']}/{len(test_ids)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"❌ Error processing {file_path}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    print(f\"\\n✅ Perplexity computation completed for all folders!\")\n",
    "\n",
    "def aggregate_perplexity_by_eta_groups(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    Aggregate average perplexity and related metrics across runs grouped by eta (fixed version).\n",
    "    \"\"\"\n",
    "    # find final perplexity result files\n",
    "    pattern = os.path.join(base_path, \"**\", \"perplexity_results_final.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "\n",
    "    # if no final files, try alternative patterns\n",
    "    if len(files) == 0:\n",
    "        patterns = [\n",
    "            \"perplexity_results_path_mapping.csv\",\n",
    "            \"perplexity_results_test.csv\",\n",
    "            \"perplexity_results.csv\"\n",
    "        ]\n",
    "        for pattern_name in patterns:\n",
    "            pattern = os.path.join(base_path, \"**\", pattern_name)\n",
    "            files = glob.glob(pattern, recursive=True)\n",
    "            if len(files) > 0:\n",
    "                print(f\"🔍 Using file pattern: {pattern_name}\")\n",
    "                break\n",
    "\n",
    "    print(f\"🔍 Found {len(files)} perplexity result files\")\n",
    "\n",
    "    if len(files) == 0:\n",
    "        print(\"❌ No perplexity result files found\")\n",
    "        return\n",
    "\n",
    "    all_data = []\n",
    "    eta_groups = {}\n",
    "\n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        parent_folder = os.path.dirname(folder_path)\n",
    "\n",
    "        # Use robust eta extraction\n",
    "        eta = extract_eta_from_folder(folder_name, default=None)\n",
    "        if eta is None:\n",
    "            print(f\"Warning: cannot extract eta from folder name {folder_name}\")\n",
    "            continue\n",
    "\n",
    "        # extract run id\n",
    "        run_match = folder_name.split('_run_')\n",
    "        if len(run_match) > 1:\n",
    "            run_id = run_match[1]\n",
    "        else:\n",
    "            print(f\"Warning: cannot extract run id from folder name {folder_name}\")\n",
    "            run_id = \"unknown\"\n",
    "\n",
    "        if eta not in eta_groups:\n",
    "            eta_groups[eta] = parent_folder\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"📖 Reading file: {folder_name} - {len(df)} rows\")\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                # ensure required field exists\n",
    "                if 'perplexity' not in row:\n",
    "                    print(f\"Warning: {file_path} missing perplexity column\")\n",
    "                    continue\n",
    "\n",
    "                all_data.append({\n",
    "                    'eta': eta,\n",
    "                    'run_id': run_id,\n",
    "                    'gamma': row.get('gamma', 0.05),\n",
    "                    'depth': row.get('depth', 3),\n",
    "                    'alpha': row.get('alpha', 0.1),\n",
    "                    'perplexity': row.get('perplexity', 0),\n",
    "                    'avg_doc_perplexity': row.get('avg_doc_perplexity', row.get('perplexity', 0)),\n",
    "                    'valid_test_docs': row.get('valid_test_docs', 0),\n",
    "                    'total_test_words': row.get('total_test_words', 0),\n",
    "                    'doc_match_rate': row.get('doc_match_rate', 0),\n",
    "                    'avg_path_length': row.get('avg_path_length', 0),\n",
    "                    'log_likelihood': row.get('log_likelihood', 0),\n",
    "                    'parent_folder': parent_folder\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "    # to DataFrame\n",
    "    summary_df = pd.DataFrame(all_data)\n",
    "\n",
    "    if summary_df.empty:\n",
    "        print(\"No valid data found\")\n",
    "        return\n",
    "\n",
    "    print(f\"📊 Data summary:\")\n",
    "    print(f\"   Total rows: {len(summary_df)}\")\n",
    "    print(f\"   Unique eta values: {sorted(summary_df['eta'].unique())}\")\n",
    "    print(f\"   Counts per eta: {summary_df['eta'].value_counts().sort_index().to_dict()}\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Perplexity aggregation by ETA\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # group by eta and save summaries\n",
    "    for eta, group_data in summary_df.groupby('eta'):\n",
    "        parent_folder = group_data['parent_folder'].iloc[0]\n",
    "\n",
    "        print(f\"\\nProcessing Eta={eta}\")\n",
    "        print(f\"Output directory: {parent_folder}\")\n",
    "        print(f\"Group size: {len(group_data)}\")\n",
    "\n",
    "        if len(group_data) == 0:\n",
    "            print(f\"Warning: no data for Eta={eta}, skip\")\n",
    "            continue\n",
    "\n",
    "        # safer aggregation dict construction\n",
    "        agg_dict = {}\n",
    "\n",
    "        numeric_cols = ['perplexity', 'avg_doc_perplexity', 'valid_test_docs',\n",
    "                       'total_test_words', 'doc_match_rate', 'avg_path_length', 'log_likelihood']\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            if col in group_data.columns:\n",
    "                valid_data = group_data[col].dropna()\n",
    "                if len(valid_data) > 0:\n",
    "                    if col in ['perplexity', 'avg_doc_perplexity', 'doc_match_rate', 'avg_path_length', 'log_likelihood']:\n",
    "                        agg_dict[col] = ['mean', 'std', 'min', 'max']\n",
    "                    else:\n",
    "                        agg_dict[col] = ['mean', 'std']\n",
    "                else:\n",
    "                    print(f\"   Warning: column {col} has no valid data\")\n",
    "\n",
    "        if 'run_id' in group_data.columns:\n",
    "            agg_dict['run_id'] = 'count'\n",
    "\n",
    "        for col in ['gamma', 'depth', 'alpha']:\n",
    "            if col in group_data.columns:\n",
    "                agg_dict[col] = 'first'\n",
    "\n",
    "        if not agg_dict:\n",
    "            print(f\"Warning: no aggregatable columns for Eta={eta}, skip\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(f\"   Aggregating using keys: {list(agg_dict.keys())}\")\n",
    "            eta_summary = group_data.agg(agg_dict).round(4)\n",
    "\n",
    "            # flatten multiindex columns\n",
    "            eta_summary.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in eta_summary.columns]\n",
    "            eta_summary = eta_summary.reset_index()\n",
    "            eta_summary.insert(0, 'eta', eta)\n",
    "\n",
    "            run_ids = ', '.join(sorted(group_data['run_id'].unique()))\n",
    "            eta_summary['run_ids'] = run_ids\n",
    "\n",
    "            output_filename = f'eta_{eta}_perplexity_summary.csv'\n",
    "            output_path = os.path.join(parent_folder, output_filename)\n",
    "            eta_summary.to_csv(output_path, index=False)\n",
    "\n",
    "            print(f\"  ✓ Saved summary: {output_path}\")\n",
    "\n",
    "            if 'run_id_count' in eta_summary.columns:\n",
    "                print(f\"  Runs: {int(eta_summary['run_id_count'].iloc[0])}\")\n",
    "\n",
    "            if 'perplexity_mean' in eta_summary.columns:\n",
    "                mean_perp = eta_summary['perplexity_mean'].iloc[0]\n",
    "                std_perp = eta_summary.get('perplexity_std', pd.Series([0])).iloc[0]\n",
    "                print(f\"  Average perplexity: {mean_perp:.4f} (±{std_perp:.4f})\")\n",
    "\n",
    "            print(f\"  Included runs: {run_ids}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing Eta={eta}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "    # overall comparison file at base_path\n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(\"Generating overall comparison file\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    try:\n",
    "        overall_agg_dict = {}\n",
    "\n",
    "        for col in ['perplexity', 'avg_doc_perplexity', 'doc_match_rate', 'avg_path_length',\n",
    "                   'valid_test_docs', 'total_test_words', 'log_likelihood']:\n",
    "            if col in summary_df.columns:\n",
    "                valid_data = summary_df[col].dropna()\n",
    "                if len(valid_data) > 0:\n",
    "                    overall_agg_dict[col] = ['mean', 'std']\n",
    "                    if col in ['perplexity', 'avg_doc_perplexity']:\n",
    "                        overall_agg_dict[col].extend(['min', 'max'])\n",
    "\n",
    "        if 'run_id' in summary_df.columns:\n",
    "            overall_agg_dict['run_id'] = 'count'\n",
    "\n",
    "        if not overall_agg_dict:\n",
    "            print(\"Warning: no columns available for overall aggregation\")\n",
    "            return None\n",
    "\n",
    "        overall_summary = summary_df.groupby('eta').agg(overall_agg_dict).round(4)\n",
    "\n",
    "        overall_summary.columns = ['_'.join(col).strip() for col in overall_summary.columns]\n",
    "        overall_summary = overall_summary.reset_index()\n",
    "\n",
    "        overall_output_path = os.path.join(base_path, 'eta_perplexity_comparison.csv')\n",
    "        overall_summary.to_csv(overall_output_path, index=False)\n",
    "        print(f\"✓ Overall comparison saved to: {overall_output_path}\")\n",
    "\n",
    "        print(f\"\\nCross-eta perplexity comparison:\")\n",
    "        print(\"Eta      Avg Perplexity(±std)   Runs\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        for _, row in overall_summary.iterrows():\n",
    "            eta = row['eta']\n",
    "            run_count = int(row.get('run_id_count', 0))\n",
    "\n",
    "            if 'perplexity_mean' in row:\n",
    "                mean_perp = row['perplexity_mean']\n",
    "                std_perp = row.get('perplexity_std', 0)\n",
    "                print(f\"{eta:6.3f}    {mean_perp:8.4f}(±{std_perp:6.4f})        {run_count:4d}\")\n",
    "            else:\n",
    "                print(f\"{eta:6.3f}    data missing                    {run_count:4d}\")\n",
    "\n",
    "        return overall_summary\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating overall comparison: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def analyze_perplexity_trends(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    Analyze perplexity trends\n",
    "    \"\"\"\n",
    "    comparison_file = os.path.join(base_path, 'eta_perplexity_comparison.csv')\n",
    "\n",
    "    if os.path.exists(comparison_file):\n",
    "        df = pd.read_csv(comparison_file)\n",
    "\n",
    "        print(f\"\\n📈 Perplexity trend analysis:\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        eta_perp_corr = df['eta'].corr(df['perplexity_mean'])\n",
    "        eta_match_corr = df['eta'].corr(df['doc_match_rate_mean'])\n",
    "        eta_path_corr = df['eta'].corr(df['avg_path_length_mean'])\n",
    "\n",
    "        print(f\"Correlation eta vs avg perplexity: {eta_perp_corr:.4f}\")\n",
    "        print(f\"Correlation eta vs doc match rate: {eta_match_corr:.4f}\")\n",
    "        print(f\"Correlation eta vs avg path length: {eta_path_corr:.4f}\")\n",
    "\n",
    "        best_eta_idx = df['perplexity_mean'].idxmin()\n",
    "        best_eta = df.loc[best_eta_idx, 'eta']\n",
    "        best_perplexity = df.loc[best_eta_idx, 'perplexity_mean']\n",
    "\n",
    "        print(f\"\\n🏆 Best performance:\")\n",
    "        print(f\"   Lowest average perplexity: {best_perplexity:.4f} (Eta={best_eta})\")\n",
    "        print(f\"   Corresponding match rate: {df.loc[best_eta_idx, 'doc_match_rate_mean']:.1%}\")\n",
    "        print(f\"   Runs: {int(df.loc[best_eta_idx, 'run_id_count'])}\")\n",
    "\n",
    "        print(f\"\\n📊 Stability (coefficient of variation):\")\n",
    "        for _, row in df.iterrows():\n",
    "            eta = row['eta']\n",
    "            cv = row['perplexity_std'] / row['perplexity_mean'] if row['perplexity_mean'] > 0 else 0\n",
    "            print(f\"   Eta {eta}: CV={cv:.4f}\")\n",
    "\n",
    "    else:\n",
    "        print(\"⚠️ Overall comparison file not found. Run aggregation first.\")\n",
    "\n",
    "# Execute full perplexity calculation and aggregation\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step2\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Starting full perplexity computation...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Compute perplexity (if not done)\n",
    "calculate_hlda_perplexity_with_path_mapping_complete(base_path, corpus, test_ratio=0.2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Starting aggregation of perplexity by eta...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 2. Aggregate by eta\n",
    "overall_summary = aggregate_perplexity_by_eta_groups(base_path) \n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Starting perplexity trend analysis...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 3. Trend analysis\n",
    "analyze_perplexity_trends(base_path)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"✅ Perplexity computation and aggregation completed!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad4e823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Starting calculation of branching factor and Gini coefficient metrics...\n",
      "================================================================================\n",
      "🔍 Found 18 entropy files to process\n",
      "\n",
      "[1/18] Processing folder: depth_3_gamma_0.05_eta_0.1_run_2\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_2/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary:\n",
      "   Layer 0 (1 nodes): Branching=44.00, Doc Gini=0.0000, Branch Gini=0.0000\n",
      "   Layer 1 (44 nodes): Branching=4.23, Doc Gini=0.6345, Branch Gini=0.3915\n",
      "   Layer 2 (186 nodes): Branching=0.00, Doc Gini=0.4864, Branch Gini=0.0000\n",
      "\n",
      "[2/18] Processing folder: depth_3_gamma_0.05_eta_0.1_run_3\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_3/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary:\n",
      "   Layer 0 (1 nodes): Branching=41.00, Doc Gini=0.0000, Branch Gini=0.0000\n",
      "   Layer 1 (41 nodes): Branching=4.22, Doc Gini=0.6697, Branch Gini=0.4892\n",
      "   Layer 2 (173 nodes): Branching=0.00, Doc Gini=0.4607, Branch Gini=0.0000\n",
      "\n",
      "[3/18] Processing folder: depth_3_gamma_0.05_eta_0.1_run_1\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/depth_3_gamma_0.05_eta_0.1_run_1/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary:\n",
      "   Layer 0 (1 nodes): Branching=40.00, Doc Gini=0.0000, Branch Gini=0.0000\n",
      "   Layer 1 (40 nodes): Branching=4.10, Doc Gini=0.6087, Branch Gini=0.4168\n",
      "   Layer 2 (164 nodes): Branching=0.00, Doc Gini=0.4949, Branch Gini=0.0000\n",
      "\n",
      "[4/18] Processing folder: depth_3_gamma_0.05_eta_0.05_run_3\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_3/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary:\n",
      "   Layer 0 (1 nodes): Branching=59.00, Doc Gini=0.0000, Branch Gini=0.0000\n",
      "   Layer 2 (262 nodes): Branching=0.00, Doc Gini=0.3689, Branch Gini=0.0000\n",
      "   Layer 1 (59 nodes): Branching=4.44, Doc Gini=0.5429, Branch Gini=0.4466\n",
      "\n",
      "[5/18] Processing folder: depth_3_gamma_0.05_eta_0.05_run_1\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_1/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary:\n",
      "   Layer 0 (1 nodes): Branching=62.00, Doc Gini=0.0000, Branch Gini=0.0000\n",
      "   Layer 1 (62 nodes): Branching=4.06, Doc Gini=0.5204, Branch Gini=0.4242\n",
      "   Layer 2 (252 nodes): Branching=0.00, Doc Gini=0.3778, Branch Gini=0.0000\n",
      "\n",
      "[6/18] Processing folder: depth_3_gamma_0.05_eta_0.05_run_2\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/depth_3_gamma_0.05_eta_0.05_run_2/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary:\n",
      "   Layer 0 (1 nodes): Branching=55.00, Doc Gini=0.0000, Branch Gini=0.0000\n",
      "   Layer 1 (55 nodes): Branching=4.42, Doc Gini=0.5330, Branch Gini=0.4275\n",
      "   Layer 2 (243 nodes): Branching=0.00, Doc Gini=0.4104, Branch Gini=0.0000\n",
      "\n",
      "[7/18] Processing folder: depth_3_gamma_0.05_eta_0.005_run_3\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_3/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary:\n",
      "   Layer 0 (1 nodes): Branching=90.00, Doc Gini=0.0000, Branch Gini=0.0000\n",
      "   Layer 1 (90 nodes): Branching=3.73, Doc Gini=0.6512, Branch Gini=0.4900\n",
      "   Layer 2 (336 nodes): Branching=0.00, Doc Gini=0.4452, Branch Gini=0.0000\n",
      "\n",
      "[8/18] Processing folder: depth_3_gamma_0.05_eta_0.005_run_1\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_1/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary:\n",
      "   Layer 0 (1 nodes): Branching=85.00, Doc Gini=0.0000, Branch Gini=0.0000\n",
      "   Layer 1 (85 nodes): Branching=4.14, Doc Gini=0.5767, Branch Gini=0.4263\n",
      "   Layer 2 (352 nodes): Branching=0.00, Doc Gini=0.4305, Branch Gini=0.0000\n",
      "\n",
      "[9/18] Processing folder: depth_3_gamma_0.05_eta_0.005_run_2\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/depth_3_gamma_0.05_eta_0.005_run_2/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary:\n",
      "   Layer 0 (1 nodes): Branching=80.00, Doc Gini=0.0000, Branch Gini=0.0000\n",
      "   Layer 1 (80 nodes): Branching=4.39, Doc Gini=0.5878, Branch Gini=0.4009\n",
      "   Layer 2 (351 nodes): Branching=0.00, Doc Gini=0.4696, Branch Gini=0.0000\n",
      "\n",
      "[10/18] Processing folder: depth_3_gamma_0.05_eta_0.02_run_3\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_3/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary:\n",
      "   Layer 0 (1 nodes): Branching=74.00, Doc Gini=0.0000, Branch Gini=0.0000\n",
      "   Layer 1 (74 nodes): Branching=4.23, Doc Gini=0.5992, Branch Gini=0.4617\n",
      "   Layer 2 (313 nodes): Branching=0.00, Doc Gini=0.4004, Branch Gini=0.0000\n",
      "\n",
      "[11/18] Processing folder: depth_3_gamma_0.05_eta_0.02_run_2\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_2/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary:\n",
      "   Layer 0 (1 nodes): Branching=72.00, Doc Gini=0.0000, Branch Gini=0.0000\n",
      "   Layer 1 (72 nodes): Branching=4.32, Doc Gini=0.5466, Branch Gini=0.4012\n",
      "   Layer 2 (311 nodes): Branching=0.00, Doc Gini=0.4395, Branch Gini=0.0000\n",
      "\n",
      "[12/18] Processing folder: depth_3_gamma_0.05_eta_0.02_run_1\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/depth_3_gamma_0.05_eta_0.02_run_1/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary:\n",
      "   Layer 0 (1 nodes): Branching=74.00, Doc Gini=0.0000, Branch Gini=0.0000\n",
      "   Layer 1 (74 nodes): Branching=4.16, Doc Gini=0.5690, Branch Gini=0.4226\n",
      "   Layer 2 (308 nodes): Branching=0.00, Doc Gini=0.4247, Branch Gini=0.0000\n",
      "\n",
      "[13/18] Processing folder: depth_3_gamma_0.05_eta_0.2_run_1\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_1/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary:\n",
      "   Layer 0 (1 nodes): Branching=28.00, Doc Gini=0.0000, Branch Gini=0.0000\n",
      "   Layer 1 (28 nodes): Branching=4.36, Doc Gini=0.6294, Branch Gini=0.4028\n",
      "   Layer 2 (122 nodes): Branching=0.00, Doc Gini=0.5425, Branch Gini=0.0000\n",
      "\n",
      "[14/18] Processing folder: depth_3_gamma_0.05_eta_0.2_run_3\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_3/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary:\n",
      "   Layer 0 (1 nodes): Branching=29.00, Doc Gini=0.0000, Branch Gini=0.0000\n",
      "   Layer 1 (29 nodes): Branching=4.38, Doc Gini=0.6420, Branch Gini=0.4073\n",
      "   Layer 2 (127 nodes): Branching=0.00, Doc Gini=0.5238, Branch Gini=0.0000\n",
      "\n",
      "[15/18] Processing folder: depth_3_gamma_0.05_eta_0.2_run_2\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/depth_3_gamma_0.05_eta_0.2_run_2/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary:\n",
      "   Layer 0 (1 nodes): Branching=31.00, Doc Gini=0.0000, Branch Gini=0.0000\n",
      "   Layer 1 (31 nodes): Branching=4.03, Doc Gini=0.6688, Branch Gini=0.4150\n",
      "   Layer 2 (125 nodes): Branching=0.00, Doc Gini=0.5554, Branch Gini=0.0000\n",
      "\n",
      "[16/18] Processing folder: depth_3_gamma_0.05_eta_0.01_run_2\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_2/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary:\n",
      "   Layer 0 (1 nodes): Branching=76.00, Doc Gini=0.0000, Branch Gini=0.0000\n",
      "   Layer 1 (76 nodes): Branching=4.29, Doc Gini=0.5609, Branch Gini=0.3717\n",
      "   Layer 2 (326 nodes): Branching=0.00, Doc Gini=0.4440, Branch Gini=0.0000\n",
      "\n",
      "[17/18] Processing folder: depth_3_gamma_0.05_eta_0.01_run_1\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_1/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary:\n",
      "   Layer 0 (1 nodes): Branching=80.00, Doc Gini=0.0000, Branch Gini=0.0000\n",
      "   Layer 1 (80 nodes): Branching=4.30, Doc Gini=0.5589, Branch Gini=0.4483\n",
      "   Layer 2 (344 nodes): Branching=0.00, Doc Gini=0.4102, Branch Gini=0.0000\n",
      "\n",
      "[18/18] Processing folder: depth_3_gamma_0.05_eta_0.01_run_3\n",
      "✓ Layer metrics saved to: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/depth_3_gamma_0.05_eta_0.01_run_3/layer_branching_gini_metrics.csv\n",
      "📊 Layer metrics summary:\n",
      "   Layer 0 (1 nodes): Branching=81.00, Doc Gini=0.0000, Branch Gini=0.0000\n",
      "   Layer 1 (81 nodes): Branching=4.33, Doc Gini=0.5794, Branch Gini=0.4351\n",
      "   Layer 2 (351 nodes): Branching=0.00, Doc Gini=0.4156, Branch Gini=0.0000\n",
      "\n",
      "================================================================================\n",
      "Starting aggregation of branching factor and Gini coefficient statistics by eta value...\n",
      "================================================================================\n",
      "================================================================================\n",
      "Aggregating layer-level branching factor and Gini coefficient metrics...\n",
      "================================================================================\n",
      "🔍 Found 18 layer metric files\n",
      "Layer-level branching factor and Gini coefficient summary statistics by ETA value\n",
      "================================================================================\n",
      "\n",
      "Processing Eta=0.005\n",
      "  Saved layer summary file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e0005_基于e01_收敛/eta_0.005_layer_branching_gini_summary.csv\n",
      "  Number of layers: 3\n",
      "    Layer 0: Branching=85.00, Doc Gini=0.0000, Branch Gini=0.0000, runs=3\n",
      "    Layer 1: Branching=4.09, Doc Gini=0.6052, Branch Gini=0.4391, runs=3\n",
      "    Layer 2: Branching=0.00, Doc Gini=0.4484, Branch Gini=0.0000, runs=3\n",
      "\n",
      "Processing Eta=0.01\n",
      "  Saved layer summary file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e001_基于e01_收敛/eta_0.01_layer_branching_gini_summary.csv\n",
      "  Number of layers: 3\n",
      "    Layer 0: Branching=79.00, Doc Gini=0.0000, Branch Gini=0.0000, runs=3\n",
      "    Layer 1: Branching=4.31, Doc Gini=0.5664, Branch Gini=0.4184, runs=3\n",
      "    Layer 2: Branching=0.00, Doc Gini=0.4233, Branch Gini=0.0000, runs=3\n",
      "\n",
      "Processing Eta=0.02\n",
      "  Saved layer summary file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e002_基于e01_收敛/eta_0.02_layer_branching_gini_summary.csv\n",
      "  Number of layers: 3\n",
      "    Layer 0: Branching=73.33, Doc Gini=0.0000, Branch Gini=0.0000, runs=3\n",
      "    Layer 1: Branching=4.24, Doc Gini=0.5716, Branch Gini=0.4285, runs=3\n",
      "    Layer 2: Branching=0.00, Doc Gini=0.4216, Branch Gini=0.0000, runs=3\n",
      "\n",
      "Processing Eta=0.05\n",
      "  Saved layer summary file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e005_基于e01_第二次_收敛/eta_0.05_layer_branching_gini_summary.csv\n",
      "  Number of layers: 3\n",
      "    Layer 0: Branching=58.67, Doc Gini=0.0000, Branch Gini=0.0000, runs=3\n",
      "    Layer 1: Branching=4.31, Doc Gini=0.5321, Branch Gini=0.4328, runs=3\n",
      "    Layer 2: Branching=0.00, Doc Gini=0.3857, Branch Gini=0.0000, runs=3\n",
      "\n",
      "Processing Eta=0.1\n",
      "  Saved layer summary file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e01_收敛/eta_0.1_layer_branching_gini_summary.csv\n",
      "  Number of layers: 3\n",
      "    Layer 0: Branching=41.67, Doc Gini=0.0000, Branch Gini=0.0000, runs=3\n",
      "    Layer 1: Branching=4.18, Doc Gini=0.6376, Branch Gini=0.4325, runs=3\n",
      "    Layer 2: Branching=0.00, Doc Gini=0.4807, Branch Gini=0.0000, runs=3\n",
      "\n",
      "Processing Eta=0.2\n",
      "  Saved layer summary file: /Volumes/My Passport/收敛结果/step2/step2_d3_g005_e02_收敛/eta_0.2_layer_branching_gini_summary.csv\n",
      "  Number of layers: 3\n",
      "    Layer 0: Branching=29.33, Doc Gini=0.0000, Branch Gini=0.0000, runs=3\n",
      "    Layer 1: Branching=4.26, Doc Gini=0.6467, Branch Gini=0.4084, runs=3\n",
      "    Layer 2: Branching=0.00, Doc Gini=0.5406, Branch Gini=0.0000, runs=3\n",
      "\n",
      "Overall layer comparison file saved to: /Volumes/My Passport/收敛结果/step2/eta_layer_branching_gini_comparison.csv\n",
      "\n",
      "================================================================================\n",
      "Displaying branching factor and Gini coefficient summary report...\n",
      "================================================================================\n",
      "====================================================================================================\n",
      "Branching Factor and Gini Coefficient Analysis Summary Report\n",
      "====================================================================================================\n",
      "\n",
      "📊 Layer-level branching factor and Gini coefficient analysis:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Layer 0 Cross-Eta comparison:\n",
      "Eta Value  Avg Branching(±std)  Doc Gini(±std)     Branch Gini(±std)     Runs\n",
      "---------------------------------------------------------------------------\n",
      " 0.005     85.00(±5.00)     0.0000(±0.0000)     0.0000(±0.0000)        3\n",
      " 0.010     79.00(±2.65)     0.0000(±0.0000)     0.0000(±0.0000)        3\n",
      " 0.020     73.33(±1.15)     0.0000(±0.0000)     0.0000(±0.0000)        3\n",
      " 0.050     58.67(±3.51)     0.0000(±0.0000)     0.0000(±0.0000)        3\n",
      " 0.100     41.67(±2.08)     0.0000(±0.0000)     0.0000(±0.0000)        3\n",
      " 0.200     29.33(±1.53)     0.0000(±0.0000)     0.0000(±0.0000)        3\n",
      "\n",
      "Layer 1 Cross-Eta comparison:\n",
      "Eta Value  Avg Branching(±std)  Doc Gini(±std)     Branch Gini(±std)     Runs\n",
      "---------------------------------------------------------------------------\n",
      " 0.005      4.09(±0.33)     0.6052(±0.0402)     0.4391(±0.0459)        3\n",
      " 0.010      4.31(±0.02)     0.5664(±0.0113)     0.4184(±0.0409)        3\n",
      " 0.020      4.24(±0.08)     0.5716(±0.0264)     0.4285(±0.0307)        3\n",
      " 0.050      4.31(±0.21)     0.5321(±0.0113)     0.4328(±0.0121)        3\n",
      " 0.100      4.18(±0.07)     0.6376(±0.0306)     0.4325(±0.0507)        3\n",
      " 0.200      4.26(±0.19)     0.6467(±0.0201)     0.4084(±0.0061)        3\n",
      "\n",
      "Layer 2 Cross-Eta comparison:\n",
      "Eta Value  Avg Branching(±std)  Doc Gini(±std)     Branch Gini(±std)     Runs\n",
      "---------------------------------------------------------------------------\n",
      " 0.005      0.00(±0.00)     0.4484(±0.0198)     0.0000(±0.0000)        3\n",
      " 0.010      0.00(±0.00)     0.4233(±0.0182)     0.0000(±0.0000)        3\n",
      " 0.020      0.00(±0.00)     0.4216(±0.0197)     0.0000(±0.0000)        3\n",
      " 0.050      0.00(±0.00)     0.3857(±0.0219)     0.0000(±0.0000)        3\n",
      " 0.100      0.00(±0.00)     0.4807(±0.0178)     0.0000(±0.0000)        3\n",
      " 0.200      0.00(±0.00)     0.5406(±0.0159)     0.0000(±0.0000)        3\n",
      "\n",
      "====================================================================================================\n",
      "✅ Branching factor and Gini coefficient analysis completed!\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def calculate_branching_and_gini_metrics(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    Calculate branching factor and Gini coefficient metrics for each model\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(base_path, \"**\", \"corrected_renyi_entropy.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    print(f\"🔍 Found {len(files)} entropy files to process\")\n",
    "    \n",
    "    for idx, file_path in enumerate(files, 1):\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        \n",
    "        print(f\"\\n[{idx}/{len(files)}] Processing folder: {folder_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Read entropy file\n",
    "            entropy_df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Check if required columns exist\n",
    "            required_cols = ['node_id', 'layer', 'document_count', 'child_count']\n",
    "            missing_cols = [col for col in required_cols if col not in entropy_df.columns]\n",
    "            \n",
    "            if missing_cols:\n",
    "                print(f\"⚠️ Missing required columns: {missing_cols}, skipping this file\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate layer-level branching factor and Gini coefficient metrics\n",
    "            layer_metrics = []\n",
    "            \n",
    "            for layer in entropy_df['layer'].unique():\n",
    "                if layer == -1:  # Skip invalid layers\n",
    "                    continue\n",
    "                    \n",
    "                layer_nodes = entropy_df[entropy_df['layer'] == layer]\n",
    "                \n",
    "                # Basic statistics\n",
    "                node_count = len(layer_nodes)\n",
    "                total_documents = layer_nodes['document_count'].sum()\n",
    "                \n",
    "                # Branching statistics\n",
    "                child_counts = layer_nodes['child_count'].values\n",
    "                total_branches = child_counts.sum()\n",
    "                \n",
    "                # Non-leaf node statistics\n",
    "                non_leaf_nodes = (child_counts > 0).sum()\n",
    "                non_leaf_counts = child_counts[child_counts > 0]\n",
    "                \n",
    "                # Branching factor statistics\n",
    "                if len(non_leaf_counts) > 0:\n",
    "                    avg_branching_factor = non_leaf_counts.mean()\n",
    "                    std_branching_factor = non_leaf_counts.std()\n",
    "                    non_leaf_avg_branching = non_leaf_counts.mean()\n",
    "                else:\n",
    "                    avg_branching_factor = 0.0\n",
    "                    std_branching_factor = 0.0\n",
    "                    non_leaf_avg_branching = 0.0\n",
    "                \n",
    "                # Gini coefficient calculation\n",
    "                def gini_coefficient(values):\n",
    "                    \"\"\"Calculate Gini coefficient\"\"\"\n",
    "                    if len(values) == 0:\n",
    "                        return 0.0\n",
    "                    values = np.array(values)\n",
    "                    values = values[values > 0]  # Only consider positive values\n",
    "                    if len(values) <= 1:\n",
    "                        return 0.0\n",
    "                    \n",
    "                    values = np.sort(values)\n",
    "                    n = len(values)\n",
    "                    cumsum = np.cumsum(values)\n",
    "                    return (n + 1 - 2 * np.sum(cumsum) / cumsum[-1]) / n\n",
    "                \n",
    "                # Document distribution Gini coefficient\n",
    "                doc_counts = layer_nodes['document_count'].values\n",
    "                gini_doc_distribution = gini_coefficient(doc_counts)\n",
    "                \n",
    "                # Branch distribution Gini coefficient\n",
    "                gini_branch_distribution = gini_coefficient(child_counts)\n",
    "                \n",
    "                layer_metrics.append({\n",
    "                    'layer': layer,\n",
    "                    'node_count': node_count,\n",
    "                    'total_branches': total_branches,\n",
    "                    'avg_branching_factor': avg_branching_factor,\n",
    "                    'std_branching_factor': std_branching_factor,\n",
    "                    'non_leaf_nodes': non_leaf_nodes,\n",
    "                    'non_leaf_avg_branching': non_leaf_avg_branching,\n",
    "                    'total_documents': total_documents,\n",
    "                    'gini_doc_distribution': gini_doc_distribution,\n",
    "                    'gini_branch_distribution': gini_branch_distribution\n",
    "                })\n",
    "            \n",
    "            # Save layer-level metrics\n",
    "            if layer_metrics:\n",
    "                layer_df = pd.DataFrame(layer_metrics)\n",
    "                layer_output_path = os.path.join(folder_path, 'layer_branching_gini_metrics.csv')\n",
    "                layer_df.to_csv(layer_output_path, index=False)\n",
    "                print(f\"✓ Layer metrics saved to: {layer_output_path}\")\n",
    "                \n",
    "                # Display brief statistics\n",
    "                print(f\"📊 Layer metrics summary:\")\n",
    "                for _, row in layer_df.iterrows():\n",
    "                    layer_num = int(row['layer'])\n",
    "                    node_count = int(row['node_count'])\n",
    "                    avg_branch = row['avg_branching_factor']\n",
    "                    doc_gini = row['gini_doc_distribution']\n",
    "                    branch_gini = row['gini_branch_distribution']\n",
    "                    print(f\"   Layer {layer_num} ({node_count} nodes): Branching={avg_branch:.2f}, Doc Gini={doc_gini:.4f}, Branch Gini={branch_gini:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"❌ Error processing file {file_path}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "def aggregate_branching_gini_by_eta(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    Aggregate branching factor and Gini coefficient statistics by eta value (layer-level only)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"Aggregating layer-level branching factor and Gini coefficient metrics...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    pattern = os.path.join(base_path, \"**\", \"layer_branching_gini_metrics.csv\")\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    print(f\"🔍 Found {len(files)} layer metric files\")\n",
    "    \n",
    "    all_layer_data = []\n",
    "    eta_groups = {}\n",
    "    \n",
    "    for file_path in files:\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        parent_folder = os.path.dirname(folder_path)\n",
    "        \n",
    "        # Extract eta value\n",
    "        eta = None\n",
    "        if 'eta_' in folder_name:\n",
    "            try:\n",
    "                eta_part = folder_name.split('eta_')[1].split('_')[0]\n",
    "                eta = float(eta_part)\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Extract run number\n",
    "        run_match = folder_name.split('_run_')\n",
    "        if len(run_match) > 1:\n",
    "            run_id = run_match[1]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if eta not in eta_groups:\n",
    "            eta_groups[eta] = parent_folder\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                all_layer_data.append({\n",
    "                    'eta': eta,\n",
    "                    'run_id': run_id,\n",
    "                    'layer': row['layer'],\n",
    "                    'node_count': row['node_count'],\n",
    "                    'total_branches': row['total_branches'],\n",
    "                    'avg_branching_factor': row['avg_branching_factor'],\n",
    "                    'std_branching_factor': row['std_branching_factor'],\n",
    "                    'non_leaf_nodes': row['non_leaf_nodes'],\n",
    "                    'non_leaf_avg_branching': row['non_leaf_avg_branching'],\n",
    "                    'total_documents': row['total_documents'],\n",
    "                    'gini_doc_distribution': row['gini_doc_distribution'],\n",
    "                    'gini_branch_distribution': row['gini_branch_distribution'],\n",
    "                    'parent_folder': parent_folder\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_path}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame and aggregate by eta\n",
    "    if all_layer_data:\n",
    "        layer_summary_df = pd.DataFrame(all_layer_data)\n",
    "        \n",
    "        print(\"Layer-level branching factor and Gini coefficient summary statistics by ETA value\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Generate layer summary files by eta\n",
    "        for eta, group_data in layer_summary_df.groupby('eta'):\n",
    "            parent_folder = group_data['parent_folder'].iloc[0]\n",
    "            \n",
    "            print(f\"\\nProcessing Eta={eta}\")\n",
    "            \n",
    "            layer_summary = group_data.groupby('layer').agg({\n",
    "                'node_count': ['mean', 'std'],\n",
    "                'total_branches': ['mean', 'std'],\n",
    "                'avg_branching_factor': ['mean', 'std'],\n",
    "                'std_branching_factor': ['mean', 'std'],\n",
    "                'non_leaf_nodes': ['mean', 'std'],\n",
    "                'non_leaf_avg_branching': ['mean', 'std'],\n",
    "                'total_documents': ['mean', 'std'],\n",
    "                'gini_doc_distribution': ['mean', 'std'],\n",
    "                'gini_branch_distribution': ['mean', 'std'],\n",
    "                'run_id': 'count'\n",
    "            }).round(4)\n",
    "            \n",
    "            # Flatten column names\n",
    "            layer_summary.columns = ['_'.join(col).strip() for col in layer_summary.columns]\n",
    "            layer_summary = layer_summary.reset_index()\n",
    "            layer_summary.insert(0, 'eta', eta)\n",
    "            \n",
    "            # Save aggregated results\n",
    "            output_filename = f'eta_{eta}_layer_branching_gini_summary.csv'\n",
    "            output_path = os.path.join(parent_folder, output_filename)\n",
    "            layer_summary.to_csv(output_path, index=False)\n",
    "            \n",
    "            print(f\"  Saved layer summary file: {output_path}\")\n",
    "            print(f\"  Number of layers: {len(layer_summary)}\")\n",
    "            \n",
    "            # Find correct count column name\n",
    "            count_col = None\n",
    "            for col in layer_summary.columns:\n",
    "                if 'run_id' in col and ('count' in col or col.endswith('_count')):\n",
    "                    count_col = col\n",
    "                    break\n",
    "            \n",
    "            # Display brief statistics\n",
    "            for _, row in layer_summary.iterrows():\n",
    "                layer_num = int(row['layer'])\n",
    "                avg_branch = row['avg_branching_factor_mean']\n",
    "                doc_gini = row['gini_doc_distribution_mean']\n",
    "                branch_gini = row['gini_branch_distribution_mean']\n",
    "                run_count = int(row[count_col]) if count_col else 0\n",
    "                \n",
    "                print(f\"    Layer {layer_num}: Branching={avg_branch:.2f}, Doc Gini={doc_gini:.4f}, Branch Gini={branch_gini:.4f}, runs={run_count}\")\n",
    "        \n",
    "        # Generate overall layer comparison file\n",
    "        overall_layer_summary = layer_summary_df.groupby(['eta', 'layer']).agg({\n",
    "            'avg_branching_factor': ['mean', 'std'],\n",
    "            'gini_doc_distribution': ['mean', 'std'],\n",
    "            'gini_branch_distribution': ['mean', 'std'],\n",
    "            'node_count': ['mean', 'std'],\n",
    "            'run_id': 'count'\n",
    "        }).round(4)\n",
    "        \n",
    "        overall_layer_summary.columns = ['_'.join(col).strip() for col in overall_layer_summary.columns]\n",
    "        overall_layer_summary = overall_layer_summary.reset_index()\n",
    "        \n",
    "        overall_layer_output_path = os.path.join(base_path, 'eta_layer_branching_gini_comparison.csv')\n",
    "        overall_layer_summary.to_csv(overall_layer_output_path, index=False)\n",
    "        print(f\"\\nOverall layer comparison file saved to: {overall_layer_output_path}\")\n",
    "\n",
    "def display_branching_gini_summary(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    Display summary report of branching factor and Gini coefficient analysis\n",
    "    \"\"\"\n",
    "    print(\"=\" * 100)\n",
    "    print(\"Branching Factor and Gini Coefficient Analysis Summary Report\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Read overall comparison file\n",
    "    layer_comparison_file = os.path.join(base_path, 'eta_layer_branching_gini_comparison.csv')\n",
    "    \n",
    "    if os.path.exists(layer_comparison_file):\n",
    "        print(\"\\n📊 Layer-level branching factor and Gini coefficient analysis:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        df = pd.read_csv(layer_comparison_file)\n",
    "        \n",
    "        # Find correct count column name\n",
    "        count_col = None\n",
    "        for col in df.columns:\n",
    "            if 'run_id' in col and ('count' in col or col.endswith('_count')):\n",
    "                count_col = col\n",
    "                break\n",
    "        \n",
    "        for layer in sorted(df['layer'].unique()):\n",
    "            print(f\"\\nLayer {int(layer)} Cross-Eta comparison:\")\n",
    "            print(\"Eta Value  Avg Branching(±std)  Doc Gini(±std)     Branch Gini(±std)     Runs\")\n",
    "            print(\"-\" * 75)\n",
    "            \n",
    "            layer_data = df[df['layer'] == layer]\n",
    "            for _, row in layer_data.iterrows():\n",
    "                eta = row['eta']\n",
    "                avg_branch = row['avg_branching_factor_mean']\n",
    "                branch_std = row['avg_branching_factor_std']\n",
    "                doc_gini = row['gini_doc_distribution_mean']\n",
    "                doc_gini_std = row['gini_doc_distribution_std']\n",
    "                branch_gini = row['gini_branch_distribution_mean']\n",
    "                branch_gini_std = row['gini_branch_distribution_std']\n",
    "                run_count = int(row[count_col]) if count_col else 0\n",
    "                \n",
    "                print(f\"{eta:6.3f}    {avg_branch:6.2f}(±{branch_std:4.2f})     {doc_gini:6.4f}(±{doc_gini_std:5.4f})     {branch_gini:6.4f}(±{branch_gini_std:5.4f})     {run_count:4d}\")\n",
    "    else:\n",
    "        print(\"⚠️ Layer comparison file not found\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"✅ Branching factor and Gini coefficient analysis completed!\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "# Execute branching factor and Gini coefficient analysis\n",
    "base_path = \"/Volumes/My Passport/收敛结果/step2\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Starting calculation of branching factor and Gini coefficient metrics...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Calculate branching factor and Gini coefficient for each model\n",
    "calculate_branching_and_gini_metrics(base_path)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Starting aggregation of branching factor and Gini coefficient statistics by eta value...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 2. Aggregate by eta\n",
    "aggregate_branching_gini_by_eta(base_path)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Displaying branching factor and Gini coefficient summary report...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 3. Display summary report\n",
    "display_branching_gini_summary(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aa11b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
